<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">9</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">9</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">29</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Radiology (4), Diagnostic Imaging (3), Medical Imaging (2)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Radiology">Radiology (4)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (3)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (2)</option>
                        
                        <option value="Oncology">Oncology (2)</option>
                        
                        <option value="Image Analysis">Image Analysis (1)</option>
                        
                        <option value="X-ray imaging">X-Ray Imaging (1)</option>
                        
                        <option value="Ultrasound (US) imaging">Ultrasound (Us) Imaging (1)</option>
                        
                        <option value="Histopathology">Histopathology (1)</option>
                        
                        <option value="Magnetic Resonance Imaging (MRI)">Magnetic Resonance Imaging (Mri) (1)</option>
                        
                        <option value="Colonoscopy">Colonoscopy (1)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2601.16073v1"
                     data-domains="Medical Imaging,Radiology,Image Analysis,Diagnostic Imaging"
                     data-keywords="Federated Learning,Foundation Models,Medical Image Segmentation,Knowledge Distillation,Dual-Scale,Computational Efficiency,Mutual Distillation,Lightweight Models"
                     data-authors="Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.16073v1.html">DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hanwen Zhang, Qiaojin Shen, Yuxi Liu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DSFedMed, a novel dual-scale federated framework designed to efficiently deploy powerful Foundation Models (FMs) for medical image segmentation in resource-constrained federated settings. It achieves this by employing mutual knowledge distillation between a centralized foundation model and lightweight client models, significantly reducing computational and communication overhead while enhancing segmentation accuracy. The framework leverages generated medical images and a learnability-guided sample selection strategy to facilitate effective knowledge transfer.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Image Analysis</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.16073v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.16073v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.16073v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.16073v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.16064v1"
                     data-domains="X-ray imaging,Ultrasound (US) imaging,Histopathology,Magnetic Resonance Imaging (MRI),Colonoscopy"
                     data-keywords="Medical Image Segmentation,Deep Learning,Phase-Aware,Frequency Domain,Generalization,CNN,Spectral Priors,Fine-Grained Localization"
                     data-authors="Shams Nafisa Ali,Taufiq Hasan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.16064v1.html">Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shams Nafisa Ali, Taufiq Hasan
                </div>

                <div class="paper-summary">
                    This paper introduces Phi-SegNet, a novel CNN-based architecture designed to enhance medical image segmentation by explicitly integrating phase-aware information at both architectural and optimization levels. By leveraging frequency-domain representations, Phi-SegNet addresses the crucial challenge of generalization across diverse imaging modalities and anatomical structures. The model consistently achieves state-of-the-art performance and exhibits robust cross-dataset generalization, underscoring the potential of spectral priors for fine-grained object localization in medical imaging.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">X-ray imaging</span>
                    
                    <span class="domain-tag">Ultrasound (US) imaging</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Magnetic Resonance Imaging (MRI)</span>
                    
                    <span class="domain-tag">Colonoscopy</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.16064v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.16064v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.16064v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.16064v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.16060v1"
                     data-domains="Radiology,Diagnostic Imaging,Anatomy,Oncology"
                     data-keywords="medical image segmentation,diffusion models,ControlNet,prompt-guided,multi-class segmentation,cross-modality,CT imaging,MR imaging"
                     data-authors="Yuan Lin,Murong Xu,Marc H√∂lle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.16060v1.html">ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuan Lin, Murong Xu, Marc H√∂lle et al.
                </div>

                <div class="paper-summary">
                    ProGiDiff introduces a novel framework that adapts pre-trained text-to-image diffusion models for medical image segmentation using a ControlNet-style conditioning mechanism. This approach enables multi-class segmentation via natural language prompts and demonstrates strong performance on CT images, with robust few-shot cross-modality transferability to MR images. It addresses critical limitations of current methods by allowing multiple proposals and human interaction.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Anatomy</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.16060v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.16060v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.16060v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.16060v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15481v1"
                     data-domains="Emergency Medicine,Hospital Management,Healthcare Operations,Public Health Informatics"
                     data-keywords="Emergency Department,Forecasting,Machine Learning,XGBoost,SARIMAX,LSTM,Hospital Admission,Resource Allocation"
                     data-authors="Jakub Antczak,James Montgomery,Ma≈Çgorzata O'Reilly,Zbigniew Palmowski,Richard Turner">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15481v1.html">Early predicting of hospital admission using machine learning algorithms: Priority queues approach</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jakub Antczak, James Montgomery, Ma≈Çgorzata O'Reilly et al.
                </div>

                <div class="paper-summary">
                    This study evaluates and compares SARIMAX, XGBoost, and LSTM machine learning models for forecasting daily Emergency Department (ED) admissions over a seven-day horizon, using five years of data from an Australian hospital. It uniquely decomposes demand by ward category and clinical complexity, addressing COVID-19 data anomalies with counterfactual values. The models consistently outperform a seasonal naive baseline, with XGBoost being most accurate for total admissions and SARIMAX for major complexity cases, though all models struggle with sudden patient surges.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Hospital Management</span>
                    
                    <span class="domain-tag">Healthcare Operations</span>
                    
                    <span class="domain-tag">Public Health Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15481v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15481v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15481v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15481v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15457v1"
                     data-domains="Public Health,Health Policy,Health Informatics,Regulatory Affairs"
                     data-keywords="RAG,LLM,Public Health Policy,CDC,Hallucination,Faithfulness,Retrieval-Augmented Generation,Cross-Encoder Re-ranking"
                     data-authors="Anuj Maharjan,Umesh Yadav">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15457v1.html">Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Anuj Maharjan, Umesh Yadav
                </div>

                <div class="paper-summary">
                    This paper empirically evaluates Retrieval-Augmented Generation (RAG) architectures to mitigate Large Language Model (LLM) hallucinations when answering questions from public health policy documents, specifically CDC guidance. It demonstrates that Advanced RAG, employing a two-stage retrieval mechanism with cross-encoder re-ranking, significantly improves output faithfulness (0.797) compared to Basic RAG (0.621) and Vanilla LLMs (0.347), which is crucial for high-stakes information integrity.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Health Policy</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Regulatory Affairs</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15457v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15457v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15457v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15457v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15442v1"
                     data-domains="cs.AI"
                     data-keywords="cs.AI,cs.LG,cs.LO,math.NA,stat.ML"
                     data-authors="Alex Goessmann,Janina Sch√ºtte,Maximilian Fr√∂hlich,Martin Eigel">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15442v1.html">A tensor network formalism for neuro-symbolic AI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 0.85</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Alex Goessmann, Janina Sch√ºtte, Maximilian Fr√∂hlich et al.
                </div>

                <div class="paper-summary">
                    The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basi...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15442v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15442v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15442v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15442v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15416v1"
                     data-domains="Radiology,Oncology (specifically lung nodule screening/diagnosis),Dentistry (orthodontics, implant planning, endodontics),Diagnostic Imaging"
                     data-keywords="CBCT reconstruction,sparse-view,deep learning,Fourier Neural Operator,high-frequency features,medical imaging,low-dose CT,anatomical details"
                     data-authors="Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15416v1.html">DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Cuong Tran Van, Trong-Thang Pham, Ngoc-Son Nguyen et al.
                </div>

                <div class="paper-summary">
                    DuFal introduces a novel dual-path framework for high-fidelity sparse-view Cone-Beam Computed Tomography (CBCT) reconstruction, specifically addressing the challenge of recovering fine-grained anatomical details (high-frequency components) often lost in undersampled data. By integrating frequency-domain and spatial-domain processing through a High-Local Factorized Fourier Neural Operator and efficient feature fusion, DuFal significantly outperforms existing state-of-the-art methods. This leads to superior preservation of high-frequency anatomical features, especially in extremely sparse-view settings, as demonstrated on LUNA16 and ToothFairy datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Oncology (specifically lung nodule screening/diagnosis)</span>
                    
                    <span class="domain-tag">Dentistry (orthodontics, implant planning, endodontics)</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15416v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15416v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15416v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15416v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15408v1"
                     data-domains="Radiology,Medical Imaging,Clinical Diagnosis,Artificial Intelligence in Medicine"
                     data-keywords="radiology report generation,visual grounding,curriculum learning,multimodal AI,hallucination reduction,medical imaging,vision-language models,anatomy grounding"
                     data-authors="Pablo Messina,Andr√©s Villa,Juan Le√≥n Alc√°zar,Karen S√°nchez,Carlos Hinojosa,Denis Parra,√Ålvaro Soto,Bernard Ghanem">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15408v1.html">CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Pablo Messina, Andr√©s Villa, Juan Le√≥n Alc√°zar et al.
                </div>

                <div class="paper-summary">
                    CURE addresses the critical issues of inaccurate visual grounding and factual inconsistencies in AI-generated radiology reports by introducing an error-aware curriculum learning framework. It fine-tunes a multimodal instructional model across multiple tasks, dynamically emphasizing challenging samples to significantly improve grounding accuracy, overall report quality, and substantially reduce hallucinations, all without requiring additional data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Clinical Diagnosis</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15408v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15408v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15408v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15408v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15392v1"
                     data-domains="Oncology,Pathology,Genomics,Precision Medicine,Biomedical Research,Bioinformatics"
                     data-keywords="Generative Adversarial Network,Gene Expression,Histopathology Images,Clinical Metadata,Multimodal Learning,Transformer Encoder,Cross Attention,Synthetic Data"
                     data-authors="Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15392v1.html">GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Francesca Pia Panaccione, Carlo Sgaravatti, Pietro Pinoli
                </div>

                <div class="paper-summary">
                    GeMM-GAN is a novel multimodal Generative Adversarial Network designed to synthesize realistic and biologically coherent gene expression profiles. It addresses the challenges of privacy regulations and high costs associated with real gene expression data by conditioning its generation on readily available histopathology images and clinical metadata. The framework demonstrates superior performance, generating functionally meaningful profiles that improve downstream disease type prediction by over 11% compared to current state-of-the-art generative models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                    <span class="domain-tag">Biomedical Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15392v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15392v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15392v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15392v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2026-01-24 06:14:09</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>