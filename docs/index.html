<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">48</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">48</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">159</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Radiology (8), Diagnostic Imaging (7), Oncology (7)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Radiology">Radiology (8)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (7)</option>
                        
                        <option value="Oncology">Oncology (7)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (6)</option>
                        
                        <option value="Neurology">Neurology (5)</option>
                        
                        <option value="Pharmacology">Pharmacology (5)</option>
                        
                        <option value="Public Health">Public Health (5)</option>
                        
                        <option value="Pathology">Pathology (5)</option>
                        
                        <option value="Digital Health">Digital Health (4)</option>
                        
                        <option value="Clinical Informatics">Clinical Informatics (4)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2512.05114v1"
                     data-domains="Pediatric Neurology,Developmental Neuroscience,Radiology,Neonatology,Neurodevelopmental Disorders Research"
                     data-keywords="Infant brain segmentation,Deep learning,Multi-contrast MRI,Pediatric neuroimaging,Domain randomization,Brain development,Image analysis,Neurodevelopmental disorders"
                     data-authors="Malte Hoffmann,Lilla Z√∂llei,Adrian V. Dalca">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05114v1.html">Deep infant brain segmentation from multi-contrast MRI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Malte Hoffmann, Lilla Z√∂llei, Adrian V. Dalca
                </div>

                <div class="paper-summary">
                    This paper introduces BabySeg, a deep learning framework designed for robust and accurate brain segmentation in infants and young children using multi-contrast MRI. It addresses the significant challenges of pediatric brain MRI acquisition and the fragmentation of existing segmentation methods by employing domain randomization and flexible feature pooling from multiple input scans. BabySeg achieves state-of-the-art performance, matching or exceeding existing tools across diverse age cohorts and imaging protocols, all while significantly reducing computational runtime.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pediatric Neurology</span>
                    
                    <span class="domain-tag">Developmental Neuroscience</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Neonatology</span>
                    
                    <span class="domain-tag">Neurodevelopmental Disorders Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05114v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05114v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05114v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05114v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05092v1"
                     data-domains="Medical Imaging (Radiology, Pathology),Genomics,Proteomics,Drug Discovery and Design,Electronic Health Records (EHR) Analysis,Digital Pathology,Synthetic Data Generation for Clinical Research"
                     data-keywords="diffusion models,generative AI,stochastic processes,medical imaging,bioinformatics,drug discovery,machine learning,synthetic data"
                     data-authors="Vincent Pauline,Tobias H√∂ppe,Kirill Neklyudov,Alexander Tong,Stefan Bauer,Andrea Dittadi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05092v1.html">Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ stat.ML</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vincent Pauline, Tobias H√∂ppe, Kirill Neklyudov et al.
                </div>

                <div class="paper-summary">
                    This paper presents a unified, self-contained theoretical framework for understanding diffusion models across general state spaces, integrating both continuous data (e.g., medical images) and discrete/categorical data (e.g., genomic sequences). By clarifying the foundational mathematics, connecting discrete-time to continuous-time limits, and deriving common variational treatments, it provides a comprehensive roadmap for applying these powerful generative AI tools to diverse medical and health research challenges.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging (Radiology, Pathology)</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Proteomics</span>
                    
                    <span class="domain-tag">Drug Discovery and Design</span>
                    
                    <span class="domain-tag">Electronic Health Records (EHR) Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05092v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05092v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05092v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05092v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05080v1"
                     data-domains="Pharmacology,Medicinal Chemistry,Drug Development,Biotechnology,Pharmaceutical Research"
                     data-keywords="Structure-based drug design,Generative models,De novo drug design,Molecular docking,Flow matching models,Protein-ligand binding,Drug discovery,Computational chemistry"
                     data-authors="Ian Dunn,Liv Toft,Tyler Katz,Juhi Gupta,Riya Shah,Ramith Hettiarachchi,David R. Koes">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05080v1.html">OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ian Dunn, Liv Toft, Tyler Katz et al.
                </div>

                <div class="paper-summary">
                    OMTRA is a novel multi-modal flow matching generative model designed to unify various structure-based drug design (SBDD) tasks, including de novo ligand design and docking. It leverages a newly curated large-scale 3D molecular conformer dataset and achieves state-of-the-art performance in key SBDD areas. Interestingly, the benefits of extensive pretraining and multi-tasking within this framework were found to be modest.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Drug Development</span>
                    
                    <span class="domain-tag">Biotechnology</span>
                    
                    <span class="domain-tag">Pharmaceutical Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05080v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05080v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05080v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05080v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05066v1"
                     data-domains="Pharmacology,Clinical Informatics,General Medicine,Decision Support Systems,Patient Safety"
                     data-keywords="LLM Chemistry,multi-LLM collaboration,medication recommendation,clinical decision support,AI reliability,hallucination mitigation,ensemble learning,patient safety"
                     data-authors="Huascar Sanchez,Briland Hitaj,Jules Bergmann,Linda Briesemeister">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05066v1.html">Multi-LLM Collaboration for Medication Recommendation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huascar Sanchez, Briland Hitaj, Jules Bergmann et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel multi-LLM collaboration strategy, guided by a framework called 'LLM Chemistry,' to enhance the reliability and consistency of medication recommendations derived from brief clinical vignettes. By quantifying the collaborative compatibility among LLMs, the approach aims to create effective, stable, and calibrated ensembles that overcome individual LLM susceptibility to hallucinations and inconsistency, showing encouraging preliminary results in real-world clinical scenarios.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">General Medicine</span>
                    
                    <span class="domain-tag">Decision Support Systems</span>
                    
                    <span class="domain-tag">Patient Safety</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05066v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05066v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05066v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05066v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05030v1"
                     data-domains="Rehabilitation Medicine,Physical Therapy,Sports Medicine,Orthopedics,Biomedical Engineering,Kinesiology"
                     data-keywords="Ground Reaction Force,Ground Reaction Moment,Deep Learning,Attention Network,Biomechanics,Rehabilitation,Insole Sensors,Gait Analysis"
                     data-authors="Xuan Li,Samuel Bello">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05030v1.html">Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xuan Li, Samuel Bello
                </div>

                <div class="paper-summary">
                    This paper introduces a novel Dual-Path Region-Guided Attention Network for accurate, insole-based estimation of three-dimensional ground reaction forces and moments (GRFs/GRMs). The network integrates anatomy-inspired spatial and temporal priors with a region-level attention mechanism and a complementary full sensor field context path. The model demonstrates superior performance over strong deep learning baselines on two datasets, achieving robust GRF/GRM estimation crucial for biomechanics and clinical rehabilitation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                    <span class="domain-tag">Physical Therapy</span>
                    
                    <span class="domain-tag">Sports Medicine</span>
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05030v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05030v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05030v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05030v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05012v1"
                     data-domains="Clinical Trials,Evidence-Based Medicine,Medical Decision Support,Drug Development and Safety"
                     data-keywords="RAG,Contrastive Learning,Evidence Re-ranking,Factuality,Transparency,Hallucinations,Clinical Trials,Medical AI"
                     data-authors="Francielle Vargas,Daniel Pedronette">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05012v1.html">Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Francielle Vargas, Daniel Pedronette
                </div>

                <div class="paper-summary">
                    This paper introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that fine-tunes retrieval embeddings using contrastive learning with subjectivity-based hard negatives to enhance factuality and transparency in RAG systems. Evaluated on clinical trial reports, CER demonstrates improved retrieval accuracy, mitigates hallucinations, and provides transparent, evidence-based retrieval crucial for safety-critical medical domains.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                    <span class="domain-tag">Evidence-Based Medicine</span>
                    
                    <span class="domain-tag">Medical Decision Support</span>
                    
                    <span class="domain-tag">Drug Development and Safety</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05012v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05012v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05012v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05012v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04980v1"
                     data-domains="Precision Medicine,Personalized Treatment,Chronic Disease Management,Drug Discovery and Development,Prognostics,Biomarker Discovery,Public Health Interventions"
                     data-keywords="Causal Inference,Causal Representation Learning,Longitudinal Data,Individual Treatment Effects,Counterfactuals,Variational Autoencoder,Recurrent Neural Networks,Contrastive Predictive Coding,Interpretability,High-dimensional Data"
                     data-authors="Mouad EL Bouchattaoui">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04980v1.html">Learning Causality for Longitudinal Data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ stat.ML</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mouad EL Bouchattaoui
                </div>

                <div class="paper-summary">
                    This thesis presents three novel contributions in causal inference and causal representation learning (CRL) for high-dimensional, time-varying data. It introduces the Causal Dynamic Variational Autoencoder (CDVAE) for improved Individual Treatment Effect (ITE) estimation by modeling unobserved heterogeneity, an efficient RNN-based framework utilizing Contrastive Predictive Coding (CPC) for long-term counterfactuals, and a model-agnostic interpretability layer to uncover how latent causes manifest in observed features.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                    <span class="domain-tag">Personalized Treatment</span>
                    
                    <span class="domain-tag">Chronic Disease Management</span>
                    
                    <span class="domain-tag">Drug Discovery and Development</span>
                    
                    <span class="domain-tag">Prognostics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04980v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04980v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04980v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04980v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04967v1"
                     data-domains="Ophthalmology,Medical Imaging,Artificial Intelligence in Medicine,Diabetology"
                     data-keywords="retinal disease,few-shot learning,deep learning,diabetic retinopathy,macular degeneration,image augmentation,CLAHE,ophthalmology"
                     data-authors="Jasmaine Khale,Ravi Prakash Srivastava">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04967v1.html">Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jasmaine Khale, Ravi Prakash Srivastava
                </div>

                <div class="paper-summary">
                    This paper introduces a balanced few-shot episodic learning framework designed for accurate retinal disease diagnosis, specifically addressing challenges posed by limited and imbalanced annotated datasets. By integrating balanced episodic sampling, targeted augmentation (including CLAHE), and a ResNet-50 encoder, the method significantly improves diagnostic accuracy and reduces bias towards majority classes, particularly benefiting underrepresented retinal conditions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                    <span class="domain-tag">Diabetology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04967v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04967v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04967v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04967v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04943v1"
                     data-domains="Geriatrics,Rehabilitation Medicine,Telemedicine,Patient Safety,Assistive Technology,Smart Hospitals,Behavioral Health Monitoring"
                     data-keywords="Multimodal Fusion,Deep Learning,Gating Mechanisms,Human Action Recognition,Adaptive Weighting,RGB,Optical Flow,Active Assisted Living"
                     data-authors="Novanto Yudistira">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04943v1.html">Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Novanto Yudistira
                </div>

                <div class="paper-summary">
                    This study introduces a novel methodology for human action recognition by adaptively fusing multimodal deep networks, utilizing RGB, optical flow, audio, and depth data with gating mechanisms. The approach selectively integrates relevant information, demonstrating superior accuracy and robustness over traditional unimodal methods in tasks like action recognition and violence detection. This enhances holistic action representation and holds significant potential for diverse applications, particularly in active assisted living.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Patient Safety</span>
                    
                    <span class="domain-tag">Assistive Technology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04943v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04943v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04943v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04943v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04938v1"
                     data-domains="Neurology,Rare Diseases,Metabolic Disorders (specifically Phenylketonuria),Digital Health,Cognitive Neuroscience"
                     data-keywords="Neurocognitive monitoring,Speech AI,Relational Graph Transformers,Phenylketonuria (PKU),Rare neurological diseases,Brain fog,Biomarkers,Predictive analytics"
                     data-authors="Raquel Norel,Michele Merler,Pavitra Modi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04938v1.html">Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Raquel Norel, Michele Merler, Pavitra Modi
                </div>

                <div class="paper-summary">
                    This paper introduces a novel method for continuous neurocognitive monitoring in rare neurological diseases, utilizing smartphone speech analysis combined with Relational Graph Transformer (RELGT) architectures. A proof-of-concept in Phenylketonuria (PKU) demonstrates that a speech-derived metric correlates significantly with a biochemical biomarker (blood phenylalanine) but not with standard cognitive tests, highlighting its potential for detecting subtle, overlooked cognitive changes. The system aims to provide predictive alerts weeks before patient decompensation by integrating heterogeneous medical data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Rare Diseases</span>
                    
                    <span class="domain-tag">Metabolic Disorders (specifically Phenylketonuria)</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Cognitive Neuroscience</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04938v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04938v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04938v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04938v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04937v1"
                     data-domains="Neurology,Neuroscience,Geriatric Medicine,Pharmacology,Diagnostic Medicine,Translational Research"
                     data-keywords="Alzheimer's disease,amyloid-Œ≤,tau pathology,neuroinflammation,combination therapy,multi-target therapy,precision medicine,biomarkers"
                     data-authors="She Xutong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04937v1.html">A Systemic Pathological Network Model and Combinatorial Intervention Strategies for Alzheimer's Disease</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ q-bio.MN</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> She Xutong
                </div>

                <div class="paper-summary">
                    This paper synthesizes the evolving understanding of Alzheimer's disease (AD) as a complex, systemic pathological network involving amyloid-Œ≤, tau, and neuroinflammation, moving beyond the linear amyloid cascade hypothesis. It advocates for a paradigm shift from current single-target therapies, which show modest efficacy despite breakthroughs like anti-amyloid monoclonal antibodies, towards biomarker-guided, personalized combination interventions that simultaneously address multiple pathologies for more effective disease modification.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                    <span class="domain-tag">Geriatric Medicine</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04937v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04937v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04937v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04937v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04890v1"
                     data-domains="Fetal Medicine,Perinatology,Diagnostic Radiology,Obstetrics,Neonatology (indirect impact)"
                     data-keywords="Fetal MRI,Head Pose Estimation,Equivariance,Symmetry-Aware,Motion Correction,Diagnostic Imaging,6-DoF,Clinical Translation"
                     data-authors="Ramya Muthukrishnan,Borjan Gagoski,Aryn Lee,P. Ellen Grant,Elfar Adalsteinsson,Polina Golland,Benjamin Billot">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04890v1.html">Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ramya Muthukrishnan, Borjan Gagoski, Aryn Lee et al.
                </div>

                <div class="paper-summary">
                    This paper introduces E(3)-Pose, a novel fast pose estimation method that explicitly models rotation equivariance and object symmetry to address the challenging problem of fetal head motion during MRI scans. By achieving robust 6-DoF head pose estimation, the method enables automatic adaptive prescription of 2D diagnostic MRI slices and demonstrates state-of-the-art accuracy and generalization on clinical fetal MRI volumes, paving the way for clinical translation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Fetal Medicine</span>
                    
                    <span class="domain-tag">Perinatology</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                    <span class="domain-tag">Obstetrics</span>
                    
                    <span class="domain-tag">Neonatology (indirect impact)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04890v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04890v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04890v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04890v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04889v1"
                     data-domains="Medical Imaging,Radiology,Diagnostic Imaging,Biomedical Engineering,Point-of-Care Diagnostics"
                     data-keywords="low-field MRI,electromagnetic interference,EMI,FENCE,RF coil,SNR,capacitive coupling,Faraday shield"
                     data-authors="Julia Pfitzer,Martin Uecker,Hermann Scharfetter">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04889v1.html">FENCE: Flexible Electric Noise Cancellation Endo-shield for the Suppression of Electromagnetic Interference in Low-Field MRI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Julia Pfitzer, Martin Uecker, Hermann Scharfetter
                </div>

                <div class="paper-summary">
                    This paper introduces FENCE (Flexible Electric Noise Cancellation Endo-shield), a novel, low-cost solution designed to mitigate electromagnetic interference (EMI) in low-field MRI systems operating without traditional Faraday cages. FENCE, implemented as flexible PCB shields placed inside RF coils, effectively blocks capacitive coupling, demonstrating significant improvements in signal-to-noise ratio (SNR) in both phantom and in-vivo head imaging experiments. This enhances image quality, maintains system portability, and improves accessibility of low-field MRI.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                    <span class="domain-tag">Point-of-Care Diagnostics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04889v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04889v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04889v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04889v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04875v1"
                     data-domains="Radiology,Diagnostic Imaging,Thoracic Medicine"
                     data-keywords="lesion detection,chest X-ray,multi-label detection,self-prompted learning,dual-text fusion,computer-aided diagnosis,deep learning,radiology"
                     data-authors="Qing Xu,Yanqian Wang,Xiangjian Hea,Yue Li,Yixuan Zhang,Rong Qu,Wenting Duan,Zhen Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04875v1.html">SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Qing Xu, Yanqian Wang, Xiangjian Hea et al.
                </div>

                <div class="paper-summary">
                    SP-Det introduces a novel self-prompted framework for multi-label lesion detection in chest X-rays, overcoming the significant challenge of labor-intensive manual annotations required by existing promptable methods. It achieves this by automatically generating rich textual prompts through a dual-text generator and enhancing feature representation via a bidirectional enhancer, ultimately outperforming state-of-the-art detection methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Thoracic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04875v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04875v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04875v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04875v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04847v1"
                     data-domains="Cardiology,Pulmonology,Respiratory Medicine,Infectious Diseases (COVID-19),Diagnostics"
                     data-keywords="Language Models,Medical Audio,Semantic Alignment,Audio Understanding,Auscultation,Cardio-respiratory,Diagnostic AI,Post-training"
                     data-authors="Tsai-Ning Wang,Lin-Lin Chen,Neil Zeghidour,Aaqib Saeed">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04847v1.html">Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.SD</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tsai-Ning Wang, Lin-Lin Chen, Neil Zeghidour et al.
                </div>

                <div class="paper-summary">
                    This paper introduces AcuLa, a lightweight post-training framework designed to imbue pre-trained audio models with clinical semantic understanding by aligning them with a medical language model. AcuLa leverages large language models to create a massive dataset of clinical reports from audio metadata and achieves state-of-the-art results across numerous cardio-respiratory diagnostic tasks, significantly improving performance on challenging detection benchmarks.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Respiratory Medicine</span>
                    
                    <span class="domain-tag">Infectious Diseases (COVID-19)</span>
                    
                    <span class="domain-tag">Diagnostics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04847v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04847v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04847v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04847v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04843v1"
                     data-domains="Psychiatry,Mental Health,Eating Disorder Treatment,Digital Health,Clinical Psychology,Public Health"
                     data-keywords="Generative AI,Eating Disorders,AI Risks,Clinical Safeguards,Mental Health Technology,Qualitative Research,Digital Ethics,Patient Safety"
                     data-authors="Amy Winecoff,Kevin Klyman">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04843v1.html">From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.HC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Amy Winecoff, Kevin Klyman
                </div>

                <div class="paper-summary">
                    This paper establishes an expert-guided taxonomy of generative AI risks for individuals vulnerable to eating disorders (EDs), highlighting how current safeguards are insufficient for subtle clinical cues. Through qualitative analysis of interviews with ED specialists, the research identifies seven critical risk categories, demonstrating how AI interactions can exacerbate ED features and intensify user harm.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Eating Disorder Treatment</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Clinical Psychology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04843v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04843v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04843v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04843v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04834v1"
                     data-domains="Digital Healthcare,Clinical Informatics,Medical Language Processing,Health Information Technology,Medical AI,Epidemiology"
                     data-keywords="LLMs,Multilingual,Information Extraction,Healthcare,EHR,Italian,Comorbidity,Zero-shot,Clinical NLP,AI in Medicine"
                     data-authors="Vignesh Kumar Kembu,Pierandrea Morandini,Marta Bianca Maria Ranzini,Antonino Nocera">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04834v1.html">Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vignesh Kumar Kembu, Pierandrea Morandini, Marta Bianca Maria Ranzini et al.
                </div>

                <div class="paper-summary">
                    This paper investigates the zero-shot multilingual capability of open-source Large Language Models (LLMs) for extracting comorbidity information from Italian Electronic Health Records (EHRs). The study reveals that while LLMs hold promise, their performance in an on-premises zero-shot setting for this specific task is inconsistent, with many struggling to generalize across various diseases when compared to traditional pattern matching and manual annotations.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Healthcare</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Medical Language Processing</span>
                    
                    <span class="domain-tag">Health Information Technology</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04834v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04834v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04834v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04834v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04829v1"
                     data-domains="Medical imaging,Computational biology (indirectly, through structural modeling),Biophysics (indirectly, through material arrangement)"
                     data-keywords="Sphere packing,AI-assisted discovery,Semidefinite programming (SDP),Bayesian optimization,Monte Carlo Tree Search,Medical imaging,Computational geometry,Sample-efficient AI"
                     data-authors="Rasul Tutunov,Alexandre Maraval,Antoine Grosnit,Xihan Li,Jun Wang,Haitham Bou-Ammar">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04829v1.html">Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Rasul Tutunov, Alexandre Maraval, Antoine Grosnit et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a sample-efficient, model-based AI framework to address the computationally intensive problem of sphere packing, specifically focusing on deriving upper bounds. By reformulating SDP construction as a sequential decision process, the 'SDP game', and combining Bayesian optimization with Monte Carlo Tree Search, the authors achieve new state-of-the-art upper bounds in dimensions 4-16, demonstrating a novel approach to AI-assisted mathematical discovery in complex, evaluation-limited problems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical imaging</span>
                    
                    <span class="domain-tag">Computational biology (indirectly, through structural modeling)</span>
                    
                    <span class="domain-tag">Biophysics (indirectly, through material arrangement)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04829v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04829v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04829v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04829v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04821v1"
                     data-domains="Dermatology,Gastroenterology,Medical Imaging,Diagnostic Radiology"
                     data-keywords="Medical Image Segmentation,Generative Models,Flow Matching,Latent Space,Variational Autoencoders,Uncertainty Quantification,Confidence Maps,Deep Learning"
                     data-authors="Huynh Trinh Ngoc,Hoang Anh Nguyen Kim,Toan Nguyen Hai,Long Tran Quoc">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04821v1.html">LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huynh Trinh Ngoc, Hoang Anh Nguyen Kim, Toan Nguyen Hai et al.
                </div>

                <div class="paper-summary">
                    LatentFM introduces a novel latent flow matching approach for medical image segmentation, leveraging two Variational Autoencoders (VAEs) to encode images and masks into a lower-dimensional space. By estimating a conditional velocity field and sampling multiple latent representations, the method generates diverse, highly accurate, and uncertainty-aware segmentation outputs, including confidence maps for clinical analysis. Comprehensive evaluations on ISIC-2018 and CVC-Clinic datasets demonstrate its superior accuracy and efficiency compared to prior baselines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04821v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04821v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04821v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04821v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04662v1"
                     data-domains="Orthopedics,Rheumatology,Pathology,Musculoskeletal Imaging,Biomechanics"
                     data-keywords="Spectral micro-CT,Calcification,Fibrocartilage,Osteoarthritis,Femoroacetabular impingement,Quantitative analysis,3D imaging,Photon-counting detector"
                     data-authors="Vittoria Mazzini,Paolo Cardarelli,Andrew L. Coathup,Eleonora Olivotto,Francesco Grassi,Enrico Tassinari,Simone Velardita,Angelo Taibi,Luca Brombal">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04662v1.html">Spectral micro-CT for quantitative analysis of calcification in fibrocartilage</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vittoria Mazzini, Paolo Cardarelli, Andrew L. Coathup et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel spectral micro-computed tomography (ŒºCT) method for the quantitative, volumetric, and non-destructive analysis of calcification in fibrocartilage. Utilizing a photon-counting detector and material decomposition, the technique generates 3D calcium maps from intact tissue samples, demonstrating high accuracy and agreement with traditional histology. It offers an enhanced characterization tool for pathological calcification in hip joint disorders.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                    <span class="domain-tag">Rheumatology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Musculoskeletal Imaging</span>
                    
                    <span class="domain-tag">Biomechanics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04662v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04662v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04662v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04662v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04629v1"
                     data-domains="Medicinal Chemistry,Pharmacology,Drug Discovery and Development,Chemical Biology,Pharmaceutical Sciences"
                     data-keywords="molecular language model,drug discovery,multi-task learning,retrosynthesis,molecular generation,biomedical AI,small molecules,pharmaceutical research"
                     data-authors="Chenyang Zuo,Siqi Fan,Zaiqing Nie">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04629v1.html">BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chenyang Zuo, Siqi Fan, Zaiqing Nie
                </div>

                <div class="paper-summary">
                    BioMedGPT-Mol introduces a novel molecular language model adapted from a general-purpose reasoning model through a meticulously designed multi-task learning framework. Leveraging a large, curated dataset, it supports molecular understanding and generation tasks, achieving remarkable performance on various benchmarks. This work demonstrates the effective and efficient post-training of large language models for specialized applications in molecular science and drug discovery.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Drug Discovery and Development</span>
                    
                    <span class="domain-tag">Chemical Biology</span>
                    
                    <span class="domain-tag">Pharmaceutical Sciences</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04629v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04629v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04629v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04629v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04618v1"
                     data-domains="Neurology,Neurorehabilitation,Brain-Computer Interfaces (BCI),Neuroscience"
                     data-keywords="Neural Decoding,Speech BCI,ECoG,Vision Transformers,Contrastive Learning,Epidural Electrodes,Paralysis,Communication"
                     data-authors="Mohamed Baha Ben Ticha,Xingchen Ran,Guillaume Saldanha,Ga√´l Le Godais,Phil√©mon Roussel,Marc Aubert,Amina Fontanell,Thomas Costecalde,Lucas Struber,Serpil Karakas,Shaomin Zhang,Philippe Kahane,Guillaume Charvet,St√©phan Chabard√®s,Blaise Yvert">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04618v1.html">Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mohamed Baha Ben Ticha, Xingchen Ran, Guillaume Saldanha et al.
                </div>

                <div class="paper-summary">
                    This paper presents an offline speech decoding pipeline that utilizes an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning, to directly regress ECoG signals into acoustic speech. The study addresses the challenge of reconstructing speech from surface ECoG and notably demonstrates the first attempt to decode speech from a fully implantable, wireless epidural recording system, offering significant potential for long-term Brain-Computer Interface (BCI) applications in communication.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Neurorehabilitation</span>
                    
                    <span class="domain-tag">Brain-Computer Interfaces (BCI)</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04618v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04618v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04618v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04618v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04599v1"
                     data-domains="Public Health,Mental Health,Digital Forensics,Patient Safety (online platforms),Health Information Management,Child Protection (online)"
                     data-keywords="malicious content,image analysis,vision-language models,semantic segmentation,zero-shot learning,content moderation,adversarial robustness,explainable AI"
                     data-authors="Sheng Hang,Chaoxiang He,Hongsheng Hu,Hanqing Hu,Bin Benjamin Zhu,Shi-Feng Sun,Dawu Gu,Shuo Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04599v1.html">Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sheng Hang, Chaoxiang He, Hongsheng Hu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel zero-shot pipeline that simultaneously detects, identifies, and pixel-accurately localizes malicious content within images in a single pass. By fusing foundation segmentation models (SAM) with vision-language models (VLM) and incorporating an ensemble strategy, the method achieves high recall and precision on a new dataset, demonstrating robust performance against adversarial attacks and offering a practical, explainable solution for fine-grained content moderation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Digital Forensics</span>
                    
                    <span class="domain-tag">Patient Safety (online platforms)</span>
                    
                    <span class="domain-tag">Health Information Management</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04599v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04599v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04599v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04599v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04580v1"
                     data-domains="Healthcare AI,Biomedical Informatics,Digital Health,Clinical Decision Support,Precision Medicine"
                     data-keywords="LLM security,confidential computing,data privacy,model distribution,healthcare AI,encryption,access control,Safetensors"
                     data-authors="Huifeng Zhu,Shijie Li,Qinfeng Li,Yier Jin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04580v1.html">A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.85</span>
                        
                        <span class="category">üìÇ cs.CR</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huifeng Zhu, Shijie Li, Qinfeng Li et al.
                </div>

                <div class="paper-summary">
                    This paper introduces CryptoTensors, a novel secure and format-compatible file structure designed for the confidential distribution of Large Language Models (LLMs) which are often fine-tuned with sensitive data from domains like healthcare. Built as an extension to the widely adopted Safetensors format, CryptoTensors integrates tensor-level encryption and embedded access control policies to protect model weights while maintaining crucial features like lazy loading and partial deserialization. The authors demonstrate its efficacy through a proof-of-concept, benchmarking its performance and validating its compatibility with existing inference frameworks with minimal overhead.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Healthcare AI</span>
                    
                    <span class="domain-tag">Biomedical Informatics</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04580v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04580v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04580v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04580v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04576v1"
                     data-domains="Radiology,Diagnostic Imaging,Oncology,Medical Physics"
                     data-keywords="Tumor segmentation,Multi-modal CT,Missing modality,Representation disentanglement,Time-Attenuation Curve,Deep learning,Radiation reduction,Hemodynamics"
                     data-authors="Zishuo Wan,Qinqin Kang,Yi Huang,Yun Bian,Dawei Ding,Ke Yan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04576v1.html">TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zishuo Wan, Qinqin Kang, Yi Huang et al.
                </div>

                <div class="paper-summary">
                    TARDis addresses the "missing modality" problem in multi-phase contrast-enhanced CT by modeling incomplete series as missing sample points on a continuous Time-Attenuation Curve, rather than just absent channels. This physics-aware framework disentangles latent features into time-invariant static (anatomy) and time-dependent dynamic (perfusion) components, utilizing a dual-path architecture to robustly hallucinate missing hemodynamic information. The method significantly outperforms state-of-the-art approaches, demonstrating robust diagnostic performance even in extreme data-sparsity scenarios, thereby holding potential for reducing radiation exposure.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04576v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04576v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04576v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04576v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04564v1"
                     data-domains="Pathology,Histopathology,Medical Imaging,Diagnostic Medicine"
                     data-keywords="deep learning,microscopic images,dataset creation,annotation quality,domain shift,pathology,supervised learning,digital pathology"
                     data-authors="Christof A. Bertram,Viktoria Weiss,Jonas Ammeling,F. Maria Schabel,Taryn A. Donovan,Frauke Wilm,Christian Marzahl,Katharina Breininger,Marc Aubreville">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04564v1.html">Dataset creation for supervised deep learning-based analysis of microscopic images - review of important considerations and recommendations</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Christof A. Bertram, Viktoria Weiss, Jonas Ammeling et al.
                </div>

                <div class="paper-summary">
                    This paper provides a comprehensive review and practical guide for creating high-quality, large-scale datasets crucial for supervised deep learning (DL) analysis of microscopic images. It addresses common challenges like domain variability and bias, outlining critical steps from image acquisition to annotation, while emphasizing key quality criteria and advanced techniques to foster robust and generalizable DL models. The ultimate goal is to advance DL applications, particularly in pathology, by improving dataset development and accessibility.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04564v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04564v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04564v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04564v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04536v1"
                     data-domains="Public Health,Emergency Medicine,Traffic Safety,Forensic Medicine,Occupational Health,Addiction Medicine"
                     data-keywords="alcohol intoxication detection,facial video analysis,deep learning,recurrent fusion model,Graph Attention Network (GAT),3D ResNet,computer vision,public health"
                     data-authors="Bita Baroutian,Atefe Aghaei,Mohsen Ebrahimi Moghaddam">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04536v1.html">Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Bita Baroutian, Atefe Aghaei, Mohsen Ebrahimi Moghaddam
                </div>

                <div class="paper-summary">
                    This study presents a novel recurrent fusion model for detecting alcohol intoxication from facial video sequences. The model combines facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted by a 3D ResNet, employing dynamic adaptive prioritization for feature fusion. Achieving 95.82% accuracy, 0.977 precision, and 0.97 recall on a new curated dataset, the approach significantly outperforms baseline methods and shows promise for practical deployment in public safety.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Traffic Safety</span>
                    
                    <span class="domain-tag">Forensic Medicine</span>
                    
                    <span class="domain-tag">Occupational Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04536v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04536v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04536v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04536v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04535v1"
                     data-domains="Clinical Decision Support,Medical Robotics,Diagnostics,Drug Discovery (implied),Patient Monitoring (implied),Healthcare Operations"
                     data-keywords="AI agents,tool simulation,large language models,deep learning,reinforcement learning,medical AI,synthetic data,generalist models"
                     data-authors="Zhenzhen Ren,Xinpeng Zhang,Zhenxing Qian,Yan Gao,Yu Shi,Shuxin Zheng,Jiyan He">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04535v1.html">GTM: Simulating the World of Tools for AI Agents</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhenzhen Ren, Xinpeng Zhang, Zhenxing Qian et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Generalist Tool Model (GTM), a 1.5-billion-parameter AI model designed to act as a universal tool simulator for Large Language Model (LLM) agents. GTM addresses the high cost and complexity of training agents with real-world tools by synthesizing diverse tool outputs, trained through a Context-Aware Response Generation (CARG) pipeline using data from over 20,000 tools across 300 domains, including medicine. The model provides a fast, cost-effective solution, exhibiting high-quality, consistent outputs and superior simulation speed in reinforcement learning scenarios.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Robotics</span>
                    
                    <span class="domain-tag">Diagnostics</span>
                    
                    <span class="domain-tag">Drug Discovery (implied)</span>
                    
                    <span class="domain-tag">Patient Monitoring (implied)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04535v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04535v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04535v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04535v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04520v1"
                     data-domains="Dermatology (skin lesion analysis),Gastroenterology (GI tract endoscopy),Radiology / Breast Imaging (breast ultrasound),Ophthalmology (optic disc/cup segmentation)"
                     data-keywords="Zero-shot segmentation,Test-time adaptation,Medical image segmentation,SAM (Segment Anything Model),Foundation models,Boundary-aware attention,Gaussian prompts,Domain shift"
                     data-authors="Chenlin Xu,Lei Zhang,Lituan Wang,Xinyu Pu,Pengfei Ma,Guangwu Qian,Zizhou Wang,Yan Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04520v1.html">Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chenlin Xu, Lei Zhang, Lituan Wang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces BA-TTA-SAM, a novel task-agnostic test-time adaptation framework designed to significantly enhance the zero-shot medical image segmentation performance of foundational models like SAM. It integrates encoder-level Gaussian prompt injection and cross-layer boundary-aware attention alignment to address domain shifts and improve representation learning. The framework achieves an average 12.4% DICE score improvement over SAM on diverse medical datasets, outperforming state-of-the-art models without requiring source-domain training data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology (skin lesion analysis)</span>
                    
                    <span class="domain-tag">Gastroenterology (GI tract endoscopy)</span>
                    
                    <span class="domain-tag">Radiology / Breast Imaging (breast ultrasound)</span>
                    
                    <span class="domain-tag">Ophthalmology (optic disc/cup segmentation)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04520v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04520v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04520v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04520v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04518v1"
                     data-domains="Oncology,Clinical Informatics,Medical AI,Health Data Science"
                     data-keywords="Chemotherapy,Clinical Timelines,LLM,EHR,Natural Language Processing (NLP),Fine-tuning,Oncology,Information Extraction"
                     data-authors="Tianmai M. Zhang,Zhaoyi Sun,Sihang Zeng,Chenxi Li,Neil F. Abernethy,Barbara D. Lam,Fei Xia,Meliha Yetisgen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04518v1.html">UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tianmai M. Zhang, Zhaoyi Sun, Sihang Zeng et al.
                </div>

                <div class="paper-summary">
                    This paper details UW-BioNLP's approach to the ChemoTimelines 2025 shared task, focusing on automatically generating patient chemotherapy timelines from raw clinical notes using advanced LLM systems. Their method employs a two-step workflow: LLMs extract chemotherapy events from individual notes, followed by algorithmic normalization and aggregation into patient-level timelines. A fine-tuned Qwen3-14B model achieved the best official score of 0.678, demonstrating the efficacy of combining thinking, fine-tuning, and dictionary-enhanced strategies.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                    <span class="domain-tag">Health Data Science</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04518v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04518v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04518v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04518v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04425v1"
                     data-domains="Neurology,Movement Disorders,Geriatrics,Rehabilitation Medicine,Diagnostic Imaging,Artificial Intelligence in Medicine"
                     data-keywords="Parkinson's disease,gait analysis,RGB-D fusion,multimodal learning,explainable AI,Large Language Models,deep learning,clinical interpretability"
                     data-authors="Manar Alnaasan,Md Selim Sarowar,Sungho Kim">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04425v1.html">Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Manar Alnaasan, Md Selim Sarowar, Sungho Kim
                </div>

                <div class="paper-summary">
                    This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data for robust Parkinson's disease (PD) gait recognition under realistic conditions. It leverages advanced deep learning (YOLOv11, MLGE, fusion mechanisms) to capture subtle gait abnormalities and incorporates a Large Language Model (LLM) to translate visual findings into clinically meaningful textual explanations, thereby improving both accuracy and interpretability.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Movement Disorders</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04425v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04425v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04425v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04425v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04401v1"
                     data-domains="Oncology,Radiology,Medical Imaging,Biomedical Engineering"
                     data-keywords="Microwave Imaging,Deep Learning,Breast Imaging,Conformal Antenna Array,Dielectric Imaging,Time-Domain Signals,Medical Physics"
                     data-authors="Wenyi Shao,Beibei Zhou">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04401v1.html">Learnt Microwave Image Reconstruction with A Conformal Antenna Array</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Wenyi Shao, Beibei Zhou
                </div>

                <div class="paper-summary">
                    This paper introduces a deep learning model for reconstructing 2D dielectric breast images using time-domain microwave signals. A key innovation is the integration of antenna positioning into the processing pipeline, enabling a conformal antenna array that adapts to individual breast sizes. This approach aims to eliminate signal attenuation in coupling liquid common with fixed arrays and allows the neural network to focus on image reconstruction within the region of interest by pre-estimating the breast surface, demonstrating good quality image reconstruction in numerical results.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04401v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04401v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04401v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04401v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04397v1"
                     data-domains="Radiology,Diagnostic Imaging,Pulmonology,Medical Artificial Intelligence"
                     data-keywords="Transfer Learning,Medical Image Classification,Deep Learning,Convolutional Neural Networks,Chest X-ray,Disease Detection,InceptionV3,ResNet"
                     data-authors="Zeeshan Ahmad,Shudi Bao,Meng Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04397v1.html">Performance Evaluation of Transfer Learning Based Medical Image Classification Techniques for Disease Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zeeshan Ahmad, Shudi Bao, Meng Chen
                </div>

                <div class="paper-summary">
                    This paper comprehensively analyzes transfer learning (TL) techniques for medical image classification using deep convolutional neural networks (CNNs) to address the challenges of training large models from scratch with limited medical data. Evaluating six pre-trained models on a custom chest X-ray dataset, the study found InceptionV3 consistently outperformed others, demonstrating that TL is largely beneficial, especially with scarce data, and that a well-trained feature extractor can enable efficient predictions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Medical Artificial Intelligence</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04397v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04397v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04397v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04397v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04354v1"
                     data-domains="Laboratory Medicine,Hospital Medicine,Inpatient Care,Health Informatics,Clinical Pathology,Value-Based Healthcare"
                     data-keywords="machine learning,clinical decision support,laboratory utilization,inpatient care,complete blood count,electronic health record,randomized controlled trial,healthcare costs"
                     data-authors="April S. Liang,Fatemeh Amrollahi,Yixing Jiang,Conor K. Corbin,Grace Y. E. Kim,David Mui,Trevor Crowell,Aakash Acharya,Sreedevi Mony,Soumya Punnathanam,Jack McKeown,Margaret Smith,Steven Lin,Arnold Milstein,Kevin Schulman,Jason Hom,Michael A. Pfeffer,Tho D. Pham,David Svec,Weihan Chu,Lisa Shieh,Christopher Sharp,Stephen P. Ma,Jonathan H. Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04354v1.html">SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> April S. Liang, Fatemeh Amrollahi, Yixing Jiang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record to reduce unnecessary repeat laboratory testing in inpatients. A randomized controlled pilot targeting complete blood count (CBC) utilization demonstrated a significant 15% relative reduction in repetitive testing without adverse safety effects, providing valuable lessons on ML-driven CDS implementation and governance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Laboratory Medicine</span>
                    
                    <span class="domain-tag">Hospital Medicine</span>
                    
                    <span class="domain-tag">Inpatient Care</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Clinical Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04354v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04354v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04354v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04354v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04333v1"
                     data-domains="Oncology,Cancer Diagnostics,Precision Medicine,Lung Cancer,Kidney Cancer,Cervical Cancer"
                     data-keywords="RNA-seq,Early Cancer Detection,Biomarker Discovery,Graph Convolutional Networks,Feature Selection,Machine Learning,Gene Expression,Oncology"
                     data-authors="Shreyas Shende,Varsha Narayanan,Vishal Fenn,Yiran Huang,Dincer Goksuluk,Gaurav Choudhary,Melih Agraz,Mengjia Xu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04333v1.html">RGE-GCN: Recursive Gene Elimination with Graph Convolutional Networks for RNA-seq based Early Cancer Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shreyas Shende, Varsha Narayanan, Vishal Fenn et al.
                </div>

                <div class="paper-summary">
                    RGE-GCN is a novel framework that integrates Graph Convolutional Networks (GCNs) with recursive gene elimination for highly accurate and interpretable early cancer detection using RNA-seq data. It identifies compact sets of predictive biomarkers by building gene expression graphs and iteratively removing less relevant genes, outperforming traditional differential expression methods. The selected genes align with well-known cancer pathways, suggesting its promise as a generalizable tool for biomarker discovery.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Cancer Diagnostics</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                    <span class="domain-tag">Lung Cancer</span>
                    
                    <span class="domain-tag">Kidney Cancer</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04333v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04333v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04333v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04333v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04314v1"
                     data-domains="Pathology,Diagnostic Imaging,Histopathology,Medical AI"
                     data-keywords="Vision Transformers,Hyperspectral Imaging,Spatial-Channel Decoupling,Infrared Pathology,Multi-channel Vision,Deep Learning,Medical Imaging,Diagnostic AI"
                     data-authors="Jiashu Liao,Pietro Li√≤,Marc de Kamps,Duygu Sarikaya">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04314v1.html">DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jiashu Liao, Pietro Li√≤, Marc de Kamps et al.
                </div>

                <div class="paper-summary">
                    DisentangleFormer addresses a fundamental limitation of Vision Transformers by introducing a novel architecture for spatial-channel decoupling, allowing independent modeling of structural and semantic dependencies in multi-channel data. This approach achieves state-of-the-art performance across various hyperspectral datasets, including critical applications in infrared pathology imaging, while simultaneously improving computational efficiency.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04314v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04314v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04314v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04314v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04287v1"
                     data-domains="Infectious Disease Epidemiology,Public Health,Global Health Security,Health Policy,Preventive Medicine"
                     data-keywords="Artificial Intelligence,Horizon Scanning,Infectious Diseases,Public Health Preparedness,Signal Detection,Risk Assessment,Foresight,Decision Support"
                     data-authors="Ian Miles,Mayumi Wakimoto,Wagner Meira,Daniela Paula,Daylene Ticiane,Bruno Rosa,Jane Biddulph,Stelios Georgiou,Valdir Ermida">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04287v1.html">Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ian Miles, Mayumi Wakimoto, Wagner Meira et al.
                </div>

                <div class="paper-summary">
                    This review explores the integration of Artificial Intelligence (AI) into Horizon Scanning for infectious diseases, focusing on identifying and responding to emerging threats. It analyzes how AI tools can enhance critical functions like signal detection, data monitoring, scenario analysis, and decision support, while also addressing associated risks and proposing strategies for effective implementation and governance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Infectious Disease Epidemiology</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Global Health Security</span>
                    
                    <span class="domain-tag">Health Policy</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04287v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04287v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04287v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04287v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04252v1"
                     data-domains="Oncology,Drug Discovery,Medicinal Chemistry,Pharmacology,Cancer Research"
                     data-keywords="TDP1,ChemBERTa,Deep Learning,Drug Discovery,pIC50,Chemoresistance,SMILES,Virtual Screening"
                     data-authors="Baichuan Zeng">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04252v1.html">Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Baichuan Zeng
                </div>

                <div class="paper-summary">
                    This paper introduces a deep learning framework leveraging fine-tuned ChemBERTa, a pre-trained chemical language model, to quantitatively predict the pIC50 values of small molecules against Tyrosyl-DNA Phosphodiesterase 1 (TDP1) directly from their SMILES strings. Utilizing a large dataset and addressing activity imbalance, the model demonstrates superior performance over classical baselines and competitive results with Random Forest, providing an accurate and robust tool for accelerating TDP1 inhibitor discovery.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Cancer Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04252v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04252v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04252v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04252v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04238v1"
                     data-domains="Radiology,Medical AI,Anatomy,Clinical Decision Support,Diagnostic Imaging"
                     data-keywords="Vision-language models (VLMs),Adversarial examples,Anatomical variants,Medical imaging,Generalization,AI bias,Clinical workflows,Benchmarking"
                     data-authors="Leon Mayer,Piotr Kalinowski,Caroline Ebersbach,Marcel Knopp,Tim R√§dsch,Evangelia Christodoulou,Annika Reinke,Fiona R. Kolbinger,Lena Maier-Hein">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04238v1.html">6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Leon Mayer, Piotr Kalinowski, Caroline Ebersbach et al.
                </div>

                <div class="paper-summary">
                    This paper introduces AdversarialAnatomyBench, the first benchmark dataset comprising naturally occurring rare anatomical variants across various medical imaging modalities, to evaluate the robustness of Vision-Language Models (VLMs). Benchmarking 22 state-of-the-art VLMs revealed a drastic accuracy drop from 74% on typical to 29% on atypical anatomy, with leading models also showing significant performance degradation, highlighting a critical generalization weakness. The findings emphasize that current VLMs struggle with uncommon medical presentations, posing substantial risks for their integration into clinical workflows.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                    <span class="domain-tag">Anatomy</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04238v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04238v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04238v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04238v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04232v1"
                     data-domains="Epidemiology,Public Health,Infectious Disease Monitoring,Health Informatics,Health Communication,Digital Health"
                     data-keywords="Digital public health,Social media,Decentralized networks,Artificial intelligence,Large Language Models,Disease surveillance,Public health monitoring,Data access"
                     data-authors="Marcel Salath√©,Sharada P. Mohanty">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04232v1.html">Decentralized Social Media and Artificial Intelligence in Digital Public Health Monitoring</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.PE</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Marcel Salath√©, Sharada P. Mohanty
                </div>

                <div class="paper-summary">
                    This viewpoint paper addresses a critical paradox in digital public health monitoring: the simultaneous rise of powerful AI tools (like LLMs) for data analysis and the dwindling access to traditional social media data due to platform policy changes. It argues that public health surveillance must adapt by exploring decentralized social networks (e.g., Mastodon, Bluesky) as alternative data sources, developing new methodologies, and advocating for policies that ensure privacy-respective access to public data for researchers.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Infectious Disease Monitoring</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Health Communication</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04232v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04232v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04232v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04232v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04228v1"
                     data-domains="Clinical Decision Support,Medical Diagnostics,Drug Discovery and Research,Evidence-Based Medicine,Personalized Treatment Planning,Public Health Policy Modeling"
                     data-keywords="Large Language Models,Logical Fallacies,Scientific Reasoning,Dual-Inference,Counterfactual Reasoning,Negation,Adversarial Training,Healthcare AI"
                     data-authors="Peter B. Walker,Hannah Davidson,Aiden Foster,Matthew Lienert,Thomas Pardue,Dale Russell">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04228v1.html">Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Peter B. Walker, Hannah Davidson, Aiden Foster et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the vulnerability of Large Language Models (LLMs) to logical fallacies, particularly in scientific reasoning involving negation or faulty premises, due to their prevalent affirmation-based inference. It demonstrates these systematic weaknesses in current LLMs and proposes a novel dual-reasoning training framework. This framework integrates affirmative generation with structured counterfactual denial, enabling LLMs to not only affirm valid inferences but also explicitly reject invalid ones, thereby enhancing their robustness and alignment with human reasoning.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Diagnostics</span>
                    
                    <span class="domain-tag">Drug Discovery and Research</span>
                    
                    <span class="domain-tag">Evidence-Based Medicine</span>
                    
                    <span class="domain-tag">Personalized Treatment Planning</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04228v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04228v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04228v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04228v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04225v1"
                     data-domains="Genetics,Genomics,Bioinformatics,Population Health,Epidemiology,Medical Research Ethics,Precision Medicine"
                     data-keywords="Genome-Wide Association Studies (GWAS),Differential Privacy (DP),Phenotype Randomization,Genetic Privacy,Optimization,Personalized Priors,Summary Statistics,Biomedical Research"
                     data-authors="Anupama Nandi,Seth Neel,Hyunghoon Cho">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04225v1.html">GOPHER: Optimization-based Phenotype Randomization for Genome-Wide Association Studies with Differential Privacy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Anupama Nandi, Seth Neel, Hyunghoon Cho
                </div>

                <div class="paper-summary">
                    This paper introduces GOPHER, a novel set of differential privacy (DP) mechanisms designed to accurately release complete genome-wide association study (GWAS) summary statistics while rigorously protecting participant privacy. By employing optimization-based randomization and personalized priors, GOPHER minimizes noise and enhances the utility of privacy-preserving GWAS results. The mechanisms are validated for accuracy on UK Biobank datasets using both real and simulated phenotypes.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Genetics</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                    <span class="domain-tag">Population Health</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04225v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04225v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04225v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04225v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04210v1"
                     data-domains="cs.AI"
                     data-keywords="cs.AI,cs.CL,cs.CY"
                     data-authors="Huy Nghiem,Swetasudha Panda,Devashish Khatwani,Huy V. Nguyen,Krishnaram Kenthapadi,Hal Daum√©">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04210v1.html">Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huy Nghiem, Swetasudha Panda, Devashish Khatwani et al.
                </div>

                <div class="paper-summary">
                    Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment fram...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04210v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04210v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04210v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04210v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04207v1"
                     data-domains="Neurology,Primary Care,Clinical Informatics,Emergency Medicine (implied for urgent evaluation)"
                     data-keywords="secondary headache,clinical decision support system,multi-agent system,large language model,primary care,diagnosis,explainable AI,neurology"
                     data-authors="Xizhi Wu,Nelly Estefanie Garduno-Rapp,Justin F Rousseau,Mounika Thakkallapally,Hang Zhang,Yuelyu Ji,Shyam Visweswaran,Yifan Peng,Yanshan Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04207v1.html">Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xizhi Wu, Nelly Estefanie Garduno-Rapp, Justin F Rousseau et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an LLM-based multi-agent clinical decision support system (CDSS) for explicit and interpretable secondary headache diagnosis from free-text clinical vignettes in primary care. Leveraging an orchestrator-specialist architecture, the system demonstrated consistently higher F1 scores for diagnostic accuracy, especially when combined with guideline-based prompting, significantly outperforming single-LLM baselines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Primary Care</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Emergency Medicine (implied for urgent evaluation)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04207v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04207v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04207v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04207v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04187v1"
                     data-domains="Pathology,Oncology,Histology,Diagnostic Medicine,Telemedicine,Neuro-oncology"
                     data-keywords="Computational pathology,Artificial intelligence,Histopathology,Digital pathology,Real-time AI,Platform-agnostic,Whole slide imaging,Telepathology"
                     data-authors="Jinzhen Hu,Kevin Faust,Parsa Babaei Zadeh,Adrienn Bourkas,Shane Eaton,Andrew Young,Anzar Alvi,Dimitrios George Oreopoulos,Ameesha Paliwal,Assem Saleh Alrumeh,Evelyn Rose Kamski-Hennekam,Phedias Diamandis">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04187v1.html">OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jinzhen Hu, Kevin Faust, Parsa Babaei Zadeh et al.
                </div>

                <div class="paper-summary">
                    OnSight Pathology introduces a novel platform-agnostic computer vision software designed to integrate real-time artificial intelligence (AI) inferences directly into existing histopathology workflows. By utilizing continuous screen captures, this self-contained executable operates locally on consumer-grade personal computers, overcoming barriers related to proprietary digital pathology solutions and complex IT integration to democratize AI adoption in diagnostic pathology.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Histology</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04187v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04187v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04187v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04187v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04034v1"
                     data-domains="Radiology,Pathology,Dermatology,Ophthalmology,Digital health"
                     data-keywords="Out-of-distribution detection,Domain generalization,Information theory,Domain feature collapse,Single-domain learning,Medical imaging,Representation learning,Transfer learning"
                     data-authors="Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04034v1.html">Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hong Yang, Devroop Kar, Qi Yu et al.
                </div>

                <div class="paper-summary">
                    This paper provides an information-theoretic explanation for the catastrophic failure of out-of-distribution (OOD) detection methods when models are trained on single-domain datasets, attributing it to "domain feature collapse" where domain-specific information is completely discarded (I(x_d; z) = 0). The authors introduce a solution, domain filtering using pretrained representations to preserve domain information, which empirically resolves this failure mode and offers insights into supervised learning limitations and transfer learning strategies.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Digital health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04034v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04034v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04034v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04034v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03994v1"
                     data-domains="Clinical decision support,Medical record summarization,Patient communication,Medical legal compliance,Healthcare administration,Telehealth platforms,Drug interaction screening"
                     data-keywords="LLM alignment,policy violation detection,out-of-distribution detection,activation-space whitening,AI governance,medical AI,compliance,training-free"
                     data-authors="Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03994v1.html">Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Oren Rachmil, Roy Betser, Itay Gershon et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel training-free method for robust policy violation detection in LLMs by framing it as an out-of-distribution problem. It leverages activation-space whitening to decorrelate and standardize hidden LLM activations, using the Euclidean norm in this transformed space as an efficient and interpretable compliance score. The approach achieves state-of-the-art results on policy benchmarks, outperforming existing guardrails and fine-tuned models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical decision support</span>
                    
                    <span class="domain-tag">Medical record summarization</span>
                    
                    <span class="domain-tag">Patient communication</span>
                    
                    <span class="domain-tag">Medical legal compliance</span>
                    
                    <span class="domain-tag">Healthcare administration</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03994v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03994v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03994v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03994v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03883v1"
                     data-domains="Oncology,Gastroenterology,Colorectal Surgery,Medical Imaging,Artificial Intelligence in Medicine"
                     data-keywords="Rectal cancer,Watch-and-wait,Local regrowth,Endoscopy,Deep learning,Swin Transformer,Siamese network,Cross-attention"
                     data-authors="Jorge Tapias Gomez,Despoina Kanata,Aneesh Rangnekar,Christina Lee,Julio Garcia-Aguilar,Joshua Jesse Smith,Harini Veeraraghavan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03883v1.html">Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jorge Tapias Gomez, Despoina Kanata, Aneesh Rangnekar et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Dual Cross-Attention Siamese Transformer (SSDCA) for early and objective detection of local rectal tumor regrowth (LR) in patients undergoing watch-and-wait (WW) surveillance. By combining longitudinal endoscopy images with a novel dual cross-attention mechanism, SSDCA achieved superior balanced accuracy (81.76%), sensitivity (90.07%), and specificity (72.86%) in distinguishing complete response from LR. The model demonstrated robustness to image artifacts and strong discriminative feature learning, addressing a critical need in rectal cancer patient management.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Colorectal Surgery</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03883v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03883v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03883v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03883v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-05 06:27:13</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>