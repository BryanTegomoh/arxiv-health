<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">48</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">48</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">162</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Diagnostic Imaging (8), Radiology (7), Clinical Decision Support (7)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (8)</option>
                        
                        <option value="Radiology">Radiology (7)</option>
                        
                        <option value="Clinical Decision Support">Clinical Decision Support (7)</option>
                        
                        <option value="Oncology">Oncology (6)</option>
                        
                        <option value="Neurology">Neurology (5)</option>
                        
                        <option value="Public Health">Public Health (5)</option>
                        
                        <option value="Digital Health">Digital Health (5)</option>
                        
                        <option value="Geriatrics">Geriatrics (4)</option>
                        
                        <option value="Diagnostics">Diagnostics (4)</option>
                        
                        <option value="Drug Discovery">Drug Discovery (4)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2512.05114v1"
                     data-domains="Pediatric Neurology,Neuroimaging,Developmental Neuroscience,Radiology,Child Health,Medical Image Analysis"
                     data-keywords="infant brain,MRI segmentation,deep learning,domain randomization,multi-contrast MRI,pediatric neuroimaging,anatomical delineation,baby brain"
                     data-authors="Malte Hoffmann,Lilla Z√∂llei,Adrian V. Dalca">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05114v1.html">Deep infant brain segmentation from multi-contrast MRI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Malte Hoffmann, Lilla Z√∂llei, Adrian V. Dalca
                </div>

                <div class="paper-summary">
                    This paper introduces BabySeg, a novel deep learning framework designed for accurate and robust brain segmentation from multi-contrast MRI in infants and young children. It addresses the existing challenges of pediatric brain MRI analysis, offering a single model solution that achieves state-of-the-art performance across diverse imaging protocols and age groups, significantly outperforming and outrunning current fragmented methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pediatric Neurology</span>
                    
                    <span class="domain-tag">Neuroimaging</span>
                    
                    <span class="domain-tag">Developmental Neuroscience</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Child Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05114v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05114v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05114v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05114v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05080v1"
                     data-domains="Pharmacology,Drug Discovery,Medicinal Chemistry,Oncology,Infectious Diseases,Neuroscience,Cardiovascular Medicine"
                     data-keywords="structure-based drug design,generative model,de novo design,flow matching,multi-task learning,protein-ligand binding,virtual screening,drug discovery"
                     data-authors="Ian Dunn,Liv Toft,Tyler Katz,Juhi Gupta,Riya Shah,Ramith Hettiarachchi,David R. Koes">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05080v1.html">OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ian Dunn, Liv Toft, Tyler Katz et al.
                </div>

                <div class="paper-summary">
                    OMTRA is a novel multi-modal flow matching generative model designed for structure-based drug design (SBDD). It unifies various SBDD tasks, including de novo ligand design and docking, under a consistent framework and achieves state-of-the-art performance on pocket-conditioned de novo design and docking. The research also introduced a significant new dataset of 500 million 3D molecular conformers for training.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Infectious Diseases</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05080v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05080v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05080v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05080v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05066v1"
                     data-domains="Clinical Pharmacology,General Internal Medicine,Pharmacy,Clinical Informatics,Medical Decision Making,Patient Safety"
                     data-keywords="LLM Collaboration,Medication Recommendation,Clinical Decision Support,AI Reliability,LLM Chemistry,Healthcare AI,Ensemble Learning,Clinical Vignettes"
                     data-authors="Huascar Sanchez,Briland Hitaj,Jules Bergmann,Linda Briesemeister">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05066v1.html">Multi-LLM Collaboration for Medication Recommendation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huascar Sanchez, Briland Hitaj, Jules Bergmann et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a multi-LLM collaboration strategy, guided by a novel "LLM Chemistry" framework, to enhance the reliability of medication recommendations derived from brief clinical vignettes. The approach addresses individual LLM inconsistencies and hallucinations, aiming to create effective, stable, and calibrated ensembles for clinical decision support. Preliminary results from real-world clinical scenarios are encouraging, suggesting a promising path toward trustworthy AI assistants in healthcare.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Pharmacology</span>
                    
                    <span class="domain-tag">General Internal Medicine</span>
                    
                    <span class="domain-tag">Pharmacy</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Medical Decision Making</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05066v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05066v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05066v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05066v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05030v1"
                     data-domains="Biomechanics,Rehabilitation Medicine,Orthopedics,Sports Medicine,Geriatrics,Physical Therapy"
                     data-keywords="Ground Reaction Force,Ground Reaction Moment,Deep Learning,Attention Network,Biomechanics,Gait Analysis,Rehabilitation,Insole Sensors"
                     data-authors="Xuan Li,Samuel Bello">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05030v1.html">Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xuan Li, Samuel Bello
                </div>

                <div class="paper-summary">
                    This paper introduces a Dual-Path Region-Guided Attention Network for accurate, insole-based estimation of three-dimensional ground reaction forces (GRFs) and moments (GRMs), vital for biomechanics and clinical rehabilitation. The network leverages anatomy-inspired spatial and temporal priors through a region-level attention mechanism alongside a complementary path capturing full sensor field context. The model demonstrates robust performance, outperforming strong CNN and CNN-LSTM baselines on two datasets with impressive normalized root mean square error (NRMSE) metrics.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Biomechanics</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                    <span class="domain-tag">Sports Medicine</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05030v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05030v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05030v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05030v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.05012v1"
                     data-domains="Clinical Trials,Evidence-Based Medicine,AI in Healthcare,Medical Decision Support Systems,Pharmacovigilance"
                     data-keywords="RAG,Contrastive Learning,Evidence Re-ranking,Factuality,Hallucination Mitigation,Transparency,Clinical Trials,Safety-Critical AI"
                     data-authors="Francielle Vargas,Daniel Pedronette">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.05012v1.html">Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Francielle Vargas, Daniel Pedronette
                </div>

                <div class="paper-summary">
                    This paper introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method for Retrieval Augmented Generation (RAG) systems that enhances factuality and transparency. CER fine-unes embeddings with contrastive learning and generates token-level attribution rationales, explicitly aligning retrieval with evidential reasoning. Evaluated on clinical trial reports, the method demonstrates improved retrieval accuracy, mitigates hallucinations, and provides transparent, reliable evidence-based retrieval critical for safety-critical domains.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                    <span class="domain-tag">Evidence-Based Medicine</span>
                    
                    <span class="domain-tag">AI in Healthcare</span>
                    
                    <span class="domain-tag">Medical Decision Support Systems</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.05012v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.05012v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.05012v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.05012v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04980v1"
                     data-domains="Personalized Medicine,Pharmacovigilance,Epidemiology,Clinical Trials,Disease Progression Modeling,Precision Diagnostics,Public Health Interventions"
                     data-keywords="Causal Inference,Longitudinal Data,Individual Treatment Effects (ITEs),Counterfactual Regression,Causal Representation Learning,Variational Autoencoders,Recurrent Neural Networks,Interpretability"
                     data-authors="Mouad EL Bouchattaoui">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04980v1.html">Learning Causality for Longitudinal Data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ stat.ML</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mouad EL Bouchattaoui
                </div>

                <div class="paper-summary">
                    This thesis presents advanced methods for causal inference and causal representation learning (CRL) specifically designed for high-dimensional, time-varying longitudinal data. It introduces the Causal Dynamic Variational Autoencoder (CDVAE) for robust Individual Treatment Effect (ITE) estimation under unobserved confounding, an efficient RNN-based framework for long-term counterfactual prediction, and a novel model-agnostic interpretability layer to elucidate how latent causes manifest in observed variables.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                    <span class="domain-tag">Disease Progression Modeling</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04980v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04980v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04980v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04980v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04967v1"
                     data-domains="Ophthalmology,Retinal Imaging,Diagnostic Medicine,Medical Artificial Intelligence"
                     data-keywords="Few-shot learning,Retinal disease diagnosis,Data imbalance,Episodic learning,CLAHE,Diabetic retinopathy,Macular degeneration,Fundus imaging"
                     data-authors="Jasmaine Khale,Ravi Prakash Srivastava">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04967v1.html">Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jasmaine Khale, Ravi Prakash Srivastava
                </div>

                <div class="paper-summary">
                    This paper introduces a balanced few-shot episodic learning framework designed for accurate retinal disease diagnosis, specifically addressing the challenge of data imbalance and scarcity in medical imaging datasets like RFMiD. By integrating balanced sampling, targeted data augmentation (including CLAHE), and a ResNet-50 encoder, the method significantly enhances diagnostic accuracy and reduces bias for underrepresented retinal diseases even with limited labeled data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Retinal Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Medical Artificial Intelligence</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04967v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04967v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04967v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04967v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04943v1"
                     data-domains="Geriatrics,Rehabilitation,Assisted Living Technologies,Telehealth,Fall Detection Systems,Behavioral Monitoring,Smart Hospitals"
                     data-keywords="Multimodal fusion,Deep learning,Human action recognition,Gating mechanisms,Active assisted living,Surveillance,Patient monitoring,Human-computer interaction"
                     data-authors="Novanto Yudistira">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04943v1.html">Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Novanto Yudistira
                </div>

                <div class="paper-summary">
                    This study introduces a novel methodology for human action recognition using adaptive fusion of multimodal deep networks (RGB, optical flow, audio, depth) with gating mechanisms. The approach demonstrates superior accuracy and robustness compared to traditional unimodal methods, achieving promising advancements across various recognition tasks including violence detection. Its core contribution lies in selectively integrating relevant information from diverse modalities to create a more holistic action representation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Rehabilitation</span>
                    
                    <span class="domain-tag">Assisted Living Technologies</span>
                    
                    <span class="domain-tag">Telehealth</span>
                    
                    <span class="domain-tag">Fall Detection Systems</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04943v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04943v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04943v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04943v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04938v1"
                     data-domains="cs.AI"
                     data-keywords="cs.AI"
                     data-authors="Raquel Norel,Michele Merler,Pavitra Modi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04938v1.html">Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Raquel Norel, Michele Merler, Pavitra Modi
                </div>

                <div class="paper-summary">
                    Patients with rare neurological diseases report cognitive symptoms -"brain fog"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04938v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04938v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04938v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04938v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04937v1"
                     data-domains="Neurology,Neurodegeneration,Geriatrics,Pharmacology,Diagnostics,Precision Medicine"
                     data-keywords="Alzheimer's disease,amyloid-Œ≤,tau pathology,neuroinflammation,combination therapy,multi-target therapy,precision medicine,biomarkers"
                     data-authors="She Xutong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04937v1.html">A Systemic Pathological Network Model and Combinatorial Intervention Strategies for Alzheimer's Disease</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ q-bio.MN</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> She Xutong
                </div>

                <div class="paper-summary">
                    This review conceptualizes Alzheimer's disease (AD) as a systemic pathological network involving AŒ≤, tau, and neuroinflammation, moving beyond the linear amyloid cascade hypothesis. It advocates for early biomarker-based detection and comprehensive combination therapies, integrating emerging technologies like gene editing and AI, to fundamentally alter AD's trajectory towards preemptive, personalized management.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Neurodegeneration</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Diagnostics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04937v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04937v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04937v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04937v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04890v1"
                     data-domains="Fetal Medicine,Radiology,Perinatology,Diagnostic Imaging"
                     data-keywords="Fetal MRI,Head Pose Estimation,Equivariant Neural Networks,Symmetry-Aware,6-DoF Pose,Medical Image Analysis,Adaptive Slice Prescription,Clinical Translation"
                     data-authors="Ramya Muthukrishnan,Borjan Gagoski,Aryn Lee,P. Ellen Grant,Elfar Adalsteinsson,Polina Golland,Benjamin Billot">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04890v1.html">Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ramya Muthukrishnan, Borjan Gagoski, Aryn Lee et al.
                </div>

                <div class="paper-summary">
                    This paper introduces E(3)-Pose, a novel and fast pose estimation method explicitly designed to account for rotation equivariance and object symmetry, particularly for fetal head pose during MRI scans. E(3)-Pose addresses the limitations of existing methods by robustly estimating 6-DoF fetal head pose, demonstrating superior accuracy and generalization on clinical MRI volumes, thus enabling automatic adaptive prescription of diagnostic MRI slices.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Fetal Medicine</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Perinatology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04890v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04890v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04890v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04890v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04889v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical Physics,Neurology,Emergency Medicine,Point-of-Care Diagnostics"
                     data-keywords="low-field MRI,electromagnetic interference,EMI mitigation,FENCE,RF coil,capacitive coupling,flexible PCB,SNR enhancement,medical imaging,portability"
                     data-authors="Julia Pfitzer,Martin Uecker,Hermann Scharfetter">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04889v1.html">FENCE: Flexible Electric Noise Cancellation Endo-shield for the Suppression of Electromagnetic Interference in Low-Field MRI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Julia Pfitzer, Martin Uecker, Hermann Scharfetter
                </div>

                <div class="paper-summary">
                    This paper introduces FENCE (Flexible Electric Noise Cancellation Endo-shield), a novel method to suppress electromagnetic interference (EMI) in low-field MRI systems without traditional Faraday shielding. By targeting capacitive EMI coupling using flexible PCB shields inside RF coils and capacitive coil segmentation, FENCE significantly enhances signal-to-noise ratio (SNR) and reduces EMI. This innovation offers a low-cost, retrofittable solution, boosting image quality while maintaining MRI system portability and accessibility.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04889v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04889v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04889v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04889v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04875v1"
                     data-domains="Radiology,Diagnostic Imaging,Pulmonology"
                     data-keywords="Lesion detection,Chest X-ray,Multi-label classification,Self-prompted learning,Medical imaging AI,Deep learning,Radiology,Diagnostic support"
                     data-authors="Qing Xu,Yanqian Wang,Xiangjian Hea,Yue Li,Yixuan Zhang,Rong Qu,Wenting Duan,Zhen Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04875v1.html">SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Qing Xu, Yanqian Wang, Xiangjian Hea et al.
                </div>

                <div class="paper-summary">
                    SP-Det proposes a novel self-prompted detection framework for multi-label lesion detection in chest X-rays, circumventing the need for labor-intensive manual expert annotations. It automatically generates rich textual context through a dual-text prompt generator and enhances feature representation via a bidirectional feature enhancer, achieving state-of-the-art accuracy on diverse datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04875v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04875v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04875v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04875v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04847v1"
                     data-domains="Cardiology,Pulmonology,Respiratory Medicine,Infectious Diseases (e.g., COVID-19),Diagnostic Medicine"
                     data-keywords="Language Models,Audio Understanding,Medical AI,Auscultation,Clinical Semantics,Post-Training Alignment,Cardio-respiratory Monitoring,Diagnostic Tools"
                     data-authors="Tsai-Ning Wang,Lin-Lin Chen,Neil Zeghidour,Aaqib Saeed">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04847v1.html">Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.SD</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tsai-Ning Wang, Lin-Lin Chen, Neil Zeghidour et al.
                </div>

                <div class="paper-summary">
                    This paper introduces AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework designed to bridge the gap between acoustic pattern detection and clinical semantic understanding in pre-trained audio models. AcuLa aligns any audio encoder with a medical language model using a novel, LLM-generated large-scale dataset, instilling clinical awareness. The framework achieves state-of-the-art diagnostic performance across diverse cardio-respiratory tasks, transforming purely acoustic models into clinically-aware diagnostic tools.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Respiratory Medicine</span>
                    
                    <span class="domain-tag">Infectious Diseases (e.g., COVID-19)</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04847v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04847v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04847v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04847v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04843v1"
                     data-domains="Eating Disorders,Psychiatry,Clinical Psychology,Public Health,Digital Health,Mental Health Technology"
                     data-keywords="eating disorders,generative AI,AI risks,mental health,qualitative research,clinical psychology,digital health,safeguard design"
                     data-authors="Amy Winecoff,Kevin Klyman">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04843v1.html">From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.HC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Amy Winecoff, Kevin Klyman
                </div>

                <div class="paper-summary">
                    This paper investigates the serious risks generative AI systems pose to individuals vulnerable to eating disorders, noting that existing safeguards are inadequate as they overlook subtle but clinically significant cues. Through semi-structured interviews with 15 eating disorder experts and abductive qualitative analysis, the authors developed a seven-category taxonomy of AI-related risks to understand how specific user interactions intersect with clinical features of eating disorders.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Eating Disorders</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Clinical Psychology</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04843v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04843v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04843v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04843v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04834v1"
                     data-domains="Digital Healthcare,Clinical Informatics,Medical Language Processing,Epidemiology (implied by comorbidity)"
                     data-keywords="LLMs,Multilingual,Zero-Shot Learning,Information Retrieval,Electronic Health Records,Italian Healthcare,Comorbidity Extraction,Clinical NLP"
                     data-authors="Vignesh Kumar Kembu,Pierandrea Morandini,Marta Bianca Maria Ranzini,Antonino Nocera">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04834v1.html">Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vignesh Kumar Kembu, Pierandrea Morandini, Marta Bianca Maria Ranzini et al.
                </div>

                <div class="paper-summary">
                    This paper investigates the zero-shot multilingual capability of open-source Large Language Models (LLMs) for information extraction from Italian Electronic Health Records (EHRs), specifically focusing on comorbidity extraction. The study reveals that while LLMs show promise, their performance in zero-shot, on-premises settings varies significantly, often struggling to generalize across different diseases and underperforming compared to traditional pattern matching and manual annotations.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Healthcare</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Medical Language Processing</span>
                    
                    <span class="domain-tag">Epidemiology (implied by comorbidity)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04834v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04834v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04834v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04834v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04829v1"
                     data-domains="Medical Imaging,Diagnostics,Computational Biology,Biomedical Engineering"
                     data-keywords="Sphere Packing,AI-Assisted Discovery,Semidefinite Programming (SDP),Bayesian Optimization,Monte Carlo Tree Search,Medical Imaging,Computational Geometry,Sample-Efficient AI"
                     data-authors="Rasul Tutunov,Alexandre Maraval,Antoine Grosnit,Xihan Li,Jun Wang,Haitham Bou-Ammar">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04829v1.html">Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Rasul Tutunov, Alexandre Maraval, Antoine Grosnit et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel, sample-efficient, model-based AI framework to tackle the challenging mathematical problem of sphere packing, which has implications for fields like medical imaging. By formulating Semidefinite Program (SDP) construction as a sequential decision process, the AI achieved new state-of-the-art upper bounds for sphere packing in dimensions 4-16. This demonstrates that model-based search can advance computational progress in longstanding geometric problems where traditional AI methods are infeasible due to high evaluation costs.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostics</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04829v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04829v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04829v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04829v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04821v1"
                     data-domains="Dermatology (skin lesion analysis),Gastroenterology (colon polyp detection),General Medical Imaging,Computational Pathology"
                     data-keywords="Medical Image Segmentation,Generative Models,Flow Matching,Latent Space,Variational Autoencoders,Uncertainty Quantification,Confidence Maps,Deep Learning"
                     data-authors="Huynh Trinh Ngoc,Hoang Anh Nguyen Kim,Toan Nguyen Hai,Long Tran Quoc">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04821v1.html">LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.99</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huynh Trinh Ngoc, Hoang Anh Nguyen Kim, Toan Nguyen Hai et al.
                </div>

                <div class="paper-summary">
                    This paper introduces LatentFM, a novel flow-matching generative model that operates in a lower-dimensional latent space for medical image segmentation. By leveraging two Variational Autoencoders (VAEs) to encode images and masks into latent representations, LatentFM estimates a conditional velocity field to synthesize diverse, uncertainty-aware segmentation outputs. The model achieves superior segmentation accuracy and efficiency while providing valuable confidence maps, as demonstrated on the ISIC-2018 and CVC-Clinic datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology (skin lesion analysis)</span>
                    
                    <span class="domain-tag">Gastroenterology (colon polyp detection)</span>
                    
                    <span class="domain-tag">General Medical Imaging</span>
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04821v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04821v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04821v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04821v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04662v1"
                     data-domains="Orthopedics,Rheumatology,Pathology,Biomedical Imaging,Musculoskeletal Research"
                     data-keywords="spectral micro-CT,calcification,fibrocartilage,quantitative analysis,photon-counting detector,osteoarthritis,femoroacetabular impingement,3D imaging"
                     data-authors="Vittoria Mazzini,Paolo Cardarelli,Andrew L. Coathup,Eleonora Olivotto,Francesco Grassi,Enrico Tassinari,Simone Velardita,Angelo Taibi,Luca Brombal">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04662v1.html">Spectral micro-CT for quantitative analysis of calcification in fibrocartilage</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vittoria Mazzini, Paolo Cardarelli, Andrew L. Coathup et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel spectral micro-computed tomography (ŒºCT) method for quantitative, non-destructive 3D analysis of calcification in fibrocartilage using a photon-counting detector. It enables volumetric visualization and quantification of calcium structures in intact tissue samples, offering significant advantages over conventional histology. The technique demonstrated high accuracy and resolution, providing detailed 3D calcium maps to enhance the characterization of hip joint disorders.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                    <span class="domain-tag">Rheumatology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Biomedical Imaging</span>
                    
                    <span class="domain-tag">Musculoskeletal Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04662v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04662v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04662v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04662v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04629v1"
                     data-domains="Drug discovery,Pharmaceutical research,Chemical synthesis,Medicinal chemistry,Computational drug design"
                     data-keywords="BioMedGPT-Mol,molecular language model,multi-task learning,drug discovery,retrosynthetic planning,small molecules,AI in medicine,generative AI"
                     data-authors="Chenyang Zuo,Siqi Fan,Zaiqing Nie">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04629v1.html">BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chenyang Zuo, Siqi Fan, Zaiqing Nie
                </div>

                <div class="paper-summary">
                    BioMedGPT-Mol is a novel molecular language model, adapted from general-purpose reasoning models via multi-task learning, designed to enhance molecular understanding and generation. Leveraging a comprehensive dataset and structured training, it achieves remarkable performance across various molecular benchmarks and demonstrates competitive capability in retrosynthetic planning.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Drug discovery</span>
                    
                    <span class="domain-tag">Pharmaceutical research</span>
                    
                    <span class="domain-tag">Chemical synthesis</span>
                    
                    <span class="domain-tag">Medicinal chemistry</span>
                    
                    <span class="domain-tag">Computational drug design</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04629v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04629v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04629v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04629v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04618v1"
                     data-domains="Neurology,Neurorehabilitation,Brain-Computer Interfaces,Epileptology"
                     data-keywords="Speech BCI,ECoG,Neural Decoding,Vision Transformers,Contrastive Learning,Wireless Implant,Epidural Electrodes,Paralysis,Communication"
                     data-authors="Mohamed Baha Ben Ticha,Xingchen Ran,Guillaume Saldanha,Ga√´l Le Godais,Phil√©mon Roussel,Marc Aubert,Amina Fontanell,Thomas Costecalde,Lucas Struber,Serpil Karakas,Shaomin Zhang,Philippe Kahane,Guillaume Charvet,St√©phan Chabard√®s,Blaise Yvert">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04618v1.html">Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mohamed Baha Ben Ticha, Xingchen Ran, Guillaume Saldanha et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an offline speech decoding pipeline leveraging an encoder-decoder deep neural architecture, Vision Transformers, and contrastive learning to directly regress acoustic speech from ECoG signals. It evaluates this approach on clinical subdural and, notably, a fully implantable and wireless epidural ECoG system (WIMAGINE), representing a significant step towards long-term, practical speech BCIs for paralyzed individuals.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Neurorehabilitation</span>
                    
                    <span class="domain-tag">Brain-Computer Interfaces</span>
                    
                    <span class="domain-tag">Epileptology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04618v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04618v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04618v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04618v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04599v1"
                     data-domains="Public Health,Mental Health,Digital Health,Social Media Moderation (health context),Substance Abuse Prevention,Child Protection (health aspect)"
                     data-keywords="Vision-Language Models,Semantic Segmentation,Content Moderation,Malicious Image Detection,Explainable AI,Zero-shot Learning,Public Health,Digital Forensics"
                     data-authors="Sheng Hang,Chaoxiang He,Hongsheng Hu,Hanqing Hu,Bin Benjamin Zhu,Shi-Feng Sun,Dawu Gu,Shuo Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04599v1.html">Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sheng Hang, Chaoxiang He, Hongsheng Hu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel zero-shot pipeline that simultaneously detects, identifies, and localizes harmful content within images, providing pixel-accurate masks for critical elements, moving beyond simple image-level NSFW flags. Evaluated on a diverse dataset of drug, sexual, violent, and extremist content, the method achieves high recall (85.8%) and precision (78.1%), demonstrating significant improvement over direct VLM localization and robust performance against adversarial attacks, presenting a practical and explainable solution for content moderation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Social Media Moderation (health context)</span>
                    
                    <span class="domain-tag">Substance Abuse Prevention</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04599v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04599v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04599v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04599v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04580v1"
                     data-domains="Clinical Decision Support,Drug Discovery,Personalized Medicine,Medical Imaging Analysis,Electronic Health Records (EHR) Processing,Genomics and Proteomics"
                     data-keywords="CryptoTensors,LLMs,Model Security,Confidentiality,Access Control,Safetensors,Healthcare Data,Secure Distribution"
                     data-authors="Huifeng Zhu,Shijie Li,Qinfeng Li,Yier Jin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04580v1.html">A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CR</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huifeng Zhu, Shijie Li, Qinfeng Li et al.
                </div>

                <div class="paper-summary">
                    This paper introduces CryptoTensors, a novel secure and format-compatible file structure designed to protect large language model (LLM) weights, especially when customized with sensitive data from domains like healthcare. Built as an extension to Safetensors, CryptoTensors integrates tensor-level encryption and access control, enabling secure, efficient, and widespread distribution of confidential LLMs with minimal overhead.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Electronic Health Records (EHR) Processing</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04580v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04580v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04580v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04580v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04576v1"
                     data-domains="cs.CV"
                     data-keywords="cs.CV"
                     data-authors="Zishuo Wan,Qinqin Kang,Yi Huang,Yun Bian,Dawei Ding,Ke Yan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04576v1.html">TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zishuo Wan, Qinqin Kang, Yi Huang et al.
                </div>

                <div class="paper-summary">
                    Tumor segmentation and diagnosis in contrast-enhanced Computed Tomography (CT) rely heavily on the physiological dynamics of contrast agents. However, obtaining a complete multi-phase series is often clinically unfeasible due to radiation concerns or scanning limitations, leading to the "missing mod...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.CV</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04576v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04576v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04576v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04576v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04564v1"
                     data-domains="pathology,histopathology,digital pathology,medical imaging,diagnostic imaging"
                     data-keywords="deep learning,microscopic images,dataset creation,image annotation,pathology,domain shift,data quality,supervised learning"
                     data-authors="Christof A. Bertram,Viktoria Weiss,Jonas Ammeling,F. Maria Schabel,Taryn A. Donovan,Frauke Wilm,Christian Marzahl,Katharina Breininger,Marc Aubreville">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04564v1.html">Dataset creation for supervised deep learning-based analysis of microscopic images -- review of important considerations and recommendations</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Christof A. Bertram, Viktoria Weiss, Jonas Ammeling et al.
                </div>

                <div class="paper-summary">
                    This paper presents a comprehensive review guiding the creation of high-quality, large-scale datasets, which are essential for supervised deep learning-based analysis of microscopic images. It details critical steps in dataset development, including image acquisition, annotation software selection, and annotation creation, while addressing challenges like domain variability and bias to ensure the generalizability and robustness of deep learning models for pathology applications.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">pathology</span>
                    
                    <span class="domain-tag">histopathology</span>
                    
                    <span class="domain-tag">digital pathology</span>
                    
                    <span class="domain-tag">medical imaging</span>
                    
                    <span class="domain-tag">diagnostic imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04564v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04564v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04564v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04564v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04536v1"
                     data-domains="Public Health,Emergency Medicine,Forensic Medicine,Occupational Health,Behavioral Health"
                     data-keywords="alcohol intoxication,facial video analysis,deep learning,recurrent neural networks,Graph Attention Network (GAT),3D ResNet,spatiotemporal features,public safety"
                     data-authors="Bita Baroutian,Atefe Aghaei,Mohsen Ebrahimi Moghaddam">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04536v1.html">Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Bita Baroutian, Atefe Aghaei, Mohsen Ebrahimi Moghaddam
                </div>

                <div class="paper-summary">
                    This study introduces a novel video-based facial sequence analysis approach for detecting alcohol intoxication using a recurrent fusion model. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features from a 3D ResNet, dynamically fusing them with adaptive prioritization. It achieves 95.82% accuracy, outperforming existing baselines and demonstrating strong potential for practical deployment in public safety.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Forensic Medicine</span>
                    
                    <span class="domain-tag">Occupational Health</span>
                    
                    <span class="domain-tag">Behavioral Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04536v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04536v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04536v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04536v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04535v1"
                     data-domains="Diagnostics,Drug Discovery,Treatment Planning,Medical Robotics,Bioinformatics,Clinical Decision Support,Medical Imaging Analysis,Surgical Simulation"
                     data-keywords="AI Agents,Tool Simulation,Large Language Models (LLM),Generalist Tool Model (GTM),Context-Aware Response Generation (CARG),Medical AI,Reinforcement Learning,Scalable Training"
                     data-authors="Zhenzhen Ren,Xinpeng Zhang,Zhenxing Qian,Yan Gao,Yu Shi,Shuxin Zheng,Jiyan He">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04535v1.html">GTM: Simulating the World of Tools for AI Agents</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhenzhen Ren, Xinpeng Zhang, Zhenxing Qian et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Generalist Tool Model (GTM), a 1.5-billion-parameter AI model designed as a universal tool simulator to overcome the high cost and complexity of training Large Language Model (LLM) agents with real-world tools. Utilizing a novel Context-Aware Response Generation (CARG) pipeline, GTM synthesizes training data from over 20,000 tools across 300 domains, including medicine, enabling it to generate fast, high-quality, and contextually appropriate outputs that mimic real tool execution, thus significantly accelerating and scaling AI agent development.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Diagnostics</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Treatment Planning</span>
                    
                    <span class="domain-tag">Medical Robotics</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04535v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04535v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04535v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04535v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04520v1"
                     data-domains="dermatology,gastroenterology,breast imaging,ophthalmology"
                     data-keywords="medical image segmentation,zero-shot learning,test-time adaptation,SAM,foundation models,boundary-aware,Gaussian prompts,domain adaptation"
                     data-authors="Chenlin Xu,Lei Zhang,Lituan Wang,Xinyu Pu,Pengfei Ma,Guangwu Qian,Zizhou Wang,Yan Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04520v1.html">Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chenlin Xu, Lei Zhang, Lituan Wang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces BA-TTA-SAM, a novel test-time adaptation framework designed to enhance the zero-shot medical image segmentation performance of the Segment Anything Model (SAM). It tackles critical challenges like data scarcity and domain shifts by integrating encoder-level Gaussian prompt injection and cross-layer boundary-aware attention alignment. The framework achieved an average 12.4% DICE score improvement over SAM's zero-shot performance across four medical datasets, outperforming state-of-the-art methods without requiring source-domain training.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">dermatology</span>
                    
                    <span class="domain-tag">gastroenterology</span>
                    
                    <span class="domain-tag">breast imaging</span>
                    
                    <span class="domain-tag">ophthalmology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04520v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04520v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04520v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04520v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04518v1"
                     data-domains="Oncology,Medical Informatics,Health AI,Clinical Research,Natural Language Processing in Medicine"
                     data-keywords="Chemotherapy Timelines,LLM Systems,Clinical NLP,EHR Extraction,Fine-Tuning,Chain-of-Thought,Oncology Informatics,Medical Information Extraction"
                     data-authors="Tianmai M. Zhang,Zhaoyi Sun,Sihang Zeng,Chenxi Li,Neil F. Abernethy,Barbara D. Lam,Fei Xia,Meliha Yetisgen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04518v1.html">UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tianmai M. Zhang, Zhaoyi Sun, Sihang Zeng et al.
                </div>

                <div class="paper-summary">
                    This paper presents UW-BioNLP's participation in the ChemoTimelines 2025 shared task, focusing on generating patient chemotherapy timelines from raw clinical notes using advanced LLM-based systems. Their approach involved a two-step workflow combining LLM event extraction with algorithmic aggregation, evaluating strategies like fine-tuning, chain-of-thought, and dictionary lookups. A fine-tuned Qwen3-14B model achieved the best official score of 0.678, demonstrating effective LLM utilization for this critical clinical NLP task.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                    <span class="domain-tag">Health AI</span>
                    
                    <span class="domain-tag">Clinical Research</span>
                    
                    <span class="domain-tag">Natural Language Processing in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04518v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04518v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04518v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04518v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04425v1"
                     data-domains="Neurology,Movement Disorders,Geriatrics,Diagnostic Imaging,Rehabilitation Medicine"
                     data-keywords="Parkinson's Disease,Gait Analysis,RGB-D Fusion,Multimodal Learning,Explainable AI,Large Language Models,Deep Learning,Clinical Transparency"
                     data-authors="Manar Alnaasan,Md Selim Sarowar,Sungho Kim">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04425v1.html">Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Manar Alnaasan, Md Selim Sarowar, Sungho Kim
                </div>

                <div class="paper-summary">
                    This paper introduces an explainable multimodal framework that integrates RGB and Depth (RGB-D) data for robust Parkinson's disease (PD) gait recognition under realistic conditions. It leverages advanced deep learning for feature extraction and fusion, coupled with a Large Language Model (LLM) to provide clinically meaningful textual explanations, thus bridging the gap between visual analysis and medical interpretability.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Movement Disorders</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04425v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04425v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04425v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04425v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04401v1"
                     data-domains="Medical Imaging,Oncology (Breast Cancer Screening),Biomedical Engineering,Diagnostic Radiology"
                     data-keywords="Microwave Imaging,Deep Learning,Breast Imaging,Conformal Antenna Array,Dielectric Imaging,Image Reconstruction,Time-Domain Signals"
                     data-authors="Wenyi Shao,Beibei Zhou">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04401v1.html">Learnt Microwave Image Reconstruction with A Conformal Antenna Array</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Wenyi Shao, Beibei Zhou
                </div>

                <div class="paper-summary">
                    This paper introduces a deep learning model for reconstructing 2D dielectric breast images from time-domain signals, distinguishing itself by integrating a conformal antenna array. Unlike fixed array systems, this approach incorporates antenna positioning into the processing pipeline, allowing the array to adapt to various breast sizes for optimal data collection. Numerical results indicate that this method, by enabling the neural network to focus on a pre-estimated breast surface within the region of interest, can reconstruct breast images with good quality.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Oncology (Breast Cancer Screening)</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04401v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04401v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04401v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04401v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04397v1"
                     data-domains="Radiology,Pulmonology,Diagnostic Imaging"
                     data-keywords="transfer learning,medical image classification,deep learning,convolutional neural networks,disease detection,chest X-ray,InceptionV3,ResNet"
                     data-authors="Zeeshan Ahmad,Shudi Bao,Meng Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04397v1.html">Performance Evaluation of Transfer Learning Based Medical Image Classification Techniques for Disease Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zeeshan Ahmad, Shudi Bao, Meng Chen
                </div>

                <div class="paper-summary">
                    This paper conducts a comprehensive evaluation of six pre-trained deep convolutional neural networks (AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3) using transfer learning for disease detection on a custom chest X-ray dataset. The study demonstrates that InceptionV3 consistently achieves superior performance, while highlighting the benefits of transfer learning, especially with limited data, and offers insights for model selection in medical image classification.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04397v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04397v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04397v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04397v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04359v1"
                     data-domains="Clinical Decision Support,Medical Diagnostics,Drug Discovery and Development,Biomedical Research,Health Information Processing,Medical Education and Training"
                     data-keywords="Reinforcement Learning,LLM Reasoning,Entropy Collapse,Semantic Entropy,Token Entropy,Curriculum Learning,KL Regularization,Policy Exploration"
                     data-authors="Hongye Cao,Zhixin Bai,Ziyue Peng,Boyan Wang,Tianpei Yang,Jing Huo,Yuyao Zhang,Yang Gao">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04359v1.html">Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.70</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hongye Cao, Zhixin Bai, Ziyue Peng et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an efficient reinforcement learning framework for large language models (LLMs) to combat entropy collapse, a common issue in RLVR that limits reasoning capabilities. By leveraging entropy signals at both semantic and token levels through a novel data organization and algorithmic design, the method enhances policy exploration and significantly improves LLM reasoning performance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Diagnostics</span>
                    
                    <span class="domain-tag">Drug Discovery and Development</span>
                    
                    <span class="domain-tag">Biomedical Research</span>
                    
                    <span class="domain-tag">Health Information Processing</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04359v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04359v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04359v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04359v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04354v1"
                     data-domains="Hospital Medicine,Internal Medicine,Clinical Pathology,Health Informatics,Quality Improvement"
                     data-keywords="machine learning,clinical decision support,laboratory utilization,complete blood count,electronic health record,healthcare costs,patient safety,implementation science"
                     data-authors="April S. Liang,Fatemeh Amrollahi,Yixing Jiang,Conor K. Corbin,Grace Y. E. Kim,David Mui,Trevor Crowell,Aakash Acharya,Sreedevi Mony,Soumya Punnathanam,Jack McKeown,Margaret Smith,Steven Lin,Arnold Milstein,Kevin Schulman,Jason Hom,Michael A. Pfeffer,Tho D. Pham,David Svec,Weihan Chu,Lisa Shieh,Christopher Sharp,Stephen P. Ma,Jonathan H. Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04354v1.html">SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-04</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> April S. Liang, Fatemeh Amrollahi, Yixing Jiang et al.
                </div>

                <div class="paper-summary">
                    SmartAlert introduces and evaluates a machine learning (ML)-driven Clinical Decision Support (CDS) system integrated into the Electronic Health Record (EHR) to predict stable laboratory results and reduce unnecessary repeat testing. A randomized controlled pilot targeting Complete Blood Count (CBC) utilization demonstrated a significant 15% relative reduction in repetitive testing without compromising patient safety, highlighting critical implementation and governance lessons.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Hospital Medicine</span>
                    
                    <span class="domain-tag">Internal Medicine</span>
                    
                    <span class="domain-tag">Clinical Pathology</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Quality Improvement</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04354v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04354v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04354v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04354v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04333v1"
                     data-domains="Oncology,Cancer Diagnostics,Precision Medicine,Genomics,Computational Biology"
                     data-keywords="RNA-seq,Early Cancer Detection,Biomarker Discovery,Graph Convolutional Networks,Gene Expression,Feature Selection,Integrated Gradients,Oncology"
                     data-authors="Shreyas Shende,Varsha Narayanan,Vishal Fenn,Yiran Huang,Dincer Goksuluk,Gaurav Choudhary,Melih Agraz,Mengjia Xu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04333v1.html">RGE-GCN: Recursive Gene Elimination with Graph Convolutional Networks for RNA-seq based Early Cancer Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shreyas Shende, Varsha Narayanan, Vishal Fenn et al.
                </div>

                <div class="paper-summary">
                    RGE-GCN is a novel framework combining Graph Convolutional Networks (GCNs) for classification with a recursive gene elimination strategy for early cancer detection from RNA-seq data. It effectively identifies a compact set of interpretable and predictive gene biomarkers by outperforming standard tools in accuracy and F1-scores, with selected genes aligning with known cancer pathways.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Cancer Diagnostics</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04333v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04333v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04333v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04333v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04314v1"
                     data-domains="Infrared Pathology,Medical Imaging,Diagnostic Imaging,Histopathology,Biochemical Sensing"
                     data-keywords="Vision Transformers,Spatial-Channel Decoupling,Hyperspectral Imaging,Infrared Pathology,Multi-Channel Vision,Disentangled Representation,Deep Learning,Medical Imaging"
                     data-authors="Jiashu Liao,Pietro Li√≤,Marc de Kamps,Duygu Sarikaya">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04314v1.html">DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jiashu Liao, Pietro Li√≤, Marc de Kamps et al.
                </div>

                <div class="paper-summary">
                    DisentangleFormer is a novel Vision Transformer architecture that addresses the fundamental limitation of entangled spatial and channel representations in multi-channel vision tasks by introducing principled spatial-channel decoupling. It achieves state-of-the-art performance on various hyperspectral and infrared pathology datasets while significantly reducing computational costs.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Infrared Pathology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Biochemical Sensing</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04314v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04314v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04314v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04314v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04287v1"
                     data-domains="Infectious Disease Epidemiology,Public Health Surveillance,Global Health Security,Emergency Preparedness,Preventive Medicine"
                     data-keywords="Artificial Intelligence,Horizon Scanning,Infectious Diseases,Public Health,Preparedness,Epidemiology,Surveillance,Risk Assessment"
                     data-authors="Ian Miles,Mayumi Wakimoto,Wagner Meira,Daniela Paula,Daylene Ticiane,Bruno Rosa,Jane Biddulph,Stelios Georgiou,Valdir Ermida">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04287v1.html">Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ian Miles, Mayumi Wakimoto, Wagner Meira et al.
                </div>

                <div class="paper-summary">
                    This review paper explores the integration of Artificial Intelligence (AI) into Horizon Scanning for Infectious Diseases, focusing on identifying and responding to emerging threats and opportunities. It demonstrates how AI tools can enhance public health preparedness by improving signal detection, data monitoring, scenario analysis, and decision support, while also addressing associated risks and proposing governance strategies.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Infectious Disease Epidemiology</span>
                    
                    <span class="domain-tag">Public Health Surveillance</span>
                    
                    <span class="domain-tag">Global Health Security</span>
                    
                    <span class="domain-tag">Emergency Preparedness</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04287v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04287v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04287v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04287v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04252v1"
                     data-domains="Oncology,Pharmacology,Drug Development,Medicinal Chemistry"
                     data-keywords="TDP1,ChemBERTa,Deep Learning,Drug Discovery,Cancer Chemoresistance,pIC50 prediction,SMILES,Chemical Transformers"
                     data-authors="Baichuan Zeng">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04252v1.html">Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Baichuan Zeng
                </div>

                <div class="paper-summary">
                    This paper introduces a deep learning framework utilizing fine-tuned ChemBERTa models to accurately predict the pIC50 values of small molecules against Tyrosyl-DNA Phosphodiesterase 1 (TDP1) directly from their SMILES strings. Addressing the challenge of cancer chemoresistance, the approach leverages a large, imbalanced dataset and systematically evaluates pre-training strategies, outperforming classical baselines and demonstrating competitive virtual screening utility. The resulting model offers a robust, 3D-structure-free tool to accelerate the discovery of TDP1 inhibitors.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Drug Development</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04252v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04252v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04252v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04252v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04238v1"
                     data-domains="Radiology,Diagnostic Imaging,Anatomy,Clinical Decision Support,Medical Informatics"
                     data-keywords="Vision-language models,Medical AI,Rare anatomical variants,Generalization,Anatomical bias,Medical imaging,Adversarial examples,Clinical workflows"
                     data-authors="Leon Mayer,Piotr Kalinowski,Caroline Ebersbach,Marcel Knopp,Tim R√§dsch,Evangelia Christodoulou,Annika Reinke,Fiona R. Kolbinger,Lena Maier-Hein">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04238v1.html">6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Leon Mayer, Piotr Kalinowski, Caroline Ebersbach et al.
                </div>

                <div class="paper-summary">
                    This paper introduces AdversarialAnatomyBench, the first benchmark specifically designed to test vision-language models (VLMs) on naturally occurring rare anatomical variants across various imaging modalities. The study reveals a critical weakness: state-of-the-art VLMs, including top performers like GPT-5 and Gemini 2.5 Pro, experience a drastic performance drop (from 74% to 29% mean accuracy) when confronted with atypical anatomy compared to typical presentations, with current mitigation strategies proving ineffective.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Anatomy</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04238v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04238v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04238v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04238v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04232v1"
                     data-domains="Public Health,Epidemiology,Digital Health,Infectious Disease Surveillance,Health Informatics,Behavioral Health"
                     data-keywords="Digital public health,Social media,Artificial intelligence,Decentralized networks,Public health monitoring,Large Language Models,Data access,Epidemiology"
                     data-authors="Marcel Salath√©,Sharada P. Mohanty">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04232v1.html">Decentralized Social Media and Artificial Intelligence in Digital Public Health Monitoring</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.PE</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Marcel Salath√©, Sharada P. Mohanty
                </div>

                <div class="paper-summary">
                    Digital public health monitoring faces a critical challenge: while advanced AI (LLMs) offers powerful analytical capabilities, traditional social media data access is diminishing due to platform policy changes. This viewpoint paper explores decentralized social networks (e.g., Mastodon, Bluesky) as alternative data sources, advocating for their adoption alongside new methodologies to focus on broad health signals and ensure privacy-respective data access for public health researchers.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Infectious Disease Surveillance</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04232v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04232v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04232v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04232v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04228v1"
                     data-domains="Clinical Decision Support,Diagnostics,Treatment Planning,Medical Research,Drug Discovery,Patient Safety,Biomedical Informatics"
                     data-keywords="Large Language Models,Logical Fallacies,Scientific Reasoning,Dual-Inference,Counterfactual Reasoning,Clinical Decision Support,AI in Healthcare,Robust AI"
                     data-authors="Peter B. Walker,Hannah Davidson,Aiden Foster,Matthew Lienert,Thomas Pardue,Dale Russell">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04228v1.html">Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Peter B. Walker, Hannah Davidson, Aiden Foster et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the susceptibility of current Large Language Models (LLMs) to logical fallacies, especially in scientific reasoning with negation or faulty premises, due to their affirmation-based inference. It demonstrates these systematic weaknesses in existing LLMs and proposes a novel dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial to foster more robust and logically sound models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Diagnostics</span>
                    
                    <span class="domain-tag">Treatment Planning</span>
                    
                    <span class="domain-tag">Medical Research</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04228v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04228v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04228v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04228v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04225v1"
                     data-domains="Genetics,Genomics,Bioinformatics,Public Health,Precision Medicine,Biomedical Research"
                     data-keywords="Genome-wide association studies (GWAS),Differential Privacy (DP),Phenotype Randomization,Summary Statistics,Genetic Privacy,UK Biobank,Optimization,Personalized Priors"
                     data-authors="Anupama Nandi,Seth Neel,Hyunghoon Cho">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04225v1.html">GOPHER: Optimization-based Phenotype Randomization for Genome-Wide Association Studies with Differential Privacy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Anupama Nandi, Seth Neel, Hyunghoon Cho
                </div>

                <div class="paper-summary">
                    This paper introduces GOPHER, a novel set of practical differentially private mechanisms for releasing comprehensive genome-wide association study (GWAS) summary statistics. It addresses the limitations of prior DP techniques by employing an optimization-based randomization to minimize error and personalized priors to reduce noise, thereby enhancing utility while providing robust privacy guarantees. The approach's accuracy is demonstrated using real and simulated phenotypes from UK Biobank datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Genetics</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04225v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04225v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04225v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04225v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04210v1"
                     data-domains="Conversational AI in Healthcare,Medical Informatics,Clinical Decision Support,Patient Education,Digital Health"
                     data-keywords="Healthcare AI,LLMs,Safety Alignment,KTO,DPO,Adversarial Robustness,Medical Assistants,Patient Safety,Preference Alignment"
                     data-authors="Huy Nghiem,Swetasudha Panda,Devashish Khatwani,Huy V. Nguyen,Krishnaram Kenthapadi,Hal Daum√©">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04210v1.html">Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huy Nghiem, Swetasudha Panda, Devashish Khatwani et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an iterative post-deployment alignment framework using Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to enhance the safety and trustworthiness of healthcare AI assistants. Evaluating four LLMs on the CARES-18K benchmark, the framework achieved up to 42% improvement in harmful query detection, while highlighting critical trade-offs with erroneous refusals and architecture-dependent calibration biases.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Conversational AI in Healthcare</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Patient Education</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04210v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04210v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04210v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04210v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04207v1"
                     data-domains="Neurology,Primary Care,Emergency Medicine,General Medicine"
                     data-keywords="secondary headache,clinical decision support,large language model,multi-agent system,primary care,diagnosis,explainable AI,orchestrator architecture"
                     data-authors="Xizhi Wu,Nelly Estefanie Garduno-Rapp,Justin F Rousseau,Mounika Thakkallapally,Hang Zhang,Yuelyu Ji,Shyam Visweswaran,Yifan Peng,Yanshan Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04207v1.html">Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xizhi Wu, Nelly Estefanie Garduno-Rapp, Justin F Rousseau et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an LLM-based multi-agent clinical decision support system utilizing an orchestrator-specialist architecture to improve the diagnosis of secondary headaches from free-text clinical vignettes in primary care. The system decomposes diagnosis into seven specialized agents, producing evidence-grounded rationales, and demonstrated superior F1 scores, especially when combined with guideline-based prompting (GPrompt), outperforming single-LLM baselines across various open-source models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Primary Care</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">General Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04207v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04207v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04207v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04207v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04187v1"
                     data-domains="Pathology,Oncology,Diagnostic medicine,Telepathology,Intraoperative consultation,Neuropathology"
                     data-keywords="Computational pathology,Artificial intelligence,Histopathology,Real-time inference,Platform-agnostic,Digital pathology,Computer vision,Mitosis detection"
                     data-authors="Jinzhen Hu,Kevin Faust,Parsa Babaei Zadeh,Adrienn Bourkas,Shane Eaton,Andrew Young,Anzar Alvi,Dimitrios George Oreopoulos,Ameesha Paliwal,Assem Saleh Alrumeh,Evelyn Rose Kamski-Hennekam,Phedias Diamandis">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04187v1.html">OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jinzhen Hu, Kevin Faust, Parsa Babaei Zadeh et al.
                </div>

                <div class="paper-summary">
                    OnSight Pathology is a novel, platform-agnostic computer vision software designed to integrate real-time AI inferences into existing histopathology workflows. By utilizing continuous screen captures, it provides automated analysis directly from any digital slide viewer or live microscope feed, overcoming barriers associated with proprietary digital pathology solutions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostic medicine</span>
                    
                    <span class="domain-tag">Telepathology</span>
                    
                    <span class="domain-tag">Intraoperative consultation</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04187v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04187v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04187v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04187v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.04034v1"
                     data-domains="Radiology,Pathology,Dermatology,Medical Imaging Analysis,Diagnostic AI,Clinical Decision Support"
                     data-keywords="Out-of-Distribution Detection,Domain Feature Collapse,Information Theory,Information Bottleneck,Supervised Learning,Medical Imaging,Transfer Learning,Pretrained Models"
                     data-authors="Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04034v1.html">Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hong Yang, Devroop Kar, Qi Yu et al.
                </div>

                <div class="paper-summary">
                    This paper offers the first theoretical explanation, using information theory, for why state-of-the-art Out-of-Distribution (OOD) detection methods catastrophically fail on models trained with single-domain datasets. It proves that such training leads to "domain feature collapse," where models discard all domain-specific information (I(x_d; z) = 0). The authors introduce "Domain Bench" and demonstrate that preserving domain information through a method called "domain filtering" successfully resolves this failure mode, providing strong empirical evidence for their theory.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Diagnostic AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04034v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04034v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04034v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04034v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03994v1"
                     data-domains="Digital Health,Clinical Decision Support Systems,Medical Informatics,Patient Privacy & Security,Healthcare Regulatory Compliance,Medical Ethics"
                     data-keywords="LLMs,Policy Violation Detection,Out-of-Distribution Detection,Activation-Space Whitening,AI Governance,Medical AI,Regulatory Compliance,Alignment"
                     data-authors="Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03994v1.html">Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Oren Rachmil, Roy Betser, Itay Gershon et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel training-free and efficient method for detecting policy violations in large language models (LLMs) by treating it as an out-of-distribution (OOD) detection problem. The approach applies activation-space whitening to decorrelate and standardize LLM hidden activations, using the Euclidean norm in this transformed space as a compliance score. It achieves state-of-the-art results on a challenging policy benchmark, surpassing existing guardrails and fine-tuned models, providing a practical framework for policy-aware AI governance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                    <span class="domain-tag">Patient Privacy & Security</span>
                    
                    <span class="domain-tag">Healthcare Regulatory Compliance</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03994v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03994v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03994v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03994v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03883v1"
                     data-domains="Oncology,Gastroenterology,Colorectal Surgery,Diagnostic Imaging,Medical Artificial Intelligence"
                     data-keywords="Rectal Cancer,Watch-and-Wait,Tumor Regrowth,Endoscopy,Deep Learning,Siamese Transformer,Cross-Attention,Clinical Complete Response,Medical Imaging,Artificial Intelligence"
                     data-authors="Jorge Tapias Gomez,Despoina Kanata,Aneesh Rangnekar,Christina Lee,Julio Garcia-Aguilar,Joshua Jesse Smith,Harini Veeraraghavan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03883v1.html">Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jorge Tapias Gomez, Despoina Kanata, Aneesh Rangnekar et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Dual Cross-Attention Siamese Swin Transformer (SSDCA) for objective and early detection of local tumor regrowth in rectal cancer patients undergoing watch-and-wait surveillance using longitudinal endoscopy images. SSDCA effectively distinguishes local regrowth from complete clinical response by leveraging pretrained Swin transformers and a novel dual cross-attention mechanism to fuse features from two endoscopic scans without spatial alignment, achieving high balanced accuracy, sensitivity, and robustness to image artifacts.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Colorectal Surgery</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical Artificial Intelligence</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03883v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03883v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03883v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03883v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-06 06:25:07</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>