<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">10</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">10</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">33</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Pathology (4), Radiology (3), Medical Imaging (3)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Pathology">Pathology (4)</option>
                        
                        <option value="Radiology">Radiology (3)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (3)</option>
                        
                        <option value="Neurology">Neurology (2)</option>
                        
                        <option value="Oncology">Oncology (2)</option>
                        
                        <option value="Geriatrics">Geriatrics (1)</option>
                        
                        <option value="Cognitive Neuroscience">Cognitive Neuroscience (1)</option>
                        
                        <option value="Psychiatry">Psychiatry (1)</option>
                        
                        <option value="Gastroenterology">Gastroenterology (1)</option>
                        
                        <option value="Digital Pathology">Digital Pathology (1)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2512.13685v1"
                     data-domains="Neurology,Geriatrics,Cognitive Neuroscience,Psychiatry"
                     data-keywords="Alzheimer's Disease,semantic analysis,spontaneous speech,language models,early detection,neurodegeneration,linguistic markers,cognitive decline,interpretability"
                     data-authors="Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Lilian Hubner,B√°rbara Malcorra,C√©sar Renn√≥-Costa,Marco Idiart,Maria-Cruz Villa-Uriol,Aline Villavicencio">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13685v1.html">Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Dylan Phelps, Rodrigo Wilkens, Edward Gow-Smith et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel pipeline for analyzing spontaneous speech in Alzheimer's Disease (AD) detection, demonstrating that language models (LMs) can identify AD based purely on underlying semantic information, independent of surface-level textual patterns. By transforming speech texts to alter syntax and vocabulary while preserving semantics, the study shows that LM classifiers maintain similar performance, highlighting their ability to detect genuine semantic impairment. This work addresses the limited interpretability of LMs and opens new avenues for early, robust AD detection.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Cognitive Neuroscience</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13685v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13685v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13685v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13685v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13668v1"
                     data-domains="Drug Discovery,Medicinal Chemistry,Pharmaceutical Research,Chemical Biology,Biotechnology"
                     data-keywords="Organic Synthesis,Drug Discovery,Large Language Models,Automated Chemistry,Chemical Synthesis Planning,Scientific Reasoning,AI in Chemistry,Medicinal Chemistry"
                     data-authors="Guoqing Liu,Junren Li,Zihan Zhao,Eray Inanc,Krzysztof Maziarz,Jose Garrido Torres,Victor Garcia Satorras,Shoko Ueda,Christopher M. Bishop,Marwin Segler">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13668v1.html">A Scientific Reasoning Model for Organic Synthesis Procedure Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Guoqing Liu, Junren Li, Zihan Zhao et al.
                </div>

                <div class="paper-summary">
                    This paper introduces QFANG, a scientific reasoning language model designed to automatically generate precise, structured experimental procedures for organic synthesis directly from reaction equations, incorporating explicit chain-of-thought reasoning. By leveraging a large, high-quality dataset of chemical reactions and action sequences from patent literature, coupled with a Chemistry-Guided Reasoning (CGR) framework and Reinforcement Learning from Verifiable Rewards (RLVR), QFANG significantly bridges the gap between computational synthesis planning and practical laboratory execution, thereby enhancing efficiency in chemical research.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Pharmaceutical Research</span>
                    
                    <span class="domain-tag">Chemical Biology</span>
                    
                    <span class="domain-tag">Biotechnology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13668v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13668v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13668v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13668v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13641v1"
                     data-domains="Radiology,Dermatology,Ophthalmology,Pathology,Endoscopy,Global Health"
                     data-keywords="Convolutional Neural Networks,AI Robustness,Disease Diagnosis,Image Corruptions,Lightweight Models,Edge Devices,Agricultural AI,Computer Vision"
                     data-authors="Gabriel Vitorino de Andrade,Saulo Roberto dos Santos,Itallo Patrick Castro Alves da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13641v1.html">From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 0.85</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Gabriel Vitorino de Andrade, Saulo Roberto dos Santos, Itallo Patrick Castro Alves da Silva et al.
                </div>

                <div class="paper-summary">
                    This paper evaluates the robustness of Convolutional Neural Networks (CNNs) for mango leaf disease diagnosis under various real-world image corruptions like noise and blur. It finds that a lightweight, specialized architecture (LCNN) significantly outperforms complex models in corrupted scenarios, highlighting the critical need for robustness assessment in AI development for agriculture, especially for edge device deployment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Endoscopy</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13641v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13641v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13641v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13641v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13632v1"
                     data-domains="cs.LG"
                     data-keywords="cs.LG"
                     data-authors="Guransh Singh,Md Shah Fahad">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13632v1.html">StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Guransh Singh, Md Shah Fahad
                </div>

                <div class="paper-summary">
                    Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) ha...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.LG</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13632v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13632v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13632v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13632v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13608v1"
                     data-domains="Radiology,Oncology,Preventive Medicine,Medical Imaging,Public Health"
                     data-keywords="DBT-DINO,Foundation Models,Digital Breast Tomosynthesis,Breast Cancer Screening,Self-supervised Learning,Breast Density Classification,Cancer Risk Prediction,Lesion Detection"
                     data-authors="Felix J. Dorfner,Manon A. Dorster,Ryan Connolly,Oscar Gentilhomme,Edward Gibbs,Steven Graham,Seth Wander,Thomas Schultz,Manisha Bahl,Dania Daye,Albert E. Kim,Christopher P. Bridge">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13608v1.html">DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Felix J. Dorfner, Manon A. Dorster, Ryan Connolly et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DBT-DINO, the first foundation model for Digital Breast Tomosynthesis (DBT), developed using self-supervised pre-training on an unprecedented dataset of over 25 million DBT slices. The model demonstrated strong performance in breast density classification and 5-year breast cancer risk prediction, showcasing the value of domain-specific pre-training for these tasks. However, its performance on general lesion detection showed variable benefits compared to an ImageNet-pretrained baseline, suggesting areas for further methodological development.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13608v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13608v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13608v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13608v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13600v1"
                     data-domains="Pathology,Urology,Oncology,Medical Imaging"
                     data-keywords="Histopathology,Domain Adaptation,Self-Supervised Learning,Pathology Foundational Models,Multiple Instance Learning,Bladder Cancer,TURBT,Neoadjuvant Chemotherapy"
                     data-authors="Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13600v1.html">DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Haoyue Zhang, Meera Chappidi, Erolcan Sayar et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DA-SSL, a domain-adaptive self-supervised adaptor designed to overcome limitations of Pathology Foundational Models (PFMs) when applied to challenging histopathology slides like transurethral resection of bladder tumor (TURBT) specimens. DA-SSL efficiently realigns PFM features to the target domain without fine-tuning the foundational model. Piloting this framework for predicting neoadjuvant chemotherapy (NAC) treatment response in bladder cancer, the authors demonstrate its effectiveness with strong performance metrics, enhancing the applicability of AI in clinically complex scenarios.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Urology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13600v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13600v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13600v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13600v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13534v1"
                     data-domains="Neurology,Radiology,Pathology,Anatomy,Oncology (implied by pathology)"
                     data-keywords="Biomedical image segmentation,Multi-protocol segmentation,Deep learning,Foundation models,Semantic consistency,Medical imaging,Artificial intelligence,Brain MRI"
                     data-authors="Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13534v1.html">Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Marianne Rakic, Siyu Gai, Etienne Chollet et al.
                </div>

                <div class="paper-summary">
                    Pancakes introduces a novel framework for multi-protocol image segmentation in biomedical domains, enabling automatic generation of diverse and semantically consistent segmentations from a single image. It addresses the limitation of existing models that either support only one protocol or require manual prompting, and is capable of processing images from previously unseen domains. The framework significantly outperforms existing foundation models in generating multiple plausible and coherent segmentation maps.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Anatomy</span>
                    
                    <span class="domain-tag">Oncology (implied by pathology)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13534v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13534v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13534v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13534v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13527v1"
                     data-domains="Cardiology,Cardiac Electrophysiology,Cardiovascular Radiology,Medical Imaging"
                     data-keywords="3D LGE MRI,Left Atrium,Scar Imaging,Atrial Fibrillation,Dark-blood,Spectral Reconstruction,CNR,Scar Quantification,Post-ablation"
                     data-authors="Mohammed S. M. Elbaz">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13527v1.html">DarkSPARC: Dark-Blood Spectral Self-Calibrated Reconstruction of 3D Left Atrial LGE MRI for Post-Ablation Scar Imaging</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mohammed S. M. Elbaz
                </div>

                <div class="paper-summary">
                    DarkSPARC is a novel, self-calibrated spectral reconstruction method designed to transform standard bright-blood 3D Left Atrial (LA) Late Gadolinium Enhancement (LGE) MRI into dark-blood images. This technique significantly enhances image quality metrics like contrast-to-noise ratio (CNR), signal-to-noise ratio (SNR), and effective CNR (eCNR), leading to more accurate post-ablation scar quantification in both numerical phantoms and in-vivo patient data. It provides reliable scar assessment without the need for additional scans or training data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Cardiac Electrophysiology</span>
                    
                    <span class="domain-tag">Cardiovascular Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13527v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13527v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13527v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13527v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13510v1"
                     data-domains="General Medicine,Clinical Decision Support Systems,Diagnostic Support,Medical Education"
                     data-keywords="Medical AI,Reasoning,Reinforcement Learning,Language Models,Clinical Decision Support,Critical Evidence Graph,Verifiable Reasoning,Clinical Reliability"
                     data-authors="Linjie Mu,Yannian Gu,Zhongzhen Huang,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13510v1.html">MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Linjie Mu, Yannian Gu, Zhongzhen Huang et al.
                </div>

                <div class="paper-summary">
                    MedCEG is a novel framework that enhances medical language models by explicitly supervising their reasoning processes with a Critical Evidence Graph (CEG) to ensure clinical validity. It introduces a Clinical Reasoning Procedure Reward (CRPR) that holistically assesses reasoning quality based on node coverage, structural correctness, and chain completeness. MedCEG demonstrates superior performance over existing methods, generating clinically valid and transparent reasoning chains, thus advancing reliable medical AI reasoning.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">General Medicine</span>
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Diagnostic Support</span>
                    
                    <span class="domain-tag">Medical Education</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13510v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13510v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13510v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13510v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.13440v1"
                     data-domains="Gastroenterology,Pathology,Digital Pathology,Histopathology"
                     data-keywords="Inflammatory Bowel Disease (IBD),Histologic Remission,Multiple Instance Learning (MIL),Whole Slide Imaging (WSI),H&E Staining,Inflammation Prediction,Digital Pathology,Interpretability,Immune Cells"
                     data-authors="Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.13440v1.html">IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-15</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Thalyssa Baiocco-Rodrigues, Antoine Olivier, Reda Belbahri et al.
                </div>

                <div class="paper-summary">
                    IMILIA is an end-to-end framework leveraging Multiple Instance Learning (MIL) to predict inflammation in Inflammatory Bowel Disease (IBD) from H&E whole slide images. It combines a highly accurate predictive model with an interpretability module that identifies specific cell types and tissue regions driving the inflammation scores, providing biologically consistent insights. The framework demonstrates robust performance across multiple cohorts and offers a novel approach to objectively assess microscopic inflammation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Digital Pathology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.13440v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.13440v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.13440v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.13440v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-16 06:15:50</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>