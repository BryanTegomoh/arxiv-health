<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">49</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">49</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">168</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Diagnostic Imaging (11), Radiology (10), Public Health (8)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (11)</option>
                        
                        <option value="Radiology">Radiology (10)</option>
                        
                        <option value="Public Health">Public Health (8)</option>
                        
                        <option value="Personalized Medicine">Personalized Medicine (7)</option>
                        
                        <option value="Neurology">Neurology (5)</option>
                        
                        <option value="Digital Health">Digital Health (4)</option>
                        
                        <option value="Oncology">Oncology (4)</option>
                        
                        <option value="Diagnostic Medicine">Diagnostic Medicine (4)</option>
                        
                        <option value="Psychiatry">Psychiatry (4)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (4)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2511.23355v1"
                     data-domains="Intensive Care Medicine,Emergency Medicine,Digital Health,Healthcare Informatics,Global Health,Clinical Monitoring"
                     data-keywords="Computer Vision,Bedside Monitor,Vital Signs,EHR Integration,OCR,Low-Resource Settings,YOLO,Physiological Data"
                     data-authors="Vinh Chau,Khoa Le Dinh Van,Hon Huynh Ngoc,Binh Nguyen Thien,Hao Nguyen Thien,Vy Nguyen Quang,Phuc Vo Hong,Yen Lam Minh,Kieu Pham Tieu,Trinh Nguyen Thi Diem,Louise Thwaites,Hai Ho Bich">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23355v1.html">A Hierarchical Computer Vision Pipeline for Physiological Data Extraction from Bedside Monitors</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vinh Chau, Khoa Le Dinh Van, Hon Huynh Ngoc et al.
                </div>

                <div class="paper-summary">
                    This paper presents a novel computer vision-based pipeline designed to automatically capture and digitize physiological data from standalone bedside monitor screens, specifically targeting low-resource healthcare settings lacking network connectivity. By employing a hierarchical framework with YOLOv11 for object detection, PaddleOCR for text extraction, and geometric rectification, the system reliably transforms unstructured screen information into structured digital data. It achieved an end-to-end extraction accuracy exceeding 98.9% for core vital signs, offering a practical solution to integrate legacy monitor data into EHR systems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Intensive Care Medicine</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Healthcare Informatics</span>
                    
                    <span class="domain-tag">Global Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23355v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23355v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23355v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23355v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23325v1"
                     data-domains="Addiction Medicine,Psychiatry,Public Health,Behavioral Health,Preventive Medicine"
                     data-keywords="gambling disorder,early risk detection,social media analysis,natural language processing,behavioral addiction,mental health,machine learning,BERT"
                     data-authors="Horacio Thompson,Marcelo Errecalde">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23325v1.html">Tackling a Challenging Corpus for Early Detection of Gambling Disorder: UNSL at MentalRiskES 2025</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Horacio Thompson, Marcelo Errecalde
                </div>

                <div class="paper-summary">
                    This paper details UNSL's participation in MentalRiskES 2025, Task 1, focusing on Early Risk Detection (ERD) of gambling disorder from social media. Utilizing a CPI+DMC approach with SS3, BERT, and SBERT models combined with historical user analysis, two of their proposals achieved the top two positions in the challenge. Despite this success, the study highlighted challenges in clearly distinguishing between high and low-risk users, underscoring the need for improved data interpretation and more transparent ERD systems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Addiction Medicine</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Behavioral Health</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23325v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23325v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23325v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23325v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23276v1"
                     data-domains="Epidemiology,Public Health,Infectious Disease Surveillance,Predictive Analytics in Medicine"
                     data-keywords="Epidemic Forecasting,Hand, Foot and Mouth Disease (HFMD),Neuro-Symbolic AI,Large Language Models (LLMs),Context-Aware Forecasting,Public Health,Infectious Disease Surveillance,Probabilistic Forecasting"
                     data-authors="Joongwon Chae,Runming Wang,Chen Xiong,Gong Yunhan,Lian Zhang,Ji Jiansong,Dongmei Yu,Peiwu Qin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23276v1.html">Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Joongwon Chae, Runming Wang, Chen Xiong et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel two-agent neuro-symbolic framework designed for context-aware Hand, Foot and Mouth Disease (HFMD) forecasting, which integrates semantic reasoning into probabilistic predictions. It utilizes a Large Language Model (LLM) for interpreting diverse contextual signals and a neuro-symbolic core for combining this interpretation with historical data. The framework demonstrates competitive point forecasting accuracy, robust 90% prediction intervals, and human-interpretable rationales on real-world HFMD datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Infectious Disease Surveillance</span>
                    
                    <span class="domain-tag">Predictive Analytics in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23276v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23276v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23276v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23276v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23274v1"
                     data-domains="Radiology,Neurology,Neuroscience,Diagnostic Imaging"
                     data-keywords="MRI,k-space,under-sampling,artefact correction,deep learning,image quality,signal-to-noise ratio,brain imaging"
                     data-authors="Georgia Kanli,Daniele Perlo,Selma Boudissa,Radovan Jirik,Olivier Keunen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23274v1.html">Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Georgia Kanli, Daniele Perlo, Selma Boudissa et al.
                </div>

                <div class="paper-summary">
                    This paper introduces USArt (Under-Sampling and Artifact correction model), a novel deep learning approach designed to simultaneously improve image quality from under-sampled MRI data and correct for common artefacts like noise and motion. Addressing a critical gap where existing methods tackle either acceleration or artefact correction independently, USArt demonstrates significant improvements in signal-to-noise ratio and contrast, achieving up to 5x acceleration while maintaining high image quality in 2D brain anatomical images.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23274v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23274v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23274v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23274v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23269v1"
                     data-domains="General Diagnostics,Medical Imaging (e.g., Radiology, Pathology),Clinical Decision Support,Medical Education,Healthcare AI"
                     data-keywords="Multimodal Medical Reasoning,Large Language Models (LLMs),Supervised Fine-Tuning (SFT),Data Curation,Reasoning Traces,Out-of-Distribution Generalization,Clinical Tasks,Vision-Language Models"
                     data-authors="Timothy Ossowski,Sheng Zhang,Qianchu Liu,Guanghui Qin,Reuben Tan,Tristan Naumann,Junjie Hu,Hoifung Poon">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23269v1.html">OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Timothy Ossowski, Sheng Zhang, Qianchu Liu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces OctoMed, a novel approach utilizing carefully curated data recipes and supervised fine-tuning (SFT) to develop a robust multimodal medical reasoning model. By leveraging structured reasoning traces and scaling experiments to a vast dataset, the model achieves state-of-the-art performance among open-source models on diverse out-of-distribution medical benchmark tasks. A key finding is the model's ability to self-calibrate its reasoning trajectory lengths based on task requirements, without explicit supervision.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">General Diagnostics</span>
                    
                    <span class="domain-tag">Medical Imaging (e.g., Radiology, Pathology)</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Education</span>
                    
                    <span class="domain-tag">Healthcare AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23269v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23269v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23269v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23269v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23238v1"
                     data-domains="Digital Health,Remote Patient Monitoring,Electronic Health Records (EHR) Analysis,Intensive Care Unit (ICU) Data Analysis,Wearable Sensor Data Analysis,Clinical Decision Support Systems"
                     data-keywords="SDE-RNN,Attention Mechanism,Irregular Time Series,Missing Data,Healthcare Data,Latent Space,Time-Varying Features,Deep Learning"
                     data-authors="Yuting Fang,Qouc Le Gia,Flora Salim">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23238v1.html">SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuting Fang, Qouc Le Gia, Flora Salim
                </div>

                <div class="paper-summary">
                    This paper introduces SDE-Attention, a novel family of Stochastic Differential Equation-Recurrent Neural Networks (SDE-RNNs) augmented with channel-level attention mechanisms applied to the latent pre-RNN state. Designed to address the prevalent challenge of irregularly sampled time series with substantial missing data in fields like healthcare, the proposed models consistently outperform a vanilla SDE-RNN baseline. Notably, the LSTM-based time-varying feature model (SDE-TVF-L) demonstrated significant accuracy gains, achieving up to a 10 percentage point improvement on univariate datasets and up to 7% on multivariate benchmarks under high missingness.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Remote Patient Monitoring</span>
                    
                    <span class="domain-tag">Electronic Health Records (EHR) Analysis</span>
                    
                    <span class="domain-tag">Intensive Care Unit (ICU) Data Analysis</span>
                    
                    <span class="domain-tag">Wearable Sensor Data Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23238v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23238v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23238v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23238v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23222v1"
                     data-domains="Medical Imaging,Diagnostic Radiology,Pathology Detection,Computer-Aided Diagnosis,AI in Healthcare"
                     data-keywords="DAONet-YOLOv8,Occlusion-Aware,Dual-Attention,Deep Learning,Object Detection,Medical Imaging Analogy,Computer Vision,Agricultural Diagnostics"
                     data-authors="Yefeng Wu,Shan Wan,Ling Wu,Yecheng Zhao">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23222v1.html">DAONet-YOLOv8: An Occlusion-Aware Dual-Attention Network for Tea Leaf Pest and Disease Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yefeng Wu, Shan Wan, Ling Wu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DAONet-YOLOv8, a novel deep learning model designed to enhance the accurate detection of tea leaf pests and diseases in challenging real-world plantation environments. It addresses issues like complex backgrounds, variable illumination, and frequent occlusions by integrating a dual-attention fusion module, an occlusion-aware detection head, and dynamic synthesis convolutions. The model significantly outperforms the YOLOv8n baseline and other mainstream detectors on a real-world tea plantation dataset, demonstrating improved precision, recall, and mAP while reducing computational parameters.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                    <span class="domain-tag">Pathology Detection</span>
                    
                    <span class="domain-tag">Computer-Aided Diagnosis</span>
                    
                    <span class="domain-tag">AI in Healthcare</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23222v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23222v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23222v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23222v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23204v1"
                     data-domains="Pathology,Histopathology,Oncology,Medical Diagnostics,Digital Medicine"
                     data-keywords="Pathology Foundation Models,Knowledge Distillation,Model Compression,Computational Pathology,Deep Learning,Multi-Teacher Distillation,Digital Pathology,Medical AI"
                     data-authors="Christian Grashei,Christian Brechenmacher,Rao Muhammad Umer,Jingsong Liu,Carsten Marr,Ewa Szczurek,Peter J. Sch√ºffler">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23204v1.html">Pathryoshka: Compressing Pathology Foundation Models via Multi-Teacher Knowledge Distillation with Nested Embeddings</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Christian Grashei, Christian Brechenmacher, Rao Muhammad Umer et al.
                </div>

                <div class="paper-summary">
                    Pathryoshka introduces a novel multi-teacher distillation framework designed to significantly compress large pathology foundation models, which are often computationally expensive and resource-intensive. By reducing model size by 86-92% while maintaining on-par performance with much larger teacher models, Pathryoshka enables efficient local deployment and democratizes access to advanced computational pathology capabilities for broader research and clinical use.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Diagnostics</span>
                    
                    <span class="domain-tag">Digital Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23204v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23204v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23204v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23204v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23162v1"
                     data-domains="Clinical Neuroscience,Neurology,Psychiatry,Cognitive Neuroscience,Brain-Computer Interfaces"
                     data-keywords="Event-related Potentials (ERP),Electroencephalography (EEG),Deep Learning,Autoencoder,Uncertainty Quantification,Few-shot Learning,Clinical Neuroscience,Brain-Computer Interface"
                     data-authors="Anders Vestergaard N√∏rskov,Kasper J√∏rgensen,Alexander Neergaard Zahid,Morten M√∏rup">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23162v1.html">Estimating the Event-Related Potential from Few EEG Trials</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Anders Vestergaard N√∏rskov, Kasper J√∏rgensen, Alexander Neergaard Zahid et al.
                </div>

                <div class="paper-summary">
                    This paper introduces EEG2ERP, a novel uncertainty-aware autoencoder designed to estimate event-related potentials (ERPs) from a limited number of electroencephalography (EEG) trials. The method significantly outperforms conventional averaging techniques, particularly in the few-trial regime, thereby addressing the long-standing challenge of requiring extensive data for reliable ERP estimation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Neuroscience</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Cognitive Neuroscience</span>
                    
                    <span class="domain-tag">Brain-Computer Interfaces</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23162v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23162v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23162v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23162v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23142v1"
                     data-domains="Neurology,Clinical Neurophysiology,Epilepsy Diagnostics,Brain Monitoring,Telemedicine"
                     data-keywords="EEG compression,neural audio codecs,deep learning,electroencephalography,signal processing,DAC,clinical neurophysiology,data efficiency"
                     data-authors="Ard Kastrati,Luca Lanzend√∂rfer,Riccardo Rigoni,John Staib Matilla,Roger Wattenhofer">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23142v1.html">Adapting Neural Audio Codecs to EEG</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ard Kastrati, Luca Lanzend√∂rfer, Riccardo Rigoni et al.
                </div>

                <div class="paper-summary">
                    This paper demonstrates that pretrained neural audio codecs, specifically the state-of-the-art DAC, can be effectively adapted for EEG compression despite inherent modality differences. By preprocessing raw EEG data to match the codec's input constraints and fine-tuning on EEG, stable and high-fidelity reconstructions are achieved. The approach, including a multi-channel extension (DAC-MC), is shown to preserve clinically relevant information on medical EEG datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Clinical Neurophysiology</span>
                    
                    <span class="domain-tag">Epilepsy Diagnostics</span>
                    
                    <span class="domain-tag">Brain Monitoring</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23142v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23142v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23142v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23142v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23124v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical Image Analysis,Image-guided Therapy"
                     data-keywords="Denoising,Unsupervised Learning,Medical Imaging,Hybrid Prior,Spectral-Spatial,Image Reconstruction,Deep Learning,Robustness"
                     data-authors="Yanqi Cheng,Chun-Wun Cheng,Jim Denholm,Thiago Lima,Javier A. Montoya-Zegarra,Richard Goodwin,Carola-Bibiane Sch√∂nlieb,Angelica I Aviles-Rivero">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23124v1.html">DNA-Prior: Unsupervised Denoise Anything via Dual-Domain Prior</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yanqi Cheng, Chun-Wun Cheng, Jim Denholm et al.
                </div>

                <div class="paper-summary">
                    DNA-Prior introduces a universal unsupervised denoising framework that reconstructs clean medical images from corrupted observations using a novel, mathematically principled hybrid prior. This framework integrates an implicit architectural prior with an explicit spectral-spatial prior, allowing for robust noise suppression and structural preservation across diverse modalities and noise conditions without requiring external training data or modality-specific tuning.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                    <span class="domain-tag">Image-guided Therapy</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23124v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23124v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23124v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23124v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23118v1"
                     data-domains="Forensic Psychiatry,Mental Health,Public Health,Psychology,Risk Assessment"
                     data-keywords="Machine learning,violence prediction,systematic review,AUC,risk of bias,clinical utility,explainable AI,causal machine learning,forensic psychiatry,risk assessment"
                     data-authors="Stefaniya Kozhevnikova,Denis Yukhnenko,Giulio Scola,Seena Fazel">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23118v1.html">Machine learning for violence prediction: a systematic review and critical appraisal</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ stat.ME</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Stefaniya Kozhevnikova, Denis Yukhnenko, Giulio Scola et al.
                </div>

                <div class="paper-summary">
                    This systematic review critically appraises machine learning (ML) models developed for violence prediction, synthesizing data from 38 studies on 40 models. It reveals that while discrimination (AUC 0.68-0.99) is often reported, crucial aspects like calibration and external validation are largely neglected, and most studies suffer from high risk of bias. The paper concludes that current black-box ML models have limited clinical utility due to methodological flaws but offer promise for identifying high-risk individuals, outlining five key recommendations for future, more robust development.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Forensic Psychiatry</span>
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Psychology</span>
                    
                    <span class="domain-tag">Risk Assessment</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23118v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23118v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23118v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23118v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23114v1"
                     data-domains="Systems Biology,Pharmacodynamics,Immunology,Oncology,Developmental Biology,Personalized Medicine,Diagnostics,Synthetic Biology"
                     data-keywords="Stochastic Reaction Networks,Koopman Operator,Spectral Analysis,Biochemical Dynamics,Single-Cell Biology,Parameter Sensitivity,Noise Structure,Flow Cytometry"
                     data-authors="Ankit Gupta,Mustafa Khammash">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23114v1.html">A Spectral Koopman Approximation Framework for Stochastic Reaction Networks</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.MN</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ankit Gupta, Mustafa Khammash
                </div>

                <div class="paper-summary">
                    This paper introduces a novel spectral Koopman approximation framework for analyzing high-dimensional stochastic reaction networks (SRNs), providing a tractable, low-dimensional representation of their continuous-time dynamics with computable error estimates. The framework efficiently predicts system behavior, including moments and event probabilities, across diverse initial states, and derives continuous-time parameter sensitivities and cross-spectral densities to probe noise and frequency characteristics. Demonstrated on biologically relevant systems, it establishes a powerful analytical tool for stochastic dynamics in biological, ecological, and computational sciences.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Systems Biology</span>
                    
                    <span class="domain-tag">Pharmacodynamics</span>
                    
                    <span class="domain-tag">Immunology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Developmental Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23114v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23114v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23114v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23114v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23082v1"
                     data-domains="Dermatology,Pediatrics,Diagnostic Medicine,Chronic Disease Management"
                     data-keywords="Atopic Dermatitis,Ensemble Learning,Skin Lesion Detection,Deep Learning,Digital Healthcare,Objective Diagnosis,Computer Vision,Pediatric Dermatology"
                     data-authors="Soobin Jeon,Sujong Kim,Dongmahn Seo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23082v1.html">Implementation of a Skin Lesion Detection System for Managing Children with Atopic Dermatitis Based on Ensemble Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Soobin Jeon, Sujong Kim, Dongmahn Seo
                </div>

                <div class="paper-summary">
                    This paper introduces ENSEL, an ensemble learning-based skin lesion detection system designed to provide objective diagnostic support for managing children with atopic dermatitis. By integrating various deep learning models, ENSEL enhances diagnostic accuracy and achieves high recall with a sub-second processing speed, validated using real-world user-taken images.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Pediatrics</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Chronic Disease Management</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23082v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23082v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23082v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23082v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23066v1"
                     data-domains="Radiology,Pediatrics,Diagnostic Imaging,Artificial Intelligence in Medicine"
                     data-keywords="Generative AI,Inpainting,Bone Age Estimation,Pediatric Radiography,Deep Learning,Medical Imaging,Artifact Removal,Clinical AI"
                     data-authors="Felipe Akio Matsuoka,Eduardo Moreno J. M. Farina,Augusto Sarquis Serpa,Soraya Monteiro,Rodrigo Ragazzini,Nitamar Abdala,Marcelo Straus Takahashi,Felipe Campos Kitamura">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23066v1.html">Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Felipe Akio Matsuoka, Eduardo Moreno J. M. Farina, Augusto Sarquis Serpa et al.
                </div>

                <div class="paper-summary">
                    This paper investigated the clinical reliability of generative inpainting for removing non-anatomical artifacts from pediatric hand radiographs, evaluating its impact on bone age estimation and gender classification using deep learning models. Contrary to expectations, inpainting significantly degraded model performance, increasing bone age MAE from 6.26 to 30.11 months and decreasing gender AUC from 0.955 to 0.704. These findings highlight that visually realistic edits can obscure subtle but clinically relevant features and introduce latent bias, underscoring the need for rigorous, task-specific validation before integrating such generative tools into clinical AI workflows.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pediatrics</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23066v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23066v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23066v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23066v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23059v1"
                     data-domains="Traditional Chinese Medicine,Medical Linguistics,Cross-cultural Healthcare,Medical Education"
                     data-keywords="Traditional Chinese Medicine,Translation,Imagistic Thinking,Prompt Engineering,Large Language Models,Huangdi Neijing,Metaphor,Metonymy"
                     data-authors="Jiatong Han">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23059v1.html">Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.85</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jiatong Han
                </div>

                <div class="paper-summary">
                    This study addresses the challenge of accurately conveying imagistic thinking, crucial to Traditional Chinese Medicine (TCM) theory, in English translations, as existing literal renderings hinder clinical application. It introduces a human-in-the-loop (HITL) framework employing prompt engineering with DeepSeek V3.1 to translate fundamental passages from the Huangdi Neijing, focusing on metaphor and metonymy. An LLM-based evaluation, utilizing ChatGPT 5 Pro and Gemini 2.5 Pro simulating real-world readers, found that prompt-adjusted LLM translations consistently outperformed human and baseline LLM translations across five cognitive dimensions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Traditional Chinese Medicine</span>
                    
                    <span class="domain-tag">Medical Linguistics</span>
                    
                    <span class="domain-tag">Cross-cultural Healthcare</span>
                    
                    <span class="domain-tag">Medical Education</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23059v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23059v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23059v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23059v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.23036v1"
                     data-domains="Intensive Care Units (ICU),Continuous Patient Monitoring,Diagnostic Support Systems,Wearable Health Devices,Disease Progression Tracking,Early Warning Systems for Sepsis/Cardiac Arrest"
                     data-keywords="XAI,Time Series,Online Monitoring,Integrated Gradients,Explainability,Healthcare AI,Temporal Dependencies,Prediction Changes"
                     data-authors="Changhun Kim,Yechan Mun,Hyeongwon Jang,Eunseo Lee,Sangchul Hahn,Eunho Yang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.23036v1.html">Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Changhun Kim, Yechan Mun, Hyeongwon Jang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Delta-XAI, a unified framework that adapts 14 existing XAI methods for explaining prediction changes in online time series monitoring, addressing the oversight of temporal dependencies in prior work. It also proposes a principled evaluation suite and a novel method, Shifted Window Integrated Gradients (SWING). Key findings include that adapted classical gradient-based methods like Integrated Gradients can outperform recent XAI approaches, and SWING consistently demonstrates superior effectiveness by systematically capturing temporal dependencies.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Intensive Care Units (ICU)</span>
                    
                    <span class="domain-tag">Continuous Patient Monitoring</span>
                    
                    <span class="domain-tag">Diagnostic Support Systems</span>
                    
                    <span class="domain-tag">Wearable Health Devices</span>
                    
                    <span class="domain-tag">Disease Progression Tracking</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.23036v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.23036v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.23036v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.23036v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22990v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical AI,Population Health Studies"
                     data-keywords="Deep Learning,Medical Image Analysis,Spurious Correlations,Shortcut Learning,Causal Inference,Mutual Information,Generalization,MRI,X-ray"
                     data-authors="Louisa Fay,Hajer Reguigui,Bin Yang,Sergios Gatidis,Thomas K√ºstner">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22990v1.html">MIMM-X: Disentangling Spurious Correlations for Medical Image Analysis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Louisa Fay, Hajer Reguigui, Bin Yang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces MIMM-X, a novel framework designed to combat shortcut learning in deep learning models for medical image analysis, where models often rely on spurious correlations instead of true causal features. MIMM-X achieves this by disentangling causal features from multiple spurious correlations through mutual information minimization. Evaluated across MRI and X-ray modalities on diverse datasets, the framework effectively mitigates shortcut learning, promoting predictions based on true underlying medical relationships.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                    <span class="domain-tag">Population Health Studies</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22990v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22990v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22990v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22990v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22935v1"
                     data-domains="Cardiology,Cardiovascular Medicine,Diagnostic Medicine,Preventive Medicine"
                     data-keywords="Electrocardiogram (ECG),Ensemble Learning,Foundation Models,Multi-task Learning,Low-Rank Adaptation (LoRA),Mixture of Experts (MoE),Cardiovascular Conditions,AI in Medicine"
                     data-authors="Yuhao Xu,Xiaoda Wang,Jiaying Lu,Sirui Ding,Defu Cao,Huaxiu Yao,Yan Liu,Xiao Hu,Carl Yang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22935v1.html">EnECG: Efficient Ensemble Learning for Electrocardiogram Multi-task Foundation Model</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuhao Xu, Xiaoda Wang, Jiaying Lu et al.
                </div>

                <div class="paper-summary">
                    EnECG proposes an efficient ensemble learning framework for multi-task electrocardiogram (ECG) analysis, addressing the challenge of leveraging interrelated cardiac abnormalities and the high computational cost of adapting large foundation models. By integrating multiple specialized foundation models with a lightweight Low-Rank Adaptation (LoRA) strategy and a Mixture of Experts (MoE) mechanism, EnECG significantly reduces computational and memory costs while maintaining strong predictive performance. This approach enhances feature extraction and predictive capabilities across various ECG tasks, making it practical for real-world clinical applications.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Cardiovascular Medicine</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22935v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22935v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22935v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22935v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22911v1"
                     data-domains="Dentistry,Oral and Maxillofacial Surgery,Orthodontics,Radiology"
                     data-keywords="Semi-Supervised Learning,Tooth Segmentation,Panoramic X-ray (OPG),CBCT,Instance Segmentation,Deep Learning,Medical Image Analysis,MICCAI Challenge"
                     data-authors="Yaqi Wang,Zhi Li,Chengyu Wu,Jun Liu,Yifan Zhang,Jiaxue Ni,Qian Luo,Jialuo Chen,Hongyuan Zhang,Jin Liu,Can Han,Kaiwen Fu,Changkai Ji,Xinxu Cai,Jing Hao,Zhihao Zheng,Shi Xu,Junqiang Chen,Qianni Zhang,Dahong Qian,Shuai Wang,Huiyu Zhou">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22911v1.html">MICCAI STS 2024 Challenge: Semi-Supervised Instance-Level Tooth Segmentation in Panoramic X-ray and CBCT Images</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yaqi Wang, Zhi Li, Chengyu Wu et al.
                </div>

                <div class="paper-summary">
                    The MICCAI STS 2024 Challenge successfully benchmarked and advanced semi-supervised learning (SSL) for instance-level tooth segmentation in OPG and CBCT images, addressing the critical issue of limited labeled data. Winning SSL models demonstrated impressive performance gains (over 44 pp IA for OPG, 61 pp Dice for CBCT) over fully-supervised baselines, proving the substantial benefit of SSL for complex medical image segmentation. The most effective approaches leveraged hybrid SSL frameworks, foundational models like SAM, and multi-stage refinement pipelines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dentistry</span>
                    
                    <span class="domain-tag">Oral and Maxillofacial Surgery</span>
                    
                    <span class="domain-tag">Orthodontics</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22911v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22911v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22911v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22911v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22879v1"
                     data-domains="Radiation Oncology,Medical Physics,Diagnostic Imaging,Cancer Therapy"
                     data-keywords="MRI-Linac,Radiofrequency coil,Torso coil,Signal-to-noise ratio,Radiotherapy,Adaptive radiotherapy,Real-time imaging,Radiolucent"
                     data-authors="Mingyan Li,Ewald Weber,David E. J. Waddington,Shanshan Shan,Paul Liu,Bin Dong,Paul J. Keall,Feng Liu,Stuart Crozier">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22879v1.html">Design and Construction of a Dedicated Radiolucent 8-element Flexible Radiofrequency (RF) Torso Coil for the 1.0T Australian MRI-Linac System</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mingyan Li, Ewald Weber, David E. J. Waddington et al.
                </div>

                <div class="paper-summary">
                    This paper details the design, construction, and validation of a dedicated, flexible 8-element receive-only radiofrequency (RF) torso coil for the 1.0T Australian MRI-Linac system. The new coil significantly improves signal-to-noise ratio (SNR) compared to the existing whole-body coil, enabling faster, higher-quality imaging essential for real-time tumor tracking and adaptive radiotherapy, while ensuring full radiolucency compatible with radiation beam delivery.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Cancer Therapy</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22879v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22879v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22879v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22879v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22873v1"
                     data-domains="Public Health,Injury Prevention,Community Health,Urban Planning,Environmental Health"
                     data-keywords="Pedestrian safety,Age classification,Gender classification,Deep learning,CNN,Traffic safety,Public health,Surveillance"
                     data-authors="Shisir Shahriar Arif,Md. Muhtashim Shahrier,Nazmul Haque,Md Asif Raihan,Md. Hadiuzzaman">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22873v1.html">CNN-Based Framework for Pedestrian Age and Gender Classification Using Far-View Surveillance in Mixed-Traffic Intersections</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shisir Shahriar Arif, Md. Muhtashim Shahrier, Nazmul Haque et al.
                </div>

                <div class="paper-summary">
                    This study proposes a deep learning framework using CNNs to classify pedestrian age (adult, teenager, child) and gender (male/female) from far-view surveillance footage in mixed-traffic urban intersections. The framework achieved high accuracy (up to 86.19%) with ResNet50 and comparable performance (84.15%) with a computationally efficient custom CNN, enabling real-time demographic monitoring. This provides crucial data for targeted safety interventions and data-driven urban planning.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Injury Prevention</span>
                    
                    <span class="domain-tag">Community Health</span>
                    
                    <span class="domain-tag">Urban Planning</span>
                    
                    <span class="domain-tag">Environmental Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22873v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22873v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22873v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22873v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22853v1"
                     data-domains="Patient Monitoring (e.g., vital signs, continuous glucose levels),Disease Progression Modeling (e.g., chronic conditions, neurological disorders),Epidemiology (e.g., infectious disease spread forecasting),Clinical Trials (e.g., predicting drug response and adverse events),Healthcare Resource Management (e.g., hospital bed occupancy, staffing needs),Biomarker Prediction and Analysis,Personalized Medicine"
                     data-keywords="Generative Time Series Forecasting,Variational Autoencoder (VAE),TARFLOW,Probabilistic Forecasting,One-Step Generation,Long-Term Forecasting,Efficient Prediction,Uncertainty Quantification"
                     data-authors="Jiawen Wei,Lan Jiang,Pengbo Wei,Ziwen Ye,Teng Song,Chen Chen,Guangrui Ma">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22853v1.html">TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jiawen Wei, Lan Jiang, Pengbo Wei et al.
                </div>

                <div class="paper-summary">
                    This paper introduces TARFVAE, a novel generative framework for efficient one-step time series forecasting, combining a Transformer-based autoregressive flow (TARFLOW) with a Variational Autoencoder (VAE). TARFVAE overcomes the limitations of existing generative models, which often suffer from laborious recurrent or denoising steps, especially in long-term forecasting, by enabling fast, full-horizon predictions directly from a learned latent space. It achieves superior performance over state-of-the-art deterministic and generative models on benchmark datasets while maintaining efficient prediction speed.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Patient Monitoring (e.g., vital signs, continuous glucose levels)</span>
                    
                    <span class="domain-tag">Disease Progression Modeling (e.g., chronic conditions, neurological disorders)</span>
                    
                    <span class="domain-tag">Epidemiology (e.g., infectious disease spread forecasting)</span>
                    
                    <span class="domain-tag">Clinical Trials (e.g., predicting drug response and adverse events)</span>
                    
                    <span class="domain-tag">Healthcare Resource Management (e.g., hospital bed occupancy, staffing needs)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22853v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22853v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22853v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22853v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22841v1"
                     data-domains="Infectious Disease Epidemiology,Public Health,Virology,Genomic Surveillance,Computational Biology,Evolutionary Biology"
                     data-keywords="SARS-CoV-2,viral variants,genetic surveillance,source-sink dynamics,mutation dynamics,receptor-binding domain,phylogenetic-free,epidemic waves"
                     data-authors="Hong Zheng,Shimin Su,Caiqi Liu,Jingzhi Lou,Lirong Cao,Yexian Zhang,Zhihui Zhang,Marc Ka Chun Chong,Benny Chung-Ying Zee,Peter Pak-Hang Cheung,Haogao Gu,Juan Pu,Leo Lit Man Poon,Hui-Ling Yen,Maggie Haitian Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22841v1.html">A novel approach to profile global circulation pathway of SARS-CoV-2 variants by site-based mutation dynamics</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.PE</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hong Zheng, Shimin Su, Caiqi Liu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Site-based mutation dynamics - Equal Power Sampling (S-EPS) framework, a novel phylogenetic-free and bias-correcting method for profiling global SARS-CoV-2 variant source-sink dynamics. Applying S-EPS to 6.6 million genomes, the study identified Africa and the Indian subcontinent as predominant sources of key mutations, established a predictive model for global spread from these sources, and highlighted the framework's utility for enhanced genetic surveillance of emerging viral threats.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Infectious Disease Epidemiology</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Virology</span>
                    
                    <span class="domain-tag">Genomic Surveillance</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22841v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22841v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22841v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22841v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22818v1"
                     data-domains="Psychology,Psychiatry,Behavioral Health,Mental Health Counseling,Addiction Treatment,Clinical Supervision"
                     data-keywords="LLMs,Motivational Interviewing (MI),Psychotherapy,Semantic Drift,Dialogue Summarization,MITI Framework,Behavioral Therapy,AI in Mental Health"
                     data-authors="Vivek Kumar,Pushpraj Singh Rajawat,Eirini Ntoutsi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22818v1.html">Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-28</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vivek Kumar, Pushpraj Singh Rajawat, Eirini Ntoutsi
                </div>

                <div class="paper-summary">
                    This paper evaluates the efficacy of large language models (LLMs) in psychotherapy by having them summarize Motivational Interviewing (MI) dialogues. Employing a mixed-methods approach with a two-stage annotation scheme based on the MITI framework and expert ground truth, the study provides insights into LLMs' understanding of psychological constructs and identifies best practices to mitigate "semantic drift" in therapeutic contexts.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Psychology</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Behavioral Health</span>
                    
                    <span class="domain-tag">Mental Health Counseling</span>
                    
                    <span class="domain-tag">Addiction Treatment</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22818v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22818v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22818v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22818v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22809v1"
                     data-domains="Public Health,Health Communication,Digital Health,Patient Education,Health Policy,Medical Informatics"
                     data-keywords="AI summaries,online search,user attitudes,information influence,health communication,public perception,AI ethics,human-computer interaction"
                     data-authors="Yiwei Xu,Saloni Dash,Sungha Kang,Wang Liao,Emma S. Spiro">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22809v1.html">AI summaries in online search influence users' attitudes</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ cs.HC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yiwei Xu, Saloni Dash, Sungha Kang et al.
                </div>

                <div class="paper-summary">
                    This study, a preregistered randomized controlled experiment (N=2,004), investigated how AI-generated summaries in online search results influence users' attitudes, behavioral intentions, and policy support across debated topics. It found that these summaries consistently shift user perceptions to align with the summary's stance, with top placement causing stronger attitudinal shifts and harm-framed summaries being perceived as more useful.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Health Communication</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Patient Education</span>
                    
                    <span class="domain-tag">Health Policy</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22809v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22809v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22809v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22809v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22791v1"
                     data-domains="Telemedicine,Disaster Medicine,Public Health Logistics,Emergency Medical Services (EMS),Remote Healthcare,Medical Supply Chain"
                     data-keywords="UAV swarm networks,Intrusion Detection System (IDS),federated learning,continuous learning,privacy-preserving,cybersecurity,lightweight,decentralized training"
                     data-authors="Kanchon Gharami,Shafika Showkat Moni">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22791v1.html">An Efficient Privacy-preserving Intrusion Detection Scheme for UAV Swarm Networks</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.CR</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Kanchon Gharami, Shafika Showkat Moni
                </div>

                <div class="paper-summary">
                    This paper presents a novel, lightweight, federated, and continuous learning-based Intrusion Detection System (IDS) specifically designed for UAV swarm networks. It addresses critical security vulnerabilities by facilitating decentralized and privacy-preserving training across diverse UAV swarms, mitigating issues like latency, privacy breaches, and model drift common in conventional IDSs. The proposed scheme demonstrates high classification accuracies, achieving up to 99.99% on benchmark datasets, significantly enhancing the secure operation of UAV systems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Disaster Medicine</span>
                    
                    <span class="domain-tag">Public Health Logistics</span>
                    
                    <span class="domain-tag">Emergency Medical Services (EMS)</span>
                    
                    <span class="domain-tag">Remote Healthcare</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22791v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22791v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22791v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22791v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22774v1"
                     data-domains="Neurology,Radiology,Geriatrics,Cognitive Neuroscience,Diagnostic Imaging"
                     data-keywords="Alzheimer's Disease,Mild Cognitive Impairment,Deep Learning,MRI,Longitudinal Data,BiLSTM,Vision Transformers,Early Prediction"
                     data-authors="Mahdieh Behjat Khatooni,Mohsen Soryani">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22774v1.html">Alzheimer's Disease Prediction Using EffNetViTLoRA and BiLSTM with Multimodal Longitudinal MRI Data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mahdieh Behjat Khatooni, Mohsen Soryani
                </div>

                <div class="paper-summary">
                    This paper introduces a novel, end-to-end hybrid deep learning model, EffNetViTLoRA integrated with BiLSTM, designed for early Alzheimer's Disease prediction using multimodal longitudinal MRI data. The model effectively captures both spatial and temporal features, achieving a state-of-the-art average accuracy of 95.05% in distinguishing progressive Mild Cognitive Impairment (pMCI) from stable MCI (sMCI) by predicting cognitive status at 48 months.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Cognitive Neuroscience</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22774v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22774v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22774v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22774v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22739v1"
                     data-domains="Computational Pathology,Histopathology,Digital Pathology"
                     data-keywords="Computational Pathology,Domain Generalization,Knowledge Distillation,Prompt Tuning,Vision-Language Models,Histopathology,Medical Imaging AI,F1-score"
                     data-authors="Amir Mohammad Ezzati,Alireza Malekhosseini,Armin Khosravi,Mohammad Hossein Rohban">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22739v1.html">All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Amir Mohammad Ezzati, Alireza Malekhosseini, Armin Khosravi et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Domain Invariant Prompt Tuning (DIPT) for knowledge distillation in computational pathology (CPath) to address domain shifts caused by variations in clinical data. DIPT learns domain-invariant continuous prompts by averaging domain-specific input tokens, enabling a student model to distill knowledge from a pathology-tuned Vision-Language Model (PLIP) and align visual features with these robust embeddings. The method significantly improves the F1-score for domain generalization in histopathology datasets, paving the way for more robust CPath model deployment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Digital Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22739v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22739v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22739v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22739v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22737v1"
                     data-domains="Digital Health,Public Health,Disability Medicine,Neurodevelopmental Disorders,Nutrition and Dietetics,Rehabilitation Medicine"
                     data-keywords="Agentic AI,Multi-Agent Systems,Disabilities,Neurodivergence,Digital Health,Personalized Nutrition,Adaptive Scheduling,Explainable AI (XAI),Assistive Technology,IoT"
                     data-authors="Salman Jan,Toqeer Ali Syed,Gohar Ali,Ali Akarma,Mohammad Riyaz Belgaum,Ahmad Ali">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22737v1.html">Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Salman Jan, Toqeer Ali Syed, Gohar Ali et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a detailed Agentic AI framework designed to empower individuals with disabilities and neurodivergence to achieve healthier lives and maintain structured daily routines. The multi-agent system utilizes a three-layer architecture to deliver adaptive, personalized support for nutrition, scheduling, and health monitoring, leveraging explainable AI and secure multi-modal data. It aims to transcend traditional assistive technologies by fostering autonomy, health, and digital equity through human-centered design.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Disability Medicine</span>
                    
                    <span class="domain-tag">Neurodevelopmental Disorders</span>
                    
                    <span class="domain-tag">Nutrition and Dietetics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22737v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22737v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22737v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22737v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22716v1"
                     data-domains="Radiation Oncology,Medical Physics,Cancer Therapy,Radiobiology"
                     data-keywords="SFRT,BPNR,RBPNR,Therapeutic Window,Proton Minibeam Radiotherapy,FLASH Radiotherapy,High-LET Radiation,Radiation Oncology"
                     data-authors="Niels Bassler,Giuseppe Schettino,Hugo Palmans,Thomas Friedrich,Kelvin Ng Wei Siang,Emanuele Scifoni,Emanuele Scifoni,Fardous Reaz">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22716v1.html">The Biologically Effective Particle Number Ratio (BPNR): a new framework to quantify the therapeutic window in SFRT and other modalities</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Niels Bassler, Giuseppe Schettino, Hugo Palmans et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Biologically Effective Particle Number Ratio (BPNR), a novel, outcome-based, and model-independent framework to quantify the therapeutic window in spatially fractionated radiation therapy (SFRT) and other advanced modalities. BPNR, defined as the ratio of total particle numbers (monitor units) required for tumor control (TCP) and normal tissue complication (NTCP), overcomes the ambiguities of traditional dose metrics, demonstrating a widened therapeutic window for proton minibeam radiotherapy (pMBRT) in a preclinical illustration.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Cancer Therapy</span>
                    
                    <span class="domain-tag">Radiobiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22716v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22716v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22716v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22716v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22619v1"
                     data-domains="Clinical Decision Support Systems,Medical Diagnostics (e.g., radiology, pathology),Drug Discovery and Development,Personalized Medicine,Public Health (e.g., misinformation by AI),Patient-facing AI (e.g., chatbots)"
                     data-keywords="AI deception,AI safety,Machine learning ethics,Sociotechnical systems,AI governance,Risk assessment,AI auditing,Frontier AI"
                     data-authors="Boyuan Chen,Sitong Fang,Jiaming Ji,Yanxu Zhu,Pengcheng Wen,Jinzhou Wu,Yingshui Tan,Boren Zheng,Mengying Yuan,Wenqi Chen,Donghai Hong,Alex Qiu,Xin Chen,Jiayi Zhou,Kaile Wang,Juntao Dai,Borong Zhang,Tianzhuo Yang,Saad Siddiqui,Isabella Duan,Yawen Duan,Brian Tse,Jen-Tse,Huang,Kun Wang,Baihui Zheng,Jiaheng Liu,Jian Yang,Yiming Li,Wenting Chen,Dongrui Liu,Lukas Vierling,Zhiheng Xi,Haobo Fu,Wenxuan Wang,Jitao Sang,Zhengyan Shi,Chi-Min Chan,Eugenie Shi,Simin Li,Juncheng Li,Wei Ji,Dong Li,Jun Song,Yinpeng Dong,Jie Fu,Bo Zheng,Min Yang,Yike Guo,Philip Torr,Zhongyuan Wang,Yaodong Yang,Tiejun Huang,Ya-Qin Zhang,Hongjiang Zhang,Andrew Yao">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22619v1.html">AI Deception: Risks, Dynamics, and Controls</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Boyuan Chen, Sitong Fang, Jiaming Ji et al.
                </div>

                <div class="paper-summary">
                    This paper provides a comprehensive review and conceptual framework for AI deception, defining it as AI systems inducing false beliefs for self-beneficial outcomes. It outlines the 'deception cycle' encompassing emergence (mechanisms like capabilities, incentives, and triggers) and treatment (detection and mitigation strategies), emphasizing deception as a critical sociotechnical safety challenge.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Medical Diagnostics (e.g., radiology, pathology)</span>
                    
                    <span class="domain-tag">Drug Discovery and Development</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Public Health (e.g., misinformation by AI)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22619v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22619v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22619v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22619v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22615v1"
                     data-domains="Radiology,Diagnostic Imaging,Infectious Diseases (COVID-19),Artificial Intelligence in Medicine"
                     data-keywords="continual learning,catastrophic forgetting,medical imaging,latent drift,replay methods,COVID-19 CT,deep learning,representation stability"
                     data-authors="Paraskevi-Antonia Theofilou,Anuhya Thota,Stefanos Kollias,Mamatha Thota">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22615v1.html">Stable-Drift: A Patient-Aware Latent Drift Replay Method for Stabilizing Representations in Continual Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Paraskevi-Antonia Theofilou, Anuhya Thota, Stefanos Kollias et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Stable-Drift, a novel latent drift-guided replay method designed to mitigate catastrophic forgetting in deep learning models continually adapting to new medical imaging data. By identifying and replaying samples exhibiting high representational instability‚Äîquantified as latent drift‚Äîthe method significantly reduces forgetting in a cross-hospital COVID-19 CT classification task compared to standard approaches.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Infectious Diseases (COVID-19)</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22615v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22615v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22615v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22615v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22566v1"
                     data-domains="Nuclear Medicine,Medical Imaging,Diagnostic Radiology,Medical Physics"
                     data-keywords="PET,Image Reconstruction,Computational Runtime,Medical Imaging,Optimization,Challenge,Open-source,Positron Emission Tomography"
                     data-authors="Casper da Costa-Luis,Matthias J. Ehrhardt,Christoph Kolbitsch,Evgueni Ovtchinnikov,Edoardo Pasca,Kris Thielemans,Charalampos Tsoumpas">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22566v1.html">PET Rapid Image Reconstruction Challenge (PETRIC)</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Casper da Costa-Luis, Matthias J. Ehrhardt, Christoph Kolbitsch et al.
                </div>

                <div class="paper-summary">
                    PETRIC introduces the first image reconstruction challenge specifically for Positron Emission Tomography (PET), aiming to minimize computational runtime of related algorithms. It established an open-source framework, curated datasets, and performance metrics, attracting four teams and nine algorithms utilizing diverse optimization and AI techniques. The challenge's solid foundation is reusable for evaluating future PET reconstruction methods and will be iterated upon.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Nuclear Medicine</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22566v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22566v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22566v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22566v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22483v1"
                     data-domains="Clinical decision support systems,Diagnostic imaging interpretation,Personalized medicine and treatment planning,Medical research and drug discovery,Mental health support,Public health informatics"
                     data-keywords="Large language models (LLMs),Quantization,Trustworthiness,Adversarial robustness,Fairness,Machine ethics,Out-of-distribution robustness,Mixed precision,Ensemble voting"
                     data-authors="Guanxi Lu,Hao Mark Chen,Zhiqiang Que,Wayne Luk,Hongxiang Fan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22483v1.html">Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Guanxi Lu, Hao Mark Chen, Zhiqiang Que et al.
                </div>

                <div class="paper-summary">
                    This paper investigates the critical impact of quantization, a technique for efficient LLM deployment, on four key trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness). It identifies significant instability in these metrics across various compression ratios and quantization methods. To address this, the authors propose a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model, consistently improving trustworthiness by up to 5.8%.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical decision support systems</span>
                    
                    <span class="domain-tag">Diagnostic imaging interpretation</span>
                    
                    <span class="domain-tag">Personalized medicine and treatment planning</span>
                    
                    <span class="domain-tag">Medical research and drug discovery</span>
                    
                    <span class="domain-tag">Mental health support</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22483v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22483v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22483v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22483v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22402v1"
                     data-domains="Diagnostic Medicine,Clinical Informatics,Medical Decision Support,Healthcare AI,Medical Natural Language Processing"
                     data-keywords="Large Language Models (LLMs),linguistic uncertainty,epistemic modality,clinical doubt,medical text,interpretability,layerwise probing,diagnostic interpretation"
                     data-authors="Srivarshinee Sridhar,Raghav Kaushik Ravi,Kripabandhu Ghosh">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22402v1.html">Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Srivarshinee Sridhar, Raghav Kaushik Ravi, Kripabandhu Ghosh
                </div>

                <div class="paper-summary">
                    This paper investigates how Large Language Models (LLMs) internally represent linguistic uncertainty in medical text, a crucial aspect for diagnostic interpretation and decision-making. By introducing a layerwise probing metric, Model Sensitivity to Uncertainty (MSU), and a contrastive dataset, the authors found that LLMs exhibit structured, depth-dependent sensitivity, progressively encoding epistemic information in deeper layers. These findings reveal critical insights into the interpretability and epistemic reliability of LLMs in clinical contexts.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Medical Decision Support</span>
                    
                    <span class="domain-tag">Healthcare AI</span>
                    
                    <span class="domain-tag">Medical Natural Language Processing</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22402v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22402v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22402v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22402v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22316v1"
                     data-domains="Clinical Decision Support,Medical Imaging Analysis,Drug Discovery,Personalized Medicine,Telemedicine,Health Informatics"
                     data-keywords="LLM quantization,single-pass,gradient noise,Straight-Through Estimator,Givens rotations,Alignment Rotation Transformation,Uniformity Rotation Transformation,resource-limited deployment"
                     data-authors="Jinying Xiao,Bin Ji,Shasha Li,Xiaodong Liu,Ma Jun,Ye Zhong,Wei Li,Xuan Xie,Qingbo Wu,Jie Yu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22316v1.html">SingleQuant: Efficient Quantization of Large Language Models in a Single Pass</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.70</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jinying Xiao, Bin Ji, Shasha Li et al.
                </div>

                <div class="paper-summary">
                    This paper introduces SingleQuant, a novel single-pass quantization framework for Large Language Models (LLMs) that addresses convergence issues and performance degradation in existing methods. By decoupling quantization truncation from gradient optimization and employing specialized rotation transformations (ART and URT), SingleQuant significantly improves quantization speed and task performance of LLMs. It achieves this by eliminating non-smoothness and gradient noise previously introduced by Straight-Through Estimators on Stiefel manifolds.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22316v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22316v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22316v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22316v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22311v1"
                     data-domains="Biotechnology,Pharmaceuticals,Drug Discovery,Vaccine Development,Biomaterials Science,Enzyme Engineering,Personalized Medicine,Diagnostics"
                     data-keywords="Protein design,Large Language Models (LLMs),Swarm intelligence,De novo design,Agent-based modeling,Biomolecular engineering,Machine learning,Therapeutic proteins"
                     data-authors="Fiona Y. Wang,Di Sheng Lee,David L. Kaplan,Markus J. Buehler">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22311v1.html">Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Fiona Y. Wang, Di Sheng Lee, David L. Kaplan et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel decentralized framework utilizing multiple Large Language Model (LLM) agents, inspired by swarm intelligence, for de novo protein sequence design. Each LLM agent, assigned to a specific residue position, iteratively proposes context-aware mutations to achieve objective-directed designs efficiently. The method generates diverse, well-defined protein sequences without fine-tuning or specialized training, validated experimentally on alpha helix and coil proteins.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Biotechnology</span>
                    
                    <span class="domain-tag">Pharmaceuticals</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Vaccine Development</span>
                    
                    <span class="domain-tag">Biomaterials Science</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22311v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22311v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22311v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22311v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22294v1"
                     data-domains="Radiology,Diagnostic Imaging,Cardiothoracic Imaging,Medical Machine Learning"
                     data-keywords="self-supervised learning,multiview learning,masked autoencoders,radiology,medical imaging,chest X-ray,foundation models,disease classification"
                     data-authors="Sonia Laguna,Andrea Agostini,Alain Ryser,Samuel Ruiperez-Campillo,Irene Cannistraci,Moritz Vandenhirtz,Stephan Mandt,Nicolas Deperrois,Farhad Nooralahzadeh,Michael Krauthammer,Thomas M. Sutter,Julia E. Vogt">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22294v1.html">Structure is Supervision: Multiview Masked Autoencoders for Radiology</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sonia Laguna, Andrea Agostini, Alain Ryser et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn robust, view-invariant representations. MVMAE-V2T extends this by incorporating radiology reports as an auxiliary text-based signal for semantic grounding, demonstrating superior performance in disease classification on large public datasets, particularly in low-label regimes.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Cardiothoracic Imaging</span>
                    
                    <span class="domain-tag">Medical Machine Learning</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22294v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22294v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22294v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22294v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22292v1"
                     data-domains="Oncology,Cancer Research,Personalized Medicine,Computational Biology,Medical Informatics"
                     data-keywords="Tumor Growth Forecasting,Neural ODEs,Universal Differential Equations,Scientific Machine Learning,Personalized Medicine,Cancer Treatment,Gompertz Model,Symbolic Recovery"
                     data-authors="Kavya Subramanian,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22292v1.html">Adaptive tumor growth forecasting via neural & universal ODEs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Kavya Subramanian, Prathamesh Dinesh Joshi, Raj Abhijit Dandekar et al.
                </div>

                <div class="paper-summary">
                    This study introduces an adaptive approach to tumor growth forecasting by leveraging Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs). It aims to overcome the limitations of classical growth models, such as the Gompertz equation, in adapting to patient-specific variability and limited data by integrating adaptive neural networks. The method performs forecasting under data constraints and includes symbolic recovery to convert learned dynamics into explicit mathematical expressions, promising improved predictive accuracy for optimized cancer treatment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Cancer Research</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22292v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22292v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22292v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22292v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22256v1"
                     data-domains="Radiology,Diagnostic Imaging,Sonography,Cardiology,Obstetrics and Gynecology,Emergency Medicine,Internal Medicine"
                     data-keywords="Ultrasound,Vision-Language Model,Foundation Model,Medical Imaging,Segmentation,Diagnosis,Multimodal AI,Deep Learning"
                     data-authors="Dengbo Chen,Ziwei Zhao,Kexin Zhang,Shishuang Zhao,Junjie Hou,Yaqian Wang,Nianxi Liao,Anlan Sun,Fei Gao,Jia Ding,Yuhang Liu,Dong Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22256v1.html">UMind-VL: A Generalist Ultrasound Vision-Language Model for Unified Grounded Perception and Comprehensive Interpretation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Dengbo Chen, Ziwei Zhao, Kexin Zhang et al.
                </div>

                <div class="paper-summary">
                    UMind-VL introduces a unified foundation model designed to bridge the gap between low-level Ultrasound Grounded Perception and high-level Ultrasound Comprehensive Interpretation. Leveraging a novel architecture with a Dynamic Convolutional Mask Decoder and a large-scale multimodal dataset (UMind-DS), it synergizes pixel-level understanding with complex clinical reasoning. The model significantly outperforms existing generalist multimodal models and achieves state-of-the-art or superior performance across diverse tasks including segmentation, detection, measurement, and diagnosis.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Sonography</span>
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Obstetrics and Gynecology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22256v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22256v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22256v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22256v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22250v1"
                     data-domains="Gastroenterology,Endoscopy,Medical Imaging,Colorectal Surgery,Diagnostic Medicine"
                     data-keywords="Colonoscopy,3D Geometry Estimation,Monocular Depth Prediction,Camera Pose Estimation,Foundation Models,Self-supervised Learning,Medical Imaging,Computer Vision"
                     data-authors="Zhiyi Jiang,Yifu Wang,Xuelian Cheng,Zongyuan Ge">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22250v1.html">ColonAdapter: Geometry Estimation Through Foundation Model Adaptation for Colonoscopy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhiyi Jiang, Yifu Wang, Xuelian Cheng et al.
                </div>

                <div class="paper-summary">
                    This paper introduces ColonAdapter, a self-supervised fine-tuning framework designed to adapt general geometric foundation models for accurate 3D geometry estimation from monocular colonoscopy images. It addresses the poor performance of existing models in clinical scenes due to challenges like specularity and homogeneous textures, achieving state-of-the-art results in camera pose estimation, monocular depth prediction, and dense 3D point map reconstruction on both synthetic and real datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Endoscopy</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Colorectal Surgery</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22250v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22250v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22250v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22250v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22239v1"
                     data-domains="Genetics,Molecular Biology,Oncology,Pharmacogenomics,Drug Discovery,Rare Diseases,Infectious Diseases"
                     data-keywords="protein-nucleic acid interaction,mutation,binding free energy,deep learning,graph neural network,protein language model,ESM-2,disease mechanism"
                     data-authors="Somnath Mondal,Tinkal Mondal,Soumajit Pramanik,Rukmankesh Mehra">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22239v1.html">DeepPNI: Language- and graph-based model for mutation-driven protein-nucleic acid energetics</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.BM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Somnath Mondal, Tinkal Mondal, Soumajit Pramanik et al.
                </div>

                <div class="paper-summary">
                    DeepPNI is a novel deep learning model designed to accurately predict mutation-induced binding free energy changes in protein-nucleic acid complexes. It integrates structural features encoded by an edge-aware RGCN and sequential features from the ESM-2 protein language model, achieving robust and generalizable performance with a 0.76 Pearson correlation coefficient across a diverse dataset of 1951 mutations. This tool addresses limitations of experimental techniques in understanding disease-relevant mutational effects.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Genetics</span>
                    
                    <span class="domain-tag">Molecular Biology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Pharmacogenomics</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22239v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22239v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22239v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22239v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22232v1"
                     data-domains="Radiology,Diagnostic Imaging,Clinical Decision Support,Disease Progression Monitoring,AI in Healthcare"
                     data-keywords="Multi-modal LLM,Medical Imaging,Compound Figures,Composite Understanding,Longitudinal Analysis,Biomedical Literature,Chest X-ray,Clinical AI"
                     data-authors="Zhen Chen,Yihang Fu,Gabriel Madera,Mauro Giuffre,Serina Applebaum,Hyunjae Kim,Hua Xu,Qingyu Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22232v1.html">From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhen Chen, Yihang Fu, Gabriel Madera et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the critical limitation of current multi-modal large language models (MLLMs) in medicine, which are largely confined to single-image understanding, by developing a framework for multi-image composite reasoning. It leverages license-permissive compound figures from biomedical literature to create M3LLM, a medical multi-image MLLM that significantly outperforms existing models and generalizes effectively to longitudinal chest X-ray analysis.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Disease Progression Monitoring</span>
                    
                    <span class="domain-tag">AI in Healthcare</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22232v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22232v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22232v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22232v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22199v1"
                     data-domains="Intensive Care Medicine,Critical Care,Clinical Informatics,Medical Artificial Intelligence,Predictive Analytics in Healthcare"
                     data-keywords="Foundation Model,Intensive Care Unit,Self-supervised Learning,Electronic Health Records,Clinical Prediction,Longformer,Multi-task Learning,Domain Generalization"
                     data-authors="Sejeong Jang,Joo Heung Yoon,Hyo Kyung Lee">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22199v1.html">PULSE-ICU: A Pretrained Unified Long-Sequence Encoder for Multi-task Prediction in Intensive Care Units</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sejeong Jang, Joo Heung Yoon, Hyo Kyung Lee
                </div>

                <div class="paper-summary">
                    PULSE-ICU is a self-supervised foundation model designed to overcome challenges of irregular ICU data for clinical prediction. It learns event-level representations from EHR sequences using a unified embedding and a Longformer-based encoder, achieving strong performance across 18 diverse prediction tasks. The model demonstrates significant improvements and robustness during external validation, suggesting a scalable framework for adaptable ICU decision support across diverse clinical environments.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Intensive Care Medicine</span>
                    
                    <span class="domain-tag">Critical Care</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Medical Artificial Intelligence</span>
                    
                    <span class="domain-tag">Predictive Analytics in Healthcare</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22199v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22199v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22199v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22199v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22178v1"
                     data-domains="Neurology,Psychiatry,Developmental Pediatrics,Medical Imaging,Neuroscience"
                     data-keywords="Autism Spectrum Disorder,Graph Convolutional Network,Chebyshev Spectral Graph,Graph Attention Networks,Multimodal Neuroimaging,rs-fMRI,sMRI,Deep Learning"
                     data-authors="Adnan Ferdous Ashrafi,Hasanul Kabir">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22178v1.html">Enhanced Graph Convolutional Network with Chebyshev Spectral Graph and Graph Attention for Autism Spectrum Disorder Classification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Adnan Ferdous Ashrafi, Hasanul Kabir
                </div>

                <div class="paper-summary">
                    This paper introduces an enhanced Graph Convolutional Network (GCN) model, integrating Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT), to improve the classification accuracy of Autism Spectrum Disorder (ASD). Leveraging multimodal neuroimaging and phenotypic data from the ABIDE I dataset, the model achieved a test accuracy of 74.82% and an AUC of 0.82, outperforming existing state-of-the-art baselines for ASD diagnosis.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Developmental Pediatrics</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22178v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22178v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22178v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22178v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22169v1"
                     data-domains="Public Health,Environmental Health,Preventive Medicine,Respiratory Health"
                     data-keywords="particulate matter,air quality forecasting,deep learning,policy optimization,real-time,public health,False Alarm Rate,CMAQ"
                     data-authors="Inha Kang,Eunki Kim,Wonjeong Ryu,Jaeyo Shin,Seungjun Yu,Yoon-Hee Kang,Seongeun Jeong,Eunhye Kim,Soontae Kim,Hyunjung Shim">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22169v1.html">Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Inha Kang, Eunki Kim, Wonjeong Ryu et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the critical need for real-time, long-horizon (48-120 hour) particulate matter (PM) concentration forecasting, especially in complex regions like East Asia, to support public health decisions. The authors introduce a new high-resolution CMAQ-OBS dataset for East Asia, significantly reducing regional error, and propose Group-Relative Policy Optimization (GRPO) to overcome issues of asymmetric operational costs and high False Alarm Rates in standard models. Their GRPO framework substantially improves forecast reliability, demonstrating a 47.3% reduction in False Alarm Rate while maintaining a competitive F1-score.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Environmental Health</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Respiratory Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22169v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22169v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22169v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22169v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22133v1"
                     data-domains="Personalized Medicine,Computational Physiology,Biomechanics,Rehabilitation Engineering,Disease Progression Modeling,Medical Device Monitoring,Drug Pharmacokinetics/Pharmacodynamics"
                     data-keywords="Probabilistic Digital Twin,Misspecified Systems,Latent Force Modeling,Gaussian Processes,Bayesian Neural Networks,Uncertainty Quantification,Dynamical Systems,Prognosis"
                     data-authors="Sahil Kashyap,Rajdip Nayek">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22133v1.html">Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sahil Kashyap, Rajdip Nayek
                </div>

                <div class="paper-summary">
                    This research introduces a probabilistic digital twin framework designed for predicting responses in dynamical systems with misspecified physics. It integrates Gaussian Process Latent Force Models (GPLFM) and Bayesian Neural Networks (BNNs) to perform end-to-end, uncertainty-aware inference and prediction. The framework effectively diagnoses model-form errors as latent forces, learns their probabilistic mapping from system states, and then propagates this uncertainty for robust state prediction, demonstrated by its accuracy across various nonlinear systems and benchmarks.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Computational Physiology</span>
                    
                    <span class="domain-tag">Biomechanics</span>
                    
                    <span class="domain-tag">Rehabilitation Engineering</span>
                    
                    <span class="domain-tag">Disease Progression Modeling</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22133v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22133v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22133v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22133v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.22131v1"
                     data-domains="Pathology,Surgical Oncology,Anatomic Pathology,Diagnostic Imaging"
                     data-keywords="Surgical margins,Digital pathology,Whole-slide imaging (WSI),Foundation model,Deep learning,Cautery artifact,Histopathology,Image analysis"
                     data-authors="Xilin Yang,Musa Aydin,Yuhong Lu,Sahan Yoruc Selcuk,Bijie Bai,Yijie Zhang,Andrew Birkeland,Katjana Ehrlich,Julien Bec,Laura Marcu,Nir Pillar,Aydogan Ozcan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.22131v1.html">Autonomous labeling of surgical resection margins using a foundation model</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-27</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xilin Yang, Musa Aydin, Yuhong Lu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a Virtual Inking Network (VIN), a deep learning model designed for autonomous and standardized localization of surgical resection margins on whole-slide images (WSIs). VIN utilizes a frozen foundation model for feature extraction and a multilayer perceptron for classifying cautery-consistent features, demonstrating ~73.3% region-level accuracy on human tonsil tissue. This method offers a reproducible, ink-free alternative to traditional physical inking, enhancing digital pathology workflows.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Surgical Oncology</span>
                    
                    <span class="domain-tag">Anatomic Pathology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.22131v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.22131v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.22131v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.22131v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-01 06:28:13</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>