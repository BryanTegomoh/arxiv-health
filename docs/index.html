<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">49</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">49</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">140</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Oncology (13), Radiology (11), Pathology (7)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Oncology">Oncology (13)</option>
                        
                        <option value="Radiology">Radiology (11)</option>
                        
                        <option value="Pathology">Pathology (7)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (6)</option>
                        
                        <option value="Computational Pathology">Computational Pathology (5)</option>
                        
                        <option value="Digital Health">Digital Health (5)</option>
                        
                        <option value="Public Health">Public Health (5)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (5)</option>
                        
                        <option value="Drug Discovery">Drug Discovery (4)</option>
                        
                        <option value="Personalized Medicine">Personalized Medicine (4)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2511.20650v1"
                     data-domains="Radiology,Diagnostic imaging,Pathology (microscopy),Computer-aided diagnosis (CAD),Medical AI"
                     data-keywords="Open-vocabulary detection,Medical imaging,Real-time AI,Deep learning,Object detection,Foundation models,Contrastive learning,Pseudo-labeling"
                     data-authors="Tooba Tehreem Sheikh,Jean Lahoud,Rao Muhammad Anwer,Fahad Shahbaz Khan,Salman Khan,Hisham Cholakkal">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20650v1.html">MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tooba Tehreem Sheikh, Jean Lahoud, Rao Muhammad Anwer et al.
                </div>

                <div class="paper-summary">
                    MedROV introduces the first real-time open-vocabulary object detection (OVOD) model specifically for diverse medical imaging, addressing the limitations of traditional closed-set detectors in identifying novel structures. It achieves this by curating a large-scale multi-modal dataset, employing a pseudo-labeling strategy, and integrating knowledge from foundation models via contrastive learning, resulting in significant performance gains and real-time inference speed. MedROV sets a new benchmark for medical detection by outperforming state-of-the-art models in both accuracy and speed for detecting known and novel pathologies.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic imaging</span>
                    
                    <span class="domain-tag">Pathology (microscopy)</span>
                    
                    <span class="domain-tag">Computer-aided diagnosis (CAD)</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20650v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20650v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20650v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20650v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20601v1"
                     data-domains="Endocrinology,Diabetes Management,Digital Health,Predictive Analytics"
                     data-keywords="blood glucose forecasting,deep sequence models,Driver-Blindness,autocorrelation,physiological drivers,insulin,meals,activity,personalization,diabetes management"
                     data-authors="Heman Shakeri">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20601v1.html">The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Heman Shakeri
                </div>

                <div class="paper-summary">
                    This paper identifies and formalizes the "Driver-Blindness" phenomenon in deep sequence models for blood glucose forecasting, where models consistently fail to leverage crucial clinical drivers like insulin, meals, and activity. It attributes this failure to architectural biases, data fidelity gaps, and physiological heterogeneity, proposing strategies to mitigate this issue and recommending routine reporting of driver-leveraging performance metrics.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Endocrinology</span>
                    
                    <span class="domain-tag">Diabetes Management</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Predictive Analytics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20601v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20601v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20601v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20601v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20526v1"
                     data-domains="Pharmacy,Pharmacology,Clinical Pharmacy,Health Education,Medical Certification,Digital Health"
                     data-keywords="Large Language Models,LLMs,Pharmacist Licensure Exam,ChatGPT-4o,DeepSeek-R1,Medical Education,AI Assessment,Digital Health,Pharmacology"
                     data-authors="Xinran Wang,Boran Zhu,Shujuan Zhou,Ziwen Long,Dehua Zhou,Shu Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20526v1.html">Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xinran Wang, Boran Zhu, Shujuan Zhou et al.
                </div>

                <div class="paper-summary">
                    This study evaluated the performance of two large language models, ChatGPT-4o and DeepSeek-R1, on 2,306 multiple-choice questions from the Chinese Pharmacist Licensing Examination (2017-2021). DeepSeek-R1 significantly outperformed ChatGPT-4o (90.0% vs. 76.1%, p < 0.001), particularly in foundational and clinical synthesis modules, demonstrating robust alignment with the exam's demands. The findings highlight the potential of domain-specific LLMs for formative evaluation in medical contexts while reinforcing the necessity of human oversight.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmacy</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Clinical Pharmacy</span>
                    
                    <span class="domain-tag">Health Education</span>
                    
                    <span class="domain-tag">Medical Certification</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20526v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20526v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20526v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20526v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20514v1"
                     data-domains="Interventional Radiology,Diagnostic Imaging,Oncology (for biopsies),Minimally Invasive Surgery"
                     data-keywords="3D needle tracking,Photoacoustic beacon,Interventional ultrasound,Core needle biopsy,Real-time guidance,Time-of-flight,B-mode imaging,Image-guided procedures"
                     data-authors="Christian Baker,Weidong Liang,Richard Colchester,Peng Lei,Francois Joubert,Sebastien Ourselin,Simeon West,Adrien Desjardins,Athanasios Diamantopoulos,Wenfeng Xia">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20514v1.html">Real-time 3D Ultrasonic Needle Tracking with a Photoacoustic Beacon</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Christian Baker, Weidong Liang, Richard Colchester et al.
                </div>

                <div class="paper-summary">
                    This paper presents a novel interventional ultrasound system that provides simultaneous 3D tracking and B-mode guidance for core needle biopsies using a specialized trackable needle. It addresses the common challenge of poor needle visibility and positional ambiguity in conventional ultrasound guidance by enabling real-time, quantitative visualization of the needle tip's 3D location superimposed on B-mode images. The system's performance was assessed in phantoms, and its usability was evaluated by clinicians.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Interventional Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Oncology (for biopsies)</span>
                    
                    <span class="domain-tag">Minimally Invasive Surgery</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20514v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20514v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20514v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20514v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20510v1"
                     data-domains="Oncology,Drug Discovery,Medicinal Chemistry"
                     data-keywords="generative AI,drug discovery,molecule generation,fragmentation,Q-learning,agentic AI,lead optimization,cancer therapeutics"
                     data-authors="Yuto Suzuki,Paul Awolade,Daniel V. LaBarbera,Farnoush Banaei-Kashani">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20510v1.html">FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuto Suzuki, Paul Awolade, Daniel V. LaBarbera et al.
                </div>

                <div class="paper-summary">
                    FRAGMENTA is an end-to-end framework designed for drug lead optimization, addressing the challenges of limited class-specific molecular data and inefficient model tuning. It integrates a novel generative model that uses dynamic Q-learning to jointly optimize molecular fragmentation and generation, with an agentic AI system for expert-driven objective refinement. This approach significantly improves the identification of high-scoring drug candidates, demonstrating superior performance in real-world cancer drug discovery experiments and advancing towards autonomous model tuning.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20510v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20510v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20510v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20510v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20507v1"
                     data-domains="Neurology,Speech-Language Pathology,Cognitive Neuroscience"
                     data-keywords="Aphasia,Large Language Models (LLMs),Neuroscience,Linguistic Disorders,Clinical Assessment,Benchmark,Speech-Language Pathology,Computational Neuroscience"
                     data-authors="Nathan Roll,Jill Kries,Flora Jin,Catherine Wang,Ann Marie Finley,Meghan Sumner,Cory Shain,Laura Gwilliams">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20507v1.html">The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Nathan Roll, Jill Kries, Flora Jin et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Text Aphasia Battery (TAB), a novel text-only benchmark designed to assess aphasia-like deficits in large language models (LLMs), adapted from the Quick Aphasia Battery (QAB). It details the TAB's design, subtests, and scoring criteria, while also validating an automated evaluation protocol using Gemini 2.5 Flash, which demonstrated reliability comparable to expert human raters.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Speech-Language Pathology</span>
                    
                    <span class="domain-tag">Cognitive Neuroscience</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20507v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20507v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20507v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20507v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20503v1"
                     data-domains="Medical Imaging,Drug Discovery,Bioinformatics,Medical Data Augmentation,Anomaly Detection (Medical),Computational Pathology"
                     data-keywords="Generative Modeling,Manifold Learning,Continuum Percolation,Random Geometric Graphs,Topological Data Analysis,Mode Collapse,Hyper-Generalization,Synthetic Data"
                     data-authors="Rui Tong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20503v1.html">Generative Modeling with Manifold Percolation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ stat.ML</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Rui Tong
                </div>

                <div class="paper-summary">
                    This paper redefines generative modeling as disentangling geometric support from probability distributions, proposing Continuum Percolation for high-dimensional support analysis. It establishes an isomorphism between topological phase transitions of Random Geometric Graphs and data manifolds, introducing a 'Percolation Shift' metric that robustly detects structural pathologies like implicit mode collapse where statistical metrics fail. The authors translate this topological insight into a differentiable loss function, demonstrating it prevents manifold shrinkage and achieves 'Hyper-Generalization' with high fidelity and verified topological expansion.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                    <span class="domain-tag">Medical Data Augmentation</span>
                    
                    <span class="domain-tag">Anomaly Detection (Medical)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20503v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20503v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20503v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20503v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20501v1"
                     data-domains="Cerebrovascular Disease Management,Diagnostic Radiology,Interventional Radiology,Medical Imaging Analysis,Neurology"
                     data-keywords="Physics-Informed Loss,Artery Segmentation,DSA,Dislocation Theory,Cerebrovascular Diseases,Deep Learning,Medical Imaging,Boundary Coherence"
                     data-authors="Muhammad Irfan,Nasir Rahim,Khalid Mahmood Malik">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20501v1.html">A Physics-Informed Loss Function for Boundary-Consistent and Robust Artery Segmentation in DSA Sequences</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Muhammad Irfan, Nasir Rahim, Khalid Mahmood Malik
                </div>

                <div class="paper-summary">
                    This paper introduces a novel Physics-Informed Loss (PIL) function for accurate and robust cerebral artery segmentation in Digital Subtraction Angiography (DSA) sequences. PIL models the interaction between predicted and ground-truth boundaries as an elastic process, inspired by dislocation theory, leading to improved geometric and structural consistency. Experimental results demonstrate PIL's superior performance over conventional loss functions across various deep learning architectures and public benchmarks, enhancing the precision and robustness of vascular segmentation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cerebrovascular Disease Management</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                    <span class="domain-tag">Interventional Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20501v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20501v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20501v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20501v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20490v1"
                     data-domains="Oncology,Precision Medicine,Clinical Informatics,Pathology"
                     data-keywords="Multimodal LLM,Oncology,Molecular Tumor Board,Clinical Decision-Making,Agentic AI,Precision Medicine,Longitudinal Data,Biomedical Reasoning"
                     data-authors="Kiril Vasilev,Alexandre Misrahi,Eeshaan Jain,Phil F Cheng,Petros Liakopoulos,Olivier Michielin,Michael Moor,Charlotte Bunne">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20490v1.html">MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Kiril Vasilev, Alexandre Misrahi, Eeshaan Jain et al.
                </div>

                <div class="paper-summary">
                    MTBBench introduces a novel agentic benchmark simulating complex, multimodal, and longitudinal oncology decision-making scenarios found in Molecular Tumor Boards (MTBs). It reveals that current LLMs struggle significantly with clinical reliability, reasoning from time-resolved data, and reconciling conflicting multimodal evidence, but an accompanying agentic framework with tools substantially improves performance in these critical areas.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20490v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20490v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20490v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20490v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20471v1"
                     data-domains="Drug Discovery,Pharmaceutical Research,Personalized Medicine,Biotechnology,Computational Biology"
                     data-keywords="Large Language Models,Creative Reasoning,Drug Discovery,Artificial Intelligence,Computational Creativity,Exploratory Reasoning,Transformative Reasoning,Novelty Assessment"
                     data-authors="Yuto Suzuki,Farnoush Banaei-Kashani">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20471v1.html">Universe of Thoughts: Enabling Creative Reasoning with Large Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuto Suzuki, Farnoush Banaei-Kashani
                </div>

                <div class="paper-summary">
                    This paper addresses the limitations of Large Language Models (LLMs) in generating creative solutions, proposing a computational framework for "creative reasoning" inspired by cognitive science. It introduces three core creative paradigms‚Äîcombinational, exploratory, and transformative reasoning‚Äîand implements them through a novel LLM-based method called "Universe of Thoughts" (UoT). The UoT method is demonstrated to achieve superior performance in creative problem-solving tasks, assessed by feasibility, utility, and novelty, compared to state-of-the-art reasoning techniques and commercial models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Pharmaceutical Research</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Biotechnology</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20471v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20471v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20471v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20471v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20472v1"
                     data-domains="Radiation Oncology,Medical Physics,Cancer Treatment"
                     data-keywords="In vivo dosimetry,Radiotherapy,Scintillator array,Cherenkov imaging,External beam,Surface dosimetry,Gamma analysis,Conformal"
                     data-authors="Roman Vasyltsiv,Allison L. Matous,Natasha Mulenga,Megan A. Clark,Brian W. Pogue,David J. Gladstone,Lesley A. Jarvis,Petr Bruza">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20472v1.html">Wide Area Surface Dosimetry with Conformal Scintillator Array for External Beam Radiotherapy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Roman Vasyltsiv, Allison L. Matous, Natasha Mulenga et al.
                </div>

                <div class="paper-summary">
                    This paper presents a novel wide-area conformable scintillator array imaging system designed for in vivo dosimetry in external beam radiotherapy. The system demonstrates the capability to generate spatially resolved, dynamic surface dose maps across complex anatomical surfaces with high linearity, accuracy, and robust reproducibility, addressing key limitations of existing dosimetry techniques.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Cancer Treatment</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20472v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20472v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20472v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20472v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20392v1"
                     data-domains="Neuroscience,Neurology,Traumatology,Computational Medicine,Biomedical Engineering"
                     data-keywords="Traumatic Brain Injury (TBI),Secondary Injury,Mechano-chemical Modeling,Glia,Neurons,Finite Element Method (FEM),Viscoelasticity,Biomarkers"
                     data-authors="Debabrata Auddya,Shiva Rudraraju">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20392v1.html">Mechano-chemical modeling of glia initiated secondary injury of neurons under mechanical load</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Debabrata Auddya, Shiva Rudraraju
                </div>

                <div class="paper-summary">
                    This paper presents a continuum multiphysics framework to model glia-initiated secondary injury in Traumatic Brain Injury (TBI). Utilizing coupled Partial Differential Equations (PDEs) and Finite Element Method (FEM), the framework spatio-temporally resolves mechano-chemical interactions between neurons, microglia, and the extracellular matrix under mechanical load, aiming to quantify key chemical species and identify biomarkers for targeted TBI treatments.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Traumatology</span>
                    
                    <span class="domain-tag">Computational Medicine</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20392v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20392v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20392v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20392v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20358v1"
                     data-domains="Radiology,Diagnostic Imaging,Pharmacology,Nephrology,Cardiology"
                     data-keywords="Manganese-based contrast agents,MRI,Contrast media,Gadolinium alternatives,Pharmacokinetics,T1 relaxometry,Porcine model,Renal clearance"
                     data-authors="P√•l B. Marthinsen,Tuva R. Hope,Wibeke Nordh√∏y,Deirdre B. Cassidy,Adrian P. L. Smith,Paul M. Evans,Atle Bj√∏rnerud">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20358v1.html">Manganese-based macrocyclic chelates as novel MRI contrast agents: In vivo imaging in a porcine model</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> P√•l B. Marthinsen, Tuva R. Hope, Wibeke Nordh√∏y et al.
                </div>

                <div class="paper-summary">
                    This study investigates novel manganese-based macrocyclic chelates (MBCAs) as potential alternatives to gadolinium-based MRI contrast agents (GBCAs) due to safety concerns. Researchers evaluated the distribution, kinetics, and T1-enhancing properties of three MBCAs in a porcine model, finding several candidates with comparable vascular T1 relaxation efficacy to a reference GBCA and predominantly renal clearance, suggesting their potential for safer contrast-enhanced MRI procedures.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Nephrology</span>
                    
                    <span class="domain-tag">Cardiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20358v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20358v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20358v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20358v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20296v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical Physics,Oncology (for monitoring),Pediatrics (dose-sensitive populations)"
                     data-keywords="Computed Tomography,Sparse-view CT,Deep Learning,Lipschitz Constraint,Deep Unfolding,Image Reconstruction,Radiation Dose Reduction,Prompt Learning"
                     data-authors="Baoshun Shi,Ke Jiang,Qiusheng Lian,Xinran Yu,Huazhu Fu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20296v1.html">Prompting Lipschitz-constrained network for multiple-in-one sparse-view CT reconstruction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Baoshun Shi, Ke Jiang, Qiusheng Lian et al.
                </div>

                <div class="paper-summary">
                    This paper introduces PromptCT, a deep unfolding framework for sparse-view Computed Tomography (SVCT) reconstruction that addresses two key limitations: the lack of explicit Lipschitz constraint proofs in prior networks and the high storage costs associated with training separate models for multiple view settings. PromptCT integrates LipNet, an explicitly provable Lipschitz-constrained network, and a prompt module, allowing a single model to handle various sparse-view configurations with theoretical convergence guarantees. The method achieves superior reconstruction quality and significantly reduces storage overhead compared to benchmark algorithms in both simulated and real data experiments.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Oncology (for monitoring)</span>
                    
                    <span class="domain-tag">Pediatrics (dose-sensitive populations)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20296v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20296v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20296v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20296v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20257v1"
                     data-domains="Public health,Environmental health,Respiratory medicine,Cardiovascular health,Preventive medicine,Epidemiology"
                     data-keywords="Air pollution forecasting,Interpretable AI,Spatiotemporal modeling,Physics-guided machine learning,Advection,Attention mechanisms,Public health,Air quality management"
                     data-authors="Zhiguo Zhang,Xiaoliang Ma,Daniel Schlesinger">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20257v1.html">Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhiguo Zhang, Xiaoliang Ma, Daniel Schlesinger
                </div>

                <div class="paper-summary">
                    This study introduces a novel physics-guided, interpretable-by-design spatiotemporal learning framework for accurate air pollution forecasting, addressing the trade-off between performance and interpretability. By decoupling pollutant behavior into a transport kernel and an explainable attention mechanism, the model achieves high predictive performance, outperforming state-of-the-art baselines, while offering crucial interpretability for real-world air quality management.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public health</span>
                    
                    <span class="domain-tag">Environmental health</span>
                    
                    <span class="domain-tag">Respiratory medicine</span>
                    
                    <span class="domain-tag">Cardiovascular health</span>
                    
                    <span class="domain-tag">Preventive medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20257v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20257v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20257v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20257v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20245v1"
                     data-domains="Endoscopy,Minimally Invasive Diagnostics,Surgical Imaging,Pathology"
                     data-keywords="multimode fiber imaging,deep learning,speckle reconstruction,mutual information,OrganAMNIST,medical imaging,fiber optics,clinical deployment"
                     data-authors="Jawaria Maqbool,M. Imran Cheema">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20245v1.html">HistoSpeckle-Net: Mutual Information-Guided Deep Learning for high-fidelity reconstruction of complex OrganAMNIST images via perturbed Multimode Fibers</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jawaria Maqbool, M. Imran Cheema
                </div>

                <div class="paper-summary">
                    HistoSpeckle-Net is a novel deep learning architecture designed for high-fidelity reconstruction of complex medical images (OrganAMNIST) from multimode fiber (MMF) speckle patterns. It introduces a distribution-aware learning strategy, utilizing histogram-based mutual information loss and multiscale Structural Similarity Index Measure (SSIM) loss, enabling robust and data-efficient performance. This approach significantly outperforms baseline models, even with limited training data and under fiber perturbations, bringing MMF imaging closer to practical clinical deployment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Endoscopy</span>
                    
                    <span class="domain-tag">Minimally Invasive Diagnostics</span>
                    
                    <span class="domain-tag">Surgical Imaging</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20245v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20245v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20245v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20245v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20221v1"
                     data-domains="Neuro-oncology,Pathology,Computational Pathology,Oncology,Diagnostic Imaging"
                     data-keywords="Glioblastoma,Deep Learning,Vision Transformer (ViT),Histopathology,Whole Slide Imaging,Patch Classification,Medical Image Analysis,Brain Tumor"
                     data-authors="Juexin Zhang,Qifeng Zhong,Ying Weng,Ke Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20221v1.html">Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Juexin Zhang, Qifeng Zhong, Ying Weng et al.
                </div>

                <div class="paper-summary">
                    This paper details a deep learning method for patch-level glioblastoma subregion classification, developed for the BraTS-Path 2025 Challenge, utilizing a fine-tuned pre-trained Vision Transformer (ViT) encoder. The model achieved competitive performance, including a 0.6509 MCC and 0.5330 F1-score on the final test set, securing second place and establishing a baseline for ViT-based histopathological analysis of this aggressive brain tumor.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neuro-oncology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20221v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20221v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20221v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20221v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20189v1"
                     data-domains="Precision Medicine,Pharmacogenomics,Clinical Trial Design,Personalized Therapy,Public Health Interventions,Epidemiology"
                     data-keywords="causal inference,treatment effects,subgroup discovery,precision medicine,structural causal model,supervised learning,CART,targeted therapy"
                     data-authors="Lincen Yang,Zhong Li,Matthijs van Leeuwen,Saber Salehkaleybar">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20189v1.html">Learning Subgroups with Maximum Treatment Effects without Causal Heuristics</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Lincen Yang, Zhong Li, Matthijs van Leeuwen et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel approach for identifying patient subgroups that exhibit the maximum average treatment effect, a critical challenge in precision medicine and other domains. By directly formulating the problem within the Structural Causal Model (SCM) framework and assuming a partition-based data-generating model, the authors demonstrate that optimal subgroup discovery can be recast as a standard supervised learning task. Their method, instantiated with CART and avoiding ad-hoc causal heuristics, significantly outperforms existing baselines in accurately identifying target subgroups on synthetic and semi-synthetic datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                    <span class="domain-tag">Pharmacogenomics</span>
                    
                    <span class="domain-tag">Clinical Trial Design</span>
                    
                    <span class="domain-tag">Personalized Therapy</span>
                    
                    <span class="domain-tag">Public Health Interventions</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20189v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20189v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20189v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20189v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20179v1"
                     data-domains="q-bio.NC"
                     data-keywords="q-bio.NC,cs.AI,cs.HC"
                     data-authors="Veith Weilnhammer,Jefferson Ortega,David Whitney">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20179v1.html">Human-computer interactions predict mental health</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ q-bio.NC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Veith Weilnhammer, Jefferson Ortega, David Whitney
                </div>

                <div class="paper-summary">
                    Scalable assessments of mental illness, the leading driver of disability worldwide, remain a critical roadblock toward accessible and equitable care. Here, we show that human-computer interactions encode multiple dimensions of self-reported mental health and their changes over time.
  We introduce M...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">q-bio.NC</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20179v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20179v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20179v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20179v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20169v1"
                     data-domains="Medical"
                     data-keywords="Anomaly Detection,Multi-Domain Benchmark,Scalability,Medical Imaging,Deep Learning,Mixture-of-Experts,Foundation Models,Pixel-level Annotation"
                     data-authors="Hai Ling,Jia Guo,Zhulin Tao,Yunkang Cao,Donglin Di,Hongyan Xu,Xiu Su,Yang Song,Lei Fan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20169v1.html">ADNet: A Large-Scale and Extensible Multi-Domain Benchmark for Anomaly Detection Across 380 Real-World Categories</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hai Ling, Jia Guo, Zhulin Tao et al.
                </div>

                <div class="paper-summary">
                    This paper introduces ADNet, a novel large-scale and multi-domain benchmark comprising 380 real-world categories, including medical imaging, designed to evaluate anomaly detection models. It highlights a critical scalability challenge for existing state-of-the-art methods in multi-class settings and proposes Dinomaly-m, a context-guided Mixture-of-Experts model, which achieves superior performance on this challenging benchmark.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20169v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20169v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20169v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20169v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20154v1"
                     data-domains="Neurology,Radiology,Geriatrics,Neuroimaging,Predictive Analytics"
                     data-keywords="Alzheimer's Disease,Disease Progression Prediction,Longitudinal Data,Irregular Sampling,Riemannian Manifold,Neural Ordinary Differential Equation,Gated Recurrent Unit,Structural MRI,Deep Learning,Neuroimaging"
                     data-authors="Xin Hong,Ying Shi,Yinhao Li,Yen-Wei Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20154v1.html">Alzheimers Disease Progression Prediction Based on Manifold Mapping of Irregularly Sampled Longitudinal Data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xin Hong, Ying Shi, Yinhao Li et al.
                </div>

                <div class="paper-summary">
                    This paper introduces R-TNAG, a novel framework designed to predict Alzheimer's disease (AD) progression from irregularly sampled longitudinal structural MRI (sMRI) data. By projecting sMRI features into a Riemannian manifold and employing a Time-aware Neural ODE alongside an Attention-based Riemannian Gated Recurrent Unit, the model effectively captures the intrinsic geometry and continuous evolution of disease. Experimental results demonstrate that R-TNAG consistently outperforms state-of-the-art methods in both disease status prediction and cognitive score regression, showing robust performance across varying data conditions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Neuroimaging</span>
                    
                    <span class="domain-tag">Predictive Analytics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20154v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20154v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20154v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20154v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20152v1"
                     data-domains="Radiology,Medical Diagnostics,Medical Image Analysis,Computational Imaging,Digital Pathology"
                     data-keywords="Flow Matching,Image Restoration,Denoising,Super-Resolution,Inpainting,Medical Imaging,Generative Models,Training-Free"
                     data-authors="Arnela Hadzic,Franz Thaler,Lea Bogensperger,Simon Johannes Joham,Martin Urschler">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20152v1.html">Restora-Flow: Mask-Guided Image Restoration with Flow Matching</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Arnela Hadzic, Franz Thaler, Lea Bogensperger et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Restora-Flow, a novel training-free method that enhances image restoration by leveraging flow matching with mask-guided sampling and a trajectory correction mechanism. It addresses the issues of lengthy sampling times and over-smoothed outputs prevalent in current diffusion and flow-based generative models. The method achieves superior perceptual quality and significantly faster processing times across mask-based image restoration tasks on both natural and medical datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Diagnostics</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                    <span class="domain-tag">Computational Imaging</span>
                    
                    <span class="domain-tag">Digital Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20152v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20152v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20152v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20152v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20145v1"
                     data-domains="Oncology,Radiology,Nuclear Medicine"
                     data-keywords="PET/CT,Automated Report Generation,Vision-Language Models,Lymphoma,Oncology,Medical Imaging,Deep Learning,3D Imaging"
                     data-authors="Wenpei Jiao,Kun Shang,Hui Li,Ke Yan,Jiajin Zhang,Guangjie Yang,Lijuan Guo,Yan Wan,Xing Yang,Dakai Jin,Zhaoheng Xie">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20145v1.html">Vision-Language Models for Automated 3D PET/CT Report Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Wenpei Jiao, Kun Shang, Hui Li et al.
                </div>

                <div class="paper-summary">
                    This paper introduces PETRG-3D, an end-to-end 3D dual-branch vision-language model designed to automate PET/CT report generation, addressing the challenges of functional imaging and inter-hospital reporting variability. By separately encoding 3D PET and CT volumes and incorporating style-adaptive prompts, the model significantly outperforms existing methods on both natural language and novel clinical efficacy metrics. This work provides a robust foundation for clinically reliable AI in PET/CT reporting, particularly in oncology.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Nuclear Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20145v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20145v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20145v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20145v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20116v1"
                     data-domains="Pulmonology,Radiology,Oncology,Preventive Medicine,Public Health Screening"
                     data-keywords="Lung Cancer Risk Prediction,LDCT Screening,Deep Learning,Transformers,Whole-Lung Imaging,Scalability,Open-Source AI,Radiomics"
                     data-authors="Johannes Brandt,Maulik Chevli,Rickmer Braren,Georgios Kaissis,Philip M√ºller,Daniel Rueckert">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20116v1.html">LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Johannes Brandt, Maulik Chevli, Rickmer Braren et al.
                </div>

                <div class="paper-summary">
                    LungEvaty is an open-source, transformer-based deep learning model designed for scalable 1-6 year lung cancer risk prediction from single low-dose CT (LDCT) scans. It processes entire lung volumes directly from large screening datasets, achieving state-of-the-art performance without pixel-level annotations, and can be further refined with Anatomically Informed Attention Guidance (AIAG).
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Public Health Screening</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20116v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20116v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20116v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20116v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20107v1"
                     data-domains="Speech-Language Pathology,Rehabilitation Medicine,Clinical Linguistics,Audiology (related to speech perception/production assessment)"
                     data-keywords="Mispronunciation Detection,Speech Therapy,Language Learning,Automatic Speech Recognition (ASR),Retrieval-Based,Training-Free,Phoneme Diagnosis,Speech Pathology"
                     data-authors="Huu Tuong Tu,Ha Viet Khanh,Tran Tien Dat,Vu Huan,Thien Van Luong,Nguyen Tien Cuong,Nguyen Thi Thu Trang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20107v1.html">Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Huu Tuong Tu, Ha Viet Khanh, Tran Tien Dat et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel training-free framework for Mispronunciation Detection and Diagnosis (MDD), a critical task for language learning and speech therapy. Unlike conventional methods requiring complex model training, their approach leverages retrieval techniques with a pretrained Automatic Speech Recognition (ASR) model. The method achieves accurate detection and diagnosis of pronunciation errors, demonstrating a superior F1 score of 69.60% on the L2-ARCTIC dataset without the computational complexity of model training.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Speech-Language Pathology</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                    <span class="domain-tag">Clinical Linguistics</span>
                    
                    <span class="domain-tag">Audiology (related to speech perception/production assessment)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20107v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20107v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20107v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20107v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20101v1"
                     data-domains="Cardiology,Radiology,Diagnostic Imaging,Cardiovascular Medicine"
                     data-keywords="Cardiomegaly,Deep Learning,Inception v3,Multi-Head Attention,Chest X-ray,Cardiovascular Disease,Automated Diagnosis,Medical Imaging"
                     data-authors="Abishek Karthik,Pandiyaraju V">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20101v1.html">Multi Head Attention Enhanced Inception v3 for Cardiomegaly Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Abishek Karthik, Pandiyaraju V
                </div>

                <div class="paper-summary">
                    This paper presents an integrated deep learning approach for the automatic detection of cardiomegaly from X-ray images, leveraging a Multi-Head Attention enhanced Inception v3 model. The proposed system achieves high performance metrics, demonstrating its potential for sensitive and accurate diagnosis of this structural abnormality in the healthcare industry. By selectively focusing on critical regions of input images, the model enhances feature representation and diagnostic capability.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Cardiovascular Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20101v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20101v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20101v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20101v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20096v1"
                     data-domains="Pulmonology,Emergency Medicine,Public Health,Environmental Health,Mental Health,Trauma Surgery"
                     data-keywords="Deep Learning,Forest Fire Detection,Early Warning System,Computer Vision,YOLOv7,Detection Transformer,Smoke Plume Detection,Synthetic Data"
                     data-authors="Sharjeel Ahmed,Daim Armaghan,Fatima Naweed,Umair Yousaf,Ahmad Zubair,Murtaza Taj">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20096v1.html">Exploring State-of-the-art models for Early Detection of Forest Fires</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.70</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sharjeel Ahmed, Daim Armaghan, Fatima Naweed et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the critical issue of missed detections in early forest fire warnings due to a lack of suitable datasets and models. The authors propose a novel dataset for early identification of smoke plumes and initial fire instances, synthesized using game simulators like Red Dead Redemption 2 and combined with existing images. They then evaluate state-of-the-art deep learning models, specifically YOLOv7 and different detection transformers, for image classification and localization on this newly compiled dataset to improve early warning systems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Environmental Health</span>
                    
                    <span class="domain-tag">Mental Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20096v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20096v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20096v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20096v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20081v1"
                     data-domains="Oncology,Diagnostic Radiology,Medical Imaging,Biomedical Engineering"
                     data-keywords="CEST MRI,Denoising,Heteroscedasticity,Variance Stabilization,Local SVD,Amide Proton Transfer (APT),Molecular Imaging,Cancer Detection"
                     data-authors="Chu Chen,Aitor Artola,Yang Liu,Se Weon Park,Raymond H. Chan,Jean-Michel Morel,Kannie W. Y. Chan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20081v1.html">Blind Adaptive Local Denoising for CEST Imaging</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chu Chen, Aitor Artola, Yang Liu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Blind Adaptive Local Denoising (BALD), a novel method designed to overcome the challenges of spatially varying and heteroscedastic noise inherent in Chemical Exchange Saturation Transfer (CEST) MRI data. BALD adaptively stabilizes noise distributions and employs a two-stage denoising process to accurately disentangle molecular signals. Validated extensively, BALD consistently outperforms existing CEST denoisers, significantly improving the accuracy of quantitative contrast mapping and downstream tasks like cancer detection, thereby accelerating CEST's clinical translation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20081v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20081v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20081v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20081v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20030v1"
                     data-domains="Medical Informatics,Bioinformatics,Computational Pathology,Radiomics,Genomics,Precision Medicine,Disease Subtyping"
                     data-keywords="Multimodal Attributed Graphs,Graph Clustering,Contrastive Learning,Graph Neural Networks,Graph Signal Processing,Feature Denoising,Medical Data Analytics,Node Representation Learning"
                     data-authors="Haoran Zheng,Renchi Yang,Hongtao Wang,Jianliang Xu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20030v1.html">Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Haoran Zheng, Renchi Yang, Hongtao Wang et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the challenges of clustering Multimodal Attributed Graphs (MMAGs), where existing multi-view methods struggle with noisy and weakly correlated features from large pre-trained models. The authors propose Dual Graph Filtering (DGF), a novel scheme that incorporates feature-wise denoising and a tri-cross contrastive training strategy to learn robust and discriminative node representations. DGF consistently and significantly outperforms state-of-the-art baselines across various benchmark MMAG datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                    <span class="domain-tag">Radiomics</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20030v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20030v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20030v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20030v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.20001v1"
                     data-domains="Public Health,Psychiatry,Clinical Psychology,Adolescent Health,Digital Health,Preventive Medicine"
                     data-keywords="Machine Learning,Mental Health Detection,Cyberbullying,Social Media Analysis,Transformer Models,MentalBERT,Explainable AI,Multiclass Classification"
                     data-authors="Edward Ajayi,Martha Kachweka,Mawuli Deku,Emily Aiken">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.20001v1.html">A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Edward Ajayi, Martha Kachweka, Mawuli Deku et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a unified multiclass machine learning framework to detect ten distinct mental health and cyberbullying categories from social media data. Utilizing a rigorous "split-then-balance" pipeline and fine-tuning transformer models, the study identified the domain-adapted MentalBERT as the top performer, achieving 0.92 accuracy and 0.76 Macro F1. The system is designed as an explainable, human-in-the-loop screening aid for moderators, not a diagnostic tool.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Clinical Psychology</span>
                    
                    <span class="domain-tag">Adolescent Health</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.20001v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.20001v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.20001v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.20001v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19953v1"
                     data-domains="Pathology,Computational Pathology,Histopathology,Oncology"
                     data-keywords="Nuclear instance segmentation,Computational pathology,Training-free,Annotation-free,Prototype-guided prompting,Segment Anything Model (SAM),Optimal transport,Histopathology"
                     data-authors="Wen Zhang,Qin Ren,Wenjing Liu,Haibin Ling,Chenyu You">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19953v1.html">Supervise Less, See More: Training-free Nuclear Instance Segmentation with Prototype-Guided Prompting</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Wen Zhang, Qin Ren, Wenjing Liu et al.
                </div>

                <div class="paper-summary">
                    SPROUT introduces a novel, fully training- and annotation-free prompting framework for accurate nuclear instance segmentation in computational pathology. It leverages histology-informed priors to construct slide-specific prototypes that guide feature alignment via partial optimal transport, transforming features into point prompts for the Segment Anything Model (SAM). This method achieves competitive performance across multiple histopathology benchmarks without requiring any supervision or retraining, establishing a scalable paradigm for the field.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19953v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19953v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19953v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19953v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19935v1"
                     data-domains="Healthcare AI,Clinical Decision Support Systems,Medical Informatics,Pharmacogenomics (for drug discovery LLMs),Radiology (for image report generation LLMs)"
                     data-keywords="Large Language Models (LLMs),Domain Adaptation,Model Pruning,LoRA,Healthcare AI,Resource-constrained,Sparsity,EfficientXpert"
                     data-authors="Songlin Zhao,Michael Pitts,Zhuwei Qin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19935v1.html">EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Songlin Zhao, Michael Pitts, Zhuwei Qin
                </div>

                <div class="paper-summary">
                    EfficientXpert proposes a lightweight domain-pruning framework to enable the deployment of specialized large language models (LLMs) in resource-constrained environments. By integrating a propagation-aware pruning criterion (Foresight Mask) and an efficient adapter-update algorithm (Partial Brain Surgeon) into the LoRA fine-tuning process, it transforms general models into sparse, domain-adapted experts in a single step. The method achieved up to 98% of dense-model performance at 40% sparsity on health and legal tasks, outperforming state-of-the-art approaches.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Healthcare AI</span>
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                    <span class="domain-tag">Pharmacogenomics (for drug discovery LLMs)</span>
                    
                    <span class="domain-tag">Radiology (for image report generation LLMs)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19935v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19935v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19935v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19935v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19889v1"
                     data-domains="Radiology,Hepatology,Oncology,Medical Imaging,Diagnostic Medicine"
                     data-keywords="Liver,CT imaging,Multi-task learning,Segmentation,Classification,Detection,Computer-aided diagnosis,Benchmark dataset"
                     data-authors="Zhe Liu,Kai Han,Siqi Ma,Yan Zhu,Jun Chen,Chongwen Lyu,Xinyi Qiu,Chengxuan Qian,Yuqing Song,Yi Liu,Liyuan Tian,Yang Ji,Yuefeng Li">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19889v1.html">LiMT: A Multi-task Liver Image Benchmark Dataset</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhe Liu, Kai Han, Siqi Ma et al.
                </div>

                <div class="paper-summary">
                    This paper introduces LiMT, a novel multi-task liver image benchmark dataset designed to overcome the limitations of existing single-task datasets in computer-aided diagnosis (CAD) of liver conditions. LiMT comprises arterial phase-enhanced CT scans from 150 cases, enabling research in liver and tumor segmentation, multi-label lesion classification, and lesion detection simultaneously. The dataset aims to facilitate the development of more robust CAD technologies by allowing exploration of task correlations and mitigating data heterogeneity challenges.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Hepatology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19889v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19889v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19889v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19889v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19877v1"
                     data-domains="Psychiatry,Mental Health,Clinical Psychology,Digital Health,Behavioral Health"
                     data-keywords="Depression detection,Multi-modal LLM,Audio-visual integration,Mental health assessment,Timestamp alignment,Non-verbal cues,AI-assisted diagnosis,DAIC-WoZ"
                     data-authors="Xiangyu Zhao,Yaling Shen,Yiwen Jiang,Zimu Wang,Jiahe Liu,Maxmartwell H Cheng,Guilherme C Oliveira,Robert Desimone,Dominic Dwyer,Zongyuan Ge">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19877v1.html">It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.MM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xiangyu Zhao, Yaling Shen, Yiwen Jiang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel multi-modal Large Language Model (LLM) framework for depression detection, integrating visual understanding into an audio language model. The approach achieves fine-grained, timestamp-level alignment of audio-visual features, which enhances the modeling of temporal dynamics while reducing data and computational resource requirements. Experiments on the DAIC-WoZ dataset demonstrate that this model significantly outperforms both single-modality and prior multi-modal methods for depression detection.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Clinical Psychology</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Behavioral Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19877v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19877v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19877v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19877v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19858v1"
                     data-domains="Clinical Informatics,Patient Safety,Healthcare Quality Improvement,Medical Record Review,Diagnostic Medicine"
                     data-keywords="Large Language Models,Medical Error Detection,Error Correction,Retrieval-Augmented Generation,Dynamic Prompting,Clinical Documentation,Patient Safety,MEDEC dataset"
                     data-authors="Farzad Ahmed,Joniel Augustine Jerome,Meliha Yetisgen,√ñzlem Uzuner">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19858v1.html">A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Farzad Ahmed, Joniel Augustine Jerome, Meliha Yetisgen et al.
                </div>

                <div class="paper-summary">
                    This paper systematically evaluates the efficacy of large language models (LLMs) in detecting and correcting medical errors within clinical documentation, comparing zero-shot, static, and retrieval-augmented dynamic prompting strategies. It demonstrates that Retrieval-augmented Dynamic Prompting (RDP) significantly outperforms other methods across diverse LLMs, leading to improved error detection accuracy, reduced false positives, and more contextually appropriate error corrections. The findings suggest RDP as a promising approach for enhancing patient safety through automated quality control in medical documentation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Patient Safety</span>
                    
                    <span class="domain-tag">Healthcare Quality Improvement</span>
                    
                    <span class="domain-tag">Medical Record Review</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19858v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19858v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19858v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19858v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19834v1"
                     data-domains="Pulmonology,Radiology,Rare Diseases,Medical Imaging,Oncology (due to BHD's cancer association)"
                     data-keywords="Birt-Hogg-Dube syndrome,Diffuse Cystic Lung Diseases,Computed Tomography,Multimodal Large Language Models,Retrieval-Augmented Generation,Medical Diagnosis,Radiology,Deep Learning"
                     data-authors="Haoqing Li,Jun Shi,Xianmeng Chen,Qiwei Jia,Rui Wang,Wei Wei,Hong An,Xiaowen Hu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19834v1.html">Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Haoqing Li, Jun Shi, Xianmeng Chen et al.
                </div>

                <div class="paper-summary">
                    This paper introduces BHD-RAG, a novel multimodal retrieval-augmented generation framework designed to improve the diagnostic accuracy of Birt-Hogg-Dube (BHD) syndrome from Computed Tomography (CT) images, particularly amidst challenges of limited clinical data and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs). By integrating DCLD-specific expertise and clinical precedents with Multimodal Large Language Models (MLLMs), BHD-RAG successfully mitigates hallucination risks and achieves superior diagnostic accuracy, generating evidence-based descriptions aligned with expert insights.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Rare Diseases</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Oncology (due to BHD's cancer association)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19834v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19834v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19834v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19834v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19830v1"
                     data-domains="Medical Imaging,Electronic Health Records (EHRs),Genomics and Proteomics,Clinical Research,Personalized Medicine,Public Health"
                     data-keywords="Multi-modal analytics,LLMs,Query optimization,Semantic processing,Data frameworks,Artificial intelligence,Healthcare applications,Database systems"
                     data-authors="Junhao Zhu,Lu Chen,Xiangyu Ke,Ziquan Fang,Tianyi Li,Yunjun Gao,Christian S. Jensen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19830v1.html">Beyond Relational: Semantic-Aware Multi-Modal Analytics with LLM-Native Query Optimization</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ cs.DB</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Junhao Zhu, Lu Chen, Xiangyu Ke et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Nirvana, a novel multi-modal data analytics framework designed to overcome the limitations of traditional relational query operators by incorporating programmable semantic operators and LLM-driven query optimization. Nirvana features an agentic logical optimizer for semantic query plan exploration and a cost-aware physical optimizer for efficient LLM backend selection, leading to significant reductions in processing time and cost compared to state-of-the-art systems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Electronic Health Records (EHRs)</span>
                    
                    <span class="domain-tag">Genomics and Proteomics</span>
                    
                    <span class="domain-tag">Clinical Research</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19830v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19830v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19830v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19830v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19816v1"
                     data-domains="Mental Health,Public Health,Psychiatry,Health Informatics,Behavioral Health,Digital Therapeutics"
                     data-keywords="Valence-Arousal-Dominance (VAD),Multiword Expressions (MWEs),Lexicon,Emotionality,Sentiment Analysis,Natural Language Processing (NLP),Public Health,Digital Health"
                     data-authors="Saif M. Mohammad">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19816v1.html">Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.85</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Saif M. Mohammad
                </div>

                <div class="paper-summary">
                    This paper introduces the NRC VAD Lexicon v2, a significant expansion of emotion norms providing human-rated Valence, Arousal, and Dominance (VAD) scores for over 10,000 English Multiword Expressions (MWEs) and an additional 25,000 unigram words. This new, highly reliable resource allows for detailed analysis of the emotional characteristics and compositionality of complex linguistic units, serving as a foundational tool for various research fields.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Behavioral Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19816v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19816v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19816v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19816v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19813v1"
                     data-domains="Oncology,Developmental Biology,Hematology,Immunology,Regenerative Medicine,Personalized Medicine"
                     data-keywords="dynamic gene drivers,single-cell genomics,gene regulatory networks,optimal transport,graph attention,time-varying regulation,developmental biology,disease progression"
                     data-authors="Jiaxin Li,Shanjun Mao">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19813v1.html">Time-Varying Network Driver Estimation (TNDE) Quantifies Stage-Specific Regulatory Effects From Single-Cell Snapshots</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-25</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.MN</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jiaxin Li, Shanjun Mao
                </div>

                <div class="paper-summary">
                    TNDE is a novel computational framework that quantifies dynamic, time-resolved gene driver effects from single-cell snapshot data, overcoming limitations of existing static gene regulatory network methods. It employs a shared graph attention encoder and partial optimal transport to handle non-equilibrium processes, demonstrating superior performance on simulations and successfully identifying stage-specific drivers in mouse erythropoiesis.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Developmental Biology</span>
                    
                    <span class="domain-tag">Hematology</span>
                    
                    <span class="domain-tag">Immunology</span>
                    
                    <span class="domain-tag">Regenerative Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19813v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19813v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19813v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19813v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19798v1"
                     data-domains="Orthopedics,Rheumatology,Chronic Disease Management,Physical Medicine and Rehabilitation,Geriatrics"
                     data-keywords="Knee Osteoarthritis,Multi-Agent AI,Precision Medicine,Chronic Disease Management,Clinical Decision Support,Treatment Prescription,Imaging Analysis,Healthcare Efficiency"
                     data-authors="Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19798v1.html">KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Weizhi Liu, Xi Chen, Zekun Jiang et al.
                </div>

                <div class="paper-summary">
                    KOM is a novel multi-agent AI system designed for the precision management of Knee Osteoarthritis (KOA), aiming to automate evaluation, risk prediction, and personalized treatment prescription. It demonstrated superior performance over general-purpose large language models in imaging analysis and prescription generation, and in a simulation study, reduced diagnostic and planning time by 38.5% while improving treatment quality when collaborating with clinicians.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                    <span class="domain-tag">Rheumatology</span>
                    
                    <span class="domain-tag">Chronic Disease Management</span>
                    
                    <span class="domain-tag">Physical Medicine and Rehabilitation</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19798v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19798v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19798v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19798v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19759v1"
                     data-domains="cs.CV"
                     data-keywords="cs.CV"
                     data-authors="Jiaqi Guo,Mingzhen Li,Hanyu Su,Santiago L√≥pez,Lexiaozi Fan,Daniel Kim,Aggelos Katsaggelos">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19759v1.html">Vision--Language Enhanced Foundation Model for Semi-supervised Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jiaqi Guo, Mingzhen Li, Hanyu Su et al.
                </div>

                <div class="paper-summary">
                    Semi-supervised learning (SSL) has emerged as an effective paradigm for medical image segmentation, reducing the reliance on extensive expert annotations. Meanwhile, vision-language models (VLMs) have demonstrated strong generalization and few-shot capabilities across diverse visual domains. In this...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.CV</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19759v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19759v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19759v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19759v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19751v1"
                     data-domains="Pathology,Dermatology,Oncology,Digital Health"
                     data-keywords="Pathology Foundation Models,Histological Grading,Cutaneous Squamous Cell Carcinoma,Whole-Slide Images,Computational Pathology,Vision-Language Models,PathFMTools,H&E Staining"
                     data-authors="Abdul Rahman Diab,Emily E. Karn,Renchin Wu,Emily S. Ruiz,William Lotter">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19751v1.html">Leveraging Foundation Models for Histological Grading in Cutaneous Squamous Cell Carcinoma using PathFMTools</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Abdul Rahman Diab, Emily E. Karn, Renchin Wu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces PathFMTools, a lightweight Python package designed to streamline the execution, analysis, and visualization of pathology foundation models (FMs). The authors demonstrate its utility by benchmarking two state-of-the-art vision-language FMs, CONCH and MUSK, for histological grading in cutaneous squamous cell carcinoma (cSCC), revealing effective adaptation strategies and the potential of FM embeddings for specialized downstream models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19751v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19751v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19751v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19751v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19739v1"
                     data-domains="clinical cardiology,medical informatics,clinical natural language processing"
                     data-keywords="text embeddings,clinical NLP,cardiology,LoRA,transformer models,BioLinkBERT,encoder-only,domain adaptation"
                     data-authors="Richard J. Young,Alice M. Matthews">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19739v1.html">Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Richard J. Young, Alice M. Matthews
                </div>

                <div class="paper-summary">
                    This study systematically evaluates ten transformer-based embedding models, adapted for clinical cardiology using Low-Rank Adaptation (LoRA) fine-tuning, to generate domain-specific text representations. It demonstrates that encoder-only architectures, particularly BioLinkBERT, achieve superior performance with significantly fewer computational resources compared to larger decoder-based models, challenging conventional assumptions about model size.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">clinical cardiology</span>
                    
                    <span class="domain-tag">medical informatics</span>
                    
                    <span class="domain-tag">clinical natural language processing</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19739v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19739v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19739v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19739v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19735v1"
                     data-domains="Drug Development,Clinical Trials,Regulatory Science (e.g., FDA, EMA),Pharmacovigilance,Health Outcomes Research,Personalized Medicine"
                     data-keywords="Randomized controlled trials (RCTs),Real-world data (RWD),Artificial intelligence (AI),Machine learning (ML),Evidence synthesis,Causal inference,Regulatory science,External validity"
                     data-authors="Shu Yang,Margaret Gamalo,Haoda Fu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19735v1.html">Integrating RCTs, RWD, AI/ML and Statistics: Next-Generation Evidence Synthesis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ stat.ME</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shu Yang, Margaret Gamalo, Haoda Fu
                </div>

                <div class="paper-summary">
                    This perspective paper advocates for a next-generation evidence synthesis approach that princply integrates Randomized Controlled Trials (RCTs), Real-World Data (RWD), Artificial Intelligence/Machine Learning (AI/ML), and traditional statistics. This integration aims to overcome the individual limitations of these methods, such as RCT cost and external validity issues, RWD causal inference challenges, and AI/ML interpretability problems, to generate robust and transparent evidence essential for modern regulatory science.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Drug Development</span>
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                    <span class="domain-tag">Regulatory Science (e.g., FDA, EMA)</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Health Outcomes Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19735v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19735v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19735v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19735v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19686v1"
                     data-domains="Histopathology,Hematology,Oncology,Pathology,Immunology,Drug Discovery"
                     data-keywords="cell counting,interpretable AI,deep learning,prototype-based models,density map estimation,biomedical imaging,explainable AI (XAI),clinical applications"
                     data-authors="Abdurahman Ali Mohammed,Wallapak Tavanapong,Catherine Fonder,Donald S. Sakaguchi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19686v1.html">CountXplain: Interpretable Cell Counting with Prototype-Based Density Map Estimation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Abdurahman Ali Mohammed, Wallapak Tavanapong, Catherine Fonder et al.
                </div>

                <div class="paper-summary">
                    CountXplain introduces a novel prototype-based deep learning method for interpretable cell counting via density map estimation. This approach integrates a prototype layer to learn representative visual patterns of cells and artifacts, validated by biologists, offering clear explanations of cell identification while maintaining high counting accuracy on public datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Hematology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Immunology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19686v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19686v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19686v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19686v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19667v1"
                     data-domains="Radiology,Oncology,Diagnostic Imaging,Public Health,Global Health"
                     data-keywords="Breast Cancer Diagnosis,Multimodal AI,Mammography,Attention Mechanisms,Image Segmentation,Clinical Data Fusion,BI-RADS,Early Detection"
                     data-authors="Istiak Ahmed,Galib Ahmed,K. Shahriar Sanjid,Md. Tanzim Hossain,Md. Nishan Khan,Md. Misbah Khan,Md. Arifur Rahman,Sheikh Anisul Haque,Sharmin Akhtar Rupa,Mohammed Mejbahuddin Mia,Mahmud Hasan Mostofa Kamal,Md. Mostafa Kamal Sarker,M. Monir Uddin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19667v1.html">OncoVision: Integrating Mammography and Clinical Data through Attention-Driven Multimodal AI for Enhanced Breast Cancer Diagnosis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Istiak Ahmed, Galib Ahmed, K. Shahriar Sanjid et al.
                </div>

                <div class="paper-summary">
                    OncoVision introduces a multimodal AI pipeline that integrates mammography images and clinical data using an attention-based encoder-decoder for enhanced breast cancer diagnosis. It achieves state-of-the-art segmentation of key regions of interest and robustly predicts clinical features, employing late-fusion strategies to improve diagnostic precision and reduce inter-observer variability. This system is operationalized as a user-friendly web application, offering real-time diagnostic support, and aims to provide scalable and equitable early detection solutions globally.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Global Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19667v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19667v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19667v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19667v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19652v1"
                     data-domains="Pathology,Oncology,Diagnostic Medicine,Computational Pathology"
                     data-keywords="Gigapixel pathology,Large Multimodal Models (LMMs),Whole-Slide Images (WSIs),Agentic systems,Digital pathology,Cancer diagnosis,MultiPathQA,GIANT framework"
                     data-authors="Thomas A. Buckley,Kian R. Weihrauch,Katherine Latham,Andrew Z. Zhou,Padmini A. Manrai,Arjun K. Manrai">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19652v1.html">Navigating Gigapixel Pathology Images with Large Multimodal Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Thomas A. Buckley, Kian R. Weihrauch, Katherine Latham et al.
                </div>

                <div class="paper-summary">
                    This paper introduces GIANT (Gigapixel Image Agent for Navigating Tissue), a novel framework that enables Large Multimodal Models (LMMs) to iteratively navigate and interpret gigapixel whole-slide pathology images like a human pathologist. It demonstrates that this agentic approach, coupled with the new MultiPathQA benchmark, significantly outperforms conventional LMM baselines and often surpasses specialized pathology models, revealing a viable path for LMMs in expert medical reasoning.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19652v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19652v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19652v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19652v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19425v1"
                     data-domains="radiology,pathology,cell biology,surgical planning,disease diagnosis,biomedical imaging"
                     data-keywords="SAM3-Adapter,Segment Anything 3,medical image segmentation,camouflage object segmentation,shadow detection,foundation models,deep learning,computer vision,state-of-the-art,image segmentation"
                     data-authors="Tianrun Chen,Runlong Cao,Xinda Yu,Lanyun Zhu,Chaotao Ding,Deyi Ji,Cheng Chen,Qi Zhu,Chunyan Xu,Papa Mao,Ying Zang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19425v1.html">SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tianrun Chen, Runlong Cao, Xinda Yu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces SAM3-Adapter, a novel adapter framework designed to significantly enhance the performance of Segment Anything 3 (SAM3) for challenging fine-grained segmentation tasks. It addresses previous limitations of SAM and SAM2 in areas such as medical image segmentation, camouflaged object detection, and shadow detection, achieving new state-of-the-art results with improved precision, generalizability, and computational efficiency.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">radiology</span>
                    
                    <span class="domain-tag">pathology</span>
                    
                    <span class="domain-tag">cell biology</span>
                    
                    <span class="domain-tag">surgical planning</span>
                    
                    <span class="domain-tag">disease diagnosis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19425v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19425v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19425v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19425v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.19399v1"
                     data-domains="Clinical Decision Support,Medical Literature Analysis,Drug Discovery and Development,Biomedical Research,Health Informatics,Patient Education"
                     data-keywords="Reinforcement Learning,Evolving Rubrics,Deep Research,Long-form QA,Healthcare AI,Large Language Models,RLER,DR Tulu"
                     data-authors="Rulin Shao,Akari Asai,Shannon Zejiang Shen,Hamish Ivison,Varsha Kishore,Jingming Zhuo,Xinran Zhao,Molly Park,Samuel G. Finlayson,David Sontag,Tyler Murray,Sewon Min,Pradeep Dasigi,Luca Soldaini,Faeze Brahman,Wen-tau Yih,Tongshuang Wu,Luke Zettlemoyer,Yoon Kim,Hannaneh Hajishirzi,Pang Wei Koh">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.19399v1.html">DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-24</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Rulin Shao, Akari Asai, Shannon Zejiang Shen et al.
                </div>

                <div class="paper-summary">
                    Existing deep research models struggle with multi-step, long-form tasks due to training on easily verifiable short-form QA. This paper introduces Reinforcement Learning with Evolving Rubrics (RLER), a novel training methodology where evaluation rubrics dynamically co-evolve with the policy model to provide discriminative, on-policy feedback. Utilizing RLER, they developed DR Tulu-8B, the first open model for open-ended, long-form deep research, which significantly outperforms other open models and matches proprietary systems across science, healthcare, and general domains.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Literature Analysis</span>
                    
                    <span class="domain-tag">Drug Discovery and Development</span>
                    
                    <span class="domain-tag">Biomedical Research</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.19399v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.19399v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.19399v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.19399v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-11-26 06:31:42</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>