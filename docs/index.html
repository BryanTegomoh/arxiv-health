<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">45</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">45</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">133</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Radiology (11), Diagnostic Imaging (9), Oncology (9)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Radiology">Radiology (11)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (9)</option>
                        
                        <option value="Oncology">Oncology (9)</option>
                        
                        <option value="Public Health">Public Health (7)</option>
                        
                        <option value="Epidemiology">Epidemiology (4)</option>
                        
                        <option value="Personalized Medicine">Personalized Medicine (4)</option>
                        
                        <option value="Pharmacovigilance">Pharmacovigilance (4)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (4)</option>
                        
                        <option value="Pathology">Pathology (4)</option>
                        
                        <option value="Regulatory Affairs">Regulatory Affairs (3)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2512.07796v1"
                     data-domains="Pharmacology,Epidemiology,Pathophysiology,Diagnostics,Public Health,Personalized Medicine"
                     data-keywords="Large Language Models,Causal Models,DEMOCRITUS,Causal Inference,Knowledge Extraction,Categorical Machine Learning,Health Informatics,Biomedical Ontologies"
                     data-authors="Sridhar Mahadevan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07796v1.html">Large Causal Models from Large Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.85</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sridhar Mahadevan
                </div>

                <div class="paper-summary">
                    This paper introduces a novel paradigm and system, DEMOCRITUS, for building Large Causal Models (LCMs) by leveraging the vast knowledge embedded in Large Language Models (LLMs). DEMOCRITUS departs from traditional hypothesis-driven causal inference by extracting and integrating fragmented causal claims from diverse textual sources into coherent relational causal triples using new categorical machine learning methods, applicable across multiple domains including medicine.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Pathophysiology</span>
                    
                    <span class="domain-tag">Diagnostics</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07796v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07796v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07796v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07796v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07756v1"
                     data-domains="Radiology,Diagnostic Imaging,Interventional Radiology,Surgical Navigation,Medical Robotics"
                     data-keywords="Ultrasound Reconstruction,Optical Flow,Mamba Network,3D Freehand Ultrasound,Pose Estimation,Uncertainty Estimation,Human-in-the-Loop,Sonography"
                     data-authors="Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07756v1.html">UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mayank Anand, Ujair Alam, Surya Prakash et al.
                </div>

                <div class="paper-summary">
                    UltrasODM is a novel dual-stream deep learning framework designed to enhance the reliability of 3D freehand ultrasound reconstruction by mitigating operator-dependent errors. It provides real-time, per-frame uncertainty estimation, saliency-based diagnostics, and actionable prompts to sonographers, significantly reducing reconstruction errors and improving diagnostic confidence.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Interventional Radiology</span>
                    
                    <span class="domain-tag">Surgical Navigation</span>
                    
                    <span class="domain-tag">Medical Robotics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07756v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07756v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07756v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07756v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07741v1"
                     data-domains="Psychiatry,Mental Health,Clinical Psychology,Neuropsychiatry"
                     data-keywords="Bayesian Network,depression,anxiety,voice analysis,speech processing,symptom prediction,psychiatric assessment,explainable AI"
                     data-authors="Agnes Norbury,George Fairs,Alexandra L. Georgescu,Matthew M. Nour,Emilia Molimpakis,Stefano Goria">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07741v1.html">A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Agnes Norbury, George Fairs, Alexandra L. Georgescu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a multimodal Bayesian Network for predicting symptom-level depression and anxiety from voice and speech data, evaluating its performance on a large dataset of over 30,000 speakers. The model demonstrates high predictive accuracy (ROC-AUC > 0.8), good calibration, and demographic fairness, proposing a transparent and explainable tool to support clinical psychiatric assessment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Clinical Psychology</span>
                    
                    <span class="domain-tag">Neuropsychiatry</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07741v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07741v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07741v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07741v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07729v1"
                     data-domains="Healthcare Monitoring,Geriatric Care,Rehabilitation,Assistive Robotics,Telemedicine"
                     data-keywords="Action Recognition,Deep Neural Networks,Brain-Inspired AI,Body Perception,Scene Perception,Healthcare Monitoring,Computer Vision,Human-like Performance"
                     data-authors="Aidas Aglinskas,Stefano Anzellotti">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07729v1.html">Improving action classification with brain-inspired deep networks</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.85</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Aidas Aglinskas, Stefano Anzellotti
                </div>

                <div class="paper-summary">
                    This paper investigates the reliance of deep neural networks (DNNs) on body versus background information for action recognition, finding that conventional DNNs predominantly use background cues. It proposes a novel brain-inspired DNN architecture with separate processing streams for body and scene data, demonstrating improved action recognition performance and a more human-like accuracy profile across varying stimulus conditions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Healthcare Monitoring</span>
                    
                    <span class="domain-tag">Geriatric Care</span>
                    
                    <span class="domain-tag">Rehabilitation</span>
                    
                    <span class="domain-tag">Assistive Robotics</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07729v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07729v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07729v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07729v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07694v1"
                     data-domains="Pharmacovigilance,Drug Safety,Regulatory Affairs,Pharmaceutical Research and Development,Clinical Trials"
                     data-keywords="MedDRA,adverse events,drug safety,pharmacovigilance,artificial intelligence,natural language processing,cosine similarity,signal detection"
                     data-authors="Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07694v1.html">Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Francois Vandenhende, Anna Georgiou, Michalis Georgiou et al.
                </div>

                <div class="paper-summary">
                    This paper introduces SafeTerm, a novel artificial intelligence system designed to automate the generation of custom MedDRA queries, which are critical for signal detection in pre-market drug safety review. SafeTerm achieves this by embedding medical terminology and MedDRA Preferred Terms (PTs) into a multidimensional vector space and using cosine similarity with extreme-value clustering to retrieve and rank relevant PTs. Validation against FDA OCMQ v3.0 demonstrated that the system can achieve high recall (>95%) at moderate thresholds and improved precision (up to 86%) at higher thresholds, making it a viable supplementary tool for pharmacovigilance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Drug Safety</span>
                    
                    <span class="domain-tag">Regulatory Affairs</span>
                    
                    <span class="domain-tag">Pharmaceutical Research and Development</span>
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07694v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07694v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07694v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07694v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07674v1"
                     data-domains="Radiology,Neuroradiology,Medical Image Analysis,Diagnostic Imaging,Artificial Intelligence in Medicine"
                     data-keywords="MRI Harmonization,Deep Learning,Medical Imaging,Data Heterogeneity,Disentanglement,CLIP Guidance,Style Transfer,DICOM Metadata"
                     data-authors="Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07674v1.html">DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mehmet Yigit Avci, Pedro Borges, Virginia Fernandez et al.
                </div>

                <div class="paper-summary">
                    DIST-CLIP addresses the critical issue of MRI data heterogeneity, which limits deep learning's clinical generalization, by proposing a unified harmonization framework. It explicitly disentangles anatomical content from image contrast, leveraging CLIP encoders for contrast representation and an Adaptive Style Transfer module. The method, guided flexibly by either target images or DICOM metadata, demonstrated significant improvements in style translation fidelity and anatomical preservation on diverse clinical datasets, thereby standardizing MRI data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Neuroradiology</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07674v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07674v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07674v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07674v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07651v1"
                     data-domains="Hepatology,Radiology,Gastroenterology"
                     data-keywords="Liver Fibrosis,MRI,Dataset,Segmentation,Staging,Semi-supervised Learning,Class Activation Map,CARE 2024"
                     data-authors="Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07651v1.html">Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuanye Liu, Hanxiao Zhang, Nannan Shi et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, a multi-phase, multi-center MRI dataset from 440 patients, designed to benchmark algorithms for liver segmentation (LiSeg) and fibrosis staging (LiFS) amidst real-world complexities like domain shifts and missing data. It also details a top-performing baseline method that uses semi-supervised learning and a multi-view consensus approach, demonstrating improved robustness in clinical settings through leveraging multi-source data and anatomical constraints.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Hepatology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07651v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07651v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07651v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07651v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07608v1"
                     data-domains="Clinical Medicine,Medical Education,Diagnostic Support,Medical AI Ethics,Healthcare Informatics"
                     data-keywords="Metric-Fair Prompting,LLMs,individual fairness,medical question answering,NLP embeddings,Lipschitz constraint,MedQA,clinical decision support"
                     data-authors="Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07608v1.html">Metric-Fair Prompting: Treating Similar Samples Similarly</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jing Wang, Jie Shen, Xing Niu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Metric-Fair Prompting, a novel framework guiding Large Language Models (LLMs) to make decisions under individual metric-fairness constraints. By processing similar medical questions in joint pairs and imposing a Lipschitz-style constraint on confidence scores, the method treats similar samples similarly, significantly improving LLM accuracy on the MedQA (US) benchmark for multiple-choice medical question answering.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Medicine</span>
                    
                    <span class="domain-tag">Medical Education</span>
                    
                    <span class="domain-tag">Diagnostic Support</span>
                    
                    <span class="domain-tag">Medical AI Ethics</span>
                    
                    <span class="domain-tag">Healthcare Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07608v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07608v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07608v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07608v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07606v1"
                     data-domains="Radiology,Pathology,Oncology,Medical Image Analysis"
                     data-keywords="Active Learning,Dense Prediction,Medical Imaging,Region Annotation,Segmentation,Decomposition Sampling,Minority Classes,Pseudo-labeling"
                     data-authors="Jingna Qiu,Frauke Wilm,Mathias √ñttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07606v1.html">Decomposition Sampling for Efficient Region Annotations in Active Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jingna Qiu, Frauke Wilm, Mathias √ñttl et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Decomposition Sampling (DECOMP), a novel active learning strategy designed to improve annotation efficiency for dense prediction tasks, especially in medical imaging. DECOMP addresses limitations of existing region-level sampling methods by decomposing images into class-specific components using pseudo-labels and guiding region selection with class-wise confidence. It consistently outperforms baselines across ROI classification, 2D, and 3D segmentation tasks by enhancing diversity and boosting performance on challenging minority classes.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07606v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07606v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07606v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07606v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07583v1"
                     data-domains="Pharmaceutical Industry,Drug Development,Regulatory Affairs,Health Policy,Medical Research,Public Health"
                     data-keywords="Large Language Models,Text Classification,Human-in-the-loop AI,Complementary Learning,Chain-of-Thought,Few-shot Learning,Pharmaceutical Alliances,Abductive Reasoning"
                     data-authors="Navid Asgari,Benjamin M. Cole">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07583v1.html">Complementary Learning Approach for Text Classification using Large Language Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Navid Asgari, Benjamin M. Cole
                </div>

                <div class="paper-summary">
                    This study proposes a cost-efficient and structured methodology for text classification that integrates the strengths of Large Language Models (LLMs) with human expertise, mitigating their respective weaknesses. Utilizing chain-of-thought and few-shot learning, the approach enables human scholars to apply abductive reasoning to interrogate both machine and human contributions, demonstrating its utility in analyzing discrepancies within pharmaceutical alliance press releases.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmaceutical Industry</span>
                    
                    <span class="domain-tag">Drug Development</span>
                    
                    <span class="domain-tag">Regulatory Affairs</span>
                    
                    <span class="domain-tag">Health Policy</span>
                    
                    <span class="domain-tag">Medical Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07583v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07583v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07583v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07583v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07576v1"
                     data-domains="Orthopedics,Radiology,Spinal Surgery,Diagnostic Imaging,Biomechanics"
                     data-keywords="Spine Segmentation,X-ray,Deep Learning,Recurrent Neural Networks,Residual Networks,Scoliosis,Medical Imaging,Encoder-Decoder"
                     data-authors="Xuecheng Li,Weikuan Jia,Komildzhon Sharipov,Sharipov Hotam Beknazarovich,Farzona S. Ataeva,Qurbonaliev Alisher,Yuanjie Zheng">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07576v1.html">R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xuecheng Li, Weikuan Jia, Komildzhon Sharipov et al.
                </div>

                <div class="paper-summary">
                    This paper introduces R2MF-Net, a novel recurrent residual multi-path encoder-decoder network designed for robust and automatic segmentation of multi-directional spine X-ray images. The network addresses the limitations of manual segmentation for scoliosis assessment by employing a cascaded coarse-to-fine architecture and specialized modules to enhance accuracy and stability across varying imaging conditions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Spinal Surgery</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Biomechanics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07576v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07576v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07576v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07576v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07574v1"
                     data-domains="Diagnostic Radiology,Oncology,Surgical Planning,Radiation Oncology,Interventional Radiology"
                     data-keywords="Liver tumor segmentation,CT imaging,Deep learning,Radiomics,U-Net,3D CNN,Medical image analysis"
                     data-authors="Xuecheng Li,Weikuan Jia,Komildzhon Sharipov,Alimov Ruslan,Lutfuloev Mazbutdzhon,Ismoilov Shuhratjon,Yuanjie Zheng">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07574v1.html">Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xuecheng Li, Weikuan Jia, Komildzhon Sharipov et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a hybrid deep learning-radiomics framework designed for precise, joint liver and liver-tumor segmentation in contrast-enhanced CT images. The proposed multi-stage approach combines an attention-enhanced cascaded U-Net for initial probability maps, inter-slice consistency enforcement, radiomics-based false-positive rejection, and a 3D patch-based CNN for final boundary refinement. This aims to overcome challenges of manual contouring, such as slowness and observer dependency, and common auto-segmentation difficulties like low contrast and blurred boundaries.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Surgical Planning</span>
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Interventional Radiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07574v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07574v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07574v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07574v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07552v1"
                     data-domains="Drug Safety,Pharmacovigilance,Regulatory Affairs,Clinical Research"
                     data-keywords="MedDRA,AI,Drug Safety,Pharmacovigilance,Adverse Events,Signal Detection,Natural Language Processing,Medical Query"
                     data-authors="Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07552v1.html">Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Francois Vandenhende, Anna Georgiou, Michalis Georgiou et al.
                </div>

                <div class="paper-summary">
                    SafeTerm AMQ is a novel AI system designed to automate the generation of MedDRA queries for drug safety signal detection by embedding medical terms and MedDRA Preferred Terms (PTs) into a vector space, then using cosine similarity and clustering to retrieve and rank relevant terms. Validated against 110 Standardised MedDRA Queries (SMQs), it demonstrated high recall (94%) at moderate thresholds and improved precision (up to 89%) at higher thresholds. This makes it a viable supplementary tool for automated MedDRA query generation, effectively balancing recall and precision for pharmacovigilance applications.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Drug Safety</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Regulatory Affairs</span>
                    
                    <span class="domain-tag">Clinical Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07552v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07552v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07552v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07552v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07433v1"
                     data-domains="Computational Neuroscience,Bioinformatics,Personalized Medicine,Medical Diagnostics,Public Health Informatics,Drug Discovery"
                     data-keywords="Graph Hyperdimensional Computing,Fairness,Bias Mitigation,Demographic Parity,Equal Opportunity,Graph Neural Networks,Cognitive Computing,Machine Learning"
                     data-authors="Yezi Liu,William Youngwoo Chung,Yang Ni,Hanning Chen,Mohsen Imani">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07433v1.html">Mitigating Bias in Graph Hyperdimensional Computing</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.70</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yezi Liu, William Youngwoo Chung, Yang Ni et al.
                </div>

                <div class="paper-summary">
                    This paper introduces FairGHDC, a novel fairness-aware training framework for Graph Hyperdimensional Computing (HDC) that aims to mitigate bias in graph-structured data. FairGHDC achieves this by incorporating a demographic-parity-based bias correction directly into the hypervector update process. The framework substantially reduces fairness gaps while maintaining high accuracy and offering a significant speedup in training time compared to traditional Graph Neural Networks.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Computational Neuroscience</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Medical Diagnostics</span>
                    
                    <span class="domain-tag">Public Health Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07433v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07433v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07433v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07433v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07426v1"
                     data-domains="cs.CV"
                     data-keywords="cs.CV,cs.AI"
                     data-authors="Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07426v1.html">When normalization hallucinates: unseen risks in AI-powered whole slide image processing</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Karel Moens, Matthew B. Blaschko, Tinne Tuytelaars et al.
                </div>

                <div class="paper-summary">
                    Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.CV</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07426v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07426v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07426v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07426v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07390v1"
                     data-domains="Diagnostic imaging (e.g., radiology, pathology),Clinical decision support systems,Personalized medicine,Medical robotics and autonomous systems,Remote patient monitoring,Disease prognosis and risk assessment"
                     data-keywords="Test-time adaptation,predictive uncertainty,model calibration,style invariance,deep learning,healthcare AI,model reliability,machine learning"
                     data-authors="Gilhyun Nam,Taewon Kim,Joonhyun Jeong,Eunho Yang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07390v1.html">Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Gilhyun Nam, Taewon Kim, Joonhyun Jeong et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Style Invariance as a Correctness Likelihood (SICL), a novel framework designed to address the critical issue of poorly calibrated predictive uncertainty in Test-Time Adaptation (TTA) models, particularly relevant for high-stakes domains like healthcare. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, acting as a plug-and-play, backpropagation-free module compatible with any TTA method. Comprehensive evaluations demonstrate that SICL significantly reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Diagnostic imaging (e.g., radiology, pathology)</span>
                    
                    <span class="domain-tag">Clinical decision support systems</span>
                    
                    <span class="domain-tag">Personalized medicine</span>
                    
                    <span class="domain-tag">Medical robotics and autonomous systems</span>
                    
                    <span class="domain-tag">Remote patient monitoring</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07390v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07390v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07390v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07390v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07314v1"
                     data-domains="Public Health,Epidemiology,Infectious Disease Modeling,Disease Surveillance,Emergency Preparedness"
                     data-keywords="Human mobility,Spatiotemporal modeling,Autoregression,Multi-scale,Trajectory generation,Transformer,AIGC,Epidemic modeling"
                     data-authors="Yuxiao Luo,Songming Zhang,Sijie Ruan,Siran Chen,Kang Liu,Yang Xu,Yu Zheng,Ling Yin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07314v1.html">M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuxiao Luo, Songming Zhang, Sijie Ruan et al.
                </div>

                <div class="paper-summary">
                    M-STAR (Multi-Scale Spatio-Temporal AutoRegression) is a novel framework designed to generate long-term human mobility trajectories, overcoming the limitations of previous AI-generated content (AIGC) methods regarding inefficiency and lack of explicit multi-scale modeling. It achieves this through a coarse-to-fine spatiotemporal prediction process, leveraging a Multi-scale Spatiotemporal Tokenizer and a Transformer-based decoder. Experiments on real-world datasets demonstrate M-STAR's superior fidelity and significantly improved generation speed compared to existing methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Infectious Disease Modeling</span>
                    
                    <span class="domain-tag">Disease Surveillance</span>
                    
                    <span class="domain-tag">Emergency Preparedness</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07314v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07314v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07314v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07314v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07310v1"
                     data-domains="Clinical Trials,Pharmacovigilance,Epidemiology,Personalized Medicine,Health Outcomes Research,Public Health Informatics,Medical Genetics"
                     data-keywords="Transformer,Tabular Data,Attention Mechanism,Treatment Effect Estimation,Causal Inference,Deep Learning,Graph Dependencies,Relationship-Aware"
                     data-authors="Andrei V. Konstantinov,Valerii A. Zuev,Lev V. Utkin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07310v1.html">Towards a Relationship-Aware Transformer for Tabular Data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Andrei V. Konstantinov, Valerii A. Zuev, Lev V. Utkin
                </div>

                <div class="paper-summary">
                    This paper proposes novel deep learning models based on a modified Transformer architecture designed to integrate external graphs of dependencies between tabular data samples, a feature often missing in conventional models. The core innovation lies in a 'relationship-aware' attention mechanism that adds a term to account for these connections, particularly useful for tasks like treatment effect estimation. The models' performance is evaluated in regression tasks on synthetic and real-world datasets, and in treatment effect estimation using the IHDP dataset, with comparisons to Gradient Boosting Decision Trees.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Health Outcomes Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07310v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07310v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07310v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07310v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07307v1"
                     data-domains="Oncology,Diagnostics,Personalized Medicine,Rare Cell Analysis,Cell Biology Research"
                     data-keywords="quantum-enhanced NMR,single-cell identification,label-free detection,nitrogen-vacancy centers,proton T1 relaxation,tumor cell lines,rare cell analysis,personalized medicine"
                     data-authors="Zhiyuan Zhao,Qian Shi,Shaoyi Xu,Xiangyu Ye,Mengze Shen,Jia Su,Ya Wang,Tianyu Xie,Qingsong Hu,Fazhan Shi,Jiangfeng Du">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07307v1.html">Single-cell identification with quantum-enhanced nuclear magnetic resonance</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ quant-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhiyuan Zhao, Qian Shi, Shaoyi Xu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel label-free single-cell identification method utilizing quantum-enhanced Nuclear Magnetic Resonance (NMR) with diamond nitrogen-vacancy centers. The technique enables the distinction of individual cells, specifically two human tumor cell lines, based on their intrinsic proton ($^1$H) spin-lattice ($T_1$) relaxation times, which act as a unique physicochemical signature.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostics</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Rare Cell Analysis</span>
                    
                    <span class="domain-tag">Cell Biology Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07307v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07307v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07307v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07307v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07275v1"
                     data-domains="Dermatology,Oncology (Skin Cancer),Diagnostic Imaging,Pathology (Digital)"
                     data-keywords="Skin Lesion Segmentation,Deep Learning,Attention Mechanisms,Multi-Scale Features,Medical Image Analysis,Encoder-Decoder Network,Dermatology,Image Segmentation"
                     data-authors="Siyu Wang,Hua Wang,Huiyu Li,Fan Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07275v1.html">Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Siyu Wang, Hua Wang, Huiyu Li et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an innovative attention-guided multi-scale encoder-decoder network for precise skin lesion segmentation, specifically addressing challenges like irregular shapes and low contrast. By integrating novel modules‚ÄîMulti-Resolution Multi-Channel Fusion (MRCF), Cross-Mix Attention Module (CMAM), and an External Attention Bridge (EAB)‚Äîthe model extracts rich, cross-scale features and mitigates information loss. Evaluations demonstrate its superior accuracy and robustness compared to existing deep learning approaches.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Oncology (Skin Cancer)</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Pathology (Digital)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07275v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07275v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07275v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07275v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07253v1"
                     data-domains="Endoscopy,Minimally Invasive Surgery,Surgical Imaging"
                     data-keywords="Endoscopic Video Enhancement,Generative Adversarial Network,Real-time,Degradation-aware,Contrastive Learning,Surgical Imaging,Deep Learning,Image Quality"
                     data-authors="Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07253v1.html">DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Handing Xu, Zhenguo Nie, Tairan Peng et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DGGAN, a novel degradation-aware Generative Adversarial Network framework designed for real-time, high-quality enhancement of degraded endoscopic videos. By extracting and propagating degradation representations across frames using contrastive learning and a fusion mechanism, the method effectively overcomes computational limitations of existing deep learning approaches. It achieves a superior balance of performance and efficiency crucial for intraoperative use, thereby offering a practical pathway for clinical application.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Endoscopy</span>
                    
                    <span class="domain-tag">Minimally Invasive Surgery</span>
                    
                    <span class="domain-tag">Surgical Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07253v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07253v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07253v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07253v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07251v1"
                     data-domains="Radiology,Diagnostic Imaging,Oncology,Medical AI"
                     data-keywords="medical imaging,contrast enhancement,diffusion models,anatomy-aware AI,CT scans,cancer detection,image quality,deep learning"
                     data-authors="Junqi Liu,Zejun Wu,Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Ibrahim E. Hamamci,Sezgin Er,Tianyu Lin,Yi Luo,Szymon P≈Çotka,Bjoern Menze,Daguang Xu,Kai Ding,Kang Wang,Yang Yang,Yucheng Tang,Alan L. Yuille,Zongwei Zhou">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07251v1.html">See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Junqi Liu, Zejun Wu, Pedro R. A. S. Bassi et al.
                </div>

                <div class="paper-summary">
                    This paper introduces SMILE, an anatomy-aware diffusion model designed for contrast enhancement in medical images without introducing common artifacts like organ distortion or false findings. SMILE selectively enhances clinically relevant regions by learning organ shapes and contrast dynamics, demonstrating superior image quality and significant improvements in cancer detection, particularly from non-contrast CT scans.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07251v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07251v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07251v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07251v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07249v1"
                     data-domains="Clinical Decision Support Systems,Predictive Health Analytics,Medical Imaging Diagnostics,Precision Medicine,Public Health Interventions,Patient Risk Stratification"
                     data-keywords="Fair Machine Learning,Algorithmic Bias,Influence Function,Sample Reweighting,Pre-processing,Healthcare AI,Demographic Parity,Equalized Odds"
                     data-authors="Jingran Yang,Min Zhang,Lingfeng Zhang,Zhaohui Wang,Yonggang Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07249v1.html">IFFair: Influence Function-driven Sample Reweighting for Fair Classification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jingran Yang, Min Zhang, Lingfeng Zhang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces IFFair, a novel pre-processing method designed to mitigate bias in machine learning models by dynamically reweighting training samples. Leveraging influence functions, IFFair adjusts sample weights based on their disparate impact on different unprivileged groups, without altering the model's architecture or decision boundaries. Experiments show that IFFair successfully reduces bias across multiple established fairness metrics while achieving a better trade-off between utility and fairness compared to existing pre-processing techniques.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Predictive Health Analytics</span>
                    
                    <span class="domain-tag">Medical Imaging Diagnostics</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                    <span class="domain-tag">Public Health Interventions</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07249v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07249v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07249v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07249v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07241v1"
                     data-domains="Neurology,Oncology,Radiology,Medical Imaging"
                     data-keywords="Brain Tumor,MRI Classification,Hybrid Deep Learning,Radiomics,SqueezeNet,EfficientNet,Automated Diagnosis,Clinical Decision Support"
                     data-authors="Md. Srabon Chowdhury,Syeda Fahmida Tanzim,Sheekar Banerjee,Ishtiak Al Mamoon,AKM Muzahidul Islam">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07241v1.html">Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Md. Srabon Chowdhury, Syeda Fahmida Tanzim, Sheekar Banerjee et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Squeezed-Eff-Net, a novel hybrid deep learning model combining SqueezeNet v1 and EfficientNet-B0, enhanced with handcrafted radiomic features for highly accurate and computationally efficient brain tumor classification from MRI scans. The model achieved 99.08% accuracy with Test Time Augmentation (TTA) on the Nickparvar Brain Tumor MRI dataset, demonstrating near-clinical reliability for automated diagnosis while maintaining computational efficiency.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07241v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07241v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07241v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07241v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07224v1"
                     data-domains="Radiology,Neuroradiology,Oncology,Medical Imaging,Computer-Aided Diagnosis"
                     data-keywords="Deep Learning,Medical Image Segmentation,Explainable AI (XAI),Shapley Values,MRI,Brain Tumor,Clinical Interpretability,Model Reliability"
                     data-authors="Tianyi Ren,Daniel Low,Pittra Jaengprajak,Juampablo Heras Rivera,Jacob Ruzevick,Mehmet Kurt">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07224v1.html">Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tianyi Ren, Daniel Low, Pittra Jaengprajak et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the critical need for explainability in deep learning medical image segmentation by proposing novel Shapley-derived agreement and uncertainty metrics. By analyzing contrast-level Shapley values across different MRI sequences and model architectures, the study demonstrates that higher model performance correlates with greater agreement with clinical imaging priorities and lower uncertainty, thus offering interpretable proxies for model reliability.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Neuroradiology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Computer-Aided Diagnosis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07224v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07224v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07224v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07224v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07206v1"
                     data-domains="Oncology,Nuclear Medicine,Radiology,Hematology"
                     data-keywords="Lymphoma,FDG-PET/CT,Deep Learning,Automated Staging,Lugano Classification,Segmentation,Treatment Stratification,nnU-Net"
                     data-authors="Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07206v1.html">AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Boyang Pan, Zeyu Zhang, Hongyu Meng et al.
                </div>

                <div class="paper-summary">
                    This paper introduces AutoLugano, a novel deep learning framework designed for fully automated, end-to-end lymphoma classification and Lugano staging directly from baseline FDG-PET/CT scans. The system integrates lesion segmentation, anatomical localization, and automated staging, demonstrating robust performance in identifying regional involvement and accurately stratifying patients into therapeutic groups (Limited vs. Advanced Stage).
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Nuclear Medicine</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Hematology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07206v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07206v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07206v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07206v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07191v1"
                     data-domains="Radiology,Diagnostic Imaging,Surgical Planning,Radiation Oncology,Medical Image Analysis"
                     data-keywords="Medical Image Segmentation,Reflectance Model,Level Set Method,Bias Field Correction,Retinex,Variational Model,ADMM,Structural Prior"
                     data-authors="Wenqi Zhao,Jiacheng Sang,Fenghua Cheng,Yonglu Shu,Dong Li,Xiaofeng Yang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07191v1.html">RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Wenqi Zhao, Jiacheng Sang, Fenghua Cheng et al.
                </div>

                <div class="paper-summary">
                    RefLSM proposes a novel variational Reflectance-based Level Set Model that explicitly integrates Retinex-inspired reflectance decomposition to address challenges in medical image segmentation due to intensity inhomogeneity and noise. By segmenting the illumination-invariant reflectance component, combined with a linear structural prior and a relaxed binary level-set for stable evolution, RefLSM achieves superior accuracy, robustness, and efficiency over existing level set methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Surgical Planning</span>
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07191v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07191v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07191v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07191v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07190v1"
                     data-domains="Medical Imaging,Diagnostic Radiology,Pathology"
                     data-keywords="Medical Image Classification,Topological Data Analysis,Persistent Homology,Persistence Diagrams,Multi-scale Features,Multi-filtration,Deep Learning,Anatomical Structures"
                     data-authors="Pengfei Gu,Huimin Li,Haoteng Tang,Dongkuan,Xu,Erik Enriquez,DongChul Kim,Bin Fu,Danny Z. Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07190v1.html">Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Pengfei Gu, Huimin Li, Haoteng Tang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel topology-guided framework for medical image classification that integrates multi-scale and multi-filtration persistent topological features, addressing limitations of current deep networks that often miss fundamental anatomical structures. It employs a "vineyard" algorithm to consolidate multi-scale persistence diagrams and a cross-attention network to process these richer topological representations, fusing them with features from conventional deep learning backbones. The approach demonstrates significant improvements in recognizing complex anatomical structures and achieving robust medical image classification on public datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07190v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07190v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07190v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07190v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07178v1"
                     data-domains="Clinical Decision Support,Medical Diagnostics,Predictive Analytics in Healthcare,Medical Imaging Interpretation"
                     data-keywords="Explainable AI,SHAP,Large Language Model,Contextual Explanations,Healthcare AI,Model Interpretability,User Evaluation,GPT"
                     data-authors="Latifa Dwiyanti,Sergio Ryan Wibisono,Hidetaka Nambo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07178v1.html">ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Latifa Dwiyanti, Sergio Ryan Wibisono, Hidetaka Nambo
                </div>

                <div class="paper-summary">
                    This paper introduces ContextualSHAP, a Python package that extends the SHAP XAI method by integrating it with Large Language Models (LLMs), specifically OpenAI's GPT, to generate contextualized textual explanations. Addressing SHAP's limitation in providing meaningful context for non-technical users, the tool uses user-defined parameters to tailor explanations. Preliminary user evaluations in a healthcare case study suggest that these combined visual-textual explanations are perceived as more understandable and contextually appropriate than visual-only outputs.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Diagnostics</span>
                    
                    <span class="domain-tag">Predictive Analytics in Healthcare</span>
                    
                    <span class="domain-tag">Medical Imaging Interpretation</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07178v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07178v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07178v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07178v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07153v1"
                     data-domains="Infectious Disease Epidemiology,Public Health,Zoonoses,Disease Surveillance,One Health"
                     data-keywords="Bat monitoring,Fixed sensing,Zoonotic disease,Disease surveillance,Acoustic detection,Infrared sensing,Radar,Wildlife epidemiology"
                     data-authors="Maatla Sefawe,Sravya Ganti,Julianna Segalla,Erwei He,Isaac Tourner,Julia Gersey">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07153v1.html">A Structured Review of Fixed and Multimodal Sensing Techniques for Bat Monitoring</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ eess.SY</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Maatla Sefawe, Sravya Ganti, Julianna Segalla et al.
                </div>

                <div class="paper-summary">
                    This structured review synthesizes recent advancements in fixed sensing modalities, such as infrared sensors, cameras, radar, and acoustic detectors, used for monitoring bat populations. It provides essential context on bat biology and comprehensively analyzes each technology's unique challenges, contributions, coverage, applications, accuracy, and limitations. The paper aims to guide future research by offering a detailed overview of these techniques crucial for ecological research and, critically, understanding disease spread.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Infectious Disease Epidemiology</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Zoonoses</span>
                    
                    <span class="domain-tag">Disease Surveillance</span>
                    
                    <span class="domain-tag">One Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07153v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07153v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07153v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07153v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07148v1"
                     data-domains="Neuro-oncology,Pathology,Diagnostic Imaging,Histopathology"
                     data-keywords="Brain Cancer,Fractal Analysis,Multifractal Analysis,Inverse Participation Ratio (IPR),Tissue Microstructure,Diagnosis,Heterogeneity,Histopathology"
                     data-authors="Mousa Alrubayan,Santanu Maity,Prabhakar Pradhan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07148v1.html">Quantitative Characterization of Brain Tissue Alterations in Brain Cancer Using Fractal, Multifractal, and IPR Metrics</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mousa Alrubayan, Santanu Maity, Prabhakar Pradhan
                </div>

                <div class="paper-summary">
                    This paper presents a multiparametric framework combining fractal analysis, multifractal analysis, and Inverse Participation Ratio (IPR) analysis to quantitatively characterize structural alterations between healthy and cancerous brain tissues. By analyzing brightfield microscopy images, the framework effectively differentiated diseased tissues through metrics highlighting increased heterogeneity, local structural variations, and nanoscale disorder. This robust approach offers significant potential to improve microscopic diagnostic methods for brain cancer detection.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neuro-oncology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07148v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07148v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07148v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07148v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07081v1"
                     data-domains="Cardiology,Geriatrics,Hospital Medicine,Health Informatics,Predictive Analytics,Population Health"
                     data-keywords="Heart failure,Readmission prediction,Clinical notes,Large Language Models (LLMs),Multi-agent system,Risk factors,Electronic Health Records (EHRs),Interpretability"
                     data-authors="Rongjia Zhou,Chengzhuo Li,Carl Yang,Jiaying Lu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07081v1.html">ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Rongjia Zhou, Chengzhuo Li, Carl Yang et al.
                </div>

                <div class="paper-summary">
                    ClinNoteAgents is an LLM-based multi-agent system designed to predict and interpret 30-day heart failure (HF) readmission risk from free-text clinical notes. It transforms unstructured notes into structured clinical/social risk factors and clinician-style abstractions, demonstrating strong performance in risk factor identification and readmission prediction. This approach offers a scalable and interpretable solution, reducing reliance on structured data and manual efforts in healthcare systems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Hospital Medicine</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Predictive Analytics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07081v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07081v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07081v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07081v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07061v1"
                     data-domains="Neurology,Neuropathology,Diagnostic Imaging,Geriatrics,Neurodegenerative Diseases"
                     data-keywords="Alzheimer's disease,fractal analysis,multifractal analysis,inverse participation ratio (IPR),microstructural alterations,early diagnosis,biomarkers,brain tissue complexity"
                     data-authors="Santanu Maity,Mousa Alrubayan,Mohammad Moshahid Khan,Prabhakar Pradhan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07061v1.html">Alterations of brain tissue structural complexity and disorder in Alzheimer's disease (AD): Fractal, multifractal, fractal transformation, and disorder strength analyses</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-08</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Santanu Maity, Mousa Alrubayan, Mohammad Moshahid Khan et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a multiparametric framework combining fractal, multifractal, a novel fractal functional distribution analysis, and inverse participation ratio (IPR) analysis to quantify subtle microstructural alterations in Alzheimer's disease (AD) brain tissue. The framework effectively identifies threshold-dependent fractal signatures and demonstrates increased structural disorder with disease progression, providing a robust method for early diagnosis and improved pathological assessment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Neuropathology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Neurodegenerative Diseases</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07061v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07061v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07061v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07061v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07051v1"
                     data-domains="Fetal Ultrasound Imaging,Radiology (CT imaging),Pulmonary Embolism Detection,Diagnostic Imaging,Obstetrics"
                     data-keywords="Medical Image Segmentation,UNet Variant,Deformable Convolutions,Parameter-Free Attention,SimAM,Lightweight Model,Real-time Segmentation,Deep Learning"
                     data-authors="Adnan Munir,Shujaat Khan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07051v1.html">DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Adnan Munir, Shujaat Khan
                </div>

                <div class="paper-summary">
                    DAUNet is a novel, lightweight UNet variant designed for medical image segmentation that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to enhance spatial adaptability and context-aware feature fusion without increasing model complexity. It significantly outperforms state-of-the-art models on challenging medical datasets (fetal ultrasound, CT pulmonary embolism) in segmentation accuracy and efficiency, proving suitable for resource-constrained clinical deployment. The model demonstrates robustness to missing context and low-contrast regions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Fetal Ultrasound Imaging</span>
                    
                    <span class="domain-tag">Radiology (CT imaging)</span>
                    
                    <span class="domain-tag">Pulmonary Embolism Detection</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Obstetrics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07051v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07051v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07051v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07051v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07040v1"
                     data-domains="Disease Diagnosis,Systems Biology,Genomics,Proteomics,Drug Discovery,Precision Medicine,Pathology"
                     data-keywords="Biological Networks,Graph-to-Image Transformation,Convolutional Neural Networks,Scalability,Interpretability,Multi-modal Integration,Disease Diagnosis,Omics Data,Systems Biology"
                     data-authors="Sakib Mostafa,Lei Xing,Md. Tauhidul Islam">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07040v1.html">Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sakib Mostafa, Lei Xing, Md. Tauhidul Islam
                </div>

                <div class="paper-summary">
                    The paper introduces Graph2Image, a novel framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This enables the use of Convolutional Neural Networks (CNNs) for scalable, memory-efficient, and interpretable analysis, overcoming limitations of traditional graph-based methods. Graph2Image demonstrated up to 67.2% improved classification accuracy on large biological networks and allowed analysis of networks exceeding 1 billion nodes on a personal computer.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Disease Diagnosis</span>
                    
                    <span class="domain-tag">Systems Biology</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Proteomics</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07040v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07040v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07040v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07040v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.07021v1"
                     data-domains="Cardiology,Clinical Decision Support,Diagnostic Medicine,Critical Care"
                     data-keywords="Deep Learning,Electrocardiogram (ECG),Clinical Knowledge Transfer,Multimodal Learning,Self-supervised Learning,Interpretability,Medical AI,MIMIC-IV-ECG"
                     data-authors="Jose Geraldo Fernandes,Luiz Facury de Souza,Pedro Robles Dutenhefner,Gisele L. Pappa,Wagner Meira">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.07021v1.html">Transferring Clinical Knowledge into ECGs Representation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jose Geraldo Fernandes, Luiz Facury de Souza, Pedro Robles Dutenhefner et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel three-stage deep learning paradigm to overcome the "black box" nature of ECG classification models by transferring knowledge from multimodal clinical data (lab exams, vitals) into a unimodal ECG encoder. The approach creates an ECG representation enriched with contextual clinical information, leading to improved diagnostic accuracy and providing physiologically grounded explanations through the prediction of associated laboratory abnormalities directly from the ECG.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Critical Care</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.07021v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.07021v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.07021v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.07021v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06990v1"
                     data-domains="Neuro-oncology,Neurosurgery,Radiation Oncology,Medical Imaging (Radiology),Computational Neuroscience"
                     data-keywords="Glioblastoma Multiforme (GBM),Reinforcement Learning (RL),Multi-Agent Systems,Encoder-Decoder,Diffusion Models,Vision Transformers,Medical Imaging,Brain Tumor,Surgical Resection,Treatment Planning"
                     data-authors="Krishna Arun,Moinak Bhattachrya,Paras Goel">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06990v1.html">Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Krishna Arun, Moinak Bhattachrya, Paras Goel
                </div>

                <div class="paper-summary">
                    This research presents an end-to-end AI system designed to assist doctors with both the diagnosis and treatment planning of Glioblastoma Multiforme (GBM), the deadliest human cancer. Utilizing a sequential decision-making framework for diagnosis and a Multi-Agent Reinforcement Learning (MARL) system with generative encoder-decoder architecture agents for treatment planning, the project significantly reduces computational costs and tumor progression inference time while improving segmentation accuracy, projecting to increase GBM survival rates.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neuro-oncology</span>
                    
                    <span class="domain-tag">Neurosurgery</span>
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Medical Imaging (Radiology)</span>
                    
                    <span class="domain-tag">Computational Neuroscience</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06990v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06990v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06990v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06990v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06987v1"
                     data-domains="Pharmaceuticals,Drug Discovery,Medicinal Chemistry,Pharmacology"
                     data-keywords="Crystal Structure Prediction,Diffusion Models,Organic Crystals,Computational Chemistry,Machine Learning,Pharmaceuticals,Drug Discovery,All-Atom Modeling"
                     data-authors="Emily Jin,Andrei Cristian Nica,Mikhail Galkin,Jarrid Rector-Brooks,Kin Long Kelvin Lee,Santiago Miret,Frances H. Arnold,Michael Bronstein,Avishek Joey Bose,Alexander Tong,Cheng-Hao Liu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06987v1.html">OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Emily Jin, Andrei Cristian Nica, Mikhail Galkin et al.
                </div>

                <div class="paper-summary">
                    OXtal introduces a novel, large-scale all-atom diffusion model for accurately predicting 3D molecular crystal structures from 2D chemical graphs, a long-standing challenge in computational chemistry. By utilizing a unique lattice-free training scheme and data augmentation, it achieves orders-of-magnitude improvements in accuracy and efficiency over prior methods, holding significant implications for pharmaceutical development.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmaceuticals</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06987v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06987v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06987v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06987v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06971v1"
                     data-domains="Public Health,Epidemiology,Hospital Management,Infectious Diseases (COVID-19),Healthcare Analytics"
                     data-keywords="Local Differential Privacy,Prediction with Expert Advice,Machine Learning,Privacy Amplification,Random Walks,Ensemble Learning,COVID-19,Healthcare Data"
                     data-authors="Ben Jacobsen,Kassem Fawaz">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06971v1.html">Prediction with Expert Advice under Local Differential Privacy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ben Jacobsen, Kassem Fawaz
                </div>

                <div class="paper-summary">
                    This paper addresses the problem of prediction with expert advice under local differential privacy (LDP), proposing two novel algorithms, RW-AdaBatch and RW-Meta, that significantly improve upon classical methods. RW-AdaBatch introduces a novel privacy amplification mechanism, while RW-Meta provides a general method for privately selecting complex, non-trivial learning experts, a significant advancement over prior work. Evaluated on real-world COVID-19 hospital data, RW-Meta demonstrated superior performance, outperforming both classical LDP and state-of-the-art central DP algorithms in predicting high COVID patient density.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Hospital Management</span>
                    
                    <span class="domain-tag">Infectious Diseases (COVID-19)</span>
                    
                    <span class="domain-tag">Healthcare Analytics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06971v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06971v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06971v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06971v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06944v1"
                     data-domains="Healthcare Utilization,Medical Diagnostics,Treatment Planning,Public Health,Health Equity,Algorithmic Bias in Medicine,Health Policy"
                     data-keywords="AI fairness,human-centered AI,fairness metrics,trade-offs,multi-objective optimization,healthcare AI,ethical AI,bias mitigation"
                     data-authors="Munshi Mahbubur Rahman,Shimei Pan,James R. Foulds">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06944v1.html">A Unifying Human-Centered AI Fairness Framework</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Munshi Mahbubur Rahman, Shimei Pan, James R. Foulds
                </div>

                <div class="paper-summary">
                    This paper introduces a unifying human-centered AI fairness framework designed to systematically integrate eight distinct fairness metrics, addressing the challenge of navigating trade-offs between competing fairness notions and predictive accuracy. The framework allows stakeholders to assign weights across multiple fairness objectives, facilitating the practical and value-sensitive deployment of fair AI systems in critical domains like healthcare.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Healthcare Utilization</span>
                    
                    <span class="domain-tag">Medical Diagnostics</span>
                    
                    <span class="domain-tag">Treatment Planning</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Health Equity</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06944v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06944v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06944v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06944v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06921v1"
                     data-domains="Neurosurgery,Surgical Education,Medical Robotics,Medical Imaging Analysis"
                     data-keywords="NeuroABench,Multimodal Large Language Models,Neurosurgical Anatomy,Surgical Video Understanding,Anatomical Identification,Benchmarking,Surgical Education"
                     data-authors="Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06921v1.html">NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ziyang Song, Zelin Zang, Xiaofan Ye et al.
                </div>

                <div class="paper-summary">
                    This paper introduces NeuroABench, the first multimodal benchmark specifically designed to evaluate Multimodal Large Language Models' (MLLMs) ability to identify neurosurgical anatomy from video. It reveals that current state-of-the-art MLLMs have significant limitations in this critical area, achieving only 40.87% accuracy, which is comparable to a novice trainee but substantially below the average neurosurgical trainee's performance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurosurgery</span>
                    
                    <span class="domain-tag">Surgical Education</span>
                    
                    <span class="domain-tag">Medical Robotics</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06921v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06921v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06921v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06921v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06919v1"
                     data-domains="Oncology,Clinical Trials,Pharmacovigilance,Patient Safety,Health Informatics"
                     data-keywords="PRO-CTCAE,MedDRA,adverse events,patient-reported outcomes,semantic space,spectral analysis,oncology,clinical trials,Safeterm"
                     data-authors="Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06919v1.html">Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Francois Vandenhende, Anna Georgiou, Michalis Georgiou et al.
                </div>

                <div class="paper-summary">
                    This paper presents an automated method for selecting an optimal, minimal, yet comprehensive subset of PRO-CTCAE items for oncology clinical trials. The approach leverages historical adverse event data and MedDRA semantics, encoded within a semantic space called Safeterm, to balance comprehensive symptom coverage with minimizing patient burden and improving compliance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Patient Safety</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06919v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06919v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06919v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06919v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06888v1"
                     data-domains="pediatrics,neonatology,respiratory medicine,developmental neurology,sleep medicine"
                     data-keywords="infant respiration monitoring,contactless sensing,computer vision,small data,video dataset,deep learning,optical flow,SIDS"
                     data-authors="Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06888v1.html">Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Liyang Song, Hardik Bishnoi, Sai Kumar Reddy Manne et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the critical lack of resources for developing contactless infant respiration monitoring by introducing the AIR-400 dataset, the largest public video dataset for infant respiration estimation. It also develops the first reproducible pipelines utilizing infant-specific ROI detection and spatiotemporal neural processing with optical flow, establishing crucial state-of-the-art benchmarks to overcome small data limitations.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">pediatrics</span>
                    
                    <span class="domain-tag">neonatology</span>
                    
                    <span class="domain-tag">respiratory medicine</span>
                    
                    <span class="domain-tag">developmental neurology</span>
                    
                    <span class="domain-tag">sleep medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06888v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06888v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06888v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06888v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06849v1"
                     data-domains="Oncology,Radiology,Medical Imaging,Orthopedics"
                     data-keywords="Vertebral Metastasis,Weakly Supervised Segmentation,CT,Diffusion Autoencoder,Generative Editing,Hide-and-Seek Attribution,Oncology,Medical Imaging"
                     data-authors="Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik M√∂ller">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06849v1.html">Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Matan Atad, Alexander W. Marka, Lisa Steinhelfer et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel weakly supervised method for segmenting vertebral metastases (both lytic and blastic) in CT scans, trained exclusively on vertebra-level healthy/malignant labels without requiring expensive voxel-level annotations. By combining a Diffusion Autoencoder for classifier-guided healthy edits with a unique "Hide-and-Seek Attribution" mechanism to identify truly malignant regions, the method achieves strong segmentation performance, significantly outperforming baselines and demonstrating the feasibility of generating reliable lesion masks from coarse labels.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06849v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06849v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06849v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06849v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.06848v1"
                     data-domains="Public Health,Environmental Health,Infectious Disease Epidemiology,Preventive Medicine,Global Health"
                     data-keywords="Pathogen Detection,Water Quality Monitoring,Edge Computing,Vision-Sensor Fusion,Machine Learning,Public Health,Microbial Contamination,Real-time Monitoring"
                     data-authors="Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.06848v1.html">AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-07</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sepyan Purnama Kristanto, Lutfi Hakim, Hermansyah
                </div>

                <div class="paper-summary">
                    AquaFusionNet is a lightweight, cross-modal vision-sensor fusion framework designed for real-time pathogen detection and water quality anomaly prediction on edge devices, specifically addressing the limitations of fragmented monitoring in small-scale drinking water systems. It unifies microscopic imaging and physicochemical sensor data via a gated cross-attention mechanism, achieving high accuracy (94.8% mAP, 96.3% anomaly prediction) and robustness at low power (4.8W) in field deployments. The system significantly reduces common failure modes of unimodal detectors under challenging environmental conditions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Environmental Health</span>
                    
                    <span class="domain-tag">Infectious Disease Epidemiology</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Global Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.06848v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.06848v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.06848v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.06848v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-09 06:27:27</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>