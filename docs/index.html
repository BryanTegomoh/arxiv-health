<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">11</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">11</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">42</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Radiology (4), Diagnostic Imaging (2), Medical Imaging (2)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Radiology">Radiology (4)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (2)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (2)</option>
                        
                        <option value="Medical AI">Medical Ai (2)</option>
                        
                        <option value="Neurology">Neurology (1)</option>
                        
                        <option value="Neurodevelopmental Disorders">Neurodevelopmental Disorders (1)</option>
                        
                        <option value="Psychiatry">Psychiatry (1)</option>
                        
                        <option value="Geriatrics">Geriatrics (1)</option>
                        
                        <option value="Neuroscience Research">Neuroscience Research (1)</option>
                        
                        <option value="Child Neurology">Child Neurology (1)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2601.08818v1"
                     data-domains="Neurology,Neurodevelopmental Disorders,Psychiatry,Geriatrics,Neuroscience Research,Radiology,Child Neurology"
                     data-keywords="brain network,cortical folding,gyri,sulci,anatomical landmarks,neuroimaging,brain disorders,network analysis"
                     data-authors="Chao Cao,Tong Chen,Nan Zhao,Minheng Chen,Michael Qu,Zeyu Zhang,Xiao Shi,Xiang Li,Tianming Liu,Lu Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08818v1.html">Gyral-Sulcal-Net: An Integrated Network Representation of Brain Folding Patterns</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.NC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chao Cao, Tong Chen, Nan Zhao et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Gyral-Sulcal-Net (GS-Net), a novel unified network representation that integrates fine-scale gyral and sulcal folding patterns of the brain, addressing the limitations of traditional regional-level network studies. Evaluated across over 1,600 brains of diverse ages and health conditions, GS-Net effectively represents cortical folding from a network perspective and offers a comprehensive framework for studying brain networks in health and disease.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Neurodevelopmental Disorders</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Neuroscience Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08818v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08818v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08818v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08818v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08797v1"
                     data-domains="Dentistry,Oral and Maxillofacial Radiology,Diagnostic Imaging,Preventive Dentistry"
                     data-keywords="dental disease detection,radiographs,context-aware AI,semantic segmentation,oral anatomy,deep learning,computer-aided diagnosis,medical image analysis"
                     data-authors="Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08797v1.html">DentalX: Context-Aware Dental Disease Detection with Radiographs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhi Qin Tan, Xiatian Zhu, Owen Addison et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DentalX, a novel context-aware deep learning model designed to improve the detection of subtle dental diseases from radiographs. By integrating a structural context extraction module that leverages an auxiliary task of semantic segmentation for dental anatomy, DentalX significantly outperforms existing object detection methods, demonstrating a mutual benefit between anatomical understanding and disease identification.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dentistry</span>
                    
                    <span class="domain-tag">Oral and Maxillofacial Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Preventive Dentistry</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08797v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08797v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08797v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08797v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08784v1"
                     data-domains="Diagnostic AI,Predictive Analytics in Healthcare,Clinical Decision Support Systems,Healthcare Resource Allocation,Medical Imaging Analysis,Personalized Medicine"
                     data-keywords="Machine Learning Fairness,Sheaf Diffusion,Graph Models,Individual Fairness,Group Fairness,Responsible AI,SHAP values,Bias Mitigation"
                     data-authors="Arturo P√©rez-Peralta,Sandra Ben√≠tez-Pe√±a,Rosa E. Lillo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08784v1.html">On the use of graph models to achieve individual and group fairness</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ stat.ML</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Arturo P√©rez-Peralta, Sandra Ben√≠tez-Pe√±a, Rosa E. Lillo
                </div>

                <div class="paper-summary">
                    This paper introduces a novel theoretical framework utilizing Sheaf Diffusion, dynamical systems, and homology to model and achieve fairness in Machine Learning algorithms. It projects input data into a bias-free space, offering a unified method to address both individual and group bias through different network topologies, while also providing interpretable SHAP values. The proposed models demonstrate satisfactory performance on standard fairness benchmarks, analyzing accuracy-fairness trade-offs and hyper-parameter effects.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Diagnostic AI</span>
                    
                    <span class="domain-tag">Predictive Analytics in Healthcare</span>
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Healthcare Resource Allocation</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08784v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08784v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08784v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08784v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08226v1"
                     data-domains="Radiology,Medical Imaging,Diagnostic Medicine,Artificial Intelligence in Healthcare"
                     data-keywords="Multi-modal AI,Chest X-ray,Disease Detection,Vision Transformer,LLM,RAG,Hallucination,Calibration"
                     data-authors="Alexander Shim,Khalil Saieh,Samuel Clarke">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08226v1.html">Knowledge-based learning in Text-RAG and Image-RAG</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Alexander Shim, Khalil Saieh, Samuel Clarke
                </div>

                <div class="paper-summary">
                    This research developed and compared a multi-modal approach, combining Vision Transformers (EVA-ViT) with Large Language Models (LLMs) like Llama or ChatGPT, within Text-RAG and Image-RAG frameworks for disease detection in chest X-ray images. The study aimed to mitigate LLM hallucination and improve diagnostic accuracy. It found that Text-RAG effectively reduces hallucination using external knowledge, while Image-RAG improves prediction confidence and calibration via KNN methods, with GPT LLM demonstrating superior performance, lower hallucination, and better calibration than Llama.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Healthcare</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08226v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08226v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08226v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08226v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08192v1"
                     data-domains="Radiology,Pulmonology,Medical Informatics"
                     data-keywords="medical imaging,vision-language models,agentic AI,chest X-ray,report generation,object detection,clinical error correction,spatial grounding"
                     data-authors="Md. Faiyaz Abdullah Sayeedi,Rashedur Rahman,Siam Tahsin Bhuiyan,Sefatul Wasi,Ashraful Islam,Saadia Binte Alam,AKM Mahbubur Rahman">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08192v1.html">Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Md. Faiyaz Abdullah Sayeedi, Rashedur Rahman, Siam Tahsin Bhuiyan et al.
                </div>

                <div class="paper-summary">
                    This paper introduces R^4, an agentic framework designed to enhance medical image analysis by overcoming the limitations of single-pass, black-box Vision-Language Models (VLMs) in reasoning, safety, and spatial grounding. R^4 decomposes the medical imaging workflow into four coordinated agents‚ÄîRouter, Retriever, Reflector, and Repairer‚Äîthat iteratively refine free-text reports and bounding box detections. Evaluated on chest X-ray analysis, the framework significantly boosts performance in report generation and weakly supervised detection without requiring gradient-based fine-tuning, making VLMs more reliable for clinical interpretation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08192v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08192v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08192v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08192v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08183v1"
                     data-domains="Gastroenterology,Endoscopy,Medical Imaging,Clinical Informatics"
                     data-keywords="Multimodal Large Language Models (MLLMs),Gastrointestinal Endoscopy,Clinical Benchmark,Diagnostic Reasoning,Spatial Grounding,Medical AI,Hallucination,GI-Bench"
                     data-authors="Yan Zhu,Te Luo,Pei-Yao Fu,Zhen Zhang,Zi-Long Wang,Yi-Fan Qu,Zi-Han Geng,Jia-Qi Xu,Lu Yao,Li-Yun Ma,Wei Su,Wei-Feng Chen,Quan-Lin Li,Shuo Wang,Ping-Hong Zhou">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08183v1.html">GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yan Zhu, Te Luo, Pei-Yao Fu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces GI-Bench, a comprehensive benchmark to systematically evaluate Multimodal Large Language Models (MLLMs) in gastrointestinal endoscopy across a five-stage clinical workflow, comparing their performance against human endoscopists and trainees. It reveals that while top MLLMs can rival junior endoscopists in diagnostic reasoning, they suffer from critical 'spatial grounding bottlenecks' and a 'fluency-accuracy paradox' due to hallucination in report generation. The study highlights significant strengths in reasoning but profound weaknesses in visual localization and factual reporting, indicating a knowledge-experience dissociation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Endoscopy</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08183v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08183v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08183v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08183v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08165v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical AI,Computational Pathology"
                     data-keywords="Medical Vision-Language Pre-training,Contrastive Learning,Representation Learning,Radiology Reports,Image Classification,Image Segmentation,Object Detection,Semantic Alignment"
                     data-authors="Phuoc-Nguyen Bui,Toan Duc Nguyen,Junghyun Bum,Duc-Tai Le,Hyunseung Choo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08165v1.html">Representation Learning with Semantic-aware Instance and Sparse Token Alignments</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Phuoc-Nguyen Bui, Toan Duc Nguyen, Junghyun Bum et al.
                </div>

                <div class="paper-summary">
                    This paper introduces SISTA, a novel multi-level alignment framework for medical vision-language pre-training (VLP) that addresses the common issue of false negatives in contrastive learning. By incorporating inter-report semantic similarity and aligning image patches with relevant word tokens, SISTA learns more robust representations. This approach significantly improves performance across various downstream tasks, including image classification, segmentation, and object detection, particularly demonstrating effectiveness with limited labeled data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08165v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08165v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08165v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08165v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08147v1"
                     data-domains="Cardiology,Oncology,Nutrition,Pharmacology,Preventive Medicine,Computational Biology"
                     data-keywords="network pharmacology,dietary flavonoids,polypharmacology,multi-target drugs,cardiovascular disease,antineoplastic,food-drug interactions,systems pharmacology"
                     data-authors="Koyo Fujisaki,Osei Horikoshi,Yukitoshi Nagahara,Kengo Morohashi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08147v1.html">Network Pharmacology Framework Characterizes Polypharmacological Properties of Dietary Flavonoids: Integration of Computational, Experimental, and Epidemiological Evidence</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Koyo Fujisaki, Osei Horikoshi, Yukitoshi Nagahara et al.
                </div>

                <div class="paper-summary">
                    This paper establishes a network pharmacology framework to characterize the polypharmacological properties of dietary flavonoids, integrating computational, experimental, and epidemiological validation. It revealed that flavonoids exhibit significantly higher multi-target activity compared to FDA-approved drugs, primarily targeting proteins relevant to cardiovascular and antineoplastic diseases. The framework successfully predicted and validated flavonoid bioactivity in cell assays and generated evidence-based food-therapeutic combinations, demonstrating its potential to bridge nutritional science and systems pharmacology.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Nutrition</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08147v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08147v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08147v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08147v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08134v1"
                     data-domains="Clinical Reasoning,Medical AI,Diagnostic Support,Clinical Decision Support"
                     data-keywords="Large Reasoning Models,Confidence Estimation,Miscalibration,High-Stakes Domains,Clinical Reasoning,Benchmark,AUROC,ECE"
                     data-authors="Reza Khanmohammadi,Erfan Miahi,Simerjot Kaur,Ivan Brugere,Charese H. Smiley,Kundan Thind,Mohammad M. Ghassemi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08134v1.html">How Reliable are Confidence Estimators for Large Reasoning Models? A Systematic Benchmark on High-Stakes Domains</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Reza Khanmohammadi, Erfan Miahi, Simerjot Kaur et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Reasoning Model Confidence estimation Benchmark (RMCB) to rigorously evaluate methods for assessing the reliability of Large Reasoning Models (LRMs) in high-stakes domains, including clinical reasoning. The study reveals a fundamental trade-off between a confidence estimator's ability to discriminate between correct/incorrect answers (AUROC) and its calibration (ECE), with no single current representation-based method excelling at both. It further demonstrates that increasing architectural complexity does not consistently improve performance over simpler baselines, suggesting limitations in current approaches.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Reasoning</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                    <span class="domain-tag">Diagnostic Support</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08134v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08134v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08134v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08134v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08127v1"
                     data-domains="Kidney pathology,Skin pathology,Breast pathology,Prostate pathology,Oncology (implied for lesion diagnosis)"
                     data-keywords="Histopathology,Lesion synthesis,Diffusion models,Generative AI,Data augmentation,Medical imaging,Digital pathology,AI in medicine"
                     data-authors="Mohamad Koohi-Moghadam,Mohammad-Ali Nikouei Mahani,Kyongtae Tyler Bae">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08127v1.html">PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-13</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mohamad Koohi-Moghadam, Mohammad-Ali Nikouei Mahani, Kyongtae Tyler Bae
                </div>

                <div class="paper-summary">
                    PathoGen introduces a novel diffusion-based generative model for synthesizing realistic lesions within benign histopathology images, addressing the severe constraint of scarce expert-annotated data for AI model development. This approach generates high-fidelity lesion morphologies, significantly enhancing downstream AI segmentation performance, particularly in data-scarce regimes, and simultaneously overcomes the manual annotation bottleneck.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Kidney pathology</span>
                    
                    <span class="domain-tag">Skin pathology</span>
                    
                    <span class="domain-tag">Breast pathology</span>
                    
                    <span class="domain-tag">Prostate pathology</span>
                    
                    <span class="domain-tag">Oncology (implied for lesion diagnosis)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08127v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08127v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08127v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08127v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.08078v1"
                     data-domains="cs.CV"
                     data-keywords="cs.CV,cs.CE,cs.CL"
                     data-authors="Guoping Xu,Jayaram K. Udupa,Weiguo Lu,You Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.08078v1.html">Exploiting DINOv3-Based Self-Supervised Features for Robust Few-Shot Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-12</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Guoping Xu, Jayaram K. Udupa, Weiguo Lu et al.
                </div>

                <div class="paper-summary">
                    Deep learning-based automatic medical image segmentation plays a critical role in clinical diagnosis and treatment planning but remains challenging in few-shot scenarios due to the scarcity of annotated training data. Recently, self-supervised foundation models such as DINOv3, which were trained on ...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.CV</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.08078v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.08078v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.08078v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.08078v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2026-01-14 06:16:29</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>