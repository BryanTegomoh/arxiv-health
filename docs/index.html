<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">11</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">11</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">42</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Medical Imaging (3), Radiology (3), Diagnostic Imaging (2)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Medical Imaging">Medical Imaging (3)</option>
                        
                        <option value="Radiology">Radiology (3)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (2)</option>
                        
                        <option value="Cardiology">Cardiology (2)</option>
                        
                        <option value="Oncology">Oncology (2)</option>
                        
                        <option value="Neurology">Neurology (2)</option>
                        
                        <option value="Clinical risk assessment">Clinical Risk Assessment (1)</option>
                        
                        <option value="Healthcare administration">Healthcare Administration (1)</option>
                        
                        <option value="Diagnostic support">Diagnostic Support (1)</option>
                        
                        <option value="Patient care enhancement">Patient Care Enhancement (1)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2512.19691v1"
                     data-domains="Clinical risk assessment,Healthcare administration,Diagnostic support,Patient care enhancement"
                     data-keywords="clinical risk scores,medical benchmarks,label noise,physician-in-the-loop,reinforcement learning,agentic verifiers,MedCalc-Bench,model alignment"
                     data-authors="Junze Ye,Daniel Tawfik,Alex J. Goodell,Nikhil V. Kotha,Mark K. Buyyounouski,Mohsen Bayati">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19691v1.html">Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Junze Ye, Daniel Tawfik, Alex J. Goodell et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the critical issue of label noise in AI benchmarks for clinical risk score calculation, specifically MedCalc-Bench, which is prone to errors from LLM-based generation. It introduces a systematic physician-in-the-loop pipeline with agentic verifiers and automated triage to audit and correct these benchmarks. The study demonstrates that training a Qwen3-8B model on these corrected labels significantly improves accuracy by 8.7% compared to training on the original noisy data, underscoring the necessity of rigorous benchmark maintenance for model alignment in safety-critical domains.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical risk assessment</span>
                    
                    <span class="domain-tag">Healthcare administration</span>
                    
                    <span class="domain-tag">Diagnostic support</span>
                    
                    <span class="domain-tag">Patient care enhancement</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19691v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19691v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19691v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19691v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.19676v1"
                     data-domains="Neuroradiology,Urology,Diagnostic Imaging,Medical Physics"
                     data-keywords="MRI super-resolution,Vision Mamba,selective state-space models,computational efficiency,anatomical detail,7T MRI,prostate MRI,deep learning"
                     data-authors="Mojtaba Safari,Shansong Wang,Vanessa L Wildman,Mingzhe Hu,Zach Eidex,Chih-Wei Chang,Erik H Middlebrooks,Richard L. J Qiu,Pretesh Patel,Ashesh B. Jania,Hui Mao,Zhen Tian,Xiaofeng Yang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19676v1.html">Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mojtaba Safari, Shansong Wang, Vanessa L Wildman et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an efficient Vision Mamba-based super-resolution (SR) framework for MRI, addressing the trade-off between fidelity and computational cost in existing deep learning methods. By utilizing multi-head selective state-space models and hybrid scanning, the proposed framework significantly enhances MRI resolution and anatomical detail while demonstrating state-of-the-art accuracy and exceptional efficiency across brain and prostate MRI datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neuroradiology</span>
                    
                    <span class="domain-tag">Urology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19676v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19676v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19676v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19676v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.19663v1"
                     data-domains="Ophthalmology,Diabetology,Diagnostic Imaging,Clinical Informatics,Preventive Medicine"
                     data-keywords="Diabetic Retinopathy,Multimodal Transformers,Cross-Modal Alignment,Vision-Language Models,Clinical Text,Fundus Images,Medical AI,Ophthalmology"
                     data-authors="Argha Kamal Samanta,Harshika Goyal,Vasudha Joshi,Tushar Mungle,Pabitra Mitra">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19663v1.html">Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel knowledge-enhanced multimodal transformer framework designed to overcome the limitations of general-domain vision-language models like CLIP in medical applications, specifically for diabetic retinopathy (DR) diagnosis. By integrating retinal images, clinical text, and structured patient data through specialized encoders and a joint transformer, the model achieves superior cross-modal alignment and state-of-the-art DR classification performance. The framework dramatically improves medical image-text retrieval accuracy and exhibits strong zero-shot generalization.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Diabetology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19663v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19663v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19663v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19663v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.19602v1"
                     data-domains="Cardiology,Medical Imaging,Diagnostic AI,Clinical Prediction Models,Biomedical Informatics"
                     data-keywords="multimodal learning,missing data,vision-tabular learning,medical imaging,cardiac MRI,robustness,contrastive learning,deep learning,biobanks,data augmentation"
                     data-authors="Marta Hasny,Laura Daza,Keno Bressem,Maxime Di Folco,Julia Schnabel">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19602v1.html">No Data? No Problem: Robust Vision-Tabular Learning with Missing Values</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Marta Hasny, Laura Daza, Keno Bressem et al.
                </div>

                <div class="paper-summary">
                    RoVTL is a novel framework addressing the critical challenge of missing tabular data in multimodal learning, particularly prevalent in medical biobanks. It achieves robust performance across varying levels of tabular data availability by employing contrastive pretraining with missingness augmentation and a unique fine-tuning approach, demonstrating superior results on medical imaging and natural image datasets compared to prior methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic AI</span>
                    
                    <span class="domain-tag">Clinical Prediction Models</span>
                    
                    <span class="domain-tag">Biomedical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19602v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19602v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19602v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19602v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.19584v1"
                     data-domains="Nuclear Medicine,Medical Imaging,Radiology,Oncology,Neurology,Cardiology"
                     data-keywords="Dynamic PET,Parametric Imaging,Patlak Model,Diffusion Model,Kinetic Modeling,Total-body PET,Image Quality,Physiological Parameters"
                     data-authors="Ziqian Huang,Boxiao Yu,Siqi Li,Savas Ozdemir,Sangjin Bae,Jae Sung Lee,Guobao Wang,Kuang Gong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19584v1.html">Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ziqian Huang, Boxiao Yu, Siqi Li et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel diffusion model-based kinetic modeling framework designed to improve the quality of parametric images estimated from dynamic PET, using the Patlak model as a specific example. By leveraging a pre-trained diffusion model's score function as a prior and incorporating kinetic model constraints, the framework successfully addresses issues of low image quality and ill-posedness in dynamic PET, demonstrating promising performance in total-body PET datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Nuclear Medicine</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19584v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19584v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19584v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19584v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.19534v1"
                     data-domains="Orbital Surgery,Craniofacial Surgery,Oculoplastic Surgery,Reconstructive Surgery,Surgical Oncology"
                     data-keywords="orbital implants,preformed plates,virtual surgery,3D Slicer,surgical planning,quantitative analysis,patient-specific,open-source"
                     data-authors="Chi Zhang,Braedon Gunn,Andrew M. Read-Fuller">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19534v1.html">SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chi Zhang, Braedon Gunn, Andrew M. Read-Fuller
                </div>

                <div class="paper-summary">
                    This paper introduces SlicerOrbitSurgerySim, an open-source 3D Slicer extension designed to quantitatively compare the fit of preformed orbital plates in a patient-specific virtual planning environment. The platform addresses a critical gap by providing tools for interactive virtual registration, objective evaluation through reproducible plate-to-orbit distance metrics, and visualization to improve preoperative decision-making and reduce intraoperative modifications.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orbital Surgery</span>
                    
                    <span class="domain-tag">Craniofacial Surgery</span>
                    
                    <span class="domain-tag">Oculoplastic Surgery</span>
                    
                    <span class="domain-tag">Reconstructive Surgery</span>
                    
                    <span class="domain-tag">Surgical Oncology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19534v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19534v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19534v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19534v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.19512v1"
                     data-domains="Clinical Anatomy,Surgery,Medical Imaging Analysis,Diagnostic Medicine,Radiology"
                     data-keywords="Multimodal Large Language Models,Anatomy Reasoning,Medical Imaging,Curriculum Learning,Question Augmentation,Group Relative Policy Optimization,Surgical Images,Medical VQA"
                     data-authors="Ziyang Song,Zelin Zang,Zuyao Chen,Xusheng Liang,Dong Yi,Jinlin Wu,Hongbin Liu,Jiebo Luo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19512v1.html">Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ziyang Song, Zelin Zang, Zuyao Chen et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Anatomy-R1, a novel approach designed to significantly enhance anatomical reasoning in Multimodal Large Language Models (MLLMs) for medical imaging, particularly clinical anatomical surgical images. It addresses key limitations of existing methods like Group Relative Policy Optimization (GRPO) by proposing Anatomical Similarity Curriculum Learning (ASCL) and Group Diversity Question Augmentation (GDQA) to improve knowledge sharing and diversify reasoning paths, leading to superior performance on medical visual question answering benchmarks.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Anatomy</span>
                    
                    <span class="domain-tag">Surgery</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19512v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19512v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19512v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19512v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.19486v1"
                     data-domains="Radiology,Oncology (Radiotherapy Planning),Neurosurgery,Cardiac Imaging,Image-Guided Surgery,Neurology,Radiation Therapy"
                     data-keywords="Deformable Medical Image Registration,Combinatorial Explosion,Deep Learning,Dynamic Networks,Receptive Fields,Attention Mechanism,Medical Imaging,Image Alignment"
                     data-authors="Shaochen Bi,Yuting He,Weiming Wang,Hao Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19486v1.html">Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shaochen Bi, Yuting He, Weiming Wang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Dynamic Stream Network (DySNet) to address the combinatorial explosion problem in Deformable Medical Image Registration (DMIR), where dual image inputs lead to exponentially growing feature combinations and interfering features. DySNet dynamically adjusts receptive fields and network weights through its Adaptive Stream Basin (AdSB) and Dynamic Stream Attention (DySA) modules, enabling it to focus on correlated feature relationships and eliminate irrelevant ones. Extensive experiments demonstrate that DySNet consistently outperforms state-of-the-art DMIR methods, showcasing superior accuracy and generalization ability.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Oncology (Radiotherapy Planning)</span>
                    
                    <span class="domain-tag">Neurosurgery</span>
                    
                    <span class="domain-tag">Cardiac Imaging</span>
                    
                    <span class="domain-tag">Image-Guided Surgery</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19486v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19486v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19486v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19486v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.19485v1"
                     data-domains="Oncology,Medical Imaging,Computational Biology,Personalized Medicine"
                     data-keywords="Evolutionary Therapy,Metastatic Cancer,NSCLC,Biomarkers,RECIST,Volumetric Imaging,Tumor Dynamics,Adaptive Therapy"
                     data-authors="Eva Moln√°rov√°,Ties A. Mulders,Marcela Spee-Dropkov√°,Louise M. Spekking,Sepinoud Azimi,Irene Grossmann,Anne-Marie C. Dingemans,Kate≈ôina Sta≈àkov√°">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.19485v1.html">Enabling Evolutionary Therapy in Metastatic Cancer Lacking Serum Biomarkers</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-22</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ q-bio.PE</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Eva Moln√°rov√°, Ties A. Mulders, Marcela Spee-Dropkov√° et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the critical need for reliable tumor monitoring to enable evolutionary therapy (ET) in metastatic cancers, particularly those lacking serum biomarkers like NSCLC. It demonstrates, using a virtual patient model, that current standard RECIST 1.1 criteria are inadequate, while 3D volumetric imaging accurately captures tumor burden and dynamics, which are essential for guiding ET.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.19485v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.19485v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.19485v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.19485v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.18500v1"
                     data-domains="Public Health (via food security),Nutritional Epidemiology,Global Health Initiatives,Preventive Medicine (indirectly, through stable food supply)"
                     data-keywords="Plant Disease Detection,Deep Learning,ResNet50,Fine-tuning,Agricultural AI,Computer Vision,Food Security,Global Health"
                     data-authors="Santwana Sagnika,Manav Malhotra,Ishtaj Kaur Deol,Soumyajit Roy,Swarnav Kumar">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.18500v1.html">PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-20</span>
                        <span class="relevance">‚≠ê 0.70</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol et al.
                </div>

                <div class="paper-summary">
                    PlantDiseaseNet-RT50 is a novel fine-tuned ResNet50 architecture designed for automated, high-accuracy plant disease detection, achieving approximately 98% accuracy, precision, and recall across various crop species. This computationally efficient deep learning solution addresses the significant threat of plant diseases to agricultural productivity, moving beyond traditional manual inspection methods to support global food security.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health (via food security)</span>
                    
                    <span class="domain-tag">Nutritional Epidemiology</span>
                    
                    <span class="domain-tag">Global Health Initiatives</span>
                    
                    <span class="domain-tag">Preventive Medicine (indirectly, through stable food supply)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.18500v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.18500v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.18500v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.18500v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.18473v1"
                     data-domains="cs.LG"
                     data-keywords="cs.LG"
                     data-authors="Khaled Berkani">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.18473v1.html">APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-20</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Khaled Berkani
                </div>

                <div class="paper-summary">
                    We propose APC-GNN++, an adaptive patient-centric Graph Neural Network for diabetes classification. Our model integrates context-aware edge attention, confidence-guided blending of node features and graph representations, and neighborhood consistency regularization to better capture clinically meani...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.LG</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.18473v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.18473v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.18473v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.18473v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-23 06:15:48</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>