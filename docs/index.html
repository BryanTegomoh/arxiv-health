<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">43</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">43</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">122</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Radiology (14), Pathology (10), Neurology (7)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Radiology">Radiology (14)</option>
                        
                        <option value="Pathology">Pathology (10)</option>
                        
                        <option value="Neurology">Neurology (7)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (6)</option>
                        
                        <option value="Oncology">Oncology (6)</option>
                        
                        <option value="Digital Health">Digital Health (6)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (5)</option>
                        
                        <option value="Telemedicine">Telemedicine (4)</option>
                        
                        <option value="Cardiology">Cardiology (4)</option>
                        
                        <option value="Medical Imaging Analysis">Medical Imaging Analysis (3)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2512.03020v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical Physics,Biomedical Engineering"
                     data-keywords="MRI reconstruction,Unrolled networks,Conditional probability flow ODEs,Diffusion models,Deep learning,Image acceleration,Computational stability,Convergence,k-space undersampling"
                     data-authors="Kehan Qi,Saumya Gupta,Qingqiao Hu,Weimin Lyu,Chao Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03020v1.html">Unrolled Networks are Conditional Probability Flows in MRI Reconstruction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Kehan Qi, Saumya Gupta, Qingqiao Hu et al.
                </div>

                <div class="paper-summary">
                    This paper establishes a theoretical connection between unrolled networks and conditional probability flow ODEs for MRI reconstruction, proving that unrolled networks are discrete implementations of these flows. Leveraging this insight, the authors propose Flow-Aligned Training (FLAT), a novel method that significantly enhances reconstruction stability and convergence, yielding high-quality MRI images with substantially fewer iterations than diffusion models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03020v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03020v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03020v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03020v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02983v1"
                     data-domains="Oncology,Immunology,Pathology,Biomarker Discovery"
                     data-keywords="Spatial Proteomics,Tumor Microenvironment (TME),Prototypical Part Networks,Interpretable AI,Precision Oncology,Spatial Biomarkers,Concept Learning,Lung Cancer"
                     data-authors="Louis McConnell,Jieran Sun,Theo Maffei,Raphael Gottardo,Marianna Rapsomaniki">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02983v1.html">ProteinPNet: Prototypical Part Networks for Concept Learning in Spatial Proteomics</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Louis McConnell, Jieran Sun, Theo Maffei et al.
                </div>

                <div class="paper-summary">
                    ProteinPNet is a novel framework utilizing prototypical part networks to discover interpretable tumor microenvironment (TME) motifs directly from spatial proteomics data. This approach learns discriminative spatial prototypes that align with different tumor subtypes and capture features related to immune infiltration and tissue modularity. The findings highlight the potential of prototype-based learning for revealing interpretable spatial biomarkers crucial for advancing precision oncology.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Immunology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Biomarker Discovery</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02983v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02983v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02983v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02983v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02917v1"
                     data-domains="Nuclear Medicine,Oncology,Radiology,Medical Imaging"
                     data-keywords="PETfectior,deep learning,denoising,SUVmax,low-count PET,18F-FDG,oncology,image quality"
                     data-authors="Yamila Rotstein Habarnau,Nicol√°s Bustos,Paola Corona,Christian Gonz√°lez,Sonia Traverso,Federico Matorra,Francisco Funes,Juan Mart√≠n Giraut,Laura Pelegrina,Gabriel Bruno,Mauro Nam√≠as">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02917v1.html">Maintaining SUV Accuracy in Low-Count PET with PETfectior: A Deep Learning Denoising Solution</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yamila Rotstein Habarnau, Nicol√°s Bustos, Paola Corona et al.
                </div>

                <div class="paper-summary">
                    This study validates PETfectior, a deep learning denoising solution, for generating high-quality PET images from half-counting statistics, aiming to reduce patient radiation exposure and costs. It demonstrates that images processed with PETfectior from reduced acquisition data maintain excellent lesion detectability, quantitative SUVmax accuracy, and subjective image quality comparable to standard-of-care scans.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Nuclear Medicine</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02917v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02917v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02917v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02917v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02816v1"
                     data-domains="Traditional Chinese Medicine,Clinical Decision Support,Medical Informatics,Medical Ethics"
                     data-keywords="Traditional Chinese Medicine,Large Language Models,Syndrome Differentiation and Treatment,Benchmark,Clinical Evaluation,AI in Medicine,Prescription Congruence,Medical Ethics"
                     data-authors="Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02816v1.html">A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Kunning Li, Jianbin Guo, Zhaoyang Shang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces TCM-BEST4SDT, a novel, comprehensive, clinical case-based benchmark designed to evaluate Large Language Models (LLMs) in the complex domain of Traditional Chinese Medicine's (TCM) Syndrome Differentiation and Treatment (SDT). Spearheaded by TCM experts, the benchmark addresses the limitations of existing evaluations by incorporating treatment decision-making and utilizing a specialized reward model to quantify prescription-syndrome congruence. Its effectiveness was validated across 15 mainstream LLMs, and it is now publicly available to foster intelligent TCM research.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Traditional Chinese Medicine</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                    <span class="domain-tag">Medical Ethics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02816v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02816v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02816v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02816v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02814v1"
                     data-domains="Radiology,Diagnostic Imaging,Clinical Examinations"
                     data-keywords="Radiology Reporting,Agentic AI,Large Language Models (LLM),Quality Control,Medical Imaging,Vision-Language Models,Automated Reporting,Clinical Efficiency"
                     data-authors="Yongrui Yu,Zhongzhen Huang,Linjie Mu,Shaoting Zhang,Xiaofan Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02814v1.html">Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yongrui Yu, Zhongzhen Huang, Linjie Mu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Radiologist Copilot, an agentic AI assistant leveraging large language models and a suite of orchestrated tools to automate the entire radiology reporting process, including crucial quality control. It addresses the limitations of existing methods that primarily focus on report generation, by emulating a radiologist's workflow to produce accurate, complete, and efficient reports.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Clinical Examinations</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02814v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02814v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02814v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02814v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02702v1"
                     data-domains="Epidemiology,Metabolic Health,Cardiovascular Research,Obesity Research,Aging Research,Population Health,Biomarker Discovery"
                     data-keywords="Whole-body MRI,Image Registration,UK Biobank,Tissue Segmentation,Graph-cut,Dice Score,Inter-subject Registration,Voxel-wise Analysis"
                     data-authors="Yasemin Utkueri,Elin Lundstr√∂m,H√•kan Ahlstr√∂m,Johan √ñfverstedt,Joel Kullberg">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02702v1.html">Tissue-mask supported inter-subject whole-body image registration in the UK Biobank -- A method benchmarking study</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yasemin Utkueri, Elin Lundstr√∂m, H√•kan Ahlstr√∂m et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel sex-stratified inter-subject whole-body MR image registration method designed for UK Biobank data, augmenting intensity-based graph-cut registration with subcutaneous adipose tissue and muscle masks. The proposed method significantly outperforms intensity-only registration and two established methods (uniGradICON, MIRTK) in terms of Dice scores and reduced label error frequency across 71 tissue masks. This advancement enables more robust body-wide spatial standardization and accurate voxel-wise correlation analyses between imaging biomarkers and non-imaging health data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Metabolic Health</span>
                    
                    <span class="domain-tag">Cardiovascular Research</span>
                    
                    <span class="domain-tag">Obesity Research</span>
                    
                    <span class="domain-tag">Aging Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02702v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02702v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02702v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02702v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02669v1"
                     data-domains="Neurology,Speech-Language Pathology,Neuroscience,Geriatrics"
                     data-keywords="Dysarthria Classification,Speech Analysis,Neurodegenerative Diseases,XGBoost,Deep Learning,Vision Transformer,BiLSTM,Severity Assessment"
                     data-authors="Gauri Deshpande,Harish Battula,Ashish Panda,Sunil Kumar Kopparapu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02669v1.html">SAND Challenge: Four Approaches for Dysartria Severity Classification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.SD</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Gauri Deshpande, Harish Battula, Ashish Panda et al.
                </div>

                <div class="paper-summary">
                    This paper investigates four distinct modeling approaches‚ÄîViT-OF, 1D-CNN, BiLSTM-OF, and Hierarchical XGBoost‚Äîfor classifying dysarthria severity into five classes using a common speech dataset. While the feature-engineered Hierarchical XGBoost ensemble achieved the highest macro-F1 score of 0.86, deep learning models also demonstrated competitive performance (0.70 macro-F1), offering complementary perspectives for automated speech analysis in neurodegenerative diseases.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Speech-Language Pathology</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02669v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02669v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02669v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02669v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02626v1"
                     data-domains="Neurology,Epilepsy,Biomedical Engineering,Wearable Technology,Digital Health"
                     data-keywords="Transfer Learning,Tensor Kernel Machines,Seizure Detection,EEG,Wearable Devices,Machine Learning,Adaptive Models,Low-rank Tensor Networks"
                     data-authors="Seline J. S. de Rooij,Borb√°la Hunyadi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02626v1.html">Adapting Tensor Kernel Machines to Enable Efficient Transfer Learning for Seizure Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Seline J. S. de Rooij, Borb√°la Hunyadi
                </div>

                <div class="paper-summary">
                    This paper proposes Adapt-TKM, an efficient transfer learning method using tensor kernel machines, for seizure detection. It leverages low-rank tensor networks and regularization to adapt patient-independent models with minimal patient-specific data. The method achieves superior seizure detection performance on behind-the-ear EEG with significantly fewer parameters and faster inference, making it suitable for resource-constrained wearable devices.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Epilepsy</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                    <span class="domain-tag">Wearable Technology</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02626v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02626v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02626v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02626v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02528v1"
                     data-domains="Epidemiology,Public Health,Infectious Disease Modeling,Computational Biology"
                     data-keywords="Stochastic epidemic models,Bayesian inference,Particle Filters,Conditional Normalizing Flows,SIR model,SEIR model,Parameter estimation,Public health,Epidemic forecasting"
                     data-authors="Vincent Wieland,Nils Wassmuth,Lorenzo Contento,Martin K√ºhn,Jan Hasenauer">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02528v1.html">Assessment of Simulation-based Inference Methods for Stochastic Compartmental Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vincent Wieland, Nils Wassmuth, Lorenzo Contento et al.
                </div>

                <div class="paper-summary">
                    This paper compares two advanced Bayesian inference methods, Particle Filters (PF) and Conditional Normalizing Flows (CNF), for estimating parameters in stochastic compartmental epidemic models (SIR and SEIR) where likelihoods are intractable. The study demonstrates that these likelihood-free methods provide accurate, robust, and effective inference capabilities for capturing epidemic dynamics, offering valuable prediction for public health decisions, and showing operational robustness under real-world conditions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Infectious Disease Modeling</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02528v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02528v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02528v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02528v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02520v1"
                     data-domains="Radiology,Medical Imaging Analysis,Neuroimaging"
                     data-keywords="zero-shot anomaly detection,anomaly segmentation,consistent anomalies,Vision Transformers,graph-based framework,3D medical imaging,MRI,volumetric tokenization"
                     data-authors="Tai Le-Gia">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02520v1.html">On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tai Le-Gia
                </div>

                <div class="paper-summary">
                    This dissertation addresses the 'consistent anomalies' problem in zero-shot anomaly classification and segmentation (AC/AS), a failure mode where recurring similar anomalies bias detection methods. It introduces CoDeGraph, a graph-based framework leveraging novel phenomena (similarity scaling, neighbor-burnout) to filter these anomalies. The work extends this capability to 3D medical imaging via a training-free volumetric tokenization strategy and bridges batch-based with text-based zero-shot methods, providing both theoretical understanding and practical solutions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Neuroimaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02520v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02520v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02520v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02520v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02499v1"
                     data-domains="Neurology,Stroke Medicine,Clinical Informatics,Emergency Medicine"
                     data-keywords="Acute Ischemic Stroke,Outcome Prediction,Large Language Models,Chain-of-Thought,LLaMA-3-8B,Clinical Notes,Modified Rankin Scale,AI in Medicine"
                     data-authors="Yongkai Liu,Helena Feng,Bin Jiang,Yixin Wang,Max Wintermark,David S. Liebeskind,Michael Moseley,Maarten Lansberg,Gregory Albers,Jeremy Heit,Greg Zaharchuk">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02499v1.html">COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yongkai Liu, Helena Feng, Bin Jiang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces COPE (Chain-Of-Thought Prediction Engine), an open-source large language model (LLM) framework leveraging a two-step Chain-of-Thought (CoT) approach with LLaMA-3-8B models to predict 90-day functional outcomes in acute ischemic stroke (AIS) from unstructured clinical notes. COPE demonstrated performance comparable to GPT-4.1 and superior to other baselines, offering an accurate, interpretable, and privacy-preserving solution for clinical outcome prediction.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Stroke Medicine</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02499v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02499v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02499v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02499v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02497v1"
                     data-domains="MRI,CT,Ultrasound,Pathology,Dermoscopy,Ophthalmology (OCT),Radiology (Chest X-ray)"
                     data-keywords="Test Time Adaptation,Domain Shift,Medical Image Segmentation,Benchmark,Deep Learning,Clinical Deployment,Cross-modality,Robustness"
                     data-authors="Wenjing Yu,Shuo Jiang,Yifei Chen,Shuo Chang,Yuanhan Wang,Beining Wu,Jie Dong,Mingxuan Liu,Shenghao Zhu,Feiwei Qin,Changmiao Wang,Qiyuan Tian">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02497v1.html">A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Wenjing Yu, Shuo Jiang, Yifei Chen et al.
                </div>

                <div class="paper-summary">
                    MedSeg-TTA introduces a comprehensive benchmark for evaluating twenty Test Time Adaptation (TTA) methods in medical image segmentation across seven diverse imaging modalities using unified protocols. The study reveals that no single TTA paradigm universally outperforms others, with different methods excelling under specific conditions like mild appearance shifts (input-level) or in boundary segmentation (feature/output-level). Crucially, the benchmark highlights significant performance degradation of several methods under large domain shifts, emphasizing the need for principled method selection for reliable clinical deployment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">MRI</span>
                    
                    <span class="domain-tag">CT</span>
                    
                    <span class="domain-tag">Ultrasound</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Dermoscopy</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02497v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02497v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02497v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02497v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02491v1"
                     data-domains="Clinical research,Epidemiology,Public health policy,Pharmacovigilance,Health economics,Personalized medicine"
                     data-keywords="Causal inference,Robustness auditing,Data quality,Cardinality repairs,Machine unlearning,Observational data,Sensitivity analysis,Data integrity"
                     data-authors="Yarden Gabbay,Haoquan Guan,Shaull Almagor,El Kindi Rezig,Brit Youngmann,Babak Salimi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02491v1.html">Stress-Testing Causal Claims via Cardinality Repairs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.DB</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yarden Gabbay, Haoquan Guan, Shaull Almagor et al.
                </div>

                <div class="paper-summary">
                    This paper introduces SubCure, a framework for auditing the robustness of causal claims derived from observational data, which are often fragile to minor data errors. SubCure employs 'cardinality repairs' to identify minimal sets of data tuples or subpopulations whose removal shifts a causal effect estimate into a user-specified target range, thereby quantifying sensitivity and pinpointing influential data regions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical research</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Public health policy</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Health economics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02491v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02491v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02491v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02491v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02485v1"
                     data-domains="Radiology,Pathology,Medical Imaging,Diagnostic Medicine,Clinical Decision Support Systems"
                     data-keywords="Vision-Language Models,Multi-agent Systems,Medical Diagnosis,Visual Question Answering,Reasoning Detachment,Unidirectional Convergence,Evidence Auditing,Clinical Decision Support"
                     data-authors="Qianhan Feng,Zhongzhen Huang,Yakun Zhu,Xiaofan Zhang,Qi Dou">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02485v1.html">UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Qianhan Feng, Zhongzhen Huang, Yakun Zhu et al.
                </div>

                <div class="paper-summary">
                    UCAgents is a hierarchical multi-agent framework designed to improve the reliability and efficiency of Vision-Language Models (VLMs) in medical diagnosis by addressing reasoning detachment and textual noise. It enforces unidirectional convergence through structured evidence auditing, formalizing a dual visual-textual noise bottleneck via information theory to suppress rhetorical drift. The framework achieves superior diagnostic accuracy (+6.0% on PathVQA) with 87.7% lower token cost, confirming its diagnostic reliability and computational efficiency critical for real-world clinical deployment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02485v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02485v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02485v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02485v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02438v1"
                     data-domains="Radiology,Pathology,Medical Imaging Analysis,Clinical Decision Support,Medical Informatics"
                     data-keywords="medical vision-language pretraining,momentum self-distillation,contrastive learning,gradient accumulation,computational efficiency,few-shot learning,zero-shot classification,multimodal learning"
                     data-authors="Phuc Pham,Nhu Pham,Ngoc Quoc Ly">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02438v1.html">Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Phuc Pham, Nhu Pham, Ngoc Quoc Ly
                </div>

                <div class="paper-summary">
                    This paper introduces a novel momentum self-distillation (MSD) method combined with gradient accumulation to enhance medical Vision-Language Pretraining (VLP) under limited computing resources. The approach efficiently addresses the large batch size requirement of contrastive learning, achieving competitive zero-shot classification, significant few-shot adaptation improvements (over 90% AUC-ROC), and 2-3% better retrieval performance on a single GPU. It aims to make robust medical VLMs more accessible by reducing computational demands while improving performance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02438v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02438v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02438v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02438v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02437v1"
                     data-domains="Ophthalmology,Neuro-ophthalmology,Medical Imaging,Preventive Medicine"
                     data-keywords="Glaucoma Detection,Causal Representation Learning,HSIC Disentanglement,Convolutional VAE,Graph Autoencoder,Lightweight AI,Retinal Fundus Images,Optic Nerve Damage"
                     data-authors="Daeyoung Kim">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02437v1.html">LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Daeyoung Kim
                </div>

                <div class="paper-summary">
                    This research introduces LightHCG, a novel and extremely lightweight Convolutional VAE-based model designed for causal glaucoma detection, addressing limitations of existing AI approaches. By employing HSIC-based latent space disentanglement and Graph Autoencoders, LightHCG learns true causal relationships among glaucoma-related physical factors in the optic nerve. This results in superior classification performance, a significant reduction in model parameters (93-99% less weights), and enhanced capabilities for AI-driven intervention analysis and clinical simulations.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Neuro-ophthalmology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02437v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02437v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02437v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02437v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02364v1"
                     data-domains="Infectious Diseases,Pulmonology,Radiology,Diagnostic Imaging,Public Health"
                     data-keywords="Tuberculosis,Machine Learning,Deep Learning,Chest X-ray,ResNet-50,SqueezeNet,Diagnosis,Computer Vision"
                     data-authors="Daanish Hindustani,Sanober Hindustani,Preston Nguyen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02364v1.html">Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Daanish Hindustani, Sanober Hindustani, Preston Nguyen
                </div>

                <div class="paper-summary">
                    This study investigates the efficacy of machine learning models, specifically a pretrained ResNet-50 and a general SqueezeNet, for tuberculosis (TB) detection using chest X-ray images. It demonstrates that SqueezeNet significantly outperforms ResNet-50, highlighting deep learning's potential to provide more efficient and accessible TB diagnostics compared to traditional methods. The research suggests crucial implications for early TB identification and treatment, particularly in resource-limited settings.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Infectious Diseases</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02364v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02364v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02364v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02364v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02363v1"
                     data-domains="Healthcare Policies,Patient Information Systems,Clinical Decision Support (indirectly),Medical Information Retrieval,Public Health Services"
                     data-keywords="Question Answering,Domain Adaptation,Knowledge Fusion,Safety-Aware Decoding,Healthcare AI,Natural Language Processing,Trustworthy AI,Large Language Models"
                     data-authors="Lei Fu,Xiang Chen,Kaige Gao Xinyue Huang,Kejian Tong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02363v1.html">Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Lei Fu, Xiang Chen, Kaige Gao Xinyue Huang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces KARMA (Knowledge-Aware Reasoning and Memory-Augmented Adaptation), a novel framework addressing challenges in domain-specific question answering for sensitive service contexts like healthcare policies. KARMA enhances factual consistency and safety by fusing heterogeneous knowledge, dynamically regulating external information, and employing a safety-aware decoder. Experimental results demonstrate KARMA's superior performance in both answer quality and safety compared to strong baselines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Healthcare Policies</span>
                    
                    <span class="domain-tag">Patient Information Systems</span>
                    
                    <span class="domain-tag">Clinical Decision Support (indirectly)</span>
                    
                    <span class="domain-tag">Medical Information Retrieval</span>
                    
                    <span class="domain-tag">Public Health Services</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02363v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02363v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02363v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02363v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02302v1"
                     data-domains="Pathology,Oncology,Histopathology,Medical Imaging AI"
                     data-keywords="Breast Cell Segmentation,Quantum-inspired Enhancement,Extreme Data Constraints,Class Imbalance,Adaptive Loss Function,Weighted Sampling,UNet++,Medical Image Annotation"
                     data-authors="Varun Kumar Dasoju,Qingsu Cheng,Zeyun Yu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02302v1.html">Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Varun Kumar Dasoju, Qingsu Cheng, Zeyun Yu
                </div>

                <div class="paper-summary">
                    This paper presents a novel framework for breast cell segmentation that achieves a Dice score of 95.5% using only 599 training images, addressing significant challenges like extreme data scarcity, severe class imbalance, and high prevalence of negative samples. The approach integrates quantum-inspired edge enhancement, a stabilized multi-component loss function, and complexity-based weighted sampling to improve boundary detection and small lesion identification, thereby drastically reducing the expert annotation time required for medical AI development.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Medical Imaging AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02302v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02302v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02302v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02302v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02265v1"
                     data-domains="pediatric urinary tract infection (UTI) risk prediction,direct anticoagulant bleeding risk prediction"
                     data-keywords="machine learning,fairness,explainability,Shapley values,bias mitigation,healthcare AI,feature importance,clinical trust"
                     data-authors="Joshua Wolff Anderson,Shyam Visweswaran">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02265v1.html">The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Joshua Wolff Anderson, Shyam Visweswaran
                </div>

                <div class="paper-summary">
                    This study investigates the previously under-explored relationship between enforcing fairness constraints and the resulting changes in model explanations within machine learning models, particularly in healthcare contexts. The authors found that improving fairness through bias mitigation techniques significantly alters Shapley-based feature importance rankings, sometimes with differential impacts across different racial subgroups. This highlights the critical need for a holistic assessment of ML models, considering accuracy, fairness, and explainability in conjunction, rather than individually.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">pediatric urinary tract infection (UTI) risk prediction</span>
                    
                    <span class="domain-tag">direct anticoagulant bleeding risk prediction</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02265v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02265v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02265v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02265v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02217v1"
                     data-domains="Pathology,Hematology,Medical Imaging Analysis,Diagnostic AI,Oncology (for blood cancer screening)"
                     data-keywords="Photonic Bayesian Machine,Uncertainty Reasoning,Bayesian Neural Networks,Chaotic Light,Medical Diagnosis,Blood Cell Classification,Out-of-Domain Detection,Trustworthy AI"
                     data-authors="F. Br√ºckerhoff-Pl√ºckelmann,H. Borras,S. U. Hulyal,L. Meyer,X. Ji,J. Hu,J. Sun,B. Klein,F. Ebert,J. Dijkstra,L. McRae,P. Schmidt,T. J. Kippenberg,H. Fr√∂ning,W. Pernice">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02217v1.html">Uncertainty Reasoning with Photonic Bayesian Machines</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> F. Br√ºckerhoff-Pl√ºckelmann, H. Borras, S. U. Hulyal et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel photonic Bayesian machine that leverages the inherent randomness of chaotic light sources to enable high-speed uncertainty reasoning within Bayesian Neural Networks. The system effectively overcomes the bottleneck of pseudo random number generation in digital AI systems, significantly accelerating probabilistic convolutions to 37.5 ps. It demonstrates simultaneous classification and out-of-domain detection of blood cell microscope images, distinguishing between aleatoric and epistemic uncertainties for trustworthy AI applications.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Hematology</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Diagnostic AI</span>
                    
                    <span class="domain-tag">Oncology (for blood cancer screening)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02217v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02217v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02217v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02217v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02201v1"
                     data-domains="Public Health Informatics,Digital Health,Telemedicine,Global Health,Healthcare Accessibility,Medical Communication"
                     data-keywords="ASR,Multilingual Speech Dataset,South African Languages,Healthcare Informatics,Low-Resource Languages,Automatic Speech Recognition,Digital Health,Benchmarking"
                     data-authors="Vukosi Marivatee,Kayode Olaleye,Sitwala Mundia,Andinda Bakainga,Unarine Netshifhefhe,Mahmooda Milanzie,Tsholofelo Hope Mogale,Thapelo Sindane,Zainab Abdulrasaq,Kesego Mokgosi,Chijioke Okorie,Nia Zion Van Wyk,Graham Morrissey,Dale Dunbar,Francois Smit,Tsosheletso Chidi,Rooweither Mabuya,Andiswa Bukula,Respect Mlambo,Tebogo Macucwa,Idris Abdulmumin,and Seani Rananga">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02201v1.html">Swivuriso: The South African Next Voices Multilingual Speech Dataset</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vukosi Marivatee, Kayode Olaleye, Sitwala Mundia et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Swivuriso, a 3000-hour multilingual speech dataset specifically designed to support and benchmark Automatic Speech Recognition (ASR) technologies across seven distinct South African languages. By covering critical domains like agriculture and healthcare, Swivuriso directly addresses significant gaps in existing ASR datasets, aiming to enhance the accessibility and utility of speech technology in these under-resourced languages.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health Informatics</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Global Health</span>
                    
                    <span class="domain-tag">Healthcare Accessibility</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02201v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02201v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02201v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02201v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02198v1"
                     data-domains="Dermoscopy,Endoscopy,Ultrasound"
                     data-keywords="Multifractal Analysis,Neural Networks,Medical Imaging,Semantic Segmentation,Channel Attention,U-Net,Inductive Priors,Deep Learning"
                     data-authors="Miguel L. Martins,Miguel T. Coimbra,Francesco Renna">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02198v1.html">Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Miguel L. Martins, Miguel T. Coimbra, Francesco Renna
                </div>

                <div class="paper-summary">
                    This paper introduces novel Monofractal and Multifractal Recalibration methods as inductive priors, implemented as channel-attention functions, to overcome limitations of existing multifractal deep learning for semantic segmentation. Utilizing a U-Net framework, multifractal recalibration demonstrates substantial performance gains on three diverse medical imaging datasets, while also providing insights into the behavior of attention layers in such architectures.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermoscopy</span>
                    
                    <span class="domain-tag">Endoscopy</span>
                    
                    <span class="domain-tag">Ultrasound</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02198v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02198v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02198v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02198v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02180v1"
                     data-domains="Cardiology,Telemedicine,Digital Health,Preventive Medicine,Diagnostic Medicine"
                     data-keywords="ECG,Foundation Models,Contrastive Learning,Self-Supervised Learning,Cardiovascular Health,Remote Monitoring,Clinical Metadata,Diagnostic Performance"
                     data-authors="Yuxuan Shu,Peter H. Charlton,Fahim Kawsar,Jussi Hernesniemi,Mohammad Malekzadeh">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02180v1.html">CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuxuan Shu, Peter H. Charlton, Fahim Kawsar et al.
                </div>

                <div class="paper-summary">
                    CLEF introduces a novel clinically-guided contrastive learning approach for single-lead ECG foundation models, leveraging established clinical risk scores from routine metadata to improve diagnostic performance. Pretrained on a large dataset of 161K MIMIC-IV patients, CLEF significantly outperforms existing self-supervised baselines across 18 clinical tasks, demonstrating enhanced accuracy and scalability for remote health monitoring.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02180v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02180v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02180v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02180v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02162v1"
                     data-domains="Oncology,Radiology,Pathology,Genomics,Bioinformatics"
                     data-keywords="Somatic Mutations,Medical Imaging,Deep Learning,Variational Autoencoder,Point Clouds,Cancer Diagnosis,Genomic Information,Precision Medicine"
                     data-authors="Rahul Mehta">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02162v1.html">Mapping of Lesion Images to Somatic Mutations</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Rahul Mehta
                </div>

                <div class="paper-summary">
                    This paper introduces LLOST, a deep latent variable model designed to predict patients' somatic mutation profiles directly from medical lesion images. By transforming images into point clouds and utilizing a dual Variational Autoencoder (VAE) architecture with shared and domain-specific latent spaces, the model aims to provide early genetic insights. The research demonstrates the model's ability to predict mutation counts and occurrences, identifying shared patterns indicative of cancer types.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02162v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02162v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02162v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02162v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01986v1"
                     data-domains="Sleep Medicine,Neurology,Digital Health,Public Health,Geriatrics"
                     data-keywords="deep learning,actigraphy,sleep-wake determination,triaxial accelerometry,polysomnography,device-agnostic,sleep disorders,wearable technology"
                     data-authors="Nasim Montazeri,Stone Yang,Dominik Luszczynski,John Zhang,Dharmendra Gurve,Andrew Centen,Maged Goubran,Andrew Lim">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01986v1.html">A robust generalizable device-agnostic deep learning model for sleep-wake determination from triaxial wrist accelerometry</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Nasim Montazeri, Stone Yang, Dominik Luszczynski et al.
                </div>

                <div class="paper-summary">
                    This study developed and validated a robust, generalizable, and device-agnostic deep learning model for accurate sleep-wake determination from triaxial wrist accelerometry. The model demonstrated high performance in detecting sleep and wakefulness, with improved wake detection, and showed consistency across different accelerometer devices and in adults with various sleep disorders.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Sleep Medicine</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01986v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01986v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01986v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01986v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01934v1"
                     data-domains="Hospital Management,Patient Monitoring,Robotic Surgery,Assisted Living,Medical Logistics,Healthcare Security,Telemedicine"
                     data-keywords="Adversarial Trajectory,Multi-Object Tracking,ID-Transfer Attack,Computer Vision,Cybersecurity,Object Association,Autonomous Systems,Physical Attack"
                     data-authors="Chenyi Wang,Yanmao Man,Raymond Muller,Ming Li,Z. Berkay Celik,Ryan Gerdes,Jonathan Petit">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01934v1.html">Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.85</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chenyi Wang, Yanmao Man, Raymond Muller et al.
                </div>

                <div class="paper-summary">
                    This paper introduces AdvTraj, the first online and physical ID-manipulation attack targeting Multi-Object Tracking (MOT) systems. By employing adversarial trajectories, AdvTraj successfully transfers an attacker's ID to a targeted object, achieving 100% success in simulations and high transferability across state-of-the-art MOT algorithms. This research reveals critical vulnerabilities in the object association phase of current MOT systems, paving the way for enhanced robustness.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Hospital Management</span>
                    
                    <span class="domain-tag">Patient Monitoring</span>
                    
                    <span class="domain-tag">Robotic Surgery</span>
                    
                    <span class="domain-tag">Assisted Living</span>
                    
                    <span class="domain-tag">Medical Logistics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01934v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01934v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01934v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01934v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01922v1"
                     data-domains="ophthalmology,radiology,pathology"
                     data-keywords="medical LVLMs,hallucination mitigation,visual-contrastive decoding,token sparsification,medical imaging,factual accuracy,visual question answering,report generation"
                     data-authors="Zahra Mahdavi,Zahra Khodakaramimaghsoud,Hooman Khaloo,Sina Bakhshandeh Taleshani,Erfan Hashemi,Javad Mirzapour Kaleybar,Omid Nejati Manzari">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01922v1.html">Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zahra Mahdavi, Zahra Khodakaramimaghsoud, Hooman Khaloo et al.
                </div>

                <div class="paper-summary">
                    Med-VCD introduces a novel sparse visual-contrastive decoding method to mitigate hallucinations in medical Large Vision Language Models (LVLMs). By employing an efficient token-sparsification strategy, it selectively reinforces visually informed tokens, leading to substantial improvements in factual and hallucination accuracy across diverse medical tasks without the time overhead of previous solutions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">ophthalmology</span>
                    
                    <span class="domain-tag">radiology</span>
                    
                    <span class="domain-tag">pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01922v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01922v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01922v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01922v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01913v1"
                     data-domains="brain,lung,cardiac,abdominal"
                     data-keywords="medical image registration,deep learning,domain-specific designs,architectural trends,U-Net,motion estimation,reproducibility,benchmark"
                     data-authors="Bailiang Jian,Jiazhen Pan,Rohit Jena,Morteza Ghahremani,Hongwei Bran Li,Daniel Rueckert,Christian Wachinger,Benedikt Wiestler">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01913v1.html">Disentangling Progress in Medical Image Registration: Beyond Trend-Driven Architectures towards Domain-Specific Strategies</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Bailiang Jian, Jiazhen Pan, Rohit Jena et al.
                </div>

                <div class="paper-summary">
                    This paper systematically disentangles the contributions of generic architectural trends (e.g., Transformers) versus domain-specific designs (e.g., motion pyramids) in deep learning-based medical image registration. It demonstrates that high-level, registration-specific strategies consistently yield more accurate, smoother, and robust deformations, providing significantly greater performance gains over a baseline U-Net than generic trend-driven blocks. The research advocates for a shift in focus towards refining domain-specific principles for driving progress in this field.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">brain</span>
                    
                    <span class="domain-tag">lung</span>
                    
                    <span class="domain-tag">cardiac</span>
                    
                    <span class="domain-tag">abdominal</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01913v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01913v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01913v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01913v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01885v1"
                     data-domains="Oncology,Pharmacology,Cell Biology,Drug Discovery,Pathology"
                     data-keywords="cell tracking,deep learning,cancer cells,transient fluorescence,mitosis,apoptosis,chemotherapy response,single-cell analysis"
                     data-authors="Florian B√ºrger,Martim Dias Gomes,Nica Gutu,Adri√°n E. Granada,No√©mie Moreau,Katarzyna Bozek">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01885v1.html">TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Florian B√ºrger, Martim Dias Gomes, Nica Gutu et al.
                </div>

                <div class="paper-summary">
                    TransientTrack is a novel deep learning-based framework designed for multi-object tracking and classification of cancer cells in multi-channel microscopy videos, specifically addressing challenges posed by transient fluorescent signals. It accurately identifies key cellular events such as mitosis and apoptosis, building complete cell trajectories and lineage information to enable detailed single-cell analysis of cell population dynamics.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Cell Biology</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01885v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01885v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01885v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01885v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01834v1"
                     data-domains="Psychiatry,Mental Health,Clinical Informatics,Digital Health,Public Health (Equity)"
                     data-keywords="Gender bias,Depression detection,Causal inference,Counterfactuals,Audio analysis,Fairness,Mental health,Machine learning"
                     data-authors="Mingxuan Hu,Hongbo Ma,Xinlan Wu,Ziqi Liu,Jiaqi Liu,Yangbin Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01834v1.html">Mitigating Gender Bias in Depression Detection via Counterfactual Inference</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mingxuan Hu, Hongbo Ma, Xinlan Wu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel Counterfactual Debiasing Framework grounded in causal inference to mitigate gender bias in audio-based depression detection models. By modeling and subtracting the direct causal effect of gender on predictions during inference, the framework ensures models rely on authentic acoustic pathological features. This approach significantly reduces gender bias and improves overall detection performance compared to existing debiasing strategies.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Public Health (Equity)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01834v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01834v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01834v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01834v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01771v1"
                     data-domains="Radiology,Image-Guided Surgery,Radiation Oncology,Neurosurgery,Diagnostic Imaging,Medical Physics"
                     data-keywords="Medical Image Registration,Rigid Registration,Non-Rigid Registration,Learnable Kernels,Edge Detection,Deep Learning,Multi-Modal Imaging,Anatomical Structure Analysis"
                     data-authors="Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Malik Galijasevic,Elke Ruth Gizewski,Astrid Ellen Grams">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01771v1.html">Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ahsan Raza Siyal, Markus Haltmeier, Ruth Steiger et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel method for robust rigid and non-rigid medical image registration by integrating learnable edge kernels with learning-based techniques. The approach initiates edge detection kernels with a perturbed predefined kernel, which then adaptively learns optimal structural features during training, consistently outperforming state-of-the-art methods across various datasets and registration setups.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Image-Guided Surgery</span>
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Neurosurgery</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01771v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01771v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01771v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01771v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02089v1"
                     data-domains="Pulmonology,Respiratory Medicine,Critical Care,Environmental Health,Primary Care,Veterinary Medicine"
                     data-keywords="Breath-gas analyzer,Respiratory diagnostics,Early disease detection,Biomarkers,Portable medical device,Spirometry,Capnography,Data-logging system"
                     data-authors="Shelby Lacouture,Mitchell Kelley,Noah Plues,Laszlo Hunyadi,Emily Sundman,Annette Sobel,Robert V. Duncan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02089v1.html">A Compact, Data-Logging Breath-Gas Analyzer</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shelby Lacouture, Mitchell Kelley, Noah Plues et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the design of a portable, inexpensive, mixed-signal data-logging system for comprehensive breath-gas analysis, aiming to revolutionize respiratory disease diagnosis. By simultaneously measuring a synergistic set of exhaled gases and mass flow, the device seeks to enable early detection of respiratory ailments, addressing the limitations of current cumbersome and late diagnostic methods. The system, developed in various form factors (mask-mounted, tabletop, handheld), is envisioned to establish a new set of clinical vitals, significantly improving disease prognosis and quality of life.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Respiratory Medicine</span>
                    
                    <span class="domain-tag">Critical Care</span>
                    
                    <span class="domain-tag">Environmental Health</span>
                    
                    <span class="domain-tag">Primary Care</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02089v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02089v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02089v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02089v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01769v1"
                     data-domains="cs.CV"
                     data-keywords="cs.CV,cs.DB"
                     data-authors="Hafsa Billah">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01769v1.html">VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hafsa Billah
                </div>

                <div class="paper-summary">
                    Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). ...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.CV</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01769v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01769v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01769v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01769v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01702v1"
                     data-domains="Cardiology,Electrophysiology,Cardiac Imaging,Personalized Medicine,Computational Medicine"
                     data-keywords="cardiac electrophysiology,arrhythmia,local activation time,operator learning,neural operator,vision transformer,geometry-independent,real-time simulation,personalized medicine,atrial fibrillation"
                     data-authors="Bei Zhou,Cesare Corrado,Shuang Qian,Maximilian Balmus,Angela W. C. Lee,Cristobal Rodero,Marco J. W. Gotte,Luuk H. G. A. Hopman,Mengyun Qiao,Steven Niederer">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01702v1.html">A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Bei Zhou, Cesare Corrado, Shuang Qian et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel geometry-independent operator-learning framework, utilizing a vision-transformer backbone, to predict local activation time (LAT) fields in the left atrium. The framework, trained on 308,700 simulations across 147 diverse patient-specific anatomies represented in a Universal Atrium Coordinate system, achieves highly accurate (5.1 ms error) and near-instantaneous (0.12 ms inference) predictions. This breakthrough enables real-time computational electrophysiology for personalized arrhythmia treatment and large-scale analyses, addressing a critical bottleneck in current clinical workflows.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Electrophysiology</span>
                    
                    <span class="domain-tag">Cardiac Imaging</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Computational Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01702v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01702v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01702v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01702v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02088v1"
                     data-domains="Neurology,Stroke Medicine,Radiology,Neuroimaging,Rehabilitation Medicine"
                     data-keywords="Acute Ischemic Stroke,Diffusion MRI,Prognosis,Deep Learning,Multimodal,ResNet-50,Functional Outcome,ADC"
                     data-authors="Sina Raeisadigh,Myles Joshua Toledo Tan,Henning M√ºller,Abderrahmane Hedjoudje">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02088v1.html">Comparing Baseline and Day-1 Diffusion MRI Using Multimodal Deep Embeddings for Stroke Outcome Prediction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sina Raeisadigh, Myles Joshua Toledo Tan, Henning M√ºller et al.
                </div>

                <div class="paper-summary">
                    This study demonstrates that 24-hour (J1) diffusion MRI, combined with clinical and lesion-volume features, provides superior prognostic value for predicting three-month functional outcomes in acute ischemic stroke patients compared to baseline (J0) imaging. Utilizing multimodal deep embeddings from 3D ResNet-50, fused with clinical variables and classified by SVM, the J1-based models achieved significantly higher predictive performance (AUC = 0.923) than J0 models (AUC <= 0.86). The robust multimodal framework offers a promising tool for personalized stroke prognosis.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Stroke Medicine</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Neuroimaging</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02088v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02088v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02088v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02088v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01681v1"
                     data-domains="Oncology,Pathology,Pulmonology,Medical Imaging,Prognostics"
                     data-keywords="mesothelioma,computational pathology,self-supervised learning,biopsy,resection,subtype classification,survival prediction,AI in medicine"
                     data-authors="Farzaneh Seyedshahi,Francesca Damiola,Sylvie Lantuejoul,Ke Yuan,John Le Quesne">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01681v1.html">Cross-Domain Validation of a Resection-Trained Self-Supervised Model on Multicentre Mesothelioma Biopsies</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Farzaneh Seyedshahi, Francesca Damiola, Sylvie Lantuejoul et al.
                </div>

                <div class="paper-summary">
                    This paper demonstrates the successful cross-domain validation of a self-supervised encoder, initially trained on large mesothelioma resection specimens, for application to small biopsy material. The model effectively captures meaningful morphological patterns from biopsies, enabling accurate prediction of patient survival and classification of tumor subtypes. This innovation broadens the practical utility of AI in mesothelioma diagnostics by overcoming the common limitation of requiring large tissue samples.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Prognostics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01681v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01681v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01681v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01681v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01675v1"
                     data-domains="Radiology,Diagnostic Imaging,Pathology,Rare Diseases,Medical AI"
                     data-keywords="Diffusion Models,Long-Tail Learning,Medical Imaging,Synthetic Data Augmentation,Residual Adapters,Mode Collapse,Gradient Conflicts,Chest X-ray"
                     data-authors="Felix N√ºtzel,Mischa Dombrowski,Bernhard Kainz">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01675v1.html">GRASP: Guided Residual Adapters with Sample-wise Partitioning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Felix N√ºtzel, Mischa Dombrowski, Bernhard Kainz
                </div>

                <div class="paper-summary">
                    GRASP addresses mode collapse in text-to-image diffusion models for long-tail medical imaging by resolving gradient conflicts between frequent and rare classes. It achieves this by statically partitioning samples into clusters using external priors and then fine-tuning models with cluster-specific residual adapters. The method significantly improves generation quality and diversity for rare medical pathologies, boosting downstream classification performance for underrepresented conditions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Rare Diseases</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01675v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01675v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01675v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01675v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01657v1"
                     data-domains="Ophthalmology,Diabetology,Cardiology,Neurology,General Medicine (for systemic disease diagnosis)"
                     data-keywords="Retinal Vessel Segmentation,Deep Learning,UNet,Kolmogorov-Arnold Network (KAN),Convolutional Neural Network (CNN),Transformer,Adaptive Sampling,Ophthalmology,Biomedical Imaging"
                     data-authors="Hongyu Xu,Panpan Meng,Meng Wang,Dayu Hu,Liming Liang,Xiaoqi Sheng">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01657v1.html">DB-KAUNet: An Adaptive Dual Branch Kolmogorov-Arnold UNet for Retinal Vessel Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hongyu Xu, Panpan Meng, Meng Wang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DB-KAUNet, an Adaptive Dual Branch Kolmogorov-Arnold UNet, designed for highly accurate retinal vessel segmentation. It addresses the limitations of traditional CNNs in capturing long-range dependencies by employing a novel heterogeneous dual-branch encoder that combines parallel CNN and Transformer pathways with specialized KANConv and KAT blocks, achieving leading performance and exceptional robustness across multiple datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Diabetology</span>
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">General Medicine (for systemic disease diagnosis)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01657v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01657v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01657v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01657v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01653v1"
                     data-domains="Cardiology,Preventive Medicine,Telemedicine,Digital Health,Critical Care"
                     data-keywords="cuffless blood pressure,wearable sensors,multi-motion-state,photoplethysmography,electrocardiography,mixture-of-experts,hypertension management,digital health"
                     data-authors="Yiqiao Chen,Fazheng Xu,Zijian Huang,Juchi He,Zhenghui Feng">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01653v1.html">Cuffless Blood Pressure Estimation from Six Wearable Sensor Modalities in Multi-Motion-State Scenarios</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ eess.SP</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yiqiao Chen, Fazheng Xu, Zijian Huang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel six-modal framework for cuffless blood pressure (BP) estimation using wearable sensors, specifically designed to maintain accuracy across various motion states. By integrating diverse sensor data and employing advanced machine learning techniques, the method achieves clinically acceptable accuracy for systolic and diastolic BP, addressing a critical limitation of existing single or dual-modal approaches in dynamic scenarios.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Critical Care</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01653v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01653v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01653v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01653v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01589v2"
                     data-domains="Radiology,Otorhinolaryngology (ENT),Emergency Medicine,Infectious Diseases,Diagnostic Imaging,Surgery (Head and Neck)"
                     data-keywords="Head and Neck Abscess,CT Imaging,Semantic Segmentation,AbscessHeNe,Deep Learning,Medical Image Analysis,Content-based Retrieval,Contrast-enhanced CT"
                     data-authors="Thao Thi Phuong Dao,Tan-Cong Nguyen,Trong-Le Do,Truong Hoang Viet,Nguyen Chi Thanh,Huynh Nguyen Thuan,Do Vo Cong Nguyen,Minh-Khoi Pham,Mai-Khiem Tran,Viet-Tham Huynh,Trong-Thuan Nguyen,Trung-Nghia Le,Vo Thanh Toan,Tam V. Nguyen,Minh-Triet Tran,Thanh Dinh Le">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01589v2.html">Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Thao Thi Phuong Dao, Tan-Cong Nguyen, Trong-Le Do et al.
                </div>

                <div class="paper-summary">
                    This paper introduces AbscessHeNe, a novel, comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices specifically for head and neck abscesses, aimed at advancing semantic segmentation. Baseline evaluations on this dataset, using CNN, Transformer, and Mamba architectures, revealed significant challenges in accurate delineation, with the best model achieving a Dice Similarity Coefficient of 0.39, underscoring the need for further research in this critical medical imaging task.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Otorhinolaryngology (ENT)</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Infectious Diseases</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01589v2.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01589v2" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01589v2" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01589v2" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01534v1"
                     data-domains="Neuroradiology,Neurology,Medical Imaging,Radiology,Artificial Intelligence in Medicine"
                     data-keywords="deep learning,unsupervised anomaly detection,brain MRI,benchmarking,bias analysis,clinical translation,medical imaging,neuroradiology"
                     data-authors="Alexander Frotscher,Christian F. Baumgartner,Thomas Wolfers">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01534v1.html">Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Alexander Frotscher, Christian F. Baumgartner, Thomas Wolfers
                </div>

                <div class="paper-summary">
                    This paper presents a large-scale, multi-center benchmark for deep unsupervised anomaly detection in brain MRI, evaluating various algorithms across diverse healthy and clinical cohorts. It reveals significant variability in performance, identifies systematic biases related to scanners, lesion characteristics, age, and sex, and concludes that algorithmic limitations, rather than data availability, currently hinder progress.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neuroradiology</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01534v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01534v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01534v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01534v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.01510v1"
                     data-domains="Abdominal Segmentation,Whole-Heart Segmentation,Prostate Segmentation,Cardiovascular Imaging,Radiology"
                     data-keywords="Domain Generalization,Medical Image Segmentation,Semantic-aware Random Convolution,Source Matching,Cross-Modality,CT,MR,Deep Learning"
                     data-authors="Franz Thaler,Martin Urschler,Mateusz Kozinski,Matthias AF Gsell,Gernot Plank,Darko Stern">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.01510v1.html">Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-01</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Franz Thaler, Martin Urschler, Mateusz Kozinski et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the challenging problem of single-source domain generalization (DG) for medical image segmentation, enabling models trained on one domain to be applied directly to different domains without adaptation. The proposed method, SRCSM, leverages semantic-aware random convolution during training and intensity mapping at test-time to achieve state-of-the-art performance. SRCSM consistently outperforms previous DG techniques and remarkably matches in-domain baseline performance in several cross-modality and cross-center segmentation tasks.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Abdominal Segmentation</span>
                    
                    <span class="domain-tag">Whole-Heart Segmentation</span>
                    
                    <span class="domain-tag">Prostate Segmentation</span>
                    
                    <span class="domain-tag">Cardiovascular Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.01510v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.01510v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.01510v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.01510v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-03 06:26:00</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>