<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">9</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">9</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">28</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Radiology (4), Oncology (4), Medical Imaging (3)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Radiology">Radiology (4)</option>
                        
                        <option value="Oncology">Oncology (4)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (3)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (3)</option>
                        
                        <option value="Pathology">Pathology (2)</option>
                        
                        <option value="Histopathology">Histopathology (2)</option>
                        
                        <option value="Neurology">Neurology (1)</option>
                        
                        <option value="Cardiology">Cardiology (1)</option>
                        
                        <option value="X-ray">X-Ray (1)</option>
                        
                        <option value="Ultrasound (US)">Ultrasound (Us) (1)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2601.16073v1"
                     data-domains="Radiology,Medical Imaging,Oncology,Neurology,Cardiology,Pathology"
                     data-keywords="Federated Learning,Foundation Models,Knowledge Distillation,Medical Image Segmentation,Dual-Scale,Lightweight Models,Computational Efficiency,Data Privacy"
                     data-authors="Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.16073v1.html">DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hanwen Zhang, Qiaojin Shen, Yuxi Liu et al.
                </div>

                <div class="paper-summary">
                    DSFedMed proposes a novel dual-scale federated framework addressing the high computational and communication demands of deploying Foundation Models (FMs) in federated medical image segmentation. It achieves this by enabling mutual knowledge distillation between a centralized FM and lightweight client models, leveraging generated high-quality medical images and a learnability-guided sample selection strategy. This approach significantly improves segmentation accuracy by 2% (Dice score) while reducing communication and inference costs by nearly 90% compared to existing federated FM baselines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Cardiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.16073v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.16073v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.16073v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.16073v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.16064v1"
                     data-domains="X-ray,Ultrasound (US),Histopathology,MRI,Colonoscopy"
                     data-keywords="medical image segmentation,deep learning,phase-aware,frequency domain,CNN,generalization,Fourier Attention,boundary precision"
                     data-authors="Shams Nafisa Ali,Taufiq Hasan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.16064v1.html">Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shams Nafisa Ali, Taufiq Hasan
                </div>

                <div class="paper-summary">
                    Phi-SegNet introduces a CNN-based architecture that integrates phase-aware information at both architectural and optimization levels to enhance medical image segmentation. By leveraging Bi-Feature Mask Former modules, Reverse Fourier Attention blocks, and a dedicated phase-aware loss, it emphasizes boundary precision and improves generalization across diverse imaging modalities. The model achieved state-of-the-art performance on five public datasets and demonstrated robust cross-dataset generalization, highlighting the potential of spectral priors for fine-grained object localization.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">X-ray</span>
                    
                    <span class="domain-tag">Ultrasound (US)</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">MRI</span>
                    
                    <span class="domain-tag">Colonoscopy</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.16064v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.16064v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.16064v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.16064v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.16060v1"
                     data-domains="Radiology,Medical Imaging,Diagnostic Imaging,Oncology,Surgery Planning"
                     data-keywords="Medical Image Segmentation,Diffusion Models,Prompt-Guided,ControlNet,Multi-Class Segmentation,Cross-Modality,CT,MR"
                     data-authors="Yuan Lin,Murong Xu,Marc H√∂lle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.16060v1.html">ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuan Lin, Murong Xu, Marc H√∂lle et al.
                </div>

                <div class="paper-summary">
                    ProGiDiff introduces a novel prompt-guided, diffusion-based framework for medical image segmentation, leveraging pre-trained image generation models to address limitations of deterministic methods. It employs a ControlNet-style conditioning mechanism with a custom encoder to generate multi-class segmentation masks, demonstrating strong performance on CT organ segmentation and successful low-rank, few-shot transfer to MR images.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Surgery Planning</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.16060v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.16060v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.16060v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.16060v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.16024v1"
                     data-domains="Pathology,Oncology,Histopathology,Diagnostic Medicine"
                     data-keywords="Virtual immunohistochemistry,Computational pathology,Hematoxylin and Eosin (H&E),Deep learning,Autoregressive models,Image synthesis,Digital pathology,Cancer diagnostics"
                     data-authors="Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.16024v1.html">PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Rongze Ma, Mengkang Lu, Zhenyu Xiang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces PAINT (Pathology-Aware Integrated Next-Scale Transformation), a novel visual autoregressive framework for virtual immunohistochemistry (IHC). PAINT reformulates IHC synthesis as a structure-first conditional generation task, leveraging a Spatial Structural Start Map (3S-Map) to ground autoregressive initialization in H&E morphology. The framework significantly outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks on the IHC4BC and MIST datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.16024v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.16024v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.16024v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.16024v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15977v1"
                     data-domains="Health services research,Public health,Healthcare administration,Health equity,Epidemiology"
                     data-keywords="Healthcare visitation,Human mobility,Hospital attributes,Population socioeconomics,Deep Gravity,Machine learning,Patient flow,Health disparities"
                     data-authors="Binbin Lin,Lei Zou,Hao Tian,Heng Cai,Yifan Yang,Bing Zhou">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15977v1.html">Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Binbin Lin, Lei Zou, Hao Tian et al.
                </div>

                <div class="paper-summary">
                    This study integrates hospital attributes, population socioeconomics, and human mobility data to predict healthcare visitation flows and analyze their influencing factors. It developed and evaluated five predictive models, identifying Deep Gravity as the top performer, and revealed how hospital characteristics and socioeconomic factors differentially impact visitation patterns based on travel distance and specific demographic groups.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Health services research</span>
                    
                    <span class="domain-tag">Public health</span>
                    
                    <span class="domain-tag">Healthcare administration</span>
                    
                    <span class="domain-tag">Health equity</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15977v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15977v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15977v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15977v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15931v1"
                     data-domains="cs.AI"
                     data-keywords="cs.AI,cs.LG"
                     data-authors="Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15931v1.html">ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-22</span>
                        <span class="relevance">‚≠ê 0.70</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xiangyu Wang, Zhixin Lv, Yongjiao Sun et al.
                </div>

                <div class="paper-summary">
                    Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to ...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15931v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15931v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15931v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15931v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15416v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical Physics,Oncology (for treatment planning),Dentistry"
                     data-keywords="sparse-view CBCT,deep learning,Fourier Neural Operator,high-frequency reconstruction,medical imaging,radiation dose reduction,anatomical detail,cone-beam CT"
                     data-authors="Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15416v1.html">DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Cuong Tran Van, Trong-Thang Pham, Ngoc-Son Nguyen et al.
                </div>

                <div class="paper-summary">
                    DuFal (Dual-Frequency-Aware Learning) is a novel deep learning framework designed for high-fidelity Cone-Beam Computed Tomography (CBCT) reconstruction from extremely sparse-view projections. It tackles the challenge of recovering fine-grained, high-frequency anatomical details, which are often lost in conventional methods, by integrating frequency-domain and spatial-domain processing through a unique dual-path architecture. Experimental results demonstrate DuFal significantly outperforms state-of-the-art methods in preserving these critical high-frequency features under severe undersampling conditions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Oncology (for treatment planning)</span>
                    
                    <span class="domain-tag">Dentistry</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15416v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15416v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15416v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15416v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15408v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical AI"
                     data-keywords="medical vision-language models,radiology reports,visual grounding,curriculum learning,multi-task training,hallucination reduction,AI in medicine,diagnostic imaging"
                     data-authors="Pablo Messina,Andr√©s Villa,Juan Le√≥n Alc√°zar,Karen S√°nchez,Carlos Hinojosa,Denis Parra,√Ålvaro Soto,Bernard Ghanem">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15408v1.html">CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Pablo Messina, Andr√©s Villa, Juan Le√≥n Alc√°zar et al.
                </div>

                <div class="paper-summary">
                    CURE introduces an error-aware curriculum learning framework to enhance the reliability and accuracy of automated radiology report generation. It addresses common issues like inaccurate visual grounding and factual inconsistency by fine-tuning a multimodal instructional model across multiple tasks. The method achieves significant improvements in grounding accuracy, report quality, and reduces hallucinations without requiring additional data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15408v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15408v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15408v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15408v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2601.15392v1"
                     data-domains="Oncology,Computational Pathology,Bioinformatics,Genomics,Precision Medicine,Medical Imaging"
                     data-keywords="Gene expression,Generative Adversarial Network (GAN),Multimodal learning,Histopathology,Clinical metadata,Transformer Encoder,Cross Attention,TCGA"
                     data-authors="Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2601.15392v1.html">GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2026-01-21</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Francesca Pia Panaccione, Carlo Sgaravatti, Pietro Pinoli
                </div>

                <div class="paper-summary">
                    GeMM-GAN is a novel Multimodal Generative Adversarial Network designed to synthesize realistic gene expression profiles. It addresses the challenges of costly and privacy-sensitive gene expression data collection by conditioning its generation on readily available histopathology images and clinical metadata. The model produces biologically coherent profiles, significantly improving downstream disease type prediction accuracy by over 11% compared to current state-of-the-art generative models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2601.15392v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2601.15392v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2601.15392v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2601.15392v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2026-01-23 06:15:41</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>