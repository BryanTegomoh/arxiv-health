<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">48</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">48</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">162</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Oncology (7), Radiology (7), Cardiology (6)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Oncology">Oncology (7)</option>
                        
                        <option value="Radiology">Radiology (7)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (6)</option>
                        
                        <option value="Cardiology">Cardiology (6)</option>
                        
                        <option value="Preventive Medicine">Preventive Medicine (5)</option>
                        
                        <option value="Telemedicine">Telemedicine (5)</option>
                        
                        <option value="Pathology">Pathology (4)</option>
                        
                        <option value="Immunology">Immunology (4)</option>
                        
                        <option value="Emergency Medicine">Emergency Medicine (4)</option>
                        
                        <option value="Personalized Medicine">Personalized Medicine (3)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2511.14753v1"
                     data-domains="Digital Health,Wearable Technology,Remote Patient Monitoring,Clinical Decision Support Systems,Healthcare IoT,Predictive Analytics in Healthcare"
                     data-keywords="Spatiotemporal Modeling,Data Sparsity,Computational Efficiency,Edge Computing,ConvLSTM,Multi-objective Optimization,Healthcare AI,Efficient AI"
                     data-authors="Junfeng Wu,Hadjer Benmeziane,Kaoutar El Maghraoui,Liu Liu,Yinan Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14753v1.html">SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Junfeng Wu, Hadjer Benmeziane, Kaoutar El Maghraoui et al.
                </div>

                <div class="paper-summary">
                    SparseST is a novel framework designed to address the high computational cost of state-of-the-art ConvLSTM models in spatiotemporal data mining, particularly for deployment on resource-constrained edge devices. It achieves efficiency by pioneering the exploitation of data sparsity and feature redundancy, which existing efficient AI methods overlook, while also providing a multi-objective composite loss function to balance model performance and computational cost. This approach offers a practical guide for adapting models to specific computational resource constraints and performance requirements.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Wearable Technology</span>
                    
                    <span class="domain-tag">Remote Patient Monitoring</span>
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Healthcare IoT</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14753v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14753v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14753v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14753v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14744v1"
                     data-domains="Pharmacology,Toxicology,Drug Development,Medicinal Chemistry,Pharmaceutical Sciences"
                     data-keywords="Drug Discovery,Toxicity Prediction,Deep Learning,Tox21,AI Benchmarking,Reproducibility,Pharmacology,Computational Chemistry"
                     data-authors="Antonia Ebner,Christoph Bartmann,Sonja Topf,Sohvi Luukkonen,Johannes Schimunek,G√ºnter Klambauer">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14744v1.html">Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Antonia Ebner, Christoph Bartmann, Sonja Topf et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the lack of clear progress in AI-driven toxicity prediction for drug discovery, a field significantly impacted by deep learning. It introduces a reproducible leaderboard hosted on Hugging Face, utilizing the original Tox21 Challenge dataset, which was previously altered in subsequent benchmarks. The study reveals that early deep learning methods from 2015 and 2017 remain highly competitive, questioning the extent of substantial advancement in toxicity prediction over the past decade.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Toxicology</span>
                    
                    <span class="domain-tag">Drug Development</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Pharmaceutical Sciences</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14744v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14744v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14744v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14744v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14702v1"
                     data-domains="Cardiology,Cardiac Electrophysiology,Cardiovascular Imaging,Medical Image Analysis,Interventional Cardiology"
                     data-keywords="Myocardial Scar Segmentation,LGE-MRI,ECG,Multimodal Fusion,Deep Learning,AHA-17 Atlas,Cardiac Imaging,Tissue Viability"
                     data-authors="Farheen Ramzan,Yusuf Kiberu,Nikesh Jathanna,Meryem Jabrane,Vicente Grau,Shahnaz Jamil-Copley,Richard H. Clayton,Chen,Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14702v1.html">Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel multimodal framework for myocardial scar segmentation from Late Gadolinium-Enhanced (LGE) MRI, integrating ECG-derived electrophysiological information and AHA-17 anatomical priors. It addresses the challenge of non-simultaneous data acquisition with a Temporal Aware Feature Fusion (TAFF) mechanism. The proposed method significantly outperforms state-of-the-art image-only baselines on a clinical dataset, achieving substantial gains in Dice score, precision, and sensitivity.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Cardiac Electrophysiology</span>
                    
                    <span class="domain-tag">Cardiovascular Imaging</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                    <span class="domain-tag">Interventional Cardiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14702v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14702v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14702v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14702v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14682v1"
                     data-domains="Preventive Medicine,Public Health,Cardiology,Nephrology,Hepatology,Endocrinology"
                     data-keywords="smoking,machine learning,health decline,biomarkers,Random Forest,SHAP analysis,early detection,disease risk"
                     data-authors="Vaskar Chakma,MD Jaheid Hasan Nerab,Abdur Rouf,Abu Sayed,Hossem MD Saim,Md. Nournabi Khan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14682v1.html">Machine Learning Models for Predicting Smoking-Related Health Decline and Disease Risk</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Vaskar Chakma, MD Jaheid Hasan Nerab, Abdur Rouf et al.
                </div>

                <div class="paper-summary">
                    This study systematically evaluated machine learning models for identifying early signs of smoking-related health decline, crucial for addressing the current issue of late diagnoses. Utilizing health screening data from 55,691 individuals, the Random Forest model emerged as the most accurate (AUC 0.926), revealing specific biomarkers like blood pressure, triglycerides, liver enzymes, and serum creatinine as strong indicators of health deterioration in smokers.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Nephrology</span>
                    
                    <span class="domain-tag">Hepatology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14682v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14682v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14682v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14682v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14676v1"
                     data-domains="Immunology,Oncology,Drug Discovery,Structural Biology,Bioinformatics,Biotherapeutics"
                     data-keywords="AlphaFold 3,CD47,antibody-antigen,binding affinity,molecular docking,drug discovery,reverse docking,AI structural prediction"
                     data-authors="Yiyang Xu,Ziyou Shen,Yanqing Lv,Shutong Tan,Chun Sun,Juan Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14676v1.html">Exploring AlphaFold 3 for CD47 Antibody-Antigen Binding Affinity: An Unexpected Discovery of Reverse docking</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.BM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yiyang Xu, Ziyou Shen, Yanqing Lv et al.
                </div>

                <div class="paper-summary">
                    This paper investigates AlphaFold 3's (AF3) capabilities for predicting antibody-antigen (CD47) complex structures and analyzing binding affinity for therapeutic antibody pre-screening. It demonstrates AF3's promise for accurate structure and binding energy predictions, while also uncovering an unexpected "reverse docking" phenomenon attributed to its advanced AI architecture.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Immunology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Structural Biology</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14676v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14676v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14676v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14676v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14669v1"
                     data-domains="Infectious Diseases,Immunology,Pharmacology,Drug Discovery,Structural Biology"
                     data-keywords="Hyperbolic embeddings,Graph neural networks,Host-pathogen interactions,Protein-protein interactions,Deep learning,GPCRs,AlphaFold,Infectious diseases"
                     data-authors="Xiaoqiong Xia,Cesar de la Fuente-Nunez">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14669v1.html">Hyperbolic Graph Embeddings Reveal the Host-Pathogen Interactome</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.MN</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xiaoqiong Xia, Cesar de la Fuente-Nunez
                </div>

                <div class="paper-summary">
                    This paper introduces ApexPPI, a deep learning framework utilizing hyperbolic graph embeddings to accurately map host-pathogen protein networks, effectively capturing their hierarchical and scale-free characteristics. By integrating diverse biological data and employing multi-task hyperbolic graph neural networks, the model predicted thousands of high-confidence interactions, including many involving human G-protein-coupled receptors (GPCRs), with dozens validated by AlphaFold 3 structural modeling.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Infectious Diseases</span>
                    
                    <span class="domain-tag">Immunology</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Structural Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14669v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14669v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14669v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14669v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14649v1"
                     data-domains="Pulmonology,Radiology,Medical Imaging,Respiratory Medicine"
                     data-keywords="airway segmentation,CT,nnU-Net,topology correction,deep learning,lung analysis,discontinuity,biomarker extraction"
                     data-authors="John M. Oyer,Ali Namvar,Benjamin A. Hoff,Wassim W. Labaki,Ella A. Kazerooni,Charles R. Hatt,Fernando J. Martinez,MeiLan K. Han,Craig J. Galb√°n,Sundaresh Ram">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14649v1.html">RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> John M. Oyer, Ali Namvar, Benjamin A. Hoff et al.
                </div>

                <div class="paper-summary">
                    RepAir is a novel three-stage framework designed for robust 3D airway segmentation from CT scans, specifically addressing the common issue of disconnected components produced by conventional U-Net-based methods. It combines an nnU-Net for initial segmentation with an anatomically informed topology correction pipeline that uses skeletonization and a 1D convolutional classifier to identify and reconnect true airway branches. Evaluated on datasets spanning healthy to severe pathological airways, RepAir significantly outperforms existing approaches, yielding more complete and anatomically consistent airway trees with high accuracy.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Respiratory Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14649v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14649v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14649v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14649v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14638v1"
                     data-domains="Genetics,Rare diseases,Clinical diagnostics,Medical informatics,Medical AI,Precision medicine"
                     data-keywords="Rare diseases,Large Language Models (LLMs),Clinical reasoning,Diagnosis,Electronic Health Records (EHRs),Instruction tuning,Chain of Thought,Graph-grounded retrieval,Decision support"
                     data-authors="Tao Yang,Dandan Huang,Yunting Lin,Pengfei Wu,Zhikun Wu,Gangyuan Ma,Yulan Lu,Xinran Dong,Dingpeng Li,Junshuang Ge,Zhiyan Zhang,Xuanzhao Huang,Wenyan Nong,Yao Zhou,Hui Tang,Hongxi Yang,Shijie Zhang,Juan Li,Xiaojun Cao,Lin Yang,Xia Gao,Kaishou Xu,Xiaoqiong Gu,Wen Zhang,Huimin Xia,Li Liu,Wenhao Zhou,Mulin Jun Li">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14638v1.html">A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tao Yang, Dandan Huang, Yunting Lin et al.
                </div>

                <div class="paper-summary">
                    This paper introduces RareSeek R1, a specialized large language model designed for clinical reasoning and diagnosis of rare diseases, addressing the challenges of long diagnostic odysseys and limitations of existing LLMs. It achieves state-of-the-art accuracy and robust generalization through advanced training and retrieval methods, performing on par with experienced physicians and providing transparent, auditable diagnostic insights.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Genetics</span>
                    
                    <span class="domain-tag">Rare diseases</span>
                    
                    <span class="domain-tag">Clinical diagnostics</span>
                    
                    <span class="domain-tag">Medical informatics</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14638v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14638v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14638v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14638v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14619v1"
                     data-domains="Neurology,Rare Diseases,Personalized Medicine,Clinical Decision Support,Treatment Optimization,Disease Management"
                     data-keywords="POMDPs,Expert Knowledge,Fuzzy Logic,Expectation Maximization,Maximum A Posteriori,Data-Efficient Learning,Healthcare Modeling,Myasthenia Gravis"
                     data-authors="Marco Locatelli,Arjen Hommersom,Roberto Clemens Cerioli,Daniela Besozzi,Fabio Stella">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14619v1.html">Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Marco Locatelli, Arjen Hommersom, Roberto Clemens Cerioli et al.
                </div>

                <div class="paper-summary">
                    This paper introduces the Fuzzy MAP EM algorithm, a novel approach for learning Partially Observable Markov Decision Process (POMDP) parameters from limited data by integrating expert knowledge. It enriches the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model, effectively reformulating parameter estimation as a Maximum A Posteriori (MAP) problem. The method demonstrates superior performance over standard EM in synthetic medical simulations under low-data and high-noise conditions and successfully recovers a clinically coherent POMDP in a Myasthenia Gravis case study.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Rare Diseases</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Treatment Optimization</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14619v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14619v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14619v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14619v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14613v1"
                     data-domains="Pathology,Oncology,Developmental Biology,Neuroscience,Immunology,Personalized Medicine"
                     data-keywords="3D spatial transcriptomics,volumetric tissue,flow matching,H&E histology,gene expression,ControlNet,ZINB prior,biomarker discovery"
                     data-authors="Mohammad Vali Sanian,Arshia Hemmat,Amirhossein Vahidi,Jonas Maaskola,Jimmy Tsz Hang Lee,Stanislaw Makarchuk,Yeliz Demirci,Nana-Jane Chipampe,Omer Bayraktar,Lassi Paavolainen,Mohammad Lotfollahi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14613v1.html">3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mohammad Vali Sanian, Arshia Hemmat, Amirhossein Vahidi et al.
                </div>

                <div class="paper-summary">
                    HoloTea is a novel 3D-aware flow-matching framework designed to generate volumetric tissue spatial transcriptomics by imputing spot-level gene expression from serial H&E histology. It leverages information from adjacent sections using a ControlNet and a 3D-consistent prior, overcoming limitations of existing 2D and 3D methods to achieve scalable and accurate 3D expression profiles. This approach significantly improves 3D expression accuracy and generalization across diverse tissue types and resolutions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Developmental Biology</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                    <span class="domain-tag">Immunology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14613v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14613v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14613v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14613v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14604v1"
                     data-domains="Osteoporosis,Endocrinology,Geriatrics,Orthopedics,Radiology,Preventive Medicine"
                     data-keywords="Bone Mineral Density (BMD),Deep Learning,Multimodal Learning,Cross-Attention,Osteoporosis,Femoral Neck,X-ray Imaging,Clinical Metadata"
                     data-authors="Yilin Zhang,Leo D. Westbury,Elaine M. Dennison,Nicholas C. Harvey,Nicholas R. Fuggle,Rahman Attar">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14604v1.html">XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yilin Zhang, Leo D. Westbury, Elaine M. Dennison et al.
                </div>

                <div class="paper-summary">
                    This paper introduces XAttn-BMD, a novel multimodal deep learning framework designed to accurately predict femoral neck Bone Mineral Density (BMD) using hip X-ray images and structured clinical metadata. It employs a bidirectional cross-attention mechanism for dynamic feature integration and a tailored Weighted Smooth L1 loss, significantly outperforming baseline models in BMD estimation robustness and generalization.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Osteoporosis</span>
                    
                    <span class="domain-tag">Endocrinology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14604v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14604v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14604v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14604v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14603v1"
                     data-domains="Nephrology,Internal Medicine,Critical Care,Clinical Informatics,Population Health"
                     data-keywords="Acute Kidney Injury,Chronic Kidney Disease,EHR,Multi-state modeling,Clustering,Risk stratification,Disease progression,Decision support systems"
                     data-authors="Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14603v1.html">A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yilu Fang, Jordan G. Nestor, Casey N. Ta et al.
                </div>

                <div class="paper-summary">
                    This study developed a data-driven method using electronic health record (EHR) data to characterize the progression from acute kidney injury (AKI) to chronic kidney disease (CKD). By identifying fifteen distinct post-AKI clinical states and their varying CKD risks, the research offers a novel approach to pinpoint high-risk AKI patients more precisely. This paves the way for targeted early interventions and the development of intelligent decision-support tools in clinical settings.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Nephrology</span>
                    
                    <span class="domain-tag">Internal Medicine</span>
                    
                    <span class="domain-tag">Critical Care</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Population Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14603v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14603v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14603v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14603v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14601v1"
                     data-domains="Neurology,Geriatrics,Radiology,Computational Neuroscience,Precision Medicine"
                     data-keywords="Alzheimer's Disease,Cognitive Decline,MRI Embeddings,Vision Transformer (ViT),Multimodal Fusion,Dynamic Time Warping,Predictive Modeling,Biomarkers"
                     data-authors="Nathaniel Putera,Daniel Vilet Rodr√≠guez,Noah Videcrantz,Julia Machnio,Mostafa Mehdipour Ghazi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14601v1.html">MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Nathaniel Putera, Daniel Vilet Rodr√≠guez, Noah Videcrantz et al.
                </div>

                <div class="paper-summary">
                    This study evaluates the complementary contributions of tabular clinical data and transformer-derived MRI embeddings for modeling cognitive decline in Alzheimer's disease. It introduces a trajectory-aware labeling strategy and utilizes an unsupervised 3D Vision Transformer (ViT) for MRI feature extraction. The research concludes that clinical features excel at predicting severe progression, while ViT MRI embeddings are superior for identifying cognitively stable individuals, thereby advocating for multimodal fusion strategies.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Computational Neuroscience</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14601v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14601v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14601v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14601v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14599v1"
                     data-domains="Neuro-oncology,Diagnostic Radiology,Radiation Oncology,Medical Image Analysis"
                     data-keywords="Brain tumor segmentation,Multi-modal MRI,Missing modalities,Deep learning,Self-distillation,Robustness,Medical imaging,Clinical diagnosis"
                     data-authors="Dongqing Xie,Yonghuang Wu,Zisheng Ai,Jun Min,Zhencun Jiang,Shaojin Geng,Lei Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14599v1.html">CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Dongqing Xie, Yonghuang Wu, Zisheng Ai et al.
                </div>

                <div class="paper-summary">
                    This paper introduces CCSD, a novel Cross-Modal Compositional Self-Distillation framework, designed to address the critical challenge of robust brain tumor segmentation despite frequently missing MRI modalities in clinical practice. CCSD employs a shared-specific encoder-decoder architecture combined with two self-distillation strategies: hierarchical modality self-distillation and progressive modality combination distillation. The framework achieves state-of-the-art performance, strong generalization, and stability across various missing-modality scenarios on public benchmarks.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neuro-oncology</span>
                    
                    <span class="domain-tag">Diagnostic Radiology</span>
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14599v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14599v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14599v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14599v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14591v1"
                     data-domains="Diagnostic imaging,Pathology,Clinical decision support,Epidemiology,Rare disease diagnosis,Precision medicine"
                     data-keywords="AI bias,class imbalance,base rate neglect,human-AI interaction,decision support,appropriate reliance,compound bias,medical diagnostics"
                     data-authors="Nick von Felten,Johannes Sch√∂ning,Klaus Opwis,Nicolas Scharowksi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14591v1.html">Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.HC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Nick von Felten, Johannes Sch√∂ning, Klaus Opwis et al.
                </div>

                <div class="paper-summary">
                    This paper investigates the complex interaction between AI bias (class imbalance) and human bias (base rate neglect) in decision-making contexts involving AI-based decision-support systems. It found that class imbalance negatively impacted participants' ability to appropriately calibrate their reliance on AI, and crucially, identified a mutually reinforcing effect between these two biases, leading to a compound human-AI bias.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Diagnostic imaging</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Clinical decision support</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Rare disease diagnosis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14591v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14591v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14591v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14591v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14588v1"
                     data-domains="neurology,geriatrics,radiology,neuroscience,dementia research"
                     data-keywords="white matter hyperintensities,deep learning,Alzheimer's disease,biomarker,regional analysis,neuroimaging,segmentation,classification"
                     data-authors="Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14588v1.html">Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Julia Machnio, Mads Nielsen, Mostafa Mehdipour Ghazi
                </div>

                <div class="paper-summary">
                    This paper introduces a deep learning framework for segmenting and localizing white matter hyperintensities (WMH) within specific anatomical regions, addressing the limitation of previous methods that only provide global lesion load. The study demonstrates that regional WMH quantification significantly outperforms global lesion burden for Alzheimer's disease (AD) classification, especially when integrated with brain atrophy metrics, offering a more robust biomarker for neurodegenerative disorders.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">neurology</span>
                    
                    <span class="domain-tag">geriatrics</span>
                    
                    <span class="domain-tag">radiology</span>
                    
                    <span class="domain-tag">neuroscience</span>
                    
                    <span class="domain-tag">dementia research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14588v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14588v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14588v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14588v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14559v1"
                     data-domains="Pharmacology,Drug Discovery,Structural Biology,Medicinal Chemistry,Bioinformatics,Computational Biology"
                     data-keywords="Drug design,Diffusion models,Protein flexibility,Ligand generation,Binding pockets,Conformational changes,Apo-holo structures,Structure-based drug design"
                     data-authors="Xinzhe Zheng,Shiyu Jiang,Gustavo Seabra,Chenglong Li,Yanjun Li">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14559v1.html">Apo2Mol: 3D Molecule Generation via Dynamic Pocket-Aware Diffusion Models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.BM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xinzhe Zheng, Shiyu Jiang, Gustavo Seabra et al.
                </div>

                <div class="paper-summary">
                    Apo2Mol introduces a novel diffusion-based generative framework for 3D small molecule design that overcomes the rigid protein assumption prevalent in current approaches. It simultaneously generates high-affinity ligand molecules and their corresponding flexible protein binding pocket conformations from an apo state, leveraging a curated dataset of experimentally resolved apo-holo structure pairs. This method achieves state-of-the-art performance in ligand generation and accurately models realistic protein conformational changes.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Structural Biology</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Bioinformatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14559v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14559v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14559v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14559v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14558v1"
                     data-domains="Pathology,Oncology,Urology,Diagnostic Imaging"
                     data-keywords="Digital Pathology,Explainable AI,XAI,Convolutional Neural Networks,Clustering,Model Interpretability,Prostate Cancer Detection,Global Explainability"
                     data-authors="Adam Bajger,Jan Obdr≈æ√°lek,Vojtƒõch K≈Ør,Rudolf Nenutil,Petr Holub,V√≠t Musil,Tom√°≈° Br√°zdil">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14558v1.html">Explaining Digital Pathology Models via Clustering Activations</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Adam Bajger, Jan Obdr≈æ√°lek, Vojtƒõch K≈Ør et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel clustering-based explainability technique for convolutional neural network models used in digital pathology. Unlike traditional saliency map methods that offer local explanations, this approach provides insights into the global behavior of the model through visualization of activation clusters, offering both a broader understanding and fine-grained information. The authors demonstrate its utility by evaluating it on an existing prostate cancer detection model, showing it can increase confidence in model operation and facilitate clinical adoption.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Urology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14558v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14558v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14558v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14558v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14545v1"
                     data-domains="Personalized Medicine,Dynamic Treatment Regimens,Chronic Disease Management,Critical Care,Oncology,Public Health Interventions,Clinical Trial Design"
                     data-keywords="Causal Inference,Time-varying Treatment Effects,Structural Nested Mean Models (SNMMs),Deep Learning,Blip Effects,Optimal Treatment Policies,Confounding Adjustment,Neural Networks"
                     data-authors="Haorui Ma,Dennis Frauen,Stefan Feuerriegel">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14545v1.html">DeepBlip: Estimating Conditional Average Treatment Effects Over Time</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ stat.ML</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Haorui Ma, Dennis Frauen, Stefan Feuerriegel
                </div>

                <div class="paper-summary">
                    DeepBlip introduces the first neural framework for Structural Nested Mean Models (SNMMs), enabling the estimation of conditional average treatment effects over time. It overcomes the prior limitation of sequential g-estimation preventing end-to-end training by utilizing a novel double optimization trick for simultaneous, gradient-based learning of all blip functions. This framework integrates sequential neural networks, provides unbiased estimates by correctly adjusting for time-varying confounding, and demonstrates state-of-the-art performance on clinical datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Dynamic Treatment Regimens</span>
                    
                    <span class="domain-tag">Chronic Disease Management</span>
                    
                    <span class="domain-tag">Critical Care</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14545v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14545v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14545v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14545v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14523v1"
                     data-domains="Biostatistics,Epidemiology,Preclinical Research,Clinical Trials,Public Health Research,Veterinary Medicine"
                     data-keywords="longitudinal data,linear mixed models,mouse body weight,model selection,linear contrasts,biostatistics,epidemiology,reproducible research"
                     data-authors="Sunday A. Adetunji">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14523v1.html">Teaching Longitudinal Linear Mixed Models End-to-End: A Reproducible Case Study in Mouse Body-Weight Growth</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ stat.ME</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sunday A. Adetunji
                </div>

                <div class="paper-summary">
                    This paper presents a comprehensive, reproducible case study demonstrating an end-to-end workflow for teaching and applying linear mixed-effects models (LMMs) to longitudinal data. Using a mouse body-weight growth experiment, it illustrates data preparation, sequential model building and selection (common-slope, fully interacted, parsimonious models), diagnostics, and interpretation via linear contrasts. The study highlights that a parsimonious LMM can effectively reveal distinct group-specific growth trajectories, outperforming simpler models and fitting as well as more complex ones.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Biostatistics</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Preclinical Research</span>
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                    <span class="domain-tag">Public Health Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14523v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14523v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14523v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14523v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14518v1"
                     data-domains="Radiology,Diagnostic Imaging,Oncology,Medical Physics,Image Processing in Medicine"
                     data-keywords="Low-Dose CT,Image Enhancement,Deep Learning,Perceptual Loss,Human Visual System,Radiology,DINOv2,Medical Imaging"
                     data-authors="Taifour Yousra Nabila,Azeddine Beghdadi,Marie Luong,Zuheng Ming,Habib Zaidi,Faouzi Alaya Cheikh">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14518v1.html">D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Taifour Yousra Nabila, Azeddine Beghdadi, Marie Luong et al.
                </div>

                <div class="paper-summary">
                    D-PerceptCT is a novel deep learning architecture inspired by the Human Visual System (HVS) designed to enhance low-dose CT (LDCT) images. It addresses the common problem of critical detail loss in existing enhancement methods by focusing on perceptually relevant features, thereby providing radiologists with clearer images for improved diagnosis. The method demonstrated superior preservation of structural and textural information on the Mayo2016 dataset compared to state-of-the-art techniques.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Image Processing in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14518v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14518v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14518v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14518v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14500v1"
                     data-domains="cond-mat.mtrl-sci"
                     data-keywords="cond-mat.mtrl-sci,physics.med-ph"
                     data-authors="Xenie Lytvynenko,Marie Urbanov√°,Ond≈ôej Lalinsk√Ω,Vil√©m Vojta,Jan B√°rta,Lenka Prouzov√° Proch√°zkov√°,V√°clav ƒåuba">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14500v1.html">Composition-Dependent Properties of $\mathrm{Ce_{x}La_{0.95-x}Tb_{0.05}F_{3}}$ Nanopowders Tailored for X-Ray Photodynamic Therapy and Cathodoluminescence Imaging</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cond-mat.mtrl-sci</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xenie Lytvynenko, Marie Urbanov√°, Ond≈ôej Lalinsk√Ω et al.
                </div>

                <div class="paper-summary">
                    This study investigates the synthesis and luminescence behavior of $\mathrm{Ce_{x}La_{0.95-x}Tb_{0.05}F_{3}}$ nanoparticles with varying $\mathrm{Ce^{3+}}$ content. The materials were prepared via a wet chemical route and thermally annealed to improve crystallinity and reduce defects. Phase composit...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cond-mat.mtrl-sci</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14500v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14500v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14500v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14500v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14469v1"
                     data-domains="Emergency Medicine,Critical Care,Surgical Robotics,Telemedicine,Patient Monitoring,Autonomous Medical Devices,Medical Surveillance"
                     data-keywords="Low-light video enhancement,Video deblurring,Event cameras,RGB fusion,Complex-valued neural networks,Spatiotemporal fusion,Deep learning,Computer Vision"
                     data-authors="Mingchen Zhong,Xin Lu,Dong Li,Senyan Xu,Ruixuan Jiang,Xueyang Fu,Baocai Yin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14469v1.html">CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mingchen Zhong, Xin Lu, Dong Li et al.
                </div>

                <div class="paper-summary">
                    CompEvent introduces a novel complex neural network framework for holistic, full-process fusion of event camera data and RGB frames to tackle challenging low-light video deblurring. By leveraging complex-valued convolutions and processing in both spatial and frequency domains, it achieves superior spatiotemporal fusion, maximizing complementary learning between modalities. This approach significantly strengthens low-light video deblurring capabilities, outperforming state-of-the-art methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Critical Care</span>
                    
                    <span class="domain-tag">Surgical Robotics</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Patient Monitoring</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14469v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14469v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14469v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14469v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14452v1"
                     data-domains="Cardiology,Critical Care Medicine,Anesthesiology,Intensive Care Units (ICU),Precision Health,Telemedicine"
                     data-keywords="Photoplethysmography (PPG),Cardiovascular Monitoring,Stroke Volume,Cardiac Output,Hybrid Model,Hemodynamic Simulations,Variational Autoencoder,Non-invasive"
                     data-authors="Emanuele Palumbo,Sorawit Saengkyongam,Maria R. Cervera,Jens Behrmann,Andrew C. Miller,Guillermo Sapiro,Christina Heinze-Deml,Antoine Wehenkel">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14452v1.html">Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Emanuele Palumbo, Sorawit Saengkyongam, Maria R. Cervera et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the challenge of non-invasively monitoring crucial cardiovascular biomarkers like stroke volume (SV) and cardiac output (CO), which traditionally require invasive arterial pressure waveform (APW) measurements. It proposes a novel hybrid model that leverages photoplethysmography (PPG) signals by combining a conditional variational autoencoder trained on real paired PPG-APW data with a conditional density estimator trained on labeled simulated APW data. The study demonstrates that this approach can effectively detect fluctuations and monitor temporal changes in SV and CO, outperforming supervised baselines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Critical Care Medicine</span>
                    
                    <span class="domain-tag">Anesthesiology</span>
                    
                    <span class="domain-tag">Intensive Care Units (ICU)</span>
                    
                    <span class="domain-tag">Precision Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14452v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14452v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14452v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14452v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14445v1"
                     data-domains="Mental Health,Clinical Psychology,Psychiatry,Preventive Medicine"
                     data-keywords="LLM,Mental Well-being,RAG,Synthetic Dialogue,Agentic AI,Self-care,Digital Therapeutics,Conversational AI"
                     data-authors="Trishala Jayesh Ahalpara">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14445v1.html">Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Trishala Jayesh Ahalpara
                </div>

                <div class="paper-summary">
                    Tell Me is an LLM-powered mental well-being system designed to provide accessible, context-aware support through three integrated components: a RAG-based conversational assistant, a synthetic client-therapist dialogue generator, and an agentic AI crew for personalized self-care. It aims to lower barriers to support, complement existing care, and broaden access to mental health resources, explicitly not as a substitute for professional therapy.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Mental Health</span>
                    
                    <span class="domain-tag">Clinical Psychology</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14445v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14445v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14445v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14445v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14439v1"
                     data-domains="24 primary medical specialties,91 secondary medical specialties,Chinese clinical guidelines"
                     data-keywords="medical AI,large language models,multimodal models,intelligent agents,clinical evaluation,safety,benchmark,Chinese medicine,healthcare AI,LLM-as-a-judge"
                     data-authors="Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14439v1.html">MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jinru Ding, Lu Lu, Chao Ding et al.
                </div>

                <div class="paper-summary">
                    MedBench v4 introduces a robust, cloud-based benchmark with over 700,000 expert-curated tasks across 24 primary and 91 secondary Chinese medical specialties to evaluate LLMs, multimodal models, and intelligent agents. The study reveals significant performance gaps in base LLMs and multimodal models, particularly in safety/ethics and cross-modal reasoning, while demonstrating that governance-aware agentic orchestration substantially improves end-to-end clinical readiness and safety. This platform offers a practical reference for auditing medical AI, aligned with Chinese clinical guidelines and regulatory priorities.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">24 primary medical specialties</span>
                    
                    <span class="domain-tag">91 secondary medical specialties</span>
                    
                    <span class="domain-tag">Chinese clinical guidelines</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14439v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14439v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14439v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14439v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14419v1"
                     data-domains="Immunology,Cell biology,Pathology,Infectious diseases,Inflammation,Oncology"
                     data-keywords="Optical flow,Region of Interest,Image compression,High-throughput imaging,Immune cell migration,JPEG2000,Live-cell microscopy,Data burden"
                     data-authors="Xiaowei Xu,Justin Sonneck,Hongxiao Wang,Roman Burkard,Hendrik Wohrle,Anton Grabmasier,Matthias Gunzer,Jianxu Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14419v1.html">FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xiaowei Xu, Justin Sonneck, Hongxiao Wang et al.
                </div>

                <div class="paper-summary">
                    FlowRoI is a novel optical-flow-based framework designed to extract Regions of Interest (RoIs) in high-throughput immune cell migration imaging data, aiming to alleviate substantial data storage and transmission burdens. It estimates optical flow between frames to derive RoI masks covering migrating cells, enabling RoI-aware JPEG2000 compression. This framework achieves 2.0-2.2x higher compression rates and superior image quality in cellular regions compared to standard JPEG2000, all while maintaining high computational efficiency.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Immunology</span>
                    
                    <span class="domain-tag">Cell biology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Infectious diseases</span>
                    
                    <span class="domain-tag">Inflammation</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14419v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14419v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14419v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14419v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14398v1"
                     data-domains="Ophthalmology,Diabetology,Preventive Medicine,Telemedicine,Public Health"
                     data-keywords="Diabetic Retinopathy,Ordinal Regression,Fundus Images,APTOS-2019,Quadratic Weighted Kappa,Image Preprocessing,Preventable Blindness,Medical Imaging"
                     data-authors="Saksham Kumar,D Sridhar Aditya,T Likhil Kumar,Thulasi Bikku,Srinivasarao Thota,Chandan Kumar">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14398v1.html">Stage Aware Diagnosis of Diabetic Retinopathy via Ordinal Regression</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Saksham Kumar, D Sridhar Aditya, T Likhil Kumar et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel, state-of-the-art Ordinal Regression-based framework for the stage-aware diagnosis of Diabetic Retinopathy (DR) using fundus images. Employing a sophisticated preprocessing pipeline on the APTOS-2019 dataset, the method achieved a Quadratic Weighted Kappa (QWK) score of 0.8992, establishing a new benchmark for accurate DR classification that aligns closely with clinical grading.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Diabetology</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14398v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14398v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14398v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14398v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14361v1"
                     data-domains="Ophthalmology,Optometry,Neurology (for specific movement disorders),Telemedicine"
                     data-keywords="Blink Application,Eyelid Movements,Mobile Health,Ocular Surface,Machine Learning,Ophthalmology,Clinical Validation,Real-time Analysis"
                     data-authors="Gustavo Adolpho Bonesso,Carlos Marcelo Gurj√£o de Godoy,Tammy Hentona Osaki,Midori Hentona Osaki,B√°rbara Moreira Ribeiro Trindade dos Santos,Regina C√©lia Coelho">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14361v1.html">Clinically-Validated Innovative Mobile Application for Assessing Blinking and Eyelid Movements</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Gustavo Adolpho Bonesso, Carlos Marcelo Gurj√£o de Godoy, Tammy Hentona Osaki et al.
                </div>

                <div class="paper-summary">
                    This paper presents the clinical validation of Bapp, an innovative mobile application leveraging Flutter and Google ML Kit for real-time, on-device analysis of blinking and eyelid movements. Validated against 45 patient videos manually annotated by ophthalmology specialists, Bapp achieved high performance with 98.4% precision, 96.9% recall, and 98.3% accuracy. This establishes Bapp as a reliable, portable, and objective tool for continuous ocular health monitoring and postoperative evaluation, addressing the limitations of existing assessment methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Optometry</span>
                    
                    <span class="domain-tag">Neurology (for specific movement disorders)</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14361v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14361v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14361v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14361v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14343v1"
                     data-domains="Orthodontics,Dentistry,Maxillofacial Surgery,Diagnostic Imaging"
                     data-keywords="3D-2D registration,intraoral scan,cephalometric radiograph,silhouette-to-contour,Chamfer distance,orthodontics,dental imaging,DRR"
                     data-authors="Yiyi Miao,Taoyu Wu,Ji Jiang,Tong Chen,Zhe Tang,Zhengyong Jiang,Angelos Stefanidis,Limin Yu,Jionglong Su">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14343v1.html">Silhouette-to-Contour Registration: Aligning Intraoral Scan Models with Cephalometric Radiographs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yiyi Miao, Taoyu Wu, Ji Jiang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DentalSCR, a novel contour-guided framework for robust and accurate 3D-2D alignment of intraoral scan models with lateral cephalometric radiographs. By establishing a unified anatomical coordinate system and employing a surface-based DRR generation with subsequent Chamfer distance optimization, DentalSCR overcomes challenges of conventional methods, achieving high-fidelity alignment with reduced landmark errors, particularly for posterior teeth, and superior performance compared to baselines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthodontics</span>
                    
                    <span class="domain-tag">Dentistry</span>
                    
                    <span class="domain-tag">Maxillofacial Surgery</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14343v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14343v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14343v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14343v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14336v1"
                     data-domains="Orthodontics,Dentistry,Oral and Maxillofacial Radiology,Dental Diagnostics"
                     data-keywords="3D intraoral scans,digital orthodontics,tooth counting,dental segmentation,knowledge-guided,arch-flattening,computer vision,dental ontology"
                     data-authors="Bohan Zhang,Yiyi Miao,Taoyu Wu,Tong Chen,Ji Jiang,Zhuoxiao Li,Zhe Tang,Limin Yu,Jionglong Su">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14336v1.html">ArchMap: Arch-Flattening and Knowledge-Guided Vision Language Model for Tooth Counting and Structured Dental Understanding</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Bohan Zhang, Yiyi Miao, Taoyu Wu et al.
                </div>

                <div class="paper-summary">
                    ArchMap is a novel training-free, knowledge-guided framework designed to provide robust structured understanding of 3D intraoral scans by addressing limitations of existing deep-learning approaches. It achieves this by standardizing raw meshes through a geometry-aware arch-flattening module and leveraging a comprehensive Dental Knowledge Base for semantic reasoning. The system demonstrates superior accuracy and stability across various dental analysis tasks compared to supervised and VLM baselines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthodontics</span>
                    
                    <span class="domain-tag">Dentistry</span>
                    
                    <span class="domain-tag">Oral and Maxillofacial Radiology</span>
                    
                    <span class="domain-tag">Dental Diagnostics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14336v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14336v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14336v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14336v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14317v1"
                     data-domains="Clinical machine learning,Healthcare analytics,Predictive medicine,Health informatics,Resource-constrained healthcare"
                     data-keywords="Clinical machine learning,Rashomon Effect,model selection,intervention efficiency,perturbation validation,robustness,capacity constraints,clinical utility"
                     data-authors="Yuwen Zhang,Viet Tran,Paul Weng">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14317v1.html">Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yuwen Zhang, Viet Tran, Paul Weng
                </div>

                <div class="paper-summary">
                    This paper addresses the challenges of selecting reliable clinical machine learning models amidst the Rashomon Effect, where multiple models exhibit comparable performance but may differ in clinical utility or robustness. It introduces two novel tools, Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF), to enable capacity-aware and robust model selection, demonstrating improved generalization and alignment with clinical resource constraints on healthcare datasets.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical machine learning</span>
                    
                    <span class="domain-tag">Healthcare analytics</span>
                    
                    <span class="domain-tag">Predictive medicine</span>
                    
                    <span class="domain-tag">Health informatics</span>
                    
                    <span class="domain-tag">Resource-constrained healthcare</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14317v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14317v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14317v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14317v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14315v1"
                     data-domains="Digital Orthodontics,Tele-dentistry,Oral and Maxillofacial Surgery,Medical Imaging,Dental Diagnostics"
                     data-keywords="Intraoral 3D Reconstruction,Tele-orthodontics,3D Gaussian Splatting,Sparse-View Photogrammetry,Wavelet Regularization,Geometry-Aware Pairing,Digital Dentistry,Occlusion Visualization"
                     data-authors="Yiyi Miao,Taoyu Wu,Tong Chen,Ji Jiang,Zhe Tang,Zhengyong Jiang,Angelos Stefanidis,Limin Yu,Jionglong Su">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14315v1.html">Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yiyi Miao, Taoyu Wu, Tong Chen et al.
                </div>

                <div class="paper-summary">
                    Dental3R is a novel pose-free, graph-guided pipeline designed for robust, high-fidelity 3D reconstruction of intraoral structures from sparse, unposed smartphone photographs. It addresses critical challenges in tele-orthodontics by integrating a Geometry-Aware Pairing Strategy for stable geometry initialization and wavelet-regularized 3D Gaussian Splatting to preserve fine anatomical details. Validation on large clinical datasets demonstrates its superior novel view synthesis quality for dental occlusion visualization compared to state-of-the-art methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Orthodontics</span>
                    
                    <span class="domain-tag">Tele-dentistry</span>
                    
                    <span class="domain-tag">Oral and Maxillofacial Surgery</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Dental Diagnostics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14315v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14315v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14315v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14315v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14312v1"
                     data-domains="Cardiology,Cardiovascular Disease Diagnosis"
                     data-keywords="H-LDM,Phonocardiogram (PCG),Latent Diffusion Models,Cardiac Diagnostics,Data Augmentation,Clinical Metadata,Cardiovascular Disease,Interpretable AI"
                     data-authors="Chenyang Xu,Siming Li,Hao Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14312v1.html">H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chenyang Xu, Siming Li, Hao Wang
                </div>

                <div class="paper-summary">
                    H-LDM introduces a Hierarchical Latent Diffusion Model for synthesizing clinically accurate and controllable Phonocardiogram (PCG) signals from structured clinical metadata. It addresses the scarcity of labeled pathological PCG data by generating synthetic data with a physiologically-disentangled latent space and fine-grained control over 17 distinct conditions. The model demonstrates state-of-the-art performance, achieving high clinical validity and significantly improving rare disease classification when used for data augmentation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Cardiovascular Disease Diagnosis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14312v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14312v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14312v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14312v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14310v1"
                     data-domains="Radiology,Diagnostic Imaging,Emergency Medicine,Interventional Radiology,Radiation Oncology"
                     data-keywords="computed tomography,CT reconstruction,ultra-sparse-view,neural attenuation fields,diffusion models,iterative reconstruction,medical imaging,image synthesis"
                     data-authors="Jiancheng Fang,Shaoyu Wang,Junlin Wang,Weiwen Wu,Yikun Zhang,Qiegen Liu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14310v1.html">Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jiancheng Fang, Shaoyu Wang, Junlin Wang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Diffusion-Refined Neural Attenuation Fields (Diff-NAF), an iterative framework designed to overcome the challenges of ultra-sparse-view sampling in multi-source stationary CT. By combining Neural Attenuation Fields with a dual-branch conditional diffusion model, Diff-NAF iteratively synthesizes and refines projections, progressively enhancing completeness and fidelity to achieve high-quality CT reconstructions from very limited data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Interventional Radiology</span>
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14310v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14310v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14310v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14310v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14302v1"
                     data-domains="Dermatology,Gastroenterology"
                     data-keywords="Federated Learning,Semi-Supervised Learning,Medical Image Segmentation,Knowledge Distillation,Foundation Models,SAM,Skin Lesion Segmentation,Polyp Segmentation"
                     data-authors="Sahar Nasirihaghighi,Negin Ghamsarian,Yiping Li,Marcel Breeuwer,Raphael Sznitman,Klaus Schoeffmann">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14302v1.html">SAM-Fed: SAM-Guided Federated Semi-Supervised Learning for Medical Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sahar Nasirihaghighi, Negin Ghamsarian, Yiping Li et al.
                </div>

                <div class="paper-summary">
                    SAM-Fed is a novel federated semi-supervised learning (FSSL) framework for medical image segmentation designed to overcome limitations of data scarcity and heterogeneous client resources. It leverages a high-capacity segmentation foundation model to guide lightweight client models through dual knowledge distillation and an adaptive agreement mechanism, enhancing pseudo-label reliability. Experiments on skin lesion and polyp segmentation demonstrate that SAM-Fed consistently outperforms state-of-the-art FSSL methods across various settings.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14302v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14302v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14302v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14302v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14286v1"
                     data-domains="Orthopedic Surgery,Spinal Surgery,Interventional Radiology,Medical Imaging,Surgical Robotics"
                     data-keywords="Bone Surface Registration,Multi-Modal Imaging,Self-Supervised Learning,Implicit Neural Representations,Computer-Assisted Orthopedic Surgery,Point Cloud Registration,Unsigned Distance Field,Robotics"
                     data-authors="Luohong Wu,Matthias Seibold,Nicola A. Cavalcanti,Yunke Ao,Roman Flepp,Aidana Massalimova,Lilian Calvet,Philipp F√ºrnstahl">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14286v1.html">NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Luohong Wu, Matthias Seibold, Nicola A. Cavalcanti et al.
                </div>

                <div class="paper-summary">
                    This paper introduces NeuralBoneReg, a novel self-supervised, surface-based framework for robust and accurate multi-modal bone surface registration in computer- and robot-assisted orthopedic surgery (CAOS). By leveraging 3D point clouds and implicit neural representations, the method achieves precise cross-registration between preoperative and intraoperative data despite significant modality heterogeneity, matching or surpassing state-of-the-art supervised approaches. The framework demonstrates strong generalizability across diverse anatomies and imaging modalities, enhancing the accuracy of surgical plan transfer.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthopedic Surgery</span>
                    
                    <span class="domain-tag">Spinal Surgery</span>
                    
                    <span class="domain-tag">Interventional Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Surgical Robotics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14286v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14286v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14286v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14286v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14255v1"
                     data-domains="Telemedicine,Clinical Documentation,Medical Transcription,Patient-Provider Communication,Healthcare AI"
                     data-keywords="African English accents,ASR,Speech Recognition,Multidomain Benchmark,LLM,Medical AI,Hallucinations,Voice Interfaces"
                     data-authors="Gabrial Zencha Ashungafac,Mardhiyah Sanni,Busayo Awobade,Alex Gichamba,Tobi Olatunji">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14255v1.html">AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Gabrial Zencha Ashungafac, Mardhiyah Sanni, Busayo Awobade et al.
                </div>

                <div class="paper-summary">
                    AfriSpeech-MultiBench is presented as the first domain-specific evaluation suite for over 100 African English accents across 10+ countries, spanning seven application domains including Medical. The benchmark rigorously evaluates diverse ASR and LLM-based speech recognition systems, revealing nuanced performance variations and highlighting critical challenges such as domain-specific named entity recognition and hallucinations.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Clinical Documentation</span>
                    
                    <span class="domain-tag">Medical Transcription</span>
                    
                    <span class="domain-tag">Patient-Provider Communication</span>
                    
                    <span class="domain-tag">Healthcare AI</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14255v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14255v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14255v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14255v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14188v1"
                     data-domains="Neurology,Cognitive Neuroscience,Rehabilitation Medicine,Infectious Diseases (Post-acute sequelae),Psychiatry"
                     data-keywords="Long COVID,brain fog,cognitive impairment,right inferior insula,neuromodulation,ultrasound stimulation,perceptual processing,false alarms,EEG,MRI"
                     data-authors="Jinhao Yang,Shaojiong Zhou,Zhibin Wang,Jiahua Xu,Jia Chen,Zhouqian Yin,Tao Wei,Chaofan Geng,Xiaoduo Liu,Xiang Li,Xiaoyu Zhou,Kun Li,Ruolei Gu,Raymond Dolan,Yi Tang,Yunzhe Liu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14188v1.html">A region-specific brain dysfunction underlies cognitive impairment in long COVID brain fog</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.NC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jinhao Yang, Shaojiong Zhou, Zhibin Wang et al.
                </div>

                <div class="paper-summary">
                    This research identifies a specific regional brain dysfunction in the right inferior insula that underlies cognitive impairment in Long COVID "brain fog," characterized by aberrant perceptual processing and increased impulsive responses. Crucially, the study demonstrates that non-invasive neuromodulation targeting this region can rescue the observed perceptual deficit, establishing it as a novel therapeutic target.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Cognitive Neuroscience</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                    <span class="domain-tag">Infectious Diseases (Post-acute sequelae)</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14188v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14188v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14188v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14188v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14187v1"
                     data-domains="Cardiovascular Surgery,Interventional Radiology,Diagnostic Imaging,Vascular Medicine,Cardiology"
                     data-keywords="Aorta Segmentation,Hierarchical Semantic Learning,Curriculum Learning,Fractal Softmax,Class Imbalance,Medical Image Analysis,Deep Learning,Vascular Imaging"
                     data-authors="Pengcheng Shi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14187v1.html">Hierarchical Semantic Learning for Multi-Class Aorta Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Pengcheng Shi
                </div>

                <div class="paper-summary">
                    This paper introduces a novel deep learning framework for multi-class aorta segmentation, addressing challenges of hierarchical anatomical relationships and severe class imbalance. It leverages a curriculum learning strategy combined with a novel fractal softmax for hierarchical semantic learning, achieving significant improvements in segmentation accuracy and efficiency. The proposed method accelerates model convergence and provides a fivefold inference speed-up, making it suitable for real-time clinical applications.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiovascular Surgery</span>
                    
                    <span class="domain-tag">Interventional Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Vascular Medicine</span>
                    
                    <span class="domain-tag">Cardiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14187v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14187v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14187v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14187v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14185v1"
                     data-domains="Public Health,Emergency Medicine,Trauma Care,Rehabilitation,Geriatrics,Preventive Medicine"
                     data-keywords="Autonomous Vehicles,AV Safety,Dataset,Motion Planning,Perception,Prediction,Traffic Safety,Injury Prevention"
                     data-authors="Xiangyu Li,Chen Wang,Yumao Liu,Dengbo He,Jiahao Zhang,Ke Ma">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14185v1.html">PAVE: An End-to-End Dataset for Production Autonomous Vehicle Evaluation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xiangyu Li, Chen Wang, Yumao Liu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces PAVE, the first end-to-end benchmark dataset specifically designed for evaluating the real behavioral safety of production autonomous vehicles (AVs). Collected entirely in autonomous-driving mode from real-world AVs, PAVE provides a rich, continuously expanding resource for analyzing AV driving behavior and ensuring their safety in diverse scenarios, unlike previous datasets primarily used for early perception and prediction training.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Trauma Care</span>
                    
                    <span class="domain-tag">Rehabilitation</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14185v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14185v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14185v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14185v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14135v1"
                     data-domains="Healthcare automation,Emergency medicine (e.g., resuscitation),Clinical operations management,AI in healthcare,Robotics in healthcare"
                     data-keywords="Multi-agent reinforcement learning (MARL),Generalized Nash Equilibrium (GNE),Fairness,Workload allocation,Healthcare automation,Demand-side management,Resuscitation simulator,Adaptive constraints"
                     data-authors="Promise Ekpo,Saesha Agarwal,Felix Grimm,Lekan Molu,Angelique Taylor">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14135v1.html">Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Promise Ekpo, Saesha Agarwal, Felix Grimm et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Fair-GNE, a novel multi-agent reinforcement learning (MARL) approach that leverages a constrained Generalized Nash Equilibrium (GNE)-seeking game framework to achieve self-enforceable and certifiably fair workload allocation in multiagent healthcare automation. Unlike existing MARL methods that rely on post-hoc reward shaping, Fair-GNE intrinsically steers group policy to an equitable collective equilibrium, significantly improving workload balance while maintaining high task success in demand-side healthcare worker settings.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Healthcare automation</span>
                    
                    <span class="domain-tag">Emergency medicine (e.g., resuscitation)</span>
                    
                    <span class="domain-tag">Clinical operations management</span>
                    
                    <span class="domain-tag">AI in healthcare</span>
                    
                    <span class="domain-tag">Robotics in healthcare</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14135v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14135v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14135v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14135v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14133v1"
                     data-domains="Oncology,Clinical Epidemiology,Pharmaceutical Research,Public Health Policy,Health Services Research,Pharmacovigilance"
                     data-keywords="Causal Inference,Survival Analysis,Synthetic Control,Time-to-Event,Hazard Trajectory,Observational Data,Cancer Treatment,Panel Data"
                     data-authors="Jessy Xinyi Han,Devavrat Shah">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14133v1.html">Synthetic Survival Control: Extending Synthetic Controls for "When-If" Decision</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jessy Xinyi Han, Devavrat Shah
                </div>

                <div class="paper-summary">
                    This paper introduces Synthetic Survival Control (SSC), a novel methodology extending synthetic control methods to estimate counterfactual hazard trajectories for time-to-event outcomes from observational data. Addressing challenges like censoring and non-random treatment assignment, SSC estimates how event timing would change under intervention by weighting observed trajectories of control units. The authors validate SSC using a multi-country cancer dataset, demonstrating that access to new therapies is associated with improved survival (lower hazard) compared to synthetic controls.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Clinical Epidemiology</span>
                    
                    <span class="domain-tag">Pharmaceutical Research</span>
                    
                    <span class="domain-tag">Public Health Policy</span>
                    
                    <span class="domain-tag">Health Services Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14133v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14133v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14133v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14133v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14119v1"
                     data-domains="Emergency Medicine,Prehospital Care,Critical Care Transport,Telehealth"
                     data-keywords="Emergency Medical Services,Real-time Analytics,Mobile Video,Multimodal Fusion,Pre-arrival,AI,LLM,rPPG,Telemedicine"
                     data-authors="Liuyi Jin,Amran Haroon,Radu Stoleru,Pasan Gunawardena,Michael Middleton,Jeeeun Kim">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14119v1.html">Real-Time Mobile Video Analytics for Pre-arrival Emergency Medical Services</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.MM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Liuyi Jin, Amran Haroon, Radu Stoleru et al.
                </div>

                <div class="paper-summary">
                    This paper introduces TeleEMS, a mobile live video analytics system designed to enhance pre-arrival emergency medical services (EMS) by integrating real-time audio and video streams. It addresses the limitations of current EMS infrastructure, which relies on manual interpretation of often overwhelming information, by providing multimodal inference to dispatchers and EMTs before arrival on scene. TeleEMS leverages domain-specialized AI models for symptom extraction, vital sign estimation, and protocol recommendations, demonstrating superior performance over general-purpose models.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Prehospital Care</span>
                    
                    <span class="domain-tag">Critical Care Transport</span>
                    
                    <span class="domain-tag">Telehealth</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14119v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14119v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14119v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14119v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14112v1"
                     data-domains="Medical Coding,Health Informatics,Clinical Documentation Improvement,Medical Billing,Public Health/Epidemiology"
                     data-keywords="ICD coding,Medical NLP,Synthetic data,Long-tail distribution,Clinical notes,Transformer models,Diagnostic codes,Data-centric AI"
                     data-authors="Truong Vo,Weiyi Wu,Kaize Ding">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14112v1.html">Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Truong Vo, Weiyi Wu, Kaize Ding
                </div>

                <div class="paper-summary">
                    This paper addresses the significant challenge of the long-tail distribution in automatic ICD coding from clinical text, where thousands of rare diagnostic codes are underrepresented. It proposes a data-centric framework that generates 90,000 high-quality synthetic discharge summaries to mitigate this data imbalance. The approach leads to modest but crucial improvements in macro-F1 for rare codes while maintaining strong micro-F1, outperforming prior state-of-the-art models and enhancing equity in long-tail ICD code prediction.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Coding</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Clinical Documentation Improvement</span>
                    
                    <span class="domain-tag">Medical Billing</span>
                    
                    <span class="domain-tag">Public Health/Epidemiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14112v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14112v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14112v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14112v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14110v1"
                     data-domains="Neonatology,Pediatric Neurology,Critical Care Medicine,Clinical Neurophysiology"
                     data-keywords="neonatal seizures,seizure prediction,EEG,ECG,convolutional neural network,machine learning,explainable AI,preictal,neonatology,critical care"
                     data-authors="Sithmini Ranasingha,Agasthi Haputhanthri,Hansa Marasinghe,Nima Wickramasinghe,Kithmin Wickremasinghe,Jithangi Wanigasinghe,Chamira U. S. Edussooriya,Joshua P. Kulasingham">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14110v1.html">A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ eess.SP</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sithmini Ranasingha, Agasthi Haputhanthri, Hansa Marasinghe et al.
                </div>

                <div class="paper-summary">
                    This research proposes a novel patient-independent convolutional neural network (CNN) model for the early prediction of neonatal seizures, distinguishing between interictal and preictal states using reduced montage EEG and ECG signals. The model achieved high accuracy, sensitivity, specificity, and F1-score (all above 96%) in predicting seizures up to 30 minutes prior to onset, demonstrating strong generalization capabilities and potential for minimally supervised deployment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neonatology</span>
                    
                    <span class="domain-tag">Pediatric Neurology</span>
                    
                    <span class="domain-tag">Critical Care Medicine</span>
                    
                    <span class="domain-tag">Clinical Neurophysiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14110v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14110v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14110v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14110v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14087v1"
                     data-domains="Radiology,Cardiology,Oncology (for tumor segmentation in planning/monitoring),General Diagnosis,Surgical Planning"
                     data-keywords="medical image segmentation,deep learning,U-Net,attention mechanisms,Grouped Coordinate Attention (GCA),ResNet-50,computer-aided diagnosis,CT imaging,MRI imaging"
                     data-authors="Jun Ding,Shang Gao">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14087v1.html">GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jun Ding, Shang Gao
                </div>

                <div class="paper-summary">
                    This paper introduces GCA-ResUNet, a novel and efficient convolutional neural network for medical image segmentation. It integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks to effectively capture long-range dependencies across channels and spatial locations, addressing limitations of traditional U-Net architectures and computational burdens of Transformers. GCA-ResUNet achieves high accuracy, evidenced by Dice scores of 86.11% on Synapse and 92.64% on ACDC datasets, outperforming state-of-the-art baselines with minimal computational overhead.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Oncology (for tumor segmentation in planning/monitoring)</span>
                    
                    <span class="domain-tag">General Diagnosis</span>
                    
                    <span class="domain-tag">Surgical Planning</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14087v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14087v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14087v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14087v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2511.14083v1"
                     data-domains="Orthopedic Surgery,Sports Medicine,Radiology,Diagnostic Imaging,Traumatology"
                     data-keywords="glenoid bone loss,shoulder instability,computed tomography (CT),deep learning,U-Net,segmentation,pre-operative planning,intraclass correlation coefficient (ICC)"
                     data-authors="Zhonghao Liu,Hanxue Gu,Qihang Li,Michael Fox,Jay M. Levin,Maciej A. Mazurowski,Brian C. Lau">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2511.14083v1.html">Automated glenoid bone loss measurement and segmentation in CT scans for pre-operative planning in shoulder instability</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-11-18</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zhonghao Liu, Hanxue Gu, Qihang Li et al.
                </div>

                <div class="paper-summary">
                    A fully automated deep learning pipeline was developed and validated for measuring glenoid bone loss on 3D CT scans using a multi-stage approach. This method demonstrated strong agreement with consensus readings and superior consistency compared to manual surgeon measurements, making it a reliable tool for pre-operative planning in shoulder instability.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthopedic Surgery</span>
                    
                    <span class="domain-tag">Sports Medicine</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Traumatology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2511.14083v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2511.14083v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2511.14083v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2511.14083v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-11-19 06:27:52</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>