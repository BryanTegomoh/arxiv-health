<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">23</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">23</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">83</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Neurology (4), Pathology (3), Oncology (3)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Neurology">Neurology (4)</option>
                        
                        <option value="Pathology">Pathology (3)</option>
                        
                        <option value="Oncology">Oncology (3)</option>
                        
                        <option value="Geriatrics">Geriatrics (3)</option>
                        
                        <option value="Rehabilitation Medicine">Rehabilitation Medicine (3)</option>
                        
                        <option value="Radiology">Radiology (3)</option>
                        
                        <option value="Pharmacology">Pharmacology (3)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (3)</option>
                        
                        <option value="Public Health">Public Health (2)</option>
                        
                        <option value="Sleep Medicine">Sleep Medicine (2)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2510.21664v1"
                     data-domains="Dermatology,Pathology,Oncology"
                     data-keywords="Dermatopathology,Whole-slide images (WSIs),Foundation models,Feature extraction,Skin lesion classification,Machine learning,Digital pathology,Automated diagnosis"
                     data-authors="Riya Gupta,Yiwei Zong,Dennis H. Murphree">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21664v1.html">Foundation Models in Dermatopathology: Skin Tissue Classification</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Riya Gupta, Yiwei Zong, Dennis H. Murphree
                </div>

                <div class="paper-summary">
                    This study evaluates the effectiveness of two foundation models, UNI and Virchow2, as feature extractors for automating the classification of whole-slide images (WSIs) in dermatopathology. By aggregating patch-level embeddings to create slide-level features, the research successfully classified WSIs into melanocytic, basaloid, and squamous lesions using various machine learning classifiers. The findings highlight the potential of foundation models to provide a scalable and efficient approach for dermatopathological diagnosis, with Virchow2-extracted features achieving up to 90% accuracy with logistic regression.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21664v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21664v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21664v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21664v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21596v1"
                     data-domains="Epilepsy,Neurology,Neurosurgery,Clinical Neurophysiology,Medical Imaging"
                     data-keywords="automated detection,interictal epileptic spikes,magnetoencephalography (MEG),deep learning,convolutional neural network (CNN),artificial neural network (ANN),noisy annotations,presurgical evaluation"
                     data-authors="Pauline Mouches,Julien Jung,Armand Demasson,Agn√®s Guinard,Romain Bouet,Rosalie Marchal,Romain Quentin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21596v1.html">Automated interictal epileptic spike detection from simple and noisy annotations in MEG data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Pauline Mouches, Julien Jung, Armand Demasson et al.
                </div>

                <div class="paper-summary">
                    This paper introduces two deep learning models, a feature-based Artificial Neural Network (ANN) and a Convolutional Neural Network (CNN), for automated interictal epileptic spike detection in Magnetoencephalography (MEG) data. Designed to operate effectively even with simple, noisy, and single-expert annotations common in clinical practice, these models significantly outperform a state-of-the-art method on a holdout test set. The research highlights the robustness of these architectures and proposes an interactive machine learning strategy to improve annotation quality and accelerate the overall diagnostic process in drug-resistant epilepsy.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Epilepsy</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Neurosurgery</span>
                    
                    <span class="domain-tag">Clinical Neurophysiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21596v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21596v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21596v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21596v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21585v1"
                     data-domains="Neurology,Clinical Neurophysiology,Sleep Medicine,Psychiatry,Cognitive Neuroscience"
                     data-keywords="EEG,Foundation Model,Pretraining,Deep Learning,Generalization,Seizure Detection,Sleep Staging,Clinical Neuroscience"
                     data-authors="Yassine El Ouahidi,Jonathan Lys,Philipp Th√∂lke,Nicolas Farrugia,Bastien Pasdeloup,Vincent Gripon,Karim Jerbi,Giulia Lioi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21585v1.html">REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yassine El Ouahidi, Jonathan Lys, Philipp Th√∂lke et al.
                </div>

                <div class="paper-summary">
                    REVE is a novel foundation model for Electroencephalography (EEG) designed to overcome the challenges of data heterogeneity across diverse recording setups. By employing a unique 4D positional encoding and large-scale masked autoencoding pretraining on an unprecedented 25,000 subjects, REVE achieves state-of-the-art performance and strong generalization across 10 downstream EEG tasks, significantly advancing AI applications in clinical neuroscience.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Clinical Neurophysiology</span>
                    
                    <span class="domain-tag">Sleep Medicine</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Cognitive Neuroscience</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21585v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21585v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21585v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21585v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21574v1"
                     data-domains="Drug Discovery,Pharmacology,Toxicology,Infectious Diseases (HIV),Pharmaceutical Research,Medicinal Chemistry"
                     data-keywords="Graph Neural Networks,GNNs,classical algorithms,pretraining,inductive bias,molecular property prediction,drug discovery,toxicity prediction,Open Graph Benchmark,CLRS"
                     data-authors="Jason Wu,Petar Veliƒçkoviƒá">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21574v1.html">Leveraging Classical Algorithms for Graph Neural Networks</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jason Wu, Petar Veliƒçkoviƒá
                </div>

                <div class="paper-summary">
                    This paper explores pretraining Graph Neural Networks (GNNs) on classical algorithms to enhance their generalization capabilities for molecular property prediction. By initializing and freezing layers of molecular prediction GNNs with weights learned from 24 classical algorithms, the authors demonstrate consistent performance improvements or ties on tasks such as HIV inhibition and clinical toxicity prediction, outperforming randomly initialized baselines.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Toxicology</span>
                    
                    <span class="domain-tag">Infectious Diseases (HIV)</span>
                    
                    <span class="domain-tag">Pharmaceutical Research</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21574v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21574v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21574v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21574v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21551v1"
                     data-domains="Cardiology,Diagnostic Medicine,Medical AI,Health Informatics,Electrophysiology"
                     data-keywords="ECG diagnosis,Zero-shot learning,Multimodal AI,Interpretability,Clinical knowledge,Differential diagnosis,Cardiovascular disease,Machine learning"
                     data-authors="Jialu Tang,Hung Manh Pham,Ignace De Lathauwer,Henk S. Schipper,Yuan Lu,Dong Ma,Aaqib Saeed">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21551v1.html">Interpretable Multimodal Zero-Shot ECG Diagnosis via Structured Clinical Knowledge Alignment</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jialu Tang, Hung Manh Pham, Ignace De Lathauwer et al.
                </div>

                <div class="paper-summary">
                    ZETA is a novel zero-shot multimodal framework for interpretable ECG diagnosis that addresses transparency and generalization challenges in automated systems. It achieves this by comparing ECG signals against structured, LLM-assisted and expert-validated positive and negative clinical observations, mimicking differential diagnosis. The framework demonstrates competitive zero-shot classification performance while providing enhanced interpretability, grounding predictions in specific, clinically relevant features.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Electrophysiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21551v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21551v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21551v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21551v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21495v1"
                     data-domains="Obstetrics and Gynecology,Maternal-Fetal Medicine,Diagnostic Imaging"
                     data-keywords="Placental Abruption,Hematoma Detection,Ultrasound Imaging,Few-Shot Learning,YOLOv11n,Computer-Aided Diagnosis,Deep Learning,Obstetrics"
                     data-authors="Xiaoqing Liu,Jitai Han,Hua Yan,Peng Li,Sida Tang,Ying Li,Kaiwen Zhang,Min Yu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21495v1.html">An Automatic Detection Method for Hematoma Features in Placental Abruption Ultrasound Images Based on Few-Shot Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xiaoqing Liu, Jitai Han, Hua Yan et al.
                </div>

                <div class="paper-summary">
                    This paper introduces EH-YOLOv11n, an improved deep learning model utilizing few-shot learning for the automatic detection of hematoma features in placental abruption ultrasound images. The model enhances detection accuracy and reliability by integrating advanced convolutional techniques and a cascaded group attention mechanism. Achieving 78% accuracy, it significantly outperforms existing YOLO variants and offers a robust, real-time solution for computer-aided diagnosis in a critical obstetric condition.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Obstetrics and Gynecology</span>
                    
                    <span class="domain-tag">Maternal-Fetal Medicine</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21495v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21495v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21495v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21495v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21479v1"
                     data-domains="Pathology,Oncology,Cancer Diagnosis,Histology"
                     data-keywords="Histopathology,Cancer Subtyping,Tissue-Cell Interaction,Deep Learning,Recurrent Neural Networks,Transformer,Computational Pathology,Digital Pathology"
                     data-authors="Yating Huang,Qijun Yang,Lintao Xiang,Hujun Yin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21479v1.html">ITC-RWKV: Interactive Tissue-Cell Modeling with Recurrent Key-Value Aggregation for Histopathological Subtyping</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yating Huang, Qijun Yang, Lintao Xiang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces ITC-RWKV, a novel dual-stream architecture designed for accurate histopathological subtyping by integrating multi-scale information. It addresses the limitation of existing foundation models by explicitly modeling cell-level features using a recurrent, linear-complexity key-value aggregation model and a bidirectional tissue-cell interaction module. Experiments on four benchmarks demonstrate superior performance, highlighting the critical role of cell-level aggregation and tissue-cell interaction in fine-grained computational pathology.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Cancer Diagnosis</span>
                    
                    <span class="domain-tag">Histology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21479v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21479v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21479v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21479v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21469v1"
                     data-domains="Geriatrics,Assistive Technology,Rehabilitation Medicine,Patient Care,Medical Robotics"
                     data-keywords="Social Robots,Resilient AI,Healthcare Technology,Elderly Care,Trust in AI,Human-Robot Interaction,Artificial Intelligence,Robotics"
                     data-authors="Domenico Palmisano,Giuseppe Palestra,Berardina Nadja De Carolis">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21469v1.html">Enhancing Social Robots through Resilient AI</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.RO</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Domenico Palmisano, Giuseppe Palestra, Berardina Nadja De Carolis
                </div>

                <div class="paper-summary">
                    This paper highlights the critical necessity of resilience in social robots, particularly as Artificial Intelligence (AI) becomes more integrated into sensitive domains like healthcare. It posits that resilient AI, characterized by its ability to maintain essential functions under adverse conditions, is fundamental for fostering trust in robots, an indispensable element when interacting with elderly populations who often demonstrate low trust in such systems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Assistive Technology</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                    <span class="domain-tag">Patient Care</span>
                    
                    <span class="domain-tag">Medical Robotics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21469v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21469v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21469v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21469v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21464v1"
                     data-domains="Radiology,Pulmonology,Cardiology,Critical Care Medicine"
                     data-keywords="Chest X-ray,Interpretability,Deep Learning,Medical Imaging AI,Sparse Autoencoders,BiomedCLIP,Diagnostic Classifier,MIMIC-CXR"
                     data-authors="Yiming Tang,Wenjia Zhong,Rushi Shah,Dianbo Liu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21464v1.html">CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray Diagnosis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yiming Tang, Wenjia Zhong, Rushi Shah et al.
                </div>

                <div class="paper-summary">
                    This paper introduces CXR-LanIC, a novel framework designed to enhance the interpretability of deep learning models for chest X-ray diagnosis, addressing the critical 'black-box' problem. It achieves this by training transcoder-based sparse autoencoders on a diagnostically-trained BiomedCLIP classifier to discover approximately 5,000 clinically relevant, monosemantic visual patterns from MIMIC-CXR data. CXR-LanIC provides transparent attribution for predictions, decomposing them into these verifiable patterns, while maintaining competitive diagnostic accuracy on key findings.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Critical Care Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21464v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21464v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21464v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21464v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21457v1"
                     data-domains="Public Health,Epidemiology,Social Medicine,Pharmacovigilance,Health Policy,Precision Medicine"
                     data-keywords="Causal Inference,Treatment Effects,Network Analysis,Graph Neural Networks,Domain Adversarial Training,Interference,Covariate Shift,Homophily"
                     data-authors="Daan Caljon,Jente Van Belle,Wouter Verbeke">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21457v1.html">Estimating Treatment Effects in Networks using Domain Adversarial Training</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Daan Caljon, Jente Van Belle, Wouter Verbeke
                </div>

                <div class="paper-summary">
                    This paper introduces HINet, a novel method that combines Graph Neural Networks (GNNs) with Domain Adversarial Training (DAT) to accurately estimate heterogeneous treatment effects in network settings. HINet addresses two critical challenges: the often-unrealistic assumption of known exposure mappings, and the newly identified problem of network-level covariate shift arising from homophily and treatment assignment interactions. The approach demonstrates effectiveness on synthetic and semi-synthetic network datasets, providing robust treatment effect estimates under complex interference patterns.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Social Medicine</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Health Policy</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21457v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21457v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21457v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21457v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21445v1"
                     data-domains="Remote patient monitoring,Geriatrics,Chronic disease management,Emergency medicine,Telemedicine,Mental health monitoring,Preventive care"
                     data-keywords="Remote health monitoring,Wearable devices,Multimodal Large Language Models (MLLMs),Internet of Things (IoT),Anomaly detection,Fall detection,Natural language processing (NLP),Human-machine interaction,Telehealth,Patient monitoring"
                     data-authors="Thanh Cong Ho,Farah Kharrat,Abderrazek Abid,Fakhri Karray">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21445v1.html">REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Thanh Cong Ho, Farah Kharrat, Abderrazek Abid et al.
                </div>

                <div class="paper-summary">
                    REMONI is an autonomous remote health monitoring system designed to enhance human-machine interaction in healthcare by integrating wearables, IoT, and Multimodal Large Language Models (MLLMs). It continuously collects physiological, activity, and visual data, utilizing anomaly detection for emergencies, and enables healthcare workers to interact with an intelligent agent for real-time patient status, activity, and emotional state. The system has been demonstrated as implementable and scalable, promising reductions in professional workload and healthcare costs.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Remote patient monitoring</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Chronic disease management</span>
                    
                    <span class="domain-tag">Emergency medicine</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21445v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21445v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21445v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21445v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21424v1"
                     data-domains="Remote Patient Monitoring,Geriatrics,Rehabilitation Medicine,Chronic Disease Management,Preventive Care,Assisted Living"
                     data-keywords="Vision Language Models,Human Activity Recognition,Remote Health Monitoring,Generative AI,Deep Learning,Healthcare AI,Activity Recognition Datasets,Intelligent Healthcare Systems"
                     data-authors="Abderrazek Abid,Thanh-Cong Ho,Fakhri Karray">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21424v1.html">Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Abderrazek Abid, Thanh-Cong Ho, Fakhri Karray
                </div>

                <div class="paper-summary">
                    This paper explores the application of Vision Language Models (VLMs) for Human Activity Recognition (HAR) in remote health monitoring, an area previously underexplored. The authors introduce a novel descriptive caption dataset and comprehensive evaluation methods to address the challenge of evaluating VLMs' dynamic outputs. Their comparative experiments show that VLMs achieve performance comparable to, and often surpass, state-of-the-art deep learning models in HAR accuracy, establishing a strong benchmark for VLM integration into intelligent healthcare systems.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Remote Patient Monitoring</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                    <span class="domain-tag">Chronic Disease Management</span>
                    
                    <span class="domain-tag">Preventive Care</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21424v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21424v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21424v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21424v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21389v1"
                     data-domains="Sleep Medicine,Neurology,Pulmonology (related to sleep-disordered breathing),Diagnostic Medicine"
                     data-keywords="Explainable AI,Polysomnography,Arousal Diagnostics,Sleep Medicine,Human-AI Collaboration,Clinical Workflow,Trustworthy AI,User Study"
                     data-authors="Stefan Kraft,Andreas Theissler,Vera Wienhausen-Wilke,Gjergji Kasneci,Hendrik Lensch">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21389v1.html">Assessing the Real-World Utility of Explainable AI for Arousal Diagnostics: An Application-Grounded User Study</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Stefan Kraft, Andreas Theissler, Vera Wienhausen-Wilke et al.
                </div>

                <div class="paper-summary">
                    This application-grounded user study evaluates the real-world utility of Explainable AI (XAI) for scoring nocturnal arousal events in polysomnography, comparing black-box (BB) and white-box (WB) AI assistance at different integration points. It demonstrates that strategically timed, transparent AI assistance significantly enhances diagnostic accuracy and reduces inter-rater variability among sleep medicine practitioners, while also achieving high user acceptance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Sleep Medicine</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Pulmonology (related to sleep-disordered breathing)</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21389v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21389v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21389v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21389v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21371v1"
                     data-domains="Infectious Disease Epidemiology,Public Health,Vaccinology,Mathematical Biology"
                     data-keywords="SIR models,Demography,Stochastic,Vaccination,Eradication,Endemic,Mathematical Epidemiology,Asymptotic Behavior"
                     data-authors="Javier L√≥pez-de-la-Cruz,Susana Merch√°n,Felipe Rivero,Javier Rodrigo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21371v1.html">SIR models with demography, random transmission coefficient and non-autonomous vaccination rate</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.PE</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Javier L√≥pez-de-la-Cruz, Susana Merch√°n, Felipe Rivero et al.
                </div>

                <div class="paper-summary">
                    This paper analyzes SIR models that incorporate realistic features like demography, a bounded random transmission coefficient, and a time-dependent vaccination strategy targeting susceptible individuals. It establishes the existence and uniqueness of global non-negative solutions for these complex models, deriving crucial conditions that determine whether a disease will be eradicated or become endemic.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Infectious Disease Epidemiology</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Vaccinology</span>
                    
                    <span class="domain-tag">Mathematical Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21371v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21371v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21371v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21371v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21362v1"
                     data-domains="Nuclear Medicine,Radiation Oncology,Medical Physics,Radiology,Personalized Medicine"
                     data-keywords="Nuclear medicine,dosimetry,3D activity maps,planar scintigraphy,patient-specific AI,reinforced learning,diffusion models,177Lu-PSMA"
                     data-authors="Alejandro Lopez-Montes,Robert Seifert,Astrid Delker,Guido Boening,Jiahui Wang,Christoph Clement,Ali Afshar-Oromieh,Axel Rominger,Kuangyu Shi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21362v1.html">Patient-specific AI for generation of 3D dosimetry imaging from two 2D-planar measurements</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Alejandro Lopez-Montes, Robert Seifert, Astrid Delker et al.
                </div>

                <div class="paper-summary">
                    This paper proposes a novel patient-specific reinforced learning approach to generate 3D activity maps for dosimetry from just two 2D planar scintigraphy images (anterior and posterior). By developing patient-specific datasets and utilizing AI models like 3DUnet and diffusion models, the method aims to replace expensive and time-consuming 3D SPECT acquisitions with fast planar imaging. The study demonstrates enhanced accuracy and organ delineation, offering a paradigm shift for nuclear medicine dosimetry.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Nuclear Medicine</span>
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21362v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21362v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21362v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21362v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21346v1"
                     data-domains="Dermatology (skin lesion analysis),Pathology (histopathological image analysis),Radiology (tumor detection, lesion characterization),Ophthalmology (retinal disease diagnosis),Medical Imaging Analysis"
                     data-keywords="Multi-modal fusion,CNN-Transformer,CLIP,Disease recognition,Image-text learning,Few-shot learning,Complex environments,Computer Vision"
                     data-authors="Lemin Liu,Fangchao Hu,Honghua Jiang,Yaru Chen,Limin Liu,Yongliang Qiao">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21346v1.html">CT-CLIP: A Multi-modal Fusion Framework for Robust Apple Leaf Disease Recognition in Complex Environments</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.60</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Lemin Liu, Fangchao Hu, Honghua Jiang et al.
                </div>

                <div class="paper-summary">
                    This study introduces CT-CLIP, a multi-modal fusion framework that integrates CNNs for local feature extraction, Vision Transformers for global structural relationships, and an Adaptive Feature Fusion Module for optimal information coupling, to address the challenge of apple leaf disease recognition amidst phenotypic heterogeneity and complex environments. By further employing a multimodal image-text learning approach using pre-trained CLIP weights, it significantly enhances recognition accuracy, especially under few-shot conditions, by aligning visual features with semantic disease descriptions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology (skin lesion analysis)</span>
                    
                    <span class="domain-tag">Pathology (histopathological image analysis)</span>
                    
                    <span class="domain-tag">Radiology (tumor detection, lesion characterization)</span>
                    
                    <span class="domain-tag">Ophthalmology (retinal disease diagnosis)</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21346v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21346v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21346v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21346v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21337v1"
                     data-domains="Cell Biology,Pharmacology,Drug Development,Biomedical Engineering,Computational Pathology"
                     data-keywords="3D cell morphology,perturbation prediction,machine learning,diffusion models,VQGAN,virtual cell,drug discovery,computational biology"
                     data-authors="Reed Naidoo,Matt De Vries,Olga Fourkioti,Vicky Bousgouni,Mar Arias-Garcia,Maria Portillo-Malumbres,Chris Bakal">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21337v1.html">Morphologically Intelligent Perturbation Prediction with FORM</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Reed Naidoo, Matt De Vries, Olga Fourkioti et al.
                </div>

                <div class="paper-summary">
                    FORM is a novel machine learning framework that predicts perturbation-induced changes in complex three-dimensional cellular structures, addressing the limitations of existing 2D computational models. It integrates a VQGAN-based morphology encoder with a diffusion model to simulate perturbed cell states, combinatorial effects, and signaling activity. This advancement aims to realize accurate 3D virtual cell models for enhanced biomedical research and drug development.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cell Biology</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Drug Development</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21337v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21337v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21337v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21337v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21324v1"
                     data-domains="Radiology,Diagnostic Imaging,Pulmonology,Emergency Medicine,Internal Medicine"
                     data-keywords="Chest X-ray,LLM-based agents,multi-stage reasoning,diagnostic planning,medical imaging,radiology,tool orchestration,AI in medicine"
                     data-authors="Jinhui Lou,Yan Yang,Zhou Yu,Zhenqi Fu,Weidong Han,Qingming Huang,Jun Yu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21324v1.html">CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jinhui Lou, Yan Yang, Zhou Yu et al.
                </div>

                <div class="paper-summary">
                    CXRAgent introduces a novel director-orchestrated, multi-stage LLM-based agent for advanced Chest X-ray interpretation, addressing limitations of existing models in adaptability and reliability. It integrates strategic tool invocation with an Evidence-driven Validator, dynamic diagnostic planning via an expert team, and collaborative decision-making, demonstrating strong performance, visual evidence generation, and generalization across diverse clinical tasks. This framework aims to enhance diagnostic accuracy and credibility in automated CXR analysis.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Internal Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21324v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21324v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21324v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21324v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21312v1"
                     data-domains="Clinical Trials,Adaptive Treatment Strategies,Personalized Medicine,Medical Ethics,Public Health Resource Allocation"
                     data-keywords="Multi-armed bandits,Nash regret,Fairness,UCB,p-mean regret,Sub-Gaussian rewards,Adaptive clinical trials,Social welfare"
                     data-authors="Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21312v1.html">Revisiting Social Welfare in Bandits: UCB is (Nearly) All You Need</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Dhruv Sarkar, Nishant Pandey, Sayak Ray Chowdhury
                </div>

                <div class="paper-summary">
                    This paper introduces a simplified and more robust approach to minimize Nash regret and its generalized form, $p$-mean regret, in stochastic multi-armed bandits, metrics designed to ensure fairness in reward distribution. It demonstrates that a standard Upper Confidence Bound (UCB) algorithm, preceded by a uniform exploration phase, achieves near-optimal performance under significantly weaker assumptions, offering a broadly applicable solution for fairness-aware decision-making.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Trials</span>
                    
                    <span class="domain-tag">Adaptive Treatment Strategies</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                    <span class="domain-tag">Medical Ethics</span>
                    
                    <span class="domain-tag">Public Health Resource Allocation</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21312v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21312v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21312v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21312v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21265v1"
                     data-domains="Neurology,Psychiatry,Neuroscience,Rehabilitation Medicine"
                     data-keywords="Transcranial Photobiomodulation (tPBM),Pulsed Wave (PW),EEG Microstates,Neuromodulation,Brain Functional Connectivity,Machine Learning,Biomarker,Neuropsychiatric Disorders"
                     data-authors="He Jiangshan,Xie Hui,Yang Yuqiang,Jia Chunli,Liang Dan,Zhang Lianghua,Wang Xiaoyu,Luo Tianyi,Dong Zexiao,Yang Huiting,Pan Yang,Zhen Yuan,Jiang Mingzhe,Chen Xueli">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21265v1.html">EEG Dynamic Microstate Patterns Induced by Pulsed Wave Transcranial Photobiomodulation Therapy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ q-bio.NC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> He Jiangshan, Xie Hui, Yang Yuqiang et al.
                </div>

                <div class="paper-summary">
                    Pulsed wave (PW) transcranial photobiomodulation (tPBM) therapy significantly alters specific brain microstate dynamics, with its neural mechanisms previously unclear. This randomized, single-blind, crossover study used EEG microstate analysis and machine learning to demonstrate that PW tPBM selectively enhances functional network efficiency and duration of microstate C, identifying it as a robust state-dependent biomarker for optimizing tPBM protocols.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                    <span class="domain-tag">Rehabilitation Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21265v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21265v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21265v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21265v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21260v1"
                     data-domains="Oncology,Pathology,Diagnostic Imaging,Prostate Cancer,Breast Cancer"
                     data-keywords="Light-sheet microscopy,LSFM,Cancer pathology,Oncology,3D imaging,Diagnostic precision,Tumor architecture,Deep tissue imaging"
                     data-authors="Uma Pisaroviƒá,Taichi Ochi,Iryna Samarska,Ludovico Silvestri,Thiemo J. A. van Nijnatten,Loes F. S. Kooreman,Tom Marcelissen,Axel zur Hausen,Anna Schueth">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21260v1.html">Light-sheet microscopy to assess cancer pathology: current views and future trends</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Uma Pisaroviƒá, Taichi Ochi, Iryna Samarska et al.
                </div>

                <div class="paper-summary">
                    This paper reviews the current applications and future potential of light-sheet fluorescence microscopy (LSFM) in cancer pathology. It highlights LSFM's capability for visualizing and analyzing a variety of cancer tissue samples, including patient-derived specimens, organoids, biopsies, and murine models, with specific examples in prostate and breast cancer. The authors propose the integration of advanced LSFM into clinical workflows for high-throughput, three-dimensional imaging of intact cancer specimens to enhance diagnostic precision and provide novel insights into tumor architecture.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Prostate Cancer</span>
                    
                    <span class="domain-tag">Breast Cancer</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21260v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21260v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21260v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21260v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21228v1"
                     data-domains="Emergency Medicine,Prehospital Care,Medical Dispatch,Clinical Informatics"
                     data-keywords="Emergency Medical Dispatch,Multi-Agent Systems,Large Language Models,Clinical Taxonomy,AI in Healthcare,Dispatcher Training,Simulation,Decision Support"
                     data-authors="Xiang Li,Huizi Yu,Wenkong Wang,Yiran Wu,Jiayan Zhou,Wenyue Hua,Xinxin Lin,Wenjia Tan,Lexuan Zhu,Bingyi Chen,Guang Chen,Ming-Li Chen,Yang Zhou,Zhao Li,Themistocles L. Assimes,Yongfeng Zhang,Qingyun Wu,Xin Ma,Lingyao Li,Lizhou Fan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21228v1.html">DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xiang Li, Huizi Yu, Wenkong Wang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces DispatchMAS, a novel taxonomy-grounded, Large Language Model (LLM)-powered multi-agent system (MAS) designed to simulate realistic Emergency Medical Dispatch (EMD) scenarios. The system, comprising Caller and Dispatcher Agents, was rigorously evaluated by physicians and through automated linguistic analysis, demonstrating high clinical plausibility, effectiveness in dispatch, and efficacy in providing guidance. This research offers a robust framework for improving EMD processes through AI-driven simulation and potential future real-time decision support.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Emergency Medicine</span>
                    
                    <span class="domain-tag">Prehospital Care</span>
                    
                    <span class="domain-tag">Medical Dispatch</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21228v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21228v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21228v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21228v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2510.21153v1"
                     data-domains="Drug Discovery,Pharmaceutical Sciences,Medicinal Chemistry,Oncology (specifically for EGFR inhibitors),Computational Biology,Pharmacology"
                     data-keywords="Reinforcement Learning,Diffusion Models,Molecular Design,Multi-objective Optimization,Uncertainty-Aware,Drug Discovery,3D Molecules,ADMET"
                     data-authors="Lianghong Chen,Dongkyu Eugene Kim,Mike Domaratzki,Pingzhao Hu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2510.21153v1.html">Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-10-24</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Lianghong Chen, Dongkyu Eugene Kim, Mike Domaratzki et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an uncertainty-aware Reinforcement Learning (RL) framework to guide 3D molecular diffusion models, addressing the challenge of multi-objective molecular design. It leverages surrogate models with predictive uncertainty to dynamically shape reward functions, optimizing both molecular quality and multiple property objectives. The method consistently outperforms baselines, generating drug-like molecules with promising binding stability comparable to known EGFR inhibitors.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Pharmaceutical Sciences</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Oncology (specifically for EGFR inhibitors)</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2510.21153v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2510.21153v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="http://arxiv.org/pdf/2510.21153v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2510.21153v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-10-27 06:25:00</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>