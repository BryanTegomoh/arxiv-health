<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">46</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">46</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">124</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Radiology (13), Oncology (12), Diagnostic Imaging (11)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Radiology">Radiology (13)</option>
                        
                        <option value="Oncology">Oncology (12)</option>
                        
                        <option value="Diagnostic Imaging">Diagnostic Imaging (11)</option>
                        
                        <option value="Medical Imaging">Medical Imaging (10)</option>
                        
                        <option value="Public Health">Public Health (5)</option>
                        
                        <option value="Neurology">Neurology (4)</option>
                        
                        <option value="Neuroscience">Neuroscience (4)</option>
                        
                        <option value="Digital Health">Digital Health (4)</option>
                        
                        <option value="Diagnostic Medicine">Diagnostic Medicine (3)</option>
                        
                        <option value="Urology">Urology (3)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2512.04034v1"
                     data-domains="cs.LG"
                     data-keywords="cs.LG"
                     data-authors="Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.04034v1.html">Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hong Yang, Devroop Kar, Qi Yu et al.
                </div>

                <div class="paper-summary">
                    Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably pr...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.LG</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.04034v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.04034v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.04034v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.04034v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03994v1"
                     data-domains="Clinical decision support,Patient information management,Medical ethics and compliance,Health informatics,Drug safety and pharmacovigilance,Telemedicine"
                     data-keywords="LLM policy alignment,Out-of-distribution detection,Activation-space whitening,AI governance,Medical AI,Training-free,Euclidean norm,Compliance"
                     data-authors="Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03994v1.html">Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Oren Rachmil, Roy Betser, Itay Gershon et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel, training-free method for detecting policy violations in large language models (LLMs), crucial for sensitive domains like medical services. It re-frames the problem as out-of-distribution detection by applying activation-space whitening to transform the LLM's hidden activations, using the Euclidean norm as a compliance score. The approach is efficient, requires minimal data, and achieves state-of-the-art results on policy benchmarks, offering a practical framework for LLM governance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical decision support</span>
                    
                    <span class="domain-tag">Patient information management</span>
                    
                    <span class="domain-tag">Medical ethics and compliance</span>
                    
                    <span class="domain-tag">Health informatics</span>
                    
                    <span class="domain-tag">Drug safety and pharmacovigilance</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03994v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03994v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03994v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03994v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03883v1"
                     data-domains="cs.CV"
                     data-keywords="cs.CV"
                     data-authors="Jorge Tapias Gomez,Despoina Kanata,Aneesh Rangnekar,Christina Lee,Julio Garcia-Aguilar,Joshua Jesse Smith,Harini Veeraraghavan">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03883v1.html">Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Jorge Tapias Gomez, Despoina Kanata, Aneesh Rangnekar et al.
                </div>

                <div class="paper-summary">
                    Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images ...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.CV</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03883v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03883v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03883v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03883v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03880v1"
                     data-domains="Orthopedics,Endocrinology,Geriatrics,Radiology,Rheumatology,Diagnostic Imaging"
                     data-keywords="Bone strength,Topological Data Analysis (TDA),Persistent Homology,Micro-CT,Osteoporosis,Fracture risk,Machine learning,Bone microarchitecture"
                     data-authors="John Rick Manzanares,Richard Leslie Abel,Pawe≈Ç D≈Çotko">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03880v1.html">Leveraging topological data analysis to estimate bone strength from micro-CT as a surrogate for advanced imaging</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> John Rick Manzanares, Richard Leslie Abel, Pawe≈Ç D≈Çotko
                </div>

                <div class="paper-summary">
                    This study introduces Topological Data Analysis (TDA) to extract novel biomechanically relevant features from high-resolution bone micro-CT images for improved bone strength prediction. Machine learning models trained with these TDA-derived features, particularly those related to internal voids, significantly outperformed models using traditional bone morphometrics, offering a promising new framework for osteoporosis risk assessment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Orthopedics</span>
                    
                    <span class="domain-tag">Endocrinology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Rheumatology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03880v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03880v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03880v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03880v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03866v1"
                     data-domains="Geriatric Medicine,Infectious Diseases,Public Health,Epidemiology,Infection Prevention and Control"
                     data-keywords="aged care,agent-based model,contact matrix,infection control,airborne transmission,vaccination,simulation,public health"
                     data-authors="Haley Stone,C. Raina MacIntyre,Mohana Kunasekaran,Chris Poulos,David Heslop">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03866v1.html">Generating a Contact Matrix for Aged Care Settings in Australia: an agent-based model study</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.PE</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Haley Stone, C. Raina MacIntyre, Mohana Kunasekaran et al.
                </div>

                <div class="paper-summary">
                    This study developed an agent-based model (ABM) to simulate detailed staff and resident interactions within Australian aged care facilities, generating comprehensive contact matrices. It revealed significant heterogeneity in contact patterns across different care levels and staff shifts, and demonstrated the ABM's utility in evaluating targeted infection control strategies, including vaccination, to substantially reduce airborne transmission risk.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Geriatric Medicine</span>
                    
                    <span class="domain-tag">Infectious Diseases</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Infection Prevention and Control</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03866v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03866v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03866v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03866v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03854v1"
                     data-domains="Pathology,Urology,Oncology,Medical Informatics,Diagnostic Medicine"
                     data-keywords="Prostate biopsy,Whole slide images,Digital pathology,Artificial intelligence,Gleason score,Histopathology,Dataset,Middle East"
                     data-authors="Peshawa J. Muhammad Ali,Navin Vincent,Saman S. Abdulla,Han N. Mohammed Fadhl,Anders Blilie,Kelvin Szolnoky,Julia Anna Mielcarz,Xiaoyi Ji,Kimmo Kartasalo,Abdulbasit K. Al-Talabani,Nita Mulliqi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03854v1.html">Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Peshawa J. Muhammad Ali, Navin Vincent, Saman S. Abdulla et al.
                </div>

                <div class="paper-summary">
                    This paper announces the public release of a novel whole-slide image dataset of prostate core needle biopsies from an underrepresented Middle Eastern population (Erbil, Iraq). The dataset aims to address the critical lack of diversity in existing digital pathology AI datasets, thereby supporting the development and validation of AI models that are generalizable across globally diverse populations.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Urology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Medical Informatics</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03854v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03854v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03854v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03854v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03848v1"
                     data-domains="Cardiology,Radiology,Medical Imaging,Diagnostic Imaging,Artificial Intelligence in Medicine"
                     data-keywords="Cardiac image analysis,Multi-task learning,Segmentation,Disease classification,Vision-language model,Self-supervised learning,Few-shot learning,Cross-modality adaptation,Foundation model"
                     data-authors="Hania Ghouse,Maryam Alsharqi,Farhad R. Nezami,Muzammil Behzad">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03848v1.html">PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hania Ghouse, Maryam Alsharqi, Farhad R. Nezami et al.
                </div>

                <div class="paper-summary">
                    PULSE introduces a novel multi-task vision-language framework that unifies cardiac anatomical segmentation, disease classification, and clinically grounded report generation within a single architecture. Leveraging self-supervised representations and a composite supervision strategy, it demonstrates robust generalization across diverse datasets and enables few-shot adaptation to new imaging modalities. This unified approach aims to overcome the fragmentation prevalent in current cardiac image analysis pipelines, moving towards a scalable foundation model for comprehensive clinical assessment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03848v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03848v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03848v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03848v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03838v1"
                     data-domains="Sepsis,General Medical Diagnostics"
                     data-keywords="Large Language Models,Medical Reasoning,Consensus Guidelines,Explainable AI,Sepsis Prediction,Fine-tuning,Multimodal AI,Time Series Forecasting"
                     data-authors="Michael Staniek,Artem Sokolov,Stefan Riezler">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03838v1.html">Training and Evaluation of Guideline-Based Medical Reasoning in LLMs</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Michael Staniek, Artem Sokolov, Stefan Riezler
                </div>

                <div class="paper-summary">
                    This paper introduces a method to teach Large Language Models (LLMs) to perform step-by-step, guideline-based medical reasoning for prediction, aiming to improve explainability and trust in medical AI. It demonstrates that small, fine-tuned LLMs, trained on verbalized medical consensus rules from EHRs, outperform larger models and achieve nearly perfect reasoning correctness, with a multimodal approach addressing the bottleneck of forecasting sparsely sampled clinical variables for early prediction.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Sepsis</span>
                    
                    <span class="domain-tag">General Medical Diagnostics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03838v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03838v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03838v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03838v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03834v1"
                     data-domains="Radiology,Medical Image Analysis,Diagnostic Imaging"
                     data-keywords="Unet,Image Segmentation,Deep Learning,Medical Imaging,Channel Pruning,Lean Architecture,MRI,CT Scans,Computational Efficiency"
                     data-authors="Ture Hassler,Ida √Ökerholm,Marcus Nordstr√∂m,Gabriele Balletti,Orcun Goksel">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03834v1.html">Lean Unet: A Compact Model for Image Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ture Hassler, Ida √Ökerholm, Marcus Nordstr√∂m et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Lean Unet (LUnet), a compact and efficient deep learning model for semantic image segmentation, specifically targeting the high memory footprint and latency issues of conventional Unet architectures used in computer-assisted radiology. LUnet proposes a flat hierarchy with constant channel counts across layers, demonstrating comparable segmentation performance to standard Unet and pruned networks while utilizing over 30 times fewer parameters.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03834v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03834v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03834v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03834v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03787v1"
                     data-domains="Healthcare Management,Infectious Diseases,Clinical Decision Support,Quality Improvement,Hospital Administration"
                     data-keywords="Clinical Pathways,Process Mining,Conformance Checking,Healthcare Plans,Patient Treatment,SARS-CoV-2,COVID-19,Adaptive Modeling"
                     data-authors="Francesco Vitale,Nicola Mazzocca">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03787v1.html">Adaptive Identification and Modeling of Clinical Pathways with Process Mining</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Francesco Vitale, Nicola Mazzocca
                </div>

                <div class="paper-summary">
                    This paper introduces a two-phase process mining method designed to adaptively identify and model clinical pathways, addressing the limitations of manual, static modeling. By leveraging conformance checking, the approach dynamically expands the knowledge base of treatment protocols for various disease variants, demonstrating high precision and model simplicity on simulated SARS-CoV-2 infection data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Healthcare Management</span>
                    
                    <span class="domain-tag">Infectious Diseases</span>
                    
                    <span class="domain-tag">Clinical Decision Support</span>
                    
                    <span class="domain-tag">Quality Improvement</span>
                    
                    <span class="domain-tag">Hospital Administration</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03787v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03787v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03787v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03787v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03784v1"
                     data-domains="Neurology,Sleep Medicine,Biomedical Engineering,Neuroscience,Psychiatry"
                     data-keywords="Sleep modulation,Closed-loop systems,Open-loop paradigms,Non-invasive brain stimulation,Sleep disorders,Personalized medicine,Neuromodulation,Brain-computer interface"
                     data-authors="Guisong Liu,Jiansong Zhang,Yinpei Luo,Guoliang Wei,Shuqing Sun,Shiyang Deng,Pengfei Wei,Nanxi Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03784v1.html">Sleep Modulation: The Challenge of Transitioning from Open Loop to Closed Loop</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.HC</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Guisong Liu, Jiansong Zhang, Yinpei Luo et al.
                </div>

                <div class="paper-summary">
                    This paper critically analyzes the limitations of current open-loop sleep modulation techniques, which lack individual adaptation and hinder clinical translation for sleep disorders. It formally conceptualizes a shift towards closed-loop sleep modulation, proposing it as a more personalized and effective intervention strategy. The authors evaluate existing modulation techniques for their integration into a closed-loop framework and identify key challenges for its implementation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Sleep Medicine</span>
                    
                    <span class="domain-tag">Biomedical Engineering</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03784v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03784v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03784v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03784v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03751v1"
                     data-domains="Radiology,Oncology,Neuroimaging,Diagnostic Medicine"
                     data-keywords="brain tumor classification,ResNet34,deep learning,multi-scale feature extraction,Inception v2,channel attention mechanism,medical image analysis,convolutional neural networks"
                     data-authors="Yufeng Li,Wenchao Zhao,Bo Dang,Weimin Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03751v1.html">Research on Brain Tumor Classification Method Based on Improved ResNet34 Network</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yufeng Li, Wenchao Zhao, Bo Dang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces an improved ResNet34-based deep learning model for brain tumor image classification, addressing the limitations of manual methods and shallow CNNs. The proposed model incorporates multi-scale feature extraction, Inception v2 modules, and a channel attention mechanism. It achieves an average classification accuracy of 98.8% with 20% fewer parameters than the original ResNet34, offering a more efficient and accurate diagnostic tool.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Neuroimaging</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03751v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03751v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03751v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03751v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03737v1"
                     data-domains="Digital Health,Online Medical Delivery Platforms,Health Information Systems,Consumer Health Informatics,Medical Natural Language Processing"
                     data-keywords="LLM,Medical Search,Retrieval-Augmented Generation,Knowledge Distillation,Online Healthcare,Relevance Assessment,Semantic Understanding,Factual Accuracy"
                     data-authors="Chuyue Wang,Jie Feng,Yuxi Wu,Hang Zhang,Zhiguo Fan,Bing Cheng,Wei Lin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03737v1.html">AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Chuyue Wang, Jie Feng, Yuxi Wu et al.
                </div>

                <div class="paper-summary">
                    AR-Med is a novel framework designed to enhance search relevance on online medical delivery platforms by leveraging LLM semantic understanding while mitigating common LLM challenges. It achieves this through a retrieval-augmented approach grounded in verified medical knowledge, knowledge distillation for efficiency, and a specialized multi-expert benchmark, leading to significant improvements in offline accuracy and online user satisfaction.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Online Medical Delivery Platforms</span>
                    
                    <span class="domain-tag">Health Information Systems</span>
                    
                    <span class="domain-tag">Consumer Health Informatics</span>
                    
                    <span class="domain-tag">Medical Natural Language Processing</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03737v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03737v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03737v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03737v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03719v1"
                     data-domains="Digital Health,Remote Patient Monitoring,Clinical Decision Support Systems,Medical Imaging Analysis,Genomics and Proteomics,Drug Discovery and Development"
                     data-keywords="Federated Learning,Over-the-Air Computing,Edge AI,Signal Processing,Distributed Machine Learning,Wireless Communication,Privacy-Preserving AI,Medical AI"
                     data-authors="Seyed Mohammad Azimi-Abarghouyi,Carlo Fischione,Kaibin Huang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03719v1.html">Over-the-Air Federated Learning: Rethinking Edge AI Through Signal Processing</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.IT</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Seyed Mohammad Azimi-Abarghouyi, Carlo Fischione, Kaibin Huang
                </div>

                <div class="paper-summary">
                    This tutorial article introduces Over-the-Air Federated Learning (AirFL) as a paradigm tightly integrating wireless signal processing and distributed machine learning to enable scalable AI at the network edge. AirFL leverages the superposition property of wireless signals for simultaneous communication and model aggregation, thereby significantly reducing latency, bandwidth, and energy consumption. The paper provides a comprehensive guide, classifying AirFL into CSIT-aware, blind, and weighted approaches, and discussing their theoretical foundations, performance, complexity, practical limitations, and future research directions.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Remote Patient Monitoring</span>
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Medical Imaging Analysis</span>
                    
                    <span class="domain-tag">Genomics and Proteomics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03719v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03719v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03719v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03719v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03687v1"
                     data-domains="Robotic Surgery,Diagnostic Imaging (e.g., Endoscopy, Microscopy),Rehabilitation Robotics,Assistive Technologies,Smart Patient Monitoring,Telemedicine,Medical Education (Simulation)"
                     data-keywords="Active Visual Perception,Dynamic Sensing,Robotics,Human-Computer Interaction,Real-time Processing,Multimodal Integration,Computer Vision,Autonomous Systems"
                     data-authors="Yian Li,Xiaoyu Guo,Hao Zhang,Shuiwang Li,Xiaowei Dai">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03687v1.html">Active Visual Perception: Opportunities and Challenges</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yian Li, Xiaoyu Guo, Hao Zhang et al.
                </div>

                <div class="paper-summary">
                    This paper provides a comprehensive overview of active visual perception, a paradigm where systems dynamically interact with their environment through sensing and action to acquire more informative data, unlike passive visual systems. It explores the significant opportunities this approach offers across various applications, including robotics and human-computer interaction, while also detailing the critical challenges that must be addressed for its widespread adoption. The review highlights the potential of active systems to overcome limitations of static sensing in complex environments by actively directing attention and interacting with objects.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Robotic Surgery</span>
                    
                    <span class="domain-tag">Diagnostic Imaging (e.g., Endoscopy, Microscopy)</span>
                    
                    <span class="domain-tag">Rehabilitation Robotics</span>
                    
                    <span class="domain-tag">Assistive Technologies</span>
                    
                    <span class="domain-tag">Smart Patient Monitoring</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03687v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03687v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03687v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03687v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03671v1"
                     data-domains="Public Health,Digital Health,Health Informatics,Patient Education,Medical Ethics,Primary Care"
                     data-keywords="Generative AI,Digital Divide,Medical Advice,Health Literacy,Misinformation,Italy,Survey,Gender Disparity"
                     data-authors="Beatrice Savoldi,Giuseppe Attanasio,Olga Gorodetskaya,Marta Marchiori Manerba,Elisa Bassignana,Silvia Casola,Matteo Negri,Tommaso Caselli,Luisa Bentivogli,Alan Ramponi,Arianna Muti,Nicoletta Balbo,Debora Nozza">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03671v1.html">Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Beatrice Savoldi, Giuseppe Attanasio, Olga Gorodetskaya et al.
                </div>

                <div class="paper-summary">
                    This empirical study maps Generative AI (GenAI) adoption, usage, and literacy across 1,906 Italian adults, revealing widespread integration into daily life, including sensitive tasks like medical advice. It highlights GenAI's emergence as a primary information source despite low user digital literacy, posing significant risks of misinformation, and identifies a pronounced gender divide in adoption and usage. The paper underscores an urgent need for targeted educational initiatives and further investigation into non-competence-based barriers to equitable participation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Patient Education</span>
                    
                    <span class="domain-tag">Medical Ethics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03671v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03671v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03671v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03671v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03667v1"
                     data-domains="Gastroenterology,Oncology,Diagnostic Imaging,Artificial Intelligence in Medicine"
                     data-keywords="Colonoscopy,Multimodal AI,Clinical Reasoning,Visual Question Answering (VQA),Large Language Models (LLMs),Medical Imaging,Diagnostic Support,Gastroenterology"
                     data-authors="Ge-Peng Ji,Jingyi Liu,Deng-Ping Fan,Nick Barnes">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03667v1.html">Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ge-Peng Ji, Jingyi Liu, Deng-Ping Fan et al.
                </div>

                <div class="paper-summary">
                    Colon-X is an open initiative introducing ColonVQA, the most comprehensive multimodal dataset for colonoscopy, aimed at advancing multimodal intelligence. The study highlights the unreliable nature of current MLLMs for clinical outputs and proposes a novel approach to clinical reasoning, culminating in ColonReason, a new reasoning dataset, and ColonR1, a model that achieves 56.61% accuracy under data-scarce conditions, significantly outperforming supervised fine-tuning.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Artificial Intelligence in Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03667v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03667v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03667v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03667v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03634v1"
                     data-domains="Clinical Decision Support Systems,Medical Diagnostics,Healthcare AI,Medical Research,Pharmacology,Patient Education"
                     data-keywords="Large Language Models (LLMs),Hallucination,Factual Consistency,Semantic Metrics,Clinical AI,Natural Language Processing,Medical Informatics,Interpretability"
                     data-authors="Ahmad Aghaebrahimian">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03634v1.html">AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ahmad Aghaebrahimian
                </div>

                <div class="paper-summary">
                    This paper introduces AlignCheck, a semantic, open-domain metric designed to assess factual consistency in Large Language Model (LLM) outputs, critically addressing the issue of hallucination in high-stakes clinical applications. It proposes an interpretable, schema-free framework that decomposes text into atomic facts and employs a novel weighted metric for a more nuanced and robust evaluation of factual accuracy.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Decision Support Systems</span>
                    
                    <span class="domain-tag">Medical Diagnostics</span>
                    
                    <span class="domain-tag">Healthcare AI</span>
                    
                    <span class="domain-tag">Medical Research</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03634v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03634v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03634v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03634v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03597v1"
                     data-domains="Oncology,Radiology,Urology,Gastroenterology,Diagnostic Imaging"
                     data-keywords="Medical Image Segmentation,Vision Transformer,Microtumor Segmentation,Miniature Organ Segmentation,Hybrid-Bridge Transformer,Multi-Scale Feature Fusion,Attention Mechanisms,Dilated Convolutions"
                     data-authors="Fuchen Zheng,Xinyi Chen,Weixuan Li,Quanjun Li,Junhua Zhou,Xiaojiao Guo,Xuhang Chen,Chi-Man Pun,Shoujun Zhou">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03597v1.html">HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Fuchen Zheng, Xinyi Chen, Weixuan Li et al.
                </div>

                <div class="paper-summary">
                    HBFormer addresses the critical limitation of existing Vision Transformers in medical image segmentation, specifically their struggle to fuse local details with global context, which is particularly detrimental for microtumors and miniature organs. This novel Hybrid-Bridge Transformer combines a U-shaped architecture with a Swin Transformer backbone and introduces a Multi-Scale Feature Fusion (MFF) decoder, designed with attention modules and specialized convolutions, to effectively integrate multi-scale and global contextual information. The architecture achieves state-of-the-art performance on challenging medical image segmentation datasets, demonstrating superior capabilities for microtumor and miniature organ segmentation.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Urology</span>
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03597v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03597v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03597v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03597v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03579v1"
                     data-domains="Medical Imaging,Genomics,Proteomics,Electronic Health Records (EHR) Analysis,Personalized Medicine,Computational Pathology,Neuroscience,Drug Discovery"
                     data-keywords="Optimal Transport,Gromov-Wasserstein,Gaussian Measures,Heterogeneous Data,Data Alignment,Barycenter,Machine Learning,Biomedical Informatics"
                     data-authors="Sanjit Dandapanthula,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan,Aaditya Ramdas,Ziv Goldfeld">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03579v1.html">Optimal Transportation and Alignment Between Gaussian Measures</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.75</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Sanjit Dandapanthula, Aleksandr Podkopaev, Shiva Prasad Kasiviswanathan et al.
                </div>

                <div class="paper-summary">
                    This paper significantly advances the computational tractability of Optimal Transport (OT) and Gromov-Wasserstein (GW) alignment by providing comprehensive closed-form solutions for Gaussian measures under quadratic cost. It addresses previously open problems, offering efficient methods for comparing, transforming, and aggregating heterogeneous datasets, particularly for uncentered Gaussians and their barycenters.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Proteomics</span>
                    
                    <span class="domain-tag">Electronic Health Records (EHR) Analysis</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03579v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03579v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03579v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03579v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03578v1"
                     data-domains="critical care monitoring,chronic disease management,personalized medicine,predictive diagnostics,remote patient monitoring,drug response prediction,prognostic modeling"
                     data-keywords="interpretable AI,time series regression,neural networks,explainability,concept learning,healthcare AI,patient monitoring,feature attribution"
                     data-authors="Florent Forest,Amaury Wei,Olga Fink">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03578v1.html">When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Florent Forest, Amaury Wei, Olga Fink
                </div>

                <div class="paper-summary">
                    This paper introduces MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture designed for Time Series Extrinsic Regression (TSER). MAGNETS addresses the limitations of black-box models and existing interpretable methods by learning human-understandable concepts without requiring annotations, explicitly revealing which temporal features drive predictions and when they are relevant through a transparent, additive decision process.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">critical care monitoring</span>
                    
                    <span class="domain-tag">chronic disease management</span>
                    
                    <span class="domain-tag">personalized medicine</span>
                    
                    <span class="domain-tag">predictive diagnostics</span>
                    
                    <span class="domain-tag">remote patient monitoring</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03578v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03578v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03578v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03578v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03577v1"
                     data-domains="Computational Pathology,Oncology,Histopathology,Diagnostic Pathology,Precision Medicine"
                     data-keywords="Computational Pathology,Whole-Slide Imaging (WSI),Immunohistochemistry (IHC),H&E Staining,Contrastive Learning,Multiple Instance Learning (MIL),Cancer Biomarkers,Slide Representation Learning"
                     data-authors="Yizhi Zhang,Lei Fan,Zhulin Tao,Donglin Di,Yang Song,Sidong Liu,Cong Cong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03577v1.html">Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yizhi Zhang, Lei Fan, Zhulin Tao et al.
                </div>

                <div class="paper-summary">
                    This paper introduces Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework designed to generate universal, transferable whole-slide image (WSI) representations by integrating H&E and multiple immunohistochemistry (IHC) stain features. Addressing the challenge of data scarcity and inter-stain misalignment, CSCL leverages a newly curated five-stain dataset to achieve robust cross-stain representation learning through patch-wise and slide-level alignment. The framework demonstrates consistent performance gains across cancer subtype classification, IHC biomarker status classification, and survival prediction, yielding high-quality, transferable H&E slide-level representations.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Computational Pathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Diagnostic Pathology</span>
                    
                    <span class="domain-tag">Precision Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03577v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03577v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03577v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03577v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03541v1"
                     data-domains="Medical Imaging,Radiology,Ophthalmology,Clinical Research,Biomedical Informatics"
                     data-keywords="DICOM,AI-ready data,SOPs,medical imaging,FAIR data,data management,de-identification,ophthalmology"
                     data-authors="Milen Nikolov,Edilberto Amorim,J Harry Caufield,Nayoon Gim,Nomi L Harris,Jared Houghtaling,Xiang Li,Danielle Morrison,Ana√Øs Rameau,Jamie Shaffer,Hari Trivedi,Monica C Munoz-Torres">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03541v1.html">Toward AI-Ready Medical Imaging Data</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ q-bio.OT</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Milen Nikolov, Edilberto Amorim, J Harry Caufield et al.
                </div>

                <div class="paper-summary">
                    This paper addresses the critical need for standardized operating procedures (SOPs) in medical imaging data management to facilitate the creation of AI-ready datasets. Developed by the NIH Bridge2AI Standards Working Group, it describes novel SOPs for the DICOM format, covering both static and cutting-edge video modalities, with a strong emphasis on data aggregation, validation, and advanced de-identification techniques.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Clinical Research</span>
                    
                    <span class="domain-tag">Biomedical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03541v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03541v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03541v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03541v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03499v1"
                     data-domains="Medical Imaging,Radiology,Pathology,Surgical Planning,Diagnostic Imaging,Computational Anatomy"
                     data-keywords="SAM,LoRA,PEFT,Neural Architecture Search,Medical Imaging,Semantic Segmentation,Inductive Bias,Visual Foundation Models"
                     data-authors="Renqi Chen,Haoyang Su,Shixiang Tang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03499v1.html">NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Renqi Chen, Haoyang Su, Shixiang Tang
                </div>

                <div class="paper-summary">
                    NAS-LoRA introduces a novel Parameter-Efficient Fine-Tuning (PEFT) method for the Segment Anything Model (SAM) by incorporating a lightweight Neural Architecture Search (NAS) block within LoRA's structure and a stage-wise optimization strategy. This approach dynamically optimizes inductive bias to bridge the semantic gap for specialized domains like medical imaging, improving adaptation performance while significantly reducing training costs by 24.14% without affecting inference speed. The method addresses SAM's inherent lack of spatial priors, which typically hinders the acquisition of high-level semantic information in domain-specific tasks.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Surgical Planning</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03499v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03499v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03499v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03499v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03497v1"
                     data-domains="Oncology,Immunology,Developmental Biology,Pathology,Neuroscience,Regenerative Medicine,Pharmacology"
                     data-keywords="Cell-cell communication,single-cell omics,spatial omics,ligand-receptor interactions,computational biology,disease mechanisms,biological hypothesis generation,transcriptomics"
                     data-authors="Xiangzheng Cheng,Haili Huang,Ye Su,Qing Nie,Xiufen Zou,Suoqin Jin">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03497v1.html">Cell-cell communication inference and analysis: biological mechanisms, computational approaches, and future opportunities</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xiangzheng Cheng, Haili Huang, Ye Su et al.
                </div>

                <div class="paper-summary">
                    This paper provides a comprehensive review of over 140 computational methods for inferring and analyzing cell-cell communication (CCC) from single-cell and spatial omics data. It highlights the critical role of CCC in biological processes and disease, outlining diverse methodological frameworks and biological questions addressed by these advanced computational tools. The review aims to introduce biological mechanisms, modeling strategies, and discuss future opportunities and challenges in this rapidly evolving field.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Immunology</span>
                    
                    <span class="domain-tag">Developmental Biology</span>
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03497v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03497v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03497v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03497v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03477v1"
                     data-domains="Ophthalmology,Glaucoma,Medical Imaging,Diagnostic Medicine"
                     data-keywords="Fairness,Vision-Language Models (VLMs),Glaucoma Diagnosis,Algorithmic Bias,Low-Rank Adaptation (LoRA),MaxAccGap Loss,Medical Imaging,Diagnostic Disparity"
                     data-authors="Zijian Gu,Yuxi Liu,Zhenhao Zhang,Song Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03477v1.html">Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Zijian Gu, Yuxi Liu, Zhenhao Zhang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces fairness-aware Low-Rank Adaptation (LoRA) for fine-tuning Vision-Language Models (VLMs) to mitigate diagnostic accuracy disparities across demographic groups in medical tasks, specifically glaucoma diagnosis. Their key contribution is a differentiable MaxAccGap loss enabling end-to-end fairness optimization. The GR-LoRA method achieved a 69% reduction in diagnostic accuracy disparities on 10,000 glaucoma fundus images while maintaining 53.15% overall accuracy, utilizing only 0.24% trainable parameters for practical deployment.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Glaucoma</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03477v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03477v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03477v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03477v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03475v1"
                     data-domains="Neurology,Geriatrics,Dementia Research,Computational Neuroscience"
                     data-keywords="Disease progression,Mixed pathology,Neurodegeneration,Event-based models,Probabilistic framework,Alzheimer's disease,Vascular dementia,Machine learning"
                     data-authors="Hongtao Hao,Joseph L. Austerweil">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03475v1.html">Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hongtao Hao, Joseph L. Austerweil
                </div>

                <div class="paper-summary">
                    This paper introduces the Joint Progression Model (JPM), a novel probabilistic framework designed to infer disease progression in individuals affected by mixed pathologies, a common scenario in neurodegeneration. Unlike standard Event-Based Models (EBMs) that assume a single disease, JPM treats individual disease trajectories as partial rankings and builds a prior over their joint progression. The framework demonstrates improved ordering accuracy (approximately 21% over a strong baseline) in synthetic experiments and produces results consistent with existing literature for mixed Alzheimer's Disease and Vascular Dementia progression using NACC data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Dementia Research</span>
                    
                    <span class="domain-tag">Computational Neuroscience</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03475v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03475v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03475v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03475v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03471v1"
                     data-domains="Endocrinology,Preventive Medicine,Primary Care,Public Health,Digital Health,Cardiometabolic Health"
                     data-keywords="Diabetes screening,Wearable AI,Non-invasive diagnosis,Type 2 diabetes,Machine learning,Physiological data,Neural networks,Digital health"
                     data-authors="Ian Henriques,Lynda Elhassar,Sarvesh Relekar,Denis Walrave,Shayan Hassantabar,Vishu Ghanakota,Adel Laoui,Mahmoud Aich,Rafia Tir,Mohamed Zerguine,Samir Louafi,Moncef Kimouche,Emmanuel Cosson,Niraj K Jha">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03471v1.html">SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ian Henriques, Lynda Elhassar, Sarvesh Relekar et al.
                </div>

                <div class="paper-summary">
                    SweetDeep introduces a compact neural network for real-time, non-invasive type 2 diabetes screening leveraging physiological and demographic data from Samsung Galaxy Watch 7 devices. Collected from 285 participants in free-living conditions, the AI solution achieved 82.5% patient-level accuracy, demonstrating a scalable and cost-effective approach to early diabetes detection.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Endocrinology</span>
                    
                    <span class="domain-tag">Preventive Medicine</span>
                    
                    <span class="domain-tag">Primary Care</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03471v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03471v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03471v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03471v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03467v1"
                     data-domains="Neurodegenerative diseases,Chronic diseases,Neurology,Geriatrics,Personalized medicine"
                     data-keywords="Bayesian models,disease subtypes,disease progression,event-based models,SuStaIn,Alzheimer's disease,machine learning,medical statistics"
                     data-authors="Hongtao Hao,Joseph L. Austerweil">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03467v1.html">Bayesian Event-Based Model for Disease Subtype and Stage Inference</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hongtao Hao, Joseph L. Austerweil
                </div>

                <div class="paper-summary">
                    This paper introduces BEBMS, a novel principled Bayesian event-based model, designed to accurately infer disease subtypes and progression stages in chronic conditions characterized by structured heterogeneity. The study demonstrates that BEBMS significantly outperforms the widely used SuStaIn model across synthetic data experiments with varying misspecification levels, and yields results more consistent with scientific consensus when applied to real-world Alzheimer's disease data.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Neurodegenerative diseases</span>
                    
                    <span class="domain-tag">Chronic diseases</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Geriatrics</span>
                    
                    <span class="domain-tag">Personalized medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03467v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03467v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03467v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03467v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03445v1"
                     data-domains="Dermatology"
                     data-keywords="Vision-Language Pretraining,Medical Image Analysis,Dermatology,Data Augmentation,Knowledge Enhancement,Zero-shot Learning,Ontology,Foundation Models"
                     data-authors="Xieji Li,Siyuan Yan,Yingsheng Liu,H. Peter Soyer,Monika Janda,Victoria Mar,Zongyuan Ge">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03445v1.html">Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xieji Li, Siyuan Yan, Yingsheng Liu et al.
                </div>

                <div class="paper-summary">
                    This paper introduces a novel Vision-Language Pretraining (VLP) framework to overcome challenges of noisy web data and complex unstructured medical texts in medical image analysis. It integrates a Multi-Agent data GENeration (MAGEN) system for enhancing data quality and an Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining module for structured learning from long texts. Validated in dermatology, the approach achieves state-of-the-art zero-shot performance in disease classification and cross-modal retrieval.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Dermatology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03445v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03445v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03445v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03445v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03359v1"
                     data-domains="Oncology,Radiology,Medical Imaging,Pulmonology"
                     data-keywords="Lung Cancer,Deep Learning,CT Scans,DenseNet169,SVM,Explainable AI,Grad-CAM,SHAP,Diagnosis,Classification"
                     data-authors="Md Rashidul Islam,Bakary Gibba,Altagi Abdallah Bakheit Abdelgadir">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03359v1.html">A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Md Rashidul Islam, Bakary Gibba, Altagi Abdallah Bakheit Abdelgadir
                </div>

                <div class="paper-summary">
                    This paper proposes a hybrid deep learning framework with explainable AI for automated lung cancer classification using CT scans. The system leverages DenseNet169 with attention mechanisms and multi-scale feature fusion, alongside an SVM model with MobileNetV2 features, achieving 98% accuracy for classifying Normal, Benign, and Malignant cases, while providing crucial interpretability.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Pulmonology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03359v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03359v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03359v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03359v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03346v1"
                     data-domains="Ophthalmology,Medical Imaging,Diagnostic Imaging"
                     data-keywords="Hierarchical attention,Subclinical keratoconus,Volumetric anomaly detection,OCT,Deep learning,Inductive bias,Sparse patterns,Medical imaging"
                     data-authors="Lynn Kandakji,William Woof,Nikolas Pontikos">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03346v1.html">Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Lynn Kandakji, William Woof, Nikolas Pontikos
                </div>

                <div class="paper-summary">
                    This study demonstrates that hierarchical attention models offer a superior and more parameter-efficient inductive bias for detecting weak, spatially distributed anomalies in volumetric medical imaging. Applied to subclinical keratoconus (SKC) detection from 3D anterior segment OCT, these models achieved 21-23% higher sensitivity and specificity compared to traditional CNNs and ViTs, attributed to precise spatial scale alignment for sparse, multi-slice abnormalities.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Ophthalmology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03346v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03346v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03346v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03346v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03345v1"
                     data-domains="Medical Imaging,Radiology,Neuroradiology"
                     data-keywords="Hallucinations,Image Restoration,Generative Models,Diffusion Models,Low-Field MRI,Medical Imaging,Image Quality Metrics,Safety-Critical AI"
                     data-authors="Seunghoi Kim,Henry F. J. Tregidgo,Chen Jin,Matteo Figini,Daniel C. Alexander">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03345v1.html">HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Seunghoi Kim, Henry F. J. Tregidgo, Chen Jin et al.
                </div>

                <div class="paper-summary">
                    This paper introduces HalluGen, a diffusion-based framework designed to synthesize realistic and controllable hallucinations in medical images, specifically for low-field MRI enhancement. It addresses the critical challenge of evaluating generative model hallucinations in safety-critical domains by creating the first large-scale, annotated hallucination dataset and demonstrating its utility in benchmarking image quality metrics and training robust hallucination detectors.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Neuroradiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03345v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03345v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03345v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03345v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03339v1"
                     data-domains="Cardiology,Diagnostic Imaging,Cardiovascular Medicine"
                     data-keywords="Ejection Fraction,Echocardiography,Interpretable AI,Prototype Learning,Deep Learning,Cardiac Function,Heart Failure,Cardiovascular Imaging"
                     data-authors="Yeganeh Ghamary,Victoria Wu,Hooman Vaseli,Christina Luong,Teresa Tsang,Siavash Bigdeli,Purang Abolmaesumi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03339v1.html">ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-03</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yeganeh Ghamary, Victoria Wu, Hooman Vaseli et al.
                </div>

                <div class="paper-summary">
                    ProtoEFNet introduces a novel video-based prototype learning model for continuous ejection fraction (EF) estimation from echocardiography, designed to be inherently interpretable. It addresses the limitations of traditional manual methods and opaque deep learning models by learning dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. The model achieves accuracy comparable to non-interpretable counterparts while providing valuable clinical insights, enhanced by a novel Prototype Angular Separation (PAS) loss.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Cardiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Cardiovascular Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03339v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03339v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03339v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03339v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03296v1"
                     data-domains="Oncology,Healthcare Management,Health Informatics,Public Health,Team-based Care"
                     data-keywords="healthcare teamwork,patient outcomes,cancer treatment,electronic health records (EHR),network analysis,machine learning,predictive analysis,collaboration,artificial intelligence (AI)"
                     data-authors="Hsiao-Ying Lu,Kwan-Liu Ma">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03296v1.html">Associating Healthcare Teamwork with Patient Outcomes for Predictive Analysis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.SI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Hsiao-Ying Lu, Kwan-Liu Ma
                </div>

                <div class="paper-summary">
                    This paper introduces an AI-driven approach to investigate the impact of healthcare professionals' (HCPs) collaboration, captured via electronic health records (EHRs), on cancer patient survival. By modeling HCP interactions as networks and applying machine learning, the study identifies predictive signals of patient survival embedded in these collaborations and pinpoints key network traits associated with improved outcomes, validated by clinical experts.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Healthcare Management</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Team-based Care</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03296v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03296v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03296v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03296v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03286v1"
                     data-domains="Oncology,Immunology,Neuroscience,Developmental Biology,Personalized Medicine,Diagnostics,Drug Discovery,Infectious Disease"
                     data-keywords="Single-cell transcriptomics,Cell type annotation,Spiking Transformer,scGPT,Deep learning,Batch effects,Novel cell types,Computational genomics"
                     data-authors="Min Huang,Rishikesan Kamaleswaran">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03286v1.html">SpikGPT: A High-Accuracy and Interpretable Spiking Attention Framework for Single-Cell Annotation</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ q-bio.QM</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Min Huang, Rishikesan Kamaleswaran
                </div>

                <div class="paper-summary">
                    SpikGPT is a novel hybrid deep learning framework for single-cell annotation that combines scGPT-derived cell embeddings with an energy-efficient spiking Transformer architecture. It achieves high accuracy, consistently matching or exceeding leading tools, and uniquely identifies previously unseen cell types by assigning low-confidence predictions to an 'Unknown' category. This framework effectively addresses challenges like strong batch effects and the discovery of novel or disease-associated cell populations.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Immunology</span>
                    
                    <span class="domain-tag">Neuroscience</span>
                    
                    <span class="domain-tag">Developmental Biology</span>
                    
                    <span class="domain-tag">Personalized Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03286v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03286v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03286v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03286v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03278v1"
                     data-domains="Public Health,Health Policy,Clinical Research,Pharmacovigilance,Healthcare Administration,Medical Informatics"
                     data-keywords="LLM-based,Multi-Agent System,Claim Verification,Relational Databases,Fact Checking,Healthcare Data,SQL Query Generation,Evidence-based Medicine"
                     data-authors="Michael Theologitis,Dan Suciu">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03278v1.html">Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.DB</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Michael Theologitis, Dan Suciu
                </div>

                <div class="paper-summary">
                    This paper introduces Thucy, a novel LLM-based multi-agent system designed for automated claim verification across complex relational databases. Thucy autonomously discovers, inspects, and reasons over data to verify claims, providing transparent evidence in the form of exact SQL queries. It significantly advances the state of the art in fact verification over structured data, achieving 94.3% accuracy on the TabFact benchmark.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public Health</span>
                    
                    <span class="domain-tag">Health Policy</span>
                    
                    <span class="domain-tag">Clinical Research</span>
                    
                    <span class="domain-tag">Pharmacovigilance</span>
                    
                    <span class="domain-tag">Healthcare Administration</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03278v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03278v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03278v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03278v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03196v1"
                     data-domains="Oncology,Radiology,Urology,Medical Imaging"
                     data-keywords="Diffusion MRI,Prostate Cancer,VERDICT,Self-supervised Learning,Ultra-strong Gradients,Deep Learning,Non-invasive Characterization,Microstructural Imaging"
                     data-authors="Tanishq Patil,Snigdha Sen,Malwina Molendowska,Kieran G. Foley,Fabrizio Fasano,Mara Cercignani,Marco Palombo,Paddy J. Slator,Eleftheria Panagiotaki">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03196v1.html">Ultra-Strong Gradient Diffusion MRI with Self-Supervised Learning for Prostate Cancer Characterization</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tanishq Patil, Snigdha Sen, Malwina Molendowska et al.
                </div>

                <div class="paper-summary">
                    This study demonstrates that applying physics-informed self-supervised VERDICT (ssVERDICT) fitting to ultra-strong gradient diffusion MRI significantly enhances prostate cancer characterization. It shows superior performance in contrast, stability, and tumor-normal differentiation compared to conventional methods and clinical gradient systems, leveraging advanced deep learning architectures.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Urology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03196v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03196v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03196v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03196v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03191v1"
                     data-domains="Oncology,Metabolism and Endocrinology,Neurology,Psychiatry,Chronobiology"
                     data-keywords="Circadian Rhythms,Casein Kinase 2 (CK2),Drosophila melanogaster,PER Protein,Phosphorylation,Clock Genes,Neurodegeneration,Metabolic Disease"
                     data-authors="Yasmin Fatima,Md. Zubair Malik,Prashant Ankur Jain">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03191v1.html">A Comprehensive Review of Casein Kinase 2 in Drosophila Circadian Timing and Its Biomedical Relevance</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ q-bio.MN</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yasmin Fatima, Md. Zubair Malik, Prashant Ankur Jain
                </div>

                <div class="paper-summary">
                    This comprehensive review synthesizes the critical role of Casein Kinase 2 (CK2) as a post-translational regulator within the Drosophila circadian clock, specifically detailing its impact on PER protein dynamics (phosphorylation, stability, nuclear entry, degradation). It highlights how CK2 dysregulation in flies not only disrupts circadian rhythms but also models broader pathological processes relevant to human health, including cancer, metabolic disease, neurodegeneration, and psychiatric disorders.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Metabolism and Endocrinology</span>
                    
                    <span class="domain-tag">Neurology</span>
                    
                    <span class="domain-tag">Psychiatry</span>
                    
                    <span class="domain-tag">Chronobiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03191v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03191v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03191v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03191v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03158v1"
                     data-domains="Public health,Epidemiology,Infectious disease surveillance,Genomics,Virology,Biomonitoring"
                     data-keywords="Wastewater genomics,Viral variant detection,Deep learning,VQ-VAE,Contrastive learning,Genomic surveillance,SARS-CoV-2,Unsupervised learning"
                     data-authors="Adele Chinda,Richmond Azumah,Hemanth Demakethepalli Venkateswara">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03158v1.html">Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.98</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Adele Chinda, Richmond Azumah, Hemanth Demakethepalli Venkateswara
                </div>

                <div class="paper-summary">
                    This paper introduces an unsupervised deep learning framework for detecting viral variants from wastewater genomic sequencing data, leveraging a Vector-Quantized Variational Autoencoder (VQ-VAE) with masked reconstruction pretraining and contrastive learning. The approach effectively addresses challenges like high sequencing noise and lack of labeled data by learning discrete genomic patterns from k-mer tokenized sequences without requiring reference genomes. Evaluated on SARS-CoV-2 data, the framework achieves high token-level accuracy and significantly improves variant discrimination through contrastive fine-tuning, providing a scalable solution for public health genomic surveillance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Public health</span>
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Infectious disease surveillance</span>
                    
                    <span class="domain-tag">Genomics</span>
                    
                    <span class="domain-tag">Virology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03158v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03158v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03158v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03158v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03127v1"
                     data-domains="Pharmaceuticals,Drug Discovery,Medicinal Chemistry,Pharmacognosy,Chemical Biology"
                     data-keywords="NMR spectroscopy,structure elucidation,deep learning,atomic diffusion model,natural products,drug discovery,small molecules,AI in chemistry"
                     data-authors="Ziyu Xiong,Yichi Zhang,Foyez Alauddin,Chu Xin Cheng,Joon Soo An,Mohammad R. Seyedsayamdost,Ellen D. Zhong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03127v1.html">Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Ziyu Xiong, Yichi Zhang, Foyez Alauddin et al.
                </div>

                <div class="paper-summary">
                    ChefNMR is an end-to-end deep learning framework designed to automate the structural elucidation of small molecules directly from 1D NMR spectra and a chemical formula. Utilizing an atomic diffusion model built on a non-equivariant transformer, it predicts molecular structures with over 65% accuracy for challenging natural products. This advancement significantly streamlines a traditionally time-consuming manual process critical for the discovery of novel therapeutics.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pharmaceuticals</span>
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Pharmacognosy</span>
                    
                    <span class="domain-tag">Chemical Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03127v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03127v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03127v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03127v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.03020v1"
                     data-domains="Medical Imaging,Radiology,Diagnostic Imaging"
                     data-keywords="MRI reconstruction,unrolled networks,conditional probability flow ODEs,deep learning,diffusion models,k-space,accelerated imaging,image reconstruction"
                     data-authors="Kehan Qi,Saumya Gupta,Qingqiao Hu,Weimin Lyu,Chao Chen">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.03020v1.html">Unrolled Networks are Conditional Probability Flows in MRI Reconstruction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Kehan Qi, Saumya Gupta, Qingqiao Hu et al.
                </div>

                <div class="paper-summary">
                    This paper establishes a theoretical connection between unrolled networks, commonly used in MRI reconstruction, and conditional probability flow Ordinary Differential Equations (ODEs). Leveraging this insight, the authors propose Flow-Aligned Training (FLAT), a method that stabilizes unrolled networks by deriving parameters from ODE discretization and aligning intermediate reconstructions with an ideal ODE trajectory. FLAT achieves high-quality MRI reconstructions with significantly fewer iterations and greater stability compared to existing deep learning approaches.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.03020v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.03020v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.03020v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.03020v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02983v1"
                     data-domains="cs.LG"
                     data-keywords="cs.LG"
                     data-authors="Louis McConnell,Jieran Sun,Theo Maffei,Raphael Gottardo,Marianna Rapsomaniki">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02983v1.html">ProteinPNet: Prototypical Part Networks for Concept Learning in Spatial Proteomics</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Louis McConnell, Jieran Sun, Theo Maffei et al.
                </div>

                <div class="paper-summary">
                    Understanding the spatial architecture of the tumor microenvironment (TME) is critical to advance precision oncology. We present ProteinPNet, a novel framework based on prototypical part networks that discovers TME motifs from spatial proteomics data. Unlike traditional post-hoc explanability models...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.LG</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02983v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02983v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02983v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02983v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02917v1"
                     data-domains="Nuclear Medicine,Oncology,Diagnostic Imaging,Medical Physics,Radiology"
                     data-keywords="PET,SUVmax,Deep Learning,Denoising,Low-Count PET,18F-FDG,Oncology,Radiopharmaceutical Cost"
                     data-authors="Yamila Rotstein Habarnau,Nicol√°s Bustos,Paola Corona,Christian Gonz√°lez,Sonia Traverso,Federico Matorra,Francisco Funes,Juan Mart√≠n Giraut,Laura Pelegrina,Gabriel Bruno,Mauro Nam√≠as">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02917v1.html">Maintaining SUV Accuracy in Low-Count PET with PETfectior: A Deep Learning Denoising Solution</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ eess.IV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yamila Rotstein Habarnau, Nicol√°s Bustos, Paola Corona et al.
                </div>

                <div class="paper-summary">
                    This paper presents an initial clinical validation of PETfectior, a deep learning-based denoising software designed to process low-count PET images and maintain SUV accuracy. The study demonstrates that PETfectior can generate high-quality diagnostic images from scans acquired with half the standard counting statistics, achieving excellent lesion detectability, accurate quantitative performance, and superior subjective image quality.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Nuclear Medicine</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical Physics</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02917v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02917v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02917v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02917v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02816v1"
                     data-domains="Traditional Chinese Medicine,Clinical Informatics,Medical Artificial Intelligence,Computational Medicine"
                     data-keywords="Large Language Models,Traditional Chinese Medicine,Syndrome Differentiation and Treatment,Benchmark,Evaluation,Clinical Cases,Reward Model,Medical AI"
                     data-authors="Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02816v1.html">A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Kunning Li, Jianbin Guo, Zhaoyang Shang et al.
                </div>

                <div class="paper-summary">
                    This paper introduces TCM-BEST4SDT, a comprehensive, expert-led, clinical case-based benchmark designed to rigorously evaluate Large Language Models (LLMs) in Traditional Chinese Medicine's (TCM) Syndrome Differentiation and Treatment (SDT). It uniquely incorporates a specialized reward model to quantify prescription-syndrome congruence, assessing LLM capabilities beyond basic knowledge to include medical ethics, content safety, and treatment decision-making, with its effectiveness corroborated across 15 mainstream LLMs.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Traditional Chinese Medicine</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                    <span class="domain-tag">Medical Artificial Intelligence</span>
                    
                    <span class="domain-tag">Computational Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02816v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02816v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02816v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02816v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.02814v1"
                     data-domains="Radiology,Diagnostic Imaging,Medical AI,Clinical Informatics"
                     data-keywords="Radiology reporting,Agentic AI,Large Language Models (LLMs),Quality control,Medical imaging,Volumetric images,AI assistant,Clinical efficiency"
                     data-authors="Yongrui Yu,Zhongzhen Huang,Linjie Mu,Shaoting Zhang,Xiaofan Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.02814v1.html">Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-02</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.AI</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Yongrui Yu, Zhongzhen Huang, Linjie Mu et al.
                </div>

                <div class="paper-summary">
                    Radiologist Copilot is an agentic AI assistant leveraging large language models and a suite of orchestrated tools to automate radiology reporting, crucially integrating a robust quality control process previously neglected by existing methods. This system emulates the holistic workflow of radiologists, providing accurate, complete, and efficient reports, and demonstrates superior performance compared to state-of-the-art approaches.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Diagnostic Imaging</span>
                    
                    <span class="domain-tag">Medical AI</span>
                    
                    <span class="domain-tag">Clinical Informatics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.02814v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.02814v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.02814v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.02814v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-04 06:26:46</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>