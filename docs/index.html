<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Health AI Hub</title>
    <meta name="description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta name="keywords" content="medical AI, health AI, arXiv, research papers, machine learning, healthcare">
    <meta name="author" content="Health AI Hub">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Health AI Hub">
    <meta property="og:description" content="AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily">
    <meta property="og:url" content="https://arxiv-health.org">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@ArXiv_Health">

    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-top">
                <div class="header-title">
                    <h1><a href="index.html" class="home-link">Health AI Hub</a></h1>
                    <p class="tagline">AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                </div>
                <a href="index.html" class="home-btn">üè† Home</a>
            </div>

            <!-- Weekly Activity Hero Section -->
            <div class="weekly-hero">
                <h2>This Week's Activity</h2>
                <div class="hero-stats">
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">11</div>
                        <div class="hero-stat-label">New Papers</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">11</div>
                        <div class="hero-stat-label">Total Curated</div>
                    </div>
                    <div class="hero-stat-item">
                        <div class="hero-stat-number">36</div>
                        <div class="hero-stat-label">Medical Domains</div>
                    </div>
                </div>
                
                <div class="hottest-domains">
                    <strong>Hottest domains this week:</strong> Oncology (2), Radiology (2), Diagnostic Medicine (2)
                </div>
                
            </div>
        </div>
    </header>

    <nav class="container">
        <div class="nav-tools">
            <div class="search-box">
                <input type="text" id="search" placeholder="üîç Search papers by title, author, keywords, or domain...">
            </div>
            <div class="filters">
                <div class="filter-group">
                    <label>Sort by:</label>
                    <select id="sort-select">
                        <option value="date">Newest First</option>
                        <option value="relevance">Relevance Score</option>
                        <option value="citations">Most Cited</option>
                        <option value="title">Title A-Z</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Domain:</label>
                    <select id="domain-filter">
                        <option value="">All Domains</option>
                        
                        <option value="Radiology">Radiology (2)</option>
                        
                        <option value="cs.LG">Cs.lg (2)</option>
                        
                        <option value="Oncology">Oncology (2)</option>
                        
                        <option value="Diagnostic Medicine">Diagnostic Medicine (2)</option>
                        
                        <option value="Clinical Documentation">Clinical Documentation (1)</option>
                        
                        <option value="Medical Dictation">Medical Dictation (1)</option>
                        
                        <option value="Telemedicine">Telemedicine (1)</option>
                        
                        <option value="Health Informatics">Health Informatics (1)</option>
                        
                        <option value="Diagnostic Reporting">Diagnostic Reporting (1)</option>
                        
                        <option value="Surgical Notes">Surgical Notes (1)</option>
                        
                    </select>
                </div>
                <div class="filter-group">
                    <label>Author:</label>
                    <input type="text" id="author-filter" placeholder="Filter by author">
                </div>
            </div>
        </div>
    </nav>

    <main class="container">
        <div class="papers-grid" id="papers-container">
            
            <article class="paper-card"
                     data-arxiv-id="2512.23686v1"
                     data-domains="Clinical Documentation,Medical Dictation,Telemedicine,Health Informatics,Diagnostic Reporting,Surgical Notes"
                     data-keywords="Automatic Speech Recognition (ASR),Context-Conditioned ASR,Medical Speech,Benchmark,Professional Speech,Word Error Rate (WER),Entity Recognition,Context-Utilization Gap (CUG)"
                     data-authors="Deepak Babu Piskala">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23686v1.html">PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Deepak Babu Piskala
                </div>

                <div class="paper-summary">
                    ProfASR-Bench addresses the limitations of current ASR benchmarks for high-stakes professional speech, particularly in medical settings, by introducing a context-conditioned evaluation suite with entity-aware metrics. The study reveals a "context-utilization gap" (CUG), demonstrating that state-of-the-art ASR models largely fail to leverage even oracle textual context to improve recognition accuracy. This benchmark provides a critical tool for developing ASR systems that can effectively utilize side information in domains with dense terminology and low error tolerance.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Clinical Documentation</span>
                    
                    <span class="domain-tag">Medical Dictation</span>
                    
                    <span class="domain-tag">Telemedicine</span>
                    
                    <span class="domain-tag">Health Informatics</span>
                    
                    <span class="domain-tag">Diagnostic Reporting</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23686v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23686v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23686v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23686v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23671v1"
                     data-domains="Epidemiology,Public health,Infectious disease modeling,Medical resource planning"
                     data-keywords="Quantile forecasting,Calibration,Online learning,Epidemic forecasting,Multi-level,No-regret,Adversarial robustness,Statistical machine learning"
                     data-authors="Tiffany Ding,Isaac Gibbs,Ryan J. Tibshirani">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23671v1.html">Calibrated Multi-Level Quantile Forecasting</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ stat.ML</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Tiffany Ding, Isaac Gibbs, Ryan J. Tibshirani
                </div>

                <div class="paper-summary">
                    This paper introduces Multi-Level Quantile Tracker (MultiQT), an online method that guarantees simultaneous calibration of multi-level quantile forecasts from any existing forecaster. MultiQT corrects forecasts to ensure they are ordered and calibrated, even under adversarial distribution shifts, and comes with a no-regret guarantee. The method significantly improves forecast calibration in real-world applications, notably in epidemic and energy forecasting.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Epidemiology</span>
                    
                    <span class="domain-tag">Public health</span>
                    
                    <span class="domain-tag">Infectious disease modeling</span>
                    
                    <span class="domain-tag">Medical resource planning</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23671v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23671v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23671v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23671v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23637v1"
                     data-domains="Consumer Health Informatics,Digital Health,Medical Natural Language Processing,Public Health (information dissemination)"
                     data-keywords="consumer healthcare,question summarization,dataset,natural language processing,benchmark,medical informatics,community question answering,domain-expert annotation"
                     data-authors="Abhishek Basu,Deepak Gupta,Dina Demner-Fushman,Shweta Yadav">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23637v1.html">A Dataset and Benchmark for Consumer Healthcare Question Summarization</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CL</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Abhishek Basu, Deepak Gupta, Dina Demner-Fushman et al.
                </div>

                <div class="paper-summary">
                    This paper introduces CHQ-Sum, a novel dataset comprising 1507 domain-expert annotated consumer health questions and their corresponding summaries, to address the challenge of verbose patient queries and the lack of specialized summarization datasets. The dataset, derived from community question answering forums, provides a critical resource for developing and benchmarking efficient Natural Language Processing (NLP) systems for healthcare question summarization, thereby propelling advancements in understanding consumer health information needs.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Consumer Health Informatics</span>
                    
                    <span class="domain-tag">Digital Health</span>
                    
                    <span class="domain-tag">Medical Natural Language Processing</span>
                    
                    <span class="domain-tag">Public Health (information dissemination)</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23637v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23637v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23637v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23637v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23597v1"
                     data-domains="Oncology,Radiology,Gastroenterology,Medical Imaging,Diagnostic Medicine"
                     data-keywords="Pancreatic Neoplasm,Early Detection,Multimodal CT Imaging,Deep Learning,Feature Aggregation,Metaheuristic Optimization,Vision Transformer,EfficientNet"
                     data-authors="Janani Annur Thiruvengadam,Kiran Mayee Nabigaru,Anusha Kovi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23597v1.html">Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Janani Annur Thiruvengadam, Kiran Mayee Nabigaru, Anusha Kovi
                </div>

                <div class="paper-summary">
                    This paper introduces the Scalable Residual Feature Aggregation (SRFA) framework, a novel AI-driven system designed for robust early pancreatic neoplasm detection in multimodal CT imaging. The framework integrates advanced deep learning techniques, metaheuristic optimization, and a hybrid classification model to overcome challenges posed by subtle tumor cues and anatomical variations. Experimental results demonstrate a significant performance improvement, achieving 96.23% accuracy, 95.58% F1-score, and 94.83% specificity, substantially outperforming existing CNN and transformer-based methods.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Gastroenterology</span>
                    
                    <span class="domain-tag">Medical Imaging</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23597v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23597v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23597v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23597v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23545v1"
                     data-domains="Pathology,Histopathology,Oncology,Diagnostic Medicine"
                     data-keywords="computational pathology,agentic AI,multimodal models,pathological diagnosis,whole-slide images,evidence-seeking,deep learning,reinforcement learning"
                     data-authors="Shengyi Hua,Jianfeng Wu,Tianle Shen,Kangzhe Hu,Zhongzhen Huang,Shujuan Ni,Zhihong Zhang,Yuan Li,Zhe Wang,Xiaofan Zhang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23545v1.html">PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 1.00</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shengyi Hua, Jianfeng Wu, Tianle Shen et al.
                </div>

                <div class="paper-summary">
                    PathFound introduces an agentic multimodal model for pathological diagnosis that emulates clinical workflows by performing iterative evidence-seeking and diagnosis refinement. Unlike static inference methods, this model proactively acquires information through initial diagnosis, evidence-seeking, and final decision stages, significantly improving diagnostic accuracy and achieving state-of-the-art performance across diverse clinical scenarios.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pathology</span>
                    
                    <span class="domain-tag">Histopathology</span>
                    
                    <span class="domain-tag">Oncology</span>
                    
                    <span class="domain-tag">Diagnostic Medicine</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23545v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23545v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23545v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23545v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23513v1"
                     data-domains="physics.med-ph"
                     data-keywords="physics.med-ph"
                     data-authors="Siqi Li,Benjamin A. Spencer,Yiran Wang,Yasser G. Abdelhafez,Heather Hunt,J. Anthony Seibert,Simon R. Cherry,Ramsey D. Badawi,Lorenzo Nardo,Guobao Wang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23513v1.html">Incorporating Tissue Composition Information in Total-Body PET Metabolic Quantification of Bone Marrow through Dual-Energy CT</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Siqi Li, Benjamin A. Spencer, Yiran Wang et al.
                </div>

                <div class="paper-summary">
                    Bone marrow (BM) metabolic quantification with 18F-fluorodeoxyglucose (FDG) positron emission tomography (PET) is of broad clinical significance for accurate assessment of BM at staging and follow-up, especially when immunotherapy is involved. However, current methods of quantifying BM may be inaccu...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">physics.med-ph</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23513v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23513v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23513v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23513v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23175v1"
                     data-domains="Drug Discovery,Pharmacology,Medicinal Chemistry,Biotherapeutics,Computational Biology"
                     data-keywords="therapeutic peptides,HELM,Transformer,drug discovery,peptide property prediction,cyclic peptides,molecular language models,bioinformatics"
                     data-authors="Seungeon Lee,Takuto Koyama,Itsuki Maeda,Shigeyuki Matsumoto,Yasushi Okuno">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23175v1.html">HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Seungeon Lee, Takuto Koyama, Itsuki Maeda et al.
                </div>

                <div class="paper-summary">
                    HELM-BERT introduces a novel transformer-based language model designed for accurately predicting properties of medium-sized therapeutic peptides. It addresses the limitations of existing molecular language models by leveraging the Hierarchical Editing Language for Macromolecules (HELM), which explicitly captures complex peptide chemistry and topology. Pre-trained on a diverse corpus of nearly 40,000 peptides, HELM-BERT significantly outperforms SMILES-based models in critical downstream tasks, demonstrating enhanced data-efficiency and bridging a crucial representational gap in peptide drug discovery.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Drug Discovery</span>
                    
                    <span class="domain-tag">Pharmacology</span>
                    
                    <span class="domain-tag">Medicinal Chemistry</span>
                    
                    <span class="domain-tag">Biotherapeutics</span>
                    
                    <span class="domain-tag">Computational Biology</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23175v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23175v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23175v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23175v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23160v1"
                     data-domains="cs.LG"
                     data-keywords="cs.LG"
                     data-authors="Xianqi Liu,Xiangru Li,Lefeng He,Ziyu Fang">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23160v1.html">A Weak Signal Learning Dataset and Its Baseline Method</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.80</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Xianqi Liu, Xiangru Li, Lefeng He et al.
                </div>

                <div class="paper-summary">
                    Weak signal learning (WSL) is a common challenge in many fields like fault diagnosis, medical imaging, and autonomous driving, where critical information is often masked by noise and interference, making feature identification difficult. Even in tasks with abundant strong signals, the key to improvi...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.LG</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23160v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23160v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23160v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23160v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23151v1"
                     data-domains="Pulmonary imaging,Abdominal imaging,Diagnostic radiology,Medical physics"
                     data-keywords="MRI,respiratory motion correction,deep learning,unrolled network,radial k-space,image reconstruction,NUFFT,CNN"
                     data-authors="Shanshan Shan,Hongli Chen,Yuhan Wei,Peng Wu,Yang Gao,Tess Reynolds,Paul Liu,Jialiang Zhang,Qidi Luo,Chunyi Liu,Paul Keall,Feng Liu,Yaqin Zhang,David E. J. Waddington,Mingyuan Gao">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23151v1.html">Two-stage Respiratory Motion-resolved Radial MR Image Reconstruction Using an Interpretable Deep Unrolled Network</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ physics.med-ph</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Shanshan Shan, Hongli Chen, Yuhan Wei et al.
                </div>

                <div class="paper-summary">
                    This paper introduces MoraNet, a two-stage deep unrolled network for respiratory motion-resolved radial MR image reconstruction, addressing motion artifacts prevalent in abdominal and pulmonary imaging. The method accurately extracts respiratory signals and reconstructs high-quality, motion-corrected images for specific motion states, demonstrating superior image quality and significantly faster processing compared to conventional compressed sensing techniques.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Pulmonary imaging</span>
                    
                    <span class="domain-tag">Abdominal imaging</span>
                    
                    <span class="domain-tag">Diagnostic radiology</span>
                    
                    <span class="domain-tag">Medical physics</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23151v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23151v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23151v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23151v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23142v1"
                     data-domains="Radiology,Radiation Oncology,Image-Guided Surgery,Neuroscience (Brain Imaging),Medical Image Analysis"
                     data-keywords="Deformable registration,Deep learning,Domain shift,Multi-modal,Local features,Feature extraction,Image registration,Robustness"
                     data-authors="Mingzhen Shao,Sarang Joshi">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23142v1.html">Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.90</span>
                        
                        <span class="category">üìÇ cs.CV</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Mingzhen Shao, Sarang Joshi
                </div>

                <div class="paper-summary">
                    This paper reveals that deep deformable registration models possess an inherent immunity to domain shift, a property stemming from their reliance on local feature representations rather than global image appearance. They introduce UniReg, a framework that decouples feature extraction from deformation estimation, demonstrating robust cross-domain and multi-modal performance comparable to traditional methods, even when trained on a single dataset, while attributing conventional CNN failures to early layer biases.
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">Radiology</span>
                    
                    <span class="domain-tag">Radiation Oncology</span>
                    
                    <span class="domain-tag">Image-Guided Surgery</span>
                    
                    <span class="domain-tag">Neuroscience (Brain Imaging)</span>
                    
                    <span class="domain-tag">Medical Image Analysis</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23142v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23142v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23142v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23142v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
            <article class="paper-card"
                     data-arxiv-id="2512.23137v1"
                     data-domains="cs.LG"
                     data-keywords="cs.LG,eess.IV,q-bio.NC"
                     data-authors="Runzhi Zhou,Xi Luo">

                <div class="paper-header">
                    <h2 class="paper-title">
                        <a href="papers/2512.23137v1.html">Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use</a>
                    </h2>
                    <div class="paper-meta">
                        <span class="date">üìÖ 2025-12-29</span>
                        <span class="relevance">‚≠ê 0.95</span>
                        
                        <span class="category">üìÇ cs.LG</span>
                    </div>
                </div>

                <div class="paper-authors">
                    <strong>Authors:</strong> Runzhi Zhou, Xi Luo
                </div>

                <div class="paper-summary">
                    Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied succe...
                </div>

                <div class="paper-domains">
                    
                    <span class="domain-tag">cs.LG</span>
                    
                </div>

                <div class="paper-links">
                    <a href="papers/2512.23137v1.html" class="btn btn-primary">Read Full Summary</a>
                    <a href="http://arxiv.org/abs/2512.23137v1" target="_blank" class="btn btn-secondary">arXiv</a>
                    <a href="https://arxiv.org/pdf/2512.23137v1" target="_blank" class="btn btn-secondary">PDF</a>
                    <button class="export-btn" data-paper-id="2512.23137v1" title="Export Citation">
                        üìö Cite
                    </button>
                </div>
            </article>
            
        </div>
    </main>

    <footer class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h3>Health AI Hub</h3>
                <p>AI-powered medical research discovery | Latest health AI papers from arXiv, curated daily</p>
                <p>Curated by <a href="mailto:bryan@arxiv-health.org">Bryan Tegomoh</a></p>
                <p>Powered by Gemini AI | Updated Daily</p>
            </div>
            <div class="footer-section">
                <h3>About</h3>
                <p><a href="about.html">Methodology</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health" target="_blank">Open Source</a></p>
                <p><a href="https://github.com/BryanTegomoh/arxiv-health/discussions" target="_blank">Discussions</a></p>
            </div>
            <div class="footer-section">
                <h3>Connect</h3>
                <p><a href="https://twitter.com/ArXiv_Health" target="_blank">Twitter/X</a></p>
                <p><a href="https://bryantegomoh.substack.com" target="_blank">Newsletter</a></p>
                <p><a href="https://arxiv.org" target="_blank">arXiv.org</a></p>
            </div>
        </div>
        <div class="footer-bottom">
            <p>¬© 2025 Health AI Hub | Last updated: 2025-12-30 06:15:49</p>
        </div>
    </footer>

    <!-- Export Modal -->
    <div id="export-modal" class="modal">
        <div class="modal-content">
            <span class="modal-close">&times;</span>
            <h2>Export Citation</h2>
            <div class="export-options">
                <button class="export-format" data-format="bibtex">BibTeX</button>
                <button class="export-format" data-format="ris">RIS (EndNote/Mendeley)</button>
                <button class="export-format" data-format="plain">Plain Text</button>
            </div>
            <textarea id="citation-output" readonly></textarea>
            <button id="copy-citation" class="btn btn-primary">Copy to Clipboard</button>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>