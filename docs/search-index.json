[
  {
    "id": "2601.16073v1",
    "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models",
    "authors": [
      "Hanwen Zhang",
      "Qiaojin Shen",
      "Yuxi Liu",
      "Yuesheng Zhu",
      "Guibo Luo"
    ],
    "summary": "DSFedMed proposes a novel dual-scale federated framework addressing the high computational and communication demands of deploying Foundation Models (FMs) in federated medical image segmentation. It achieves this by enabling mutual knowledge distillation between a centralized FM and lightweight client models, leveraging generated high-quality medical images and a learnability-guided sample selection strategy. This approach significantly improves segmentation accuracy by 2% (Dice score) while reducing communication and inference costs by nearly 90% compared to existing federated FM baselines.",
    "keywords": [
      "Federated Learning",
      "Foundation Models",
      "Knowledge Distillation",
      "Medical Image Segmentation",
      "Dual-Scale",
      "Lightweight Models",
      "Computational Efficiency",
      "Data Privacy"
    ],
    "domains": [
      "Radiology",
      "Medical Imaging",
      "Oncology",
      "Neurology",
      "Cardiology",
      "Pathology"
    ],
    "url": "papers/2601.16073v1.html"
  },
  {
    "id": "2601.16064v1",
    "title": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation",
    "authors": [
      "Shams Nafisa Ali",
      "Taufiq Hasan"
    ],
    "summary": "Phi-SegNet introduces a CNN-based architecture that integrates phase-aware information at both architectural and optimization levels to enhance medical image segmentation. By leveraging Bi-Feature Mask Former modules, Reverse Fourier Attention blocks, and a dedicated phase-aware loss, it emphasizes boundary precision and improves generalization across diverse imaging modalities. The model achieved state-of-the-art performance on five public datasets and demonstrated robust cross-dataset generalization, highlighting the potential of spectral priors for fine-grained object localization.",
    "keywords": [
      "medical image segmentation",
      "deep learning",
      "phase-aware",
      "frequency domain",
      "CNN",
      "generalization",
      "Fourier Attention",
      "boundary precision"
    ],
    "domains": [
      "X-ray",
      "Ultrasound (US)",
      "Histopathology",
      "MRI",
      "Colonoscopy"
    ],
    "url": "papers/2601.16064v1.html"
  },
  {
    "id": "2601.16060v1",
    "title": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation",
    "authors": [
      "Yuan Lin",
      "Murong Xu",
      "Marc H\u00f6lle",
      "Chinmay Prabhakar",
      "Andreas Maier",
      "Vasileios Belagiannis",
      "Bjoern Menze",
      "Suprosanna Shit"
    ],
    "summary": "ProGiDiff introduces a novel prompt-guided, diffusion-based framework for medical image segmentation, leveraging pre-trained image generation models to address limitations of deterministic methods. It employs a ControlNet-style conditioning mechanism with a custom encoder to generate multi-class segmentation masks, demonstrating strong performance on CT organ segmentation and successful low-rank, few-shot transfer to MR images.",
    "keywords": [
      "Medical Image Segmentation",
      "Diffusion Models",
      "Prompt-Guided",
      "ControlNet",
      "Multi-Class Segmentation",
      "Cross-Modality",
      "CT",
      "MR"
    ],
    "domains": [
      "Radiology",
      "Medical Imaging",
      "Diagnostic Imaging",
      "Oncology",
      "Surgery Planning"
    ],
    "url": "papers/2601.16060v1.html"
  },
  {
    "id": "2601.16024v1",
    "title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry",
    "authors": [
      "Rongze Ma",
      "Mengkang Lu",
      "Zhenyu Xiang",
      "Yongsheng Pan",
      "Yicheng Wu",
      "Qingjie Zeng",
      "Yong Xia"
    ],
    "summary": "This paper introduces PAINT (Pathology-Aware Integrated Next-Scale Transformation), a novel visual autoregressive framework for virtual immunohistochemistry (IHC). PAINT reformulates IHC synthesis as a structure-first conditional generation task, leveraging a Spatial Structural Start Map (3S-Map) to ground autoregressive initialization in H&E morphology. The framework significantly outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks on the IHC4BC and MIST datasets.",
    "keywords": [
      "Virtual immunohistochemistry",
      "Computational pathology",
      "Hematoxylin and Eosin (H&E)",
      "Deep learning",
      "Autoregressive models",
      "Image synthesis",
      "Digital pathology",
      "Cancer diagnostics"
    ],
    "domains": [
      "Pathology",
      "Oncology",
      "Histopathology",
      "Diagnostic Medicine"
    ],
    "url": "papers/2601.16024v1.html"
  },
  {
    "id": "2601.15977v1",
    "title": "Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data",
    "authors": [
      "Binbin Lin",
      "Lei Zou",
      "Hao Tian",
      "Heng Cai",
      "Yifan Yang",
      "Bing Zhou"
    ],
    "summary": "This study integrates hospital attributes, population socioeconomics, and human mobility data to predict healthcare visitation flows and analyze their influencing factors. It developed and evaluated five predictive models, identifying Deep Gravity as the top performer, and revealed how hospital characteristics and socioeconomic factors differentially impact visitation patterns based on travel distance and specific demographic groups.",
    "keywords": [
      "Healthcare visitation",
      "Human mobility",
      "Hospital attributes",
      "Population socioeconomics",
      "Deep Gravity",
      "Machine learning",
      "Patient flow",
      "Health disparities"
    ],
    "domains": [
      "Health services research",
      "Public health",
      "Healthcare administration",
      "Health equity",
      "Epidemiology"
    ],
    "url": "papers/2601.15977v1.html"
  },
  {
    "id": "2601.15931v1",
    "title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search",
    "authors": [
      "Xiangyu Wang",
      "Zhixin Lv",
      "Yongjiao Sun",
      "Anrui Han",
      "Ye Yuan",
      "Hangxu Ji"
    ],
    "summary": "Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to ...",
    "keywords": [
      "cs.AI",
      "cs.LG"
    ],
    "domains": [
      "cs.AI"
    ],
    "url": "papers/2601.15931v1.html"
  },
  {
    "id": "2601.15416v1",
    "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction",
    "authors": [
      "Cuong Tran Van",
      "Trong-Thang Pham",
      "Ngoc-Son Nguyen",
      "Duy Minh Ho Nguyen",
      "Ngan Le"
    ],
    "summary": "DuFal (Dual-Frequency-Aware Learning) is a novel deep learning framework designed for high-fidelity Cone-Beam Computed Tomography (CBCT) reconstruction from extremely sparse-view projections. It tackles the challenge of recovering fine-grained, high-frequency anatomical details, which are often lost in conventional methods, by integrating frequency-domain and spatial-domain processing through a unique dual-path architecture. Experimental results demonstrate DuFal significantly outperforms state-of-the-art methods in preserving these critical high-frequency features under severe undersampling conditions.",
    "keywords": [
      "sparse-view CBCT",
      "deep learning",
      "Fourier Neural Operator",
      "high-frequency reconstruction",
      "medical imaging",
      "radiation dose reduction",
      "anatomical detail",
      "cone-beam CT"
    ],
    "domains": [
      "Radiology",
      "Diagnostic Imaging",
      "Medical Physics",
      "Oncology (for treatment planning)",
      "Dentistry"
    ],
    "url": "papers/2601.15416v1.html"
  },
  {
    "id": "2601.15408v1",
    "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation",
    "authors": [
      "Pablo Messina",
      "Andr\u00e9s Villa",
      "Juan Le\u00f3n Alc\u00e1zar",
      "Karen S\u00e1nchez",
      "Carlos Hinojosa",
      "Denis Parra",
      "\u00c1lvaro Soto",
      "Bernard Ghanem"
    ],
    "summary": "CURE introduces an error-aware curriculum learning framework to enhance the reliability and accuracy of automated radiology report generation. It addresses common issues like inaccurate visual grounding and factual inconsistency by fine-tuning a multimodal instructional model across multiple tasks. The method achieves significant improvements in grounding accuracy, report quality, and reduces hallucinations without requiring additional data.",
    "keywords": [
      "medical vision-language models",
      "radiology reports",
      "visual grounding",
      "curriculum learning",
      "multi-task training",
      "hallucination reduction",
      "AI in medicine",
      "diagnostic imaging"
    ],
    "domains": [
      "Radiology",
      "Diagnostic Imaging",
      "Medical AI"
    ],
    "url": "papers/2601.15408v1.html"
  },
  {
    "id": "2601.15392v1",
    "title": "GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation",
    "authors": [
      "Francesca Pia Panaccione",
      "Carlo Sgaravatti",
      "Pietro Pinoli"
    ],
    "summary": "GeMM-GAN is a novel Multimodal Generative Adversarial Network designed to synthesize realistic gene expression profiles. It addresses the challenges of costly and privacy-sensitive gene expression data collection by conditioning its generation on readily available histopathology images and clinical metadata. The model produces biologically coherent profiles, significantly improving downstream disease type prediction accuracy by over 11% compared to current state-of-the-art generative models.",
    "keywords": [
      "Gene expression",
      "Generative Adversarial Network (GAN)",
      "Multimodal learning",
      "Histopathology",
      "Clinical metadata",
      "Transformer Encoder",
      "Cross Attention",
      "TCGA"
    ],
    "domains": [
      "Oncology",
      "Computational Pathology",
      "Bioinformatics",
      "Genomics",
      "Precision Medicine",
      "Medical Imaging"
    ],
    "url": "papers/2601.15392v1.html"
  }
]