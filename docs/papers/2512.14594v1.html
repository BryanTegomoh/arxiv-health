<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction - Health AI Hub</title>
    <meta name="description" content="This paper introduces KEMM, an LLM-driven knowledge-enhanced multimodal model for cancer survival prediction, addressing challenges with high-dimensional data a">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.14594v1" target="_blank">2512.14594v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-16
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Chenyu Zhao, Yingxue Xu, Fengtao Zhou, Yihui Wang, Hao Chen
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.14594v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.14594v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces KEMM, an LLM-driven knowledge-enhanced multimodal model for cancer survival prediction, addressing challenges with high-dimensional data and insufficient labels. KEMM integrates LLM-refined expert reports and LLM-generated prognostic background knowledge, leveraging a Knowledge-Enhanced Cross-Modal (KECM) attention module to extract discriminative features. The model achieves state-of-the-art performance across five cancer datasets.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research significantly enhances the accuracy of cancer survival prediction by integrating clinical expert insights and general prognostic knowledge via LLMs, directly impacting personalized medicine, treatment stratification, and patient counseling in oncology.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves using Large Language Models (LLMs) to enhance multimodal cancer survival prediction. LLMs are specifically used to refine pathologist expert reports into clinically focused diagnostic statements and to generate concise prognostic background knowledge for various cancer types. This knowledge is then integrated via a 'knowledge-enhanced cross-modal attention module' to improve the accuracy of survival prediction from complex medical imaging and genomic data.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Addresses Core Challenges**: The paper tackles the difficulties in multimodal cancer survival prediction, specifically focusing on high-dimensional and redundant pathology images (WSIs) and genomic data, and the inadequacy of simple survival follow-up labels for supervision.</li>
                    
                    <li>**LLM-Driven Knowledge Integration**: KEMM incorporates two types of knowledge refined or generated by Large Language Models (LLMs): (1) expert reports (pathologist statements, LLM-refined) providing succinct diagnostic insights, and (2) prognostic background knowledge (PBK, LLM-generated) offering general prognostic information for different cancer types.</li>
                    
                    <li>**Knowledge-Enhanced Cross-Modal (KECM) Attention**: A novel KECM attention module is introduced. This module effectively guides the multimodal network to selectively focus on discriminative and survival-relevant features from the inherently redundant WSI and genomic modalities, utilizing the integrated LLM-derived knowledge.</li>
                    
                    <li>**Enhanced Supervision Signal**: By integrating rich, clinically focused knowledge, KEMM moves beyond simple survival labels, providing a more robust and nuanced supervisory signal for the complex task of predicting cancer survival.</li>
                    
                    <li>**State-of-the-Art Performance**: Extensive experiments conducted across five distinct cancer datasets demonstrate that KEMM consistently achieves state-of-the-art performance in multimodal cancer survival prediction, outperforming existing methods.</li>
                    
                    <li>**Improved Feature Extraction and Alignment**: The KECM attention mechanism, informed by expert and prognostic knowledge, facilitates more effective extraction of critical features and better alignment between disparate WSI and genomic modalities.</li>
                    
                    <li>**Potential for Clinical Translation**: The demonstrated superior performance suggests a strong potential for KEMM to improve prognostic accuracy in clinical settings, aiding in more personalized patient management.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>KEMM is a multimodal deep learning model integrating high-dimensional Whole Slide Images (WSIs) and genomic data. Its core innovation lies in leveraging LLMs to refine pathologist-provided expert reports (case-by-case diagnostic statements) and to generate concise prognostic background knowledge (PBK) for various cancer types. This knowledge is then incorporated via a novel Knowledge-Enhanced Cross-Modal (KECM) attention module, which guides the network to selectively focus on discriminative and survival-relevant features from the redundant multimodal inputs, ultimately predicting patient survival outcomes.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>KEMM achieved state-of-the-art performance in multimodal cancer survival prediction. Its ability to effectively integrate LLM-driven expert and prognostic knowledge via the KECM attention module allowed for superior extraction of discriminative features and better alignment across high-dimensional pathology and genomic modalities, leading to more accurate survival outcomes across five diverse cancer datasets.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The enhanced accuracy of cancer survival prediction offered by KEMM holds substantial clinical impact. It can lead to more precise patient risk stratification, enabling oncologists to design more personalized treatment plans, guide targeted therapies, and improve the selection of patients for clinical trials. This ultimately contributes to better patient management and potentially improved long-term outcomes in cancer care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract. Potential implicit limitations could include the reliance on the quality and availability of pathologist reports, the generalizability and robustness of LLM-generated knowledge across extremely rare cancer types, and computational demands associated with processing high-dimensional data and LLM integration.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract. However, the planned release of code upon acceptance suggests a direction towards broader community validation and further development. Future research could explore integrating other forms of clinical data (e.g., radiology reports, treatment history), enhancing the interpretability of the KECM module, or investigating real-time clinical deployment challenges.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Genomics</span>
                    
                    <span class="tag">Prognostics</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Cancer survival prediction</span>
                    
                    <span class="tag tag-keyword">Multimodal learning</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLM)</span>
                    
                    <span class="tag tag-keyword">Pathology images (WSI)</span>
                    
                    <span class="tag tag-keyword">Genomic data</span>
                    
                    <span class="tag tag-keyword">Prognostic knowledge</span>
                    
                    <span class="tag tag-keyword">Deep learning</span>
                    
                    <span class="tag tag-keyword">Attention mechanisms</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>