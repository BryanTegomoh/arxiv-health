<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Toward Global Large Language Models in Medicine - Health AI Hub</title>
    <meta name="description" content="This paper addresses the critical imbalance in medical Large Language Models (LLMs) which are predominantly trained on high-resource languages, thereby limiting">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Toward Global Large Language Models in Medicine</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.02186v1" target="_blank">2601.02186v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Rui Yang, Huitao Li, Weihao Xuan, Heli Qi, Xin Li, Kunyu Yu, Yingjian Chen, Rongrong Wang, Jacques Behmoaras, Tianxi Cai, Bibhas Chakraborty, Qingyu Chen, Lionel Tim-Ee Cheng, Marie-Louise Damwanza, Chido Dzinotyiwei, Aosong Feng, Chuan Hong, Yusuke Iwasawa, Yuhe Ke, Linah Kitala, Taehoon Ko, Jisan Lee, Irene Li, Jonathan Chong Kai Liew, Hongfang Liu, Lian Leng Low, Edison Marrese-Taylor, Yutaka Matsuo, Isheanesu Misi, Yilin Ning, Jasmine Chiat Ling Ong, Marcus Eng Hock Ong, Enrico Petretto, Hossein Rouhizadeh, Abiram Sandralegar, Oren Schreier, Iain Bee Huat Tan, Patrick Tan, Daniel Shu Wei Ting, Junjue Wang, Chunhua Weng, Matthew Yu Heng Wong, Fang Wu, Yunze Xiao, Xuhai Xu, Qingcheng Zeng, Zhuo Zheng, Yifan Peng, Douglas Teodoro, Nan Liu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.02186v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.02186v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper addresses the critical imbalance in medical Large Language Models (LLMs) which are predominantly trained on high-resource languages, thereby limiting global healthcare access and equity. The authors introduce GlobMed, a large multilingual medical dataset spanning 12 languages (including low-resource ones), and GlobMed-Bench, a comprehensive benchmark to evaluate existing LLMs across multilingual medical tasks. To overcome observed disparities, they developed GlobMed-LLMs, a suite of multilingual medical LLMs, demonstrating significant performance improvements, particularly for low-resource languages.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research directly addresses a fundamental challenge in global health equity by enabling advanced medical AI technologies to be accessible and effective across diverse linguistic populations. It holds the potential to significantly improve the quality and accessibility of medical information and healthcare services for underserved communities worldwide.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research focuses on the development, evaluation, and application of multilingual Large Language Models (LLMs) to enhance healthcare quality, expand global access to medical information, and support various medical tasks, particularly for communities using low-resource languages. This aims to create more equitable and accessible AI-driven medical solutions worldwide.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Identification:** Existing LLMs are primarily trained on high-resource languages, creating a significant barrier to equitable access to advanced medical information and healthcare globally, especially for low-resource language communities.</li>
                    
                    <li>**GlobMed Dataset:** A novel, large-scale multilingual medical dataset was constructed, comprising over 500,000 entries across 12 languages, specifically including four low-resource languages to promote linguistic diversity.</li>
                    
                    <li>**GlobMed-Bench Benchmark:** A systematic benchmark was established to evaluate 56 state-of-the-art proprietary and open-weight LLMs on various multilingual medical tasks.</li>
                    
                    <li>**Performance Disparities:** GlobMed-Bench revealed substantial performance disparities among existing LLMs, with particularly poor performance observed in tasks involving low-resource languages.</li>
                    
                    <li>**GlobMed-LLMs Development:** A new suite of multilingual medical LLMs, named GlobMed-LLMs, was developed and trained on the GlobMed dataset, with model sizes ranging from 1.7 billion to 8 billion parameters.</li>
                    
                    <li>**Significant Performance Gains:** GlobMed-LLMs achieved an average performance improvement of over 40% compared to baseline models, and a more than threefold increase in performance specifically on low-resource languages.</li>
                    
                    <li>**Foundation for Equity:** The developed datasets, benchmarks, and models lay a crucial foundation for fostering more equitable development and application of medical LLMs worldwide, extending the benefits of AI to broader language communities.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involved three main components: 1) Constructing **GlobMed**, a large multilingual medical dataset with over 500,000 entries in 12 languages, including four low-resource languages. 2) Establishing **GlobMed-Bench**, a systematic benchmark designed to assess the performance of 56 state-of-the-art LLMs (proprietary and open-weight) across multiple multilingual medical tasks. 3) Developing and training **GlobMed-LLMs**, a suite of custom multilingual medical LLMs ranging from 1.7B to 8B parameters, specifically on the GlobMed dataset.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study's key findings are that current state-of-the-art LLMs exhibit significant performance disparities across different languages in medical contexts, particularly struggling with low-resource languages. In contrast, the newly developed GlobMed-LLMs demonstrated substantial improvements, achieving an average performance increase of over 40% relative to baselines, and a remarkable greater than threefold performance increase specifically for low-resource languages, thereby bridging critical language gaps.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact of this work is profound, as it paves the way for deploying AI-powered medical tools that can understand and generate medical information in a wider array of languages. This could lead to improved diagnostics, enhanced patient education, better adherence to treatment protocols, and more efficient healthcare delivery for patients and providers in linguistically diverse regions, particularly where access to specialized medical expertise is limited due to language barriers.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the limitations of *existing* LLMs (their training bias towards high-resource languages) which this research aims to address. It does not explicitly detail specific limitations or caveats of the developed GlobMed dataset, GlobMed-Bench, or GlobMed-LLMs within the provided text. However, the scope of 12 languages, while significant, still represents a subset of global linguistic diversity.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors indicate that these resources provide an important foundation for advancing the equitable development and application of LLMs globally. This implies future directions include expanding language coverage, further improving model performance and generalization, and facilitating the real-world deployment and integration of these multilingual medical LLMs to ensure broader language communities can truly benefit from technological advances in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Global Health</span>
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Health Equity</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Medical Education</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">large language models</span>
                    
                    <span class="tag tag-keyword">multilingual AI</span>
                    
                    <span class="tag tag-keyword">medical AI</span>
                    
                    <span class="tag tag-keyword">global health</span>
                    
                    <span class="tag tag-keyword">health equity</span>
                    
                    <span class="tag tag-keyword">low-resource languages</span>
                    
                    <span class="tag tag-keyword">medical informatics</span>
                    
                    <span class="tag tag-keyword">healthcare access</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>182 pages, 65 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>