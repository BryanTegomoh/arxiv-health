<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning - Health AI Hub</title>
    <meta name="description" content="This paper introduces OpenUS, the first reproducible, open-source ultrasound foundation model designed to overcome challenges like operator dependence and data ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.11510v1" target="_blank">2511.11510v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-14
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xiaoyu Zheng, Xu Chen, Awais Rauf, Qifan Fu, Benedetta Monosi, Felice Rivellese, Myles J. Lewis, Shaogang Gong, Gregory Slabaugh
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.11510v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.11510v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces OpenUS, the first reproducible, open-source ultrasound foundation model designed to overcome challenges like operator dependence and data variability in US image analysis. It leverages a Vision Mamba backbone with a novel self-adaptive masked contrastive learning framework, pre-trained on the largest public ultrasound dataset, enabling label-efficient adaptation for diverse downstream tasks.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This work is crucial for medicine as it aims to reduce the subjectivity and variability in ultrasound interpretation, enabling the development of more standardized, generalizable, and efficient AI tools to assist clinicians in various diagnostic and interventional procedures.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research develops a foundational AI model for automated analysis and interpretation of medical ultrasound images. Its application is to reduce the operator-dependency and variability in ultrasound interpretation, thereby improving the consistency, efficiency, and accuracy of diagnosis across various anatomical regions, disease types, and clinical settings. This AI can serve as a backbone for developing specific tools for diagnosis, prognosis, and treatment monitoring in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenges of high operator-dependency, variability across anatomical regions/protocols/devices, speckle, low contrast, and limited annotations in US image interpretation, which hinder generalizable AI.</li>
                    
                    <li>Proposes OpenUS, the first reproducible and open-source foundation model specifically for ultrasound image analysis.</li>
                    
                    <li>Employs a Vision Mamba backbone, which is adept at capturing both local and global long-range dependencies within ultrasound images.</li>
                    
                    <li>Introduces a novel self-adaptive masking framework for pre-training, combining contrastive learning with masked image modeling, which uses a teacher's attention map and student reconstruction loss to refine clinically-relevant masking.</li>
                    
                    <li>Utilizes a dynamic learning schedule to progressively adjust the difficulty of the pre-training process, enhancing model learning.</li>
                    
                    <li>Built on the largest compiled public ultrasound dataset to date, comprising over 308K images from 42 diverse datasets, ensuring broad coverage of anatomical regions, institutions, devices, and disease types.</li>
                    
                    <li>The pre-trained OpenUS model serves as a robust backbone, facilitating easy and label-efficient fine-tuning for specific downstream clinical tasks.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>OpenUS employs a Vision Mamba backbone for feature extraction. For pre-training, it utilizes a novel self-adaptive masking framework that integrates contrastive learning with masked image modeling (MIM). This framework dynamically refines masking by combining a teacher's attention map with student reconstruction loss, focusing on clinically relevant regions. A dynamic learning schedule is also applied to progressively adjust pre-training difficulty. The model is developed using a large, diverse public ultrasound dataset.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The paper introduces OpenUS as the first reproducible, open-source ultrasound foundation model, successfully pre-trained on the largest-to-date public dataset. This model leverages advanced architectural and learning strategies, demonstrating its capability to serve as an adaptable and label-efficient backbone for fine-tuning on various specific downstream ultrasound analysis tasks.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>OpenUS has the potential to significantly reduce operator dependence in ultrasound diagnostics, leading to more consistent and accurate interpretations across different clinical settings and devices. By enabling label-efficient AI development, it can accelerate the deployment of advanced diagnostic tools, making high-quality ultrasound analysis more accessible and reliable, ultimately improving patient outcomes and standardizing care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the capabilities and solutions provided by OpenUS. It does not explicitly state limitations of the OpenUS model itself, but it does enumerate general challenges inherent to US imaging (speckle, low contrast, limited standardized annotations, operator dependence) that OpenUS aims to address.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract suggests that OpenUS can be easily adapted to specific downstream tasks via label-efficient fine-tuning, implying future work would involve applying and evaluating the model across a wider array of clinical applications and anatomical regions. Explicit future research directions beyond this general adaptability are not detailed.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Obstetrics and Gynecology</span>
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">General Surgery</span>
                    
                    <span class="tag">Point-of-Care Ultrasound</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Ultrasound</span>
                    
                    <span class="tag tag-keyword">Foundation Model</span>
                    
                    <span class="tag tag-keyword">Vision Mamba</span>
                    
                    <span class="tag tag-keyword">Self-Adaptive Masking</span>
                    
                    <span class="tag tag-keyword">Contrastive Learning</span>
                    
                    <span class="tag tag-keyword">Medical Imaging AI</span>
                    
                    <span class="tag tag-keyword">Label-Efficient Learning</span>
                    
                    <span class="tag tag-keyword">Open-Source</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacher's attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at https://github.com/XZheng0427/OpenUS.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>