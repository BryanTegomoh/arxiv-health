<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions - Health AI Hub</title>
    <meta name="description" content="This paper offers the first theoretical explanation for the catastrophic failure of state-of-the-art Out-of-Distribution (OOD) detection methods when models are">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04034v1" target="_blank">2512.04034v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Hong Yang, Devroop Kar, Qi Yu, Alex Ororbia, Travis Desell
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04034v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04034v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper offers the first theoretical explanation for the catastrophic failure of state-of-the-art Out-of-Distribution (OOD) detection methods when models are trained on single-domain datasets. It proves that supervised learning on single-domain data inevitably leads to "domain feature collapse," where domain-specific information is completely discarded, resolving this failure mode by preserving domain information through a novel domain filtering technique using pretrained representations.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critically relevant to medical AI, as models are often trained on single-domain datasets (e.g., images from one hospital or scanner). Domain feature collapse explains why such models catastrophically fail when deployed in new clinical environments with different data distributions, directly impacting their safety and efficacy for patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research aims to improve the safety and reliability of AI systems used in medical diagnostics and healthcare by explaining and offering solutions to a fundamental limitation in detecting out-of-distribution samples. This capability is essential for AI models to correctly identify when they are presented with data outside their training experience (e.g., a medical image from a different scanner, a rare disease not seen during training, or a non-medical artifact), preventing misdiagnosis or confident errors and thus enabling more trustworthy deployment in clinical settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>State-of-the-art OOD detection methods catastrophically fail (e.g., 53% FPR@95 on MNIST) when models are trained on single-domain datasets, a previously unexplained phenomenon.</li>
                    
                    <li>The paper provides an information-theoretic explanation, proving that supervised learning on single-domain data causes "domain feature collapse," meaning the mutual information I(x_d; z) between domain-specific features (x_d) and learned representations (z) becomes zero.</li>
                    
                    <li>This collapse is a fundamental consequence of information bottleneck optimization, where models prioritize learning class-specific features while discarding domain-specific features.</li>
                    
                    <li>The analysis is extended using Fano's inequality to quantify partial domain feature collapse in practical scenarios, acknowledging that complete collapse might be an idealization.</li>
                    
                    <li>To validate the theory, a new benchmark called Domain Bench is introduced, comprising single-domain datasets designed for this specific investigation.</li>
                    
                    <li>The proposed solution, "domain filtering" (using pretrained representations to explicitly preserve I(x_d; z) > 0), empirically demonstrates its effectiveness in resolving the OOD detection failure mode.</li>
                    
                    <li>The work highlights fundamental limitations of supervised learning in narrow domains and has significant implications for transfer learning strategies, particularly regarding when to fine-tune versus freeze pretrained models.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology is primarily theoretical, using information theory to prove the concept of 'domain feature collapse' via information bottleneck optimization. This theoretical analysis is then extended with Fano's inequality for practical quantification. Empirical validation involves introducing 'Domain Bench,' a benchmark of single-domain datasets, and demonstrating the effectiveness of 'domain filtering' (using pretrained representations to preserve domain information) as a solution.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>["Supervised learning on single-domain data fundamentally results in 'domain feature collapse,' where domain-specific information (I(x_d; z)) is completely discarded from learned representations.", 'This collapse is a direct, provable consequence of information bottleneck optimization, causing models to rely solely on class-specific features.', 'The discarding of domain features leads to catastrophic OOD detection failure, exemplified by a 53% FPR@95 on MNIST for out-of-domain samples.', "Preserving I(x_d; z) > 0 through a straightforward 'domain filtering' technique using pretrained representations effectively resolves this OOD detection failure mode."]</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work has profound clinical impact by identifying a core reason for the lack of generalizability and robustness in medical AI, particularly for diagnostic systems. Understanding domain feature collapse allows for the development of more reliable AI models that can perform consistently across diverse hospital settings, scanner types, and patient demographics. Implementing strategies like domain filtering could significantly improve the trustworthiness and deployability of AI in healthcare, preventing misdiagnoses or failures when models encounter unseen clinical data distributions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The core limitation highlighted is inherent to current supervised learning approaches when applied to narrow, single-domain datasets. This fundamental limitation leads to 'domain feature collapse' and subsequent 'catastrophic failure' of OOD detection methods, implying a severe lack of robustness and generalizability for AI models trained under such conditions.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings open crucial avenues for future research in transfer learning, particularly informing critical decisions on when to fine-tune versus freeze pretrained models for optimal performance across diverse domains. It suggests exploring methods to strategically manage or explicitly preserve domain-specific information in learned representations to enhance model robustness and generalizability.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Imaging (Radiology, Pathology)</span>
                    
                    <span class="tag">Diagnostic AI</span>
                    
                    <span class="tag">Clinical Decision Support Systems</span>
                    
                    <span class="tag">Personalized Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Out-of-Distribution Detection</span>
                    
                    <span class="tag tag-keyword">Domain Feature Collapse</span>
                    
                    <span class="tag tag-keyword">Information Theory</span>
                    
                    <span class="tag tag-keyword">Information Bottleneck</span>
                    
                    <span class="tag tag-keyword">Supervised Learning</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Transfer Learning</span>
                    
                    <span class="tag tag-keyword">Domain Filtering</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>