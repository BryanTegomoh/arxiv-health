<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions - Health AI Hub</title>
    <meta name="description" content="This paper provides an information-theoretic explanation for the catastrophic failure of out-of-distribution (OOD) detection methods when models are trained on ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04034v1" target="_blank">2512.04034v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Hong Yang, Devroop Kar, Qi Yu, Alex Ororbia, Travis Desell
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04034v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04034v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper provides an information-theoretic explanation for the catastrophic failure of out-of-distribution (OOD) detection methods when models are trained on single-domain datasets, attributing it to "domain feature collapse" where domain-specific information is completely discarded (I(x_d; z) = 0). The authors introduce a solution, domain filtering using pretrained representations to preserve domain information, which empirically resolves this failure mode and offers insights into supervised learning limitations and transfer learning strategies.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as AI models, particularly in medical imaging, are frequently trained on single-domain datasets from specific institutions or scanners. Understanding and mitigating domain feature collapse is crucial for ensuring the reliability and safety of AI deployments in diverse clinical settings, where encountering out-of-domain patient data is common.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research directly informs the development of more robust and reliable AI systems for health and medicine. By explaining why AI models fail catastrophically on out-of-domain medical inputs and proposing solutions, it helps build AI that can better identify when it's operating outside its comfort zone (e.g., encountering an unusual medical image), thereby preventing misdiagnoses or inappropriate interventions. This is crucial for applications like medical image analysis (radiology, pathology), predictive analytics, and any AI system deployed in diverse clinical environments where data distributions can shift unexpectedly.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The paper offers the first theoretical explanation for why OOD detection methods fail catastrophically when models are trained solely on single-domain datasets.</li>
                    
                    <li>It proves that supervised learning on single-domain data leads to "domain feature collapse," meaning domain-specific information (I(x_d; z)) is entirely discarded in learned representations.</li>
                    
                    <li>This collapse is identified as a fundamental consequence of information bottleneck optimization, causing models to rely only on class-specific features while ignoring domain features.</li>
                    
                    <li>The failure manifests as poor OOD detection performance (e.g., 53% FPR@95 on MNIST) when encountering samples from previously unseen domains.</li>
                    
                    <li>The analysis is extended using Fano's inequality to quantify partial domain feature collapse in practical scenarios.</li>
                    
                    <li>To validate the theory, the authors introduce "Domain Bench," a benchmark of single-domain datasets, demonstrating that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) successfully resolves the OOD detection failure.</li>
                    
                    <li>The findings have broader implications for understanding limitations of supervised learning in narrow domains, guiding transfer learning strategies, and informing decisions on when to fine-tune versus freeze pretrained models.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employs an information-theoretic framework to theoretically prove domain feature collapse based on information bottleneck optimization. This theoretical analysis is extended using Fano's inequality for quantifying partial collapse. Empirical validation is performed by introducing "Domain Bench," a benchmark of single-domain datasets, and demonstrating the effectiveness of "domain filtering" (using pretrained representations to preserve domain information) in resolving OOD detection failures.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The central finding is the theoretical proof of "domain feature collapse" in models trained on single-domain data, where domain-specific information (I(x_d; z)) is completely discarded. This fundamental limitation leads to catastrophic failure in OOD detection. The research also found that actively preserving domain information through a method like domain filtering (utilizing pretrained representations) effectively resolves this OOD detection failure, providing strong empirical validation for the theoretical framework.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work directly impacts the development and deployment of AI in clinical settings by explaining a critical failure mode of OOD detection. It suggests that AI models trained solely on institution-specific medical data may generalize poorly to data from different hospitals, scanners, or patient populations. The proposed solution (domain filtering) offers a pathway to build more robust and reliable medical AI systems, enabling safer and more accurate diagnoses and prognoses across varied clinical environments. It also provides principled guidance for adapting existing pretrained models for clinical use, a common practice in medical AI.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>While the abstract highlights the solution's conceptual straightforwardness, an implicit limitation is the reliance on pretrained representations for domain filtering, which may not always be available or optimally suited for highly specialized medical tasks. The focus on single-domain training also suggests that the full complexity of multi-domain or federated learning scenarios is not fully addressed, though the theory provides foundational insights.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings have implications for developing more robust transfer learning strategies, particularly in medical imaging. Future work could explore how to optimally fine-tune or freeze pretrained models to prevent domain feature collapse while adapting to new medical tasks. Investigating methods for inherent domain information preservation during single-domain training, beyond filtering, is another potential direction.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Dermatology</span>
                    
                    <span class="tag">Ophthalmology</span>
                    
                    <span class="tag">Digital health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Out-of-distribution detection</span>
                    
                    <span class="tag tag-keyword">Domain generalization</span>
                    
                    <span class="tag tag-keyword">Information theory</span>
                    
                    <span class="tag tag-keyword">Domain feature collapse</span>
                    
                    <span class="tag tag-keyword">Single-domain learning</span>
                    
                    <span class="tag tag-keyword">Medical imaging</span>
                    
                    <span class="tag tag-keyword">Representation learning</span>
                    
                    <span class="tag tag-keyword">Transfer learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>