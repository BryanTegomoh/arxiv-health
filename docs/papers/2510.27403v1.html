<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FedMuon: Accelerating Federated Learning with Matrix Orthogonalization - Health AI Hub</title>
    <meta name="description" content="This paper introduces FedMuon, a novel optimizer for Federated Learning (FL) that addresses the core communication bottleneck by leveraging matrix orthogonaliza">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>FedMuon: Accelerating Federated Learning with Matrix Orthogonalization</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27403v1" target="_blank">2510.27403v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Junkang Liu, Fanhua Shang, Junchao Zhou, Hongying Liu, Yuanyuan Liu, Jin Liu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27403v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27403v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces FedMuon, a novel optimizer for Federated Learning (FL) that addresses the core communication bottleneck by leveraging matrix orthogonalization. FedMuon overcomes the challenges of client drift and moment reinitialization encountered when applying matrix optimizers in non-IID FL settings, primarily through momentum aggregation and local-global alignment. The approach significantly reduces communication rounds and improves model accuracy across various tasks.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Federated Learning is crucial for developing robust AI models in healthcare while preserving patient data privacy by training across multiple institutions without direct data sharing. Improvements in FL efficiency, like FedMuon, enable faster development and deployment of advanced medical AI tools, accelerating insights from diverse patient populations.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>FedMuon enables more efficient and robust training of AI models (particularly vision and language models) on sensitive, decentralized medical data. This facilitates applications such as improved diagnostics from medical images (e.g., radiology, pathology), better insights from clinical notes and patient records, and more effective clinical decision support systems, all while adhering to strict privacy regulations like HIPAA and GDPR by keeping patient data localized.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The primary bottleneck in Federated Learning (FL) is the number of communication rounds, necessitating more effective local updates.</li>
                    
                    <li>Existing element-wise local optimizers (e.g., SGD, Adam) neglect the geometric structure of weight matrices, leading to pathological directions, poor condition numbers, and slow convergence.</li>
                    
                    <li>The Muon optimizer, which uses matrix orthogonalization, effectively accelerates FL convergence in IID (Independent and Identically Distributed) settings when applied locally.</li>
                    
                    <li>Direct application of Muon to non-IID (Non-Independent and Identically Distributed) FL induces significant client drift due to independent local preconditioners and issues with moment reinitialization.</li>
                    
                    <li>FedMuon addresses non-IID challenges through two key techniques: (1) momentum aggregation, where clients use aggregated global momentum for local initialization, and (2) local-global alignment, which aligns local gradients with the global update direction.</li>
                    
                    <li>Theoretically, FedMuon is proven to achieve a linear speedup convergence rate without requiring heterogeneity assumptions.</li>
                    
                    <li>Empirical validation on language and vision models demonstrates that FedMuon significantly reduces communication rounds and improves test accuracy compared to several baseline FL optimizers.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors propose FedMuon, an optimizer built upon the Muon optimizer's matrix orthogonalization capabilities. It introduces two main components: (1) momentum aggregation, where clients utilize aggregated global momentum for local optimizer initialization, and (2) local-global alignment, which ensures local gradients are aligned with the global update direction to mitigate client drift. The methodology includes theoretical analysis to prove a linear speedup convergence rate and extensive empirical validation on language and vision models, comparing FedMuon's performance against existing FL optimizers.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>FedMuon effectively addresses the communication bottleneck in FL, particularly in non-IID settings, by leveraging matrix orthogonalization. It overcomes client drift, a major challenge in heterogeneous FL environments, through momentum aggregation and local-global alignment. Theoretically, FedMuon guarantees a linear speedup convergence rate. Empirically, it significantly reduces communication rounds and enhances test accuracy on both language and vision models, demonstrating superior performance over baselines.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By enabling faster and more accurate training of federated models, FedMuon can accelerate the development and deployment of AI applications in healthcare. This could lead to more efficient and precise diagnostic tools, optimized treatment protocols, personalized patient care, and a quicker turnaround for developing AI-driven solutions from distributed clinical data, all while adhering to strict privacy regulations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract identifies the challenges FedMuon aims to solve as limitations of previous approaches: independent matrix orthogonalization in non-IID settings leads to strong client drift due to local preconditioners, and issues with moment reinitialization. The paper presents FedMuon as the solution to these, implying FedMuon itself directly addresses these previous limitations effectively.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Imaging Analysis (e.g., radiology, pathology)</span>
                    
                    <span class="tag">Clinical Decision Support Systems</span>
                    
                    <span class="tag">Drug Discovery and Development</span>
                    
                    <span class="tag">Personalized Medicine</span>
                    
                    <span class="tag">Public Health Surveillance</span>
                    
                    <span class="tag">Genomics and Proteomics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Federated Learning</span>
                    
                    <span class="tag tag-keyword">Matrix Orthogonalization</span>
                    
                    <span class="tag tag-keyword">Communication Efficiency</span>
                    
                    <span class="tag tag-keyword">Deep Learning Optimizer</span>
                    
                    <span class="tag tag-keyword">Client Drift</span>
                    
                    <span class="tag tag-keyword">Non-IID Data</span>
                    
                    <span class="tag tag-keyword">Convergence Rate</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Privacy-Preserving AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The core bottleneck of Federated Learning (FL) lies in the communication
rounds. That is, how to achieve more effective local updates is crucial for
reducing communication rounds. Existing FL methods still primarily use
element-wise local optimizers (Adam/SGD), neglecting the geometric structure of
the weight matrices. This often leads to the amplification of pathological
directions in the weights during local updates, leading deterioration in the
condition number and slow convergence. Therefore, we introduce the Muon
optimizer in local, which has matrix orthogonalization to optimize
matrix-structured parameters. Experimental results show that, in IID setting,
Local Muon significantly accelerates the convergence of FL and reduces
communication rounds compared to Local SGD and Local AdamW. However, in non-IID
setting, independent matrix orthogonalization based on the local distributions
of each client induces strong client drift. Applying Muon in non-IID FL poses
significant challenges: (1) client preconditioner leading to client drift; (2)
moment reinitialization. To address these challenges, we propose a novel
Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1)
momentum aggregation, where clients use the aggregated momentum for local
initialization; (2) local-global alignment, where the local gradients are
aligned with the global update direction to significantly reduce client drift.
Theoretically, we prove that \texttt{FedMuon} achieves a linear speedup
convergence rate without the heterogeneity assumption, where $S$ is the number
of participating clients per round, $K$ is the number of local iterations, and
$R$ is the total number of communication rounds. Empirically, we validate the
effectiveness of FedMuon on language and vision models. Compared to several
baselines, FedMuon significantly reduces communication rounds and improves test
accuracy.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>