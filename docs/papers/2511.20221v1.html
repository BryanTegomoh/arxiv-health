<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder - Health AI Hub</title>
    <meta name="description" content="This paper details a deep learning method for patch-level glioblastoma subregion classification, developed for the BraTS-Path 2025 Challenge, utilizing a fine-t">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20221v1" target="_blank">2511.20221v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-25
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Juexin Zhang, Qifeng Zhong, Ying Weng, Ke Chen
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20221v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20221v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper details a deep learning method for patch-level glioblastoma subregion classification, developed for the BraTS-Path 2025 Challenge, utilizing a fine-tuned pre-trained Vision Transformer (ViT) encoder. The model achieved competitive performance, including a 0.6509 MCC and 0.5330 F1-score on the final test set, securing second place and establishing a baseline for ViT-based histopathological analysis of this aggressive brain tumor.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Automated, objective classification of glioblastoma subregions using deep learning can significantly improve diagnostic accuracy and consistency, facilitate more precise patient stratification, and guide personalized treatment strategies for this highly aggressive brain tumor.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves using a contrastive learning-based Vision Transformer (ViT) encoder to automatically classify subregions of glioblastoma from whole slide images. This aims to provide objective and automated analysis to aid in the diagnosis and patient stratification of this aggressive brain tumor, thereby supporting clinical decision-making and potentially improving patient outcomes.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of glioblastoma's significant molecular and pathological heterogeneity, which complicates diagnosis and patient stratification.</li>
                    
                    <li>Developed a deep learning approach for automated, objective patch-level glioblastoma subregion classification using whole slide images.</li>
                    
                    <li>The core methodology involves fine-tuning a pre-trained Vision Transformer (ViT) encoder, which is described as 'Contrastive Learning-Based' in the title, combined with a dedicated classification head.</li>
                    
                    <li>The model was trained on the official BraTS-Path 2025 Challenge dataset and evaluated on its validation and test sets.</li>
                    
                    <li>Achieved a Matthews Correlation Coefficient (MCC) of 0.7064 and an F1-score of 0.7676 on the online validation set.</li>
                    
                    <li>On the final test set, the model secured second place in the BraTS-Pathology 2025 Challenge with an MCC of 0.6509 and an F1-score of 0.5330.</li>
                    
                    <li>Establishes a solid baseline for the application of Vision Transformer architectures in histopathological image analysis, particularly for complex tumor subregion classification.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The core methodology involved fine-tuning a pre-trained Vision Transformer (ViT) encoder, which is explicitly characterized in the title as 'Contrastive Learning-Based'. This encoder was integrated with a dedicated classification head and trained on the official BraTS-Path 2025 Challenge training dataset to perform patch-level glioblastoma subregion classification.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The model demonstrated strong performance in the BraTS-Pathology 2025 Challenge, securing second place. Quantitatively, it achieved an MCC of 0.7064 and an F1-score of 0.7676 on the online validation set, and an MCC of 0.6509 and an F1-score of 0.5330 on the final unseen test set. These results establish ViT-based models as a robust baseline for histopathological analysis of glioblastoma.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research holds the potential to introduce objective and automated tools into clinical pathology workflows for glioblastoma diagnosis and characterization. By standardizing subregion classification, it could lead to more consistent patient stratification, help in guiding personalized treatment plans, and ultimately improve prognostic accuracy and patient outcomes for individuals suffering from glioblastoma.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>A significant performance gap was observed between the online validation set (F1-score of 0.7676) and the final unseen test set (F1-score of 0.5330), indicating potential issues with model generalization or robustness to real-world data variability.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future efforts will specifically focus on addressing and bridging the observed performance gap on unseen validation data, aiming to enhance the model's generalization capabilities and robustness.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neuro-oncology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Computational Pathology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Glioblastoma</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Vision Transformer (ViT)</span>
                    
                    <span class="tag tag-keyword">Histopathology</span>
                    
                    <span class="tag tag-keyword">Whole Slide Imaging</span>
                    
                    <span class="tag tag-keyword">Patch Classification</span>
                    
                    <span class="tag tag-keyword">Medical Image Analysis</span>
                    
                    <span class="tag tag-keyword">Brain Tumor</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The significant molecular and pathological heterogeneity of glioblastoma, an aggressive brain tumor, complicates diagnosis and patient stratification. While traditional histopathological assessment remains the standard, deep learning offers a promising path toward objective and automated analysis of whole slide images. For the BraTS-Path 2025 Challenge, we developed a method that fine-tunes a pre-trained Vision Transformer (ViT) encoder with a dedicated classification head on the official training dataset. Our model's performance on the online validation set, evaluated via the Synapse platform, yielded a Matthews Correlation Coefficient (MCC) of 0.7064 and an F1-score of 0.7676. On the final test set, the model achieved an MCC of 0.6509 and an F1-score of 0.5330, which secured our team second place in the BraTS-Pathology 2025 Challenge. Our results establish a solid baseline for ViT-based histopathological analysis, and future efforts will focus on bridging the performance gap observed on the unseen validation data.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted by the International Brain Tumor Segmentation (BraTS) challenge organized at MICCAI 2025 conference</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>