<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs - Health AI Hub</title>
    <meta name="description" content="This paper identifies a new class of LLM hallucinations driven by spurious correlations in training data, where superficial associations (e.g., surnames and nat">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.07318v1" target="_blank">2511.07318v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Shaowen Wang, Yiqi Dong, Ruinian Chang, Tansheng Zhu, Yuebo Sun, Kaifeng Lyu, Jian Li
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.07318v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.07318v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper identifies a new class of LLM hallucinations driven by spurious correlations in training data, where superficial associations (e.g., surnames and nationality) lead to plausible but incorrect outputs. These hallucinations are confidently generated, resistant to model scaling and fine-tuning, and critically, evade existing detection methods like confidence-based filtering and inner-state probing. The research highlights an urgent need for novel detection approaches specifically designed for these robust, undetected biases.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Large Language Models are increasingly integrated into healthcare for tasks like clinical decision support, diagnostic assistance, and patient information. If these models confidently generate medically incorrect advice or information due to spurious correlations (e.g., falsely linking a demographic trait to a specific disease, or a common symptom to a rare condition), it could lead to severe patient harm, misdiagnosis, or inappropriate treatment.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is applicable to any AI application that utilizes LLMs for generating text, providing information, making recommendations, or assisting with analysis in medicine, healthcare, or biosecurity. Examples include AI-powered diagnostic aids, AI chatbots for patient queries or physician support, systems for summarizing medical literature, AI tools for identifying potential biothreats, and AI assistants for clinical workflow. The paper's findings highlight a critical risk to the reliability and safety of such applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Identifies spurious correlations as a critical, previously underexplored cause of LLM hallucinations.</li>
                    
                    <li>Demonstrates that hallucinations caused by spurious correlations are confidently generated, immune to model scaling, and resist refusal fine-tuning.</li>
                    
                    <li>Shows that state-of-the-art hallucination detection methods (confidence-based filtering, inner-state probing) fundamentally fail against these spurious-correlation-driven errors.</li>
                    
                    <li>Evaluations were conducted using systematically controlled synthetic experiments and empirical tests on leading LLMs, including GPT-5.</li>
                    
                    <li>Provides theoretical analysis explaining why statistical biases from spurious correlations intrinsically undermine confidence-based detection techniques.</li>
                    
                    <li>Emphasizes the urgent need for new hallucination detection strategies specifically targeting errors from spurious correlations.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors employed systematically controlled synthetic experiments to isolate and study the effect of spurious correlations. They conducted empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5) to assess the prevalence and characteristics of these hallucinations. Existing hallucination detection methods, such as confidence-based filtering and inner-state probing, were tested for their efficacy, complemented by theoretical analysis to explain their fundamental failure in the presence of such biases.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Spurious correlations in training data induce a robust class of LLM hallucinations that are confidently generated, persist despite model scaling and fine-tuning, and fundamentally evade current hallucination detection techniques. Theoretical analysis confirms that these statistical biases inherently undermine confidence-based detection mechanisms.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The inability of current detection methods to identify these spurious-correlation-driven hallucinations means that LLMs deployed in clinical settings could confidently generate medically incorrect information that goes unnoticed. This poses a significant risk for misdiagnosis, suboptimal treatment plans, the dissemination of misinformation, and could severely erode trust in AI-driven healthcare solutions, potentially endangering patient safety. New, robust detection and mitigation mechanisms are critical before widespread clinical adoption of LLMs.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The paper highlights the inherent limitations of *current* hallucination detection methods against spurious correlations, rather than limitations of the study itself. The challenge lies in the fundamental nature of these biases and the confidence with which LLMs produce such incorrect outputs, making them difficult to detect post-generation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings emphasize the urgent need for developing entirely new approaches and detection mechanisms explicitly designed to identify and address hallucinations caused by spurious correlations, particularly crucial for safety-critical applications like medicine.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Diagnostic Assistance</span>
                    
                    <span class="tag">Medical Information Retrieval</span>
                    
                    <span class="tag">Patient Education</span>
                    
                    <span class="tag">Medical Research</span>
                    
                    <span class="tag">Public Health Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">hallucinations</span>
                    
                    <span class="tag tag-keyword">spurious correlations</span>
                    
                    <span class="tag tag-keyword">hallucination detection</span>
                    
                    <span class="tag tag-keyword">confidence-based filtering</span>
                    
                    <span class="tag tag-keyword">inner-state probing</span>
                    
                    <span class="tag tag-keyword">model bias</span>
                    
                    <span class="tag tag-keyword">AI safety</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Despite substantial advances, large language models (LLMs) continue to
exhibit hallucinations, generating plausible yet incorrect responses. In this
paper, we highlight a critical yet previously underexplored class of
hallucinations driven by spurious correlations -- superficial but statistically
prominent associations between features (e.g., surnames) and attributes (e.g.,
nationality) present in the training data. We demonstrate that these spurious
correlations induce hallucinations that are confidently generated, immune to
model scaling, evade current detection methods, and persist even after refusal
fine-tuning. Through systematically controlled synthetic experiments and
empirical evaluations on state-of-the-art open-source and proprietary LLMs
(including GPT-5), we show that existing hallucination detection methods, such
as confidence-based filtering and inner-state probing, fundamentally fail in
the presence of spurious correlations. Our theoretical analysis further
elucidates why these statistical biases intrinsically undermine
confidence-based detection techniques. Our findings thus emphasize the urgent
need for new approaches explicitly designed to address hallucinations caused by
spurious correlations.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>