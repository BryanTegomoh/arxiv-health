<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel framework that employs Large Language Models (LLMs) to detect and sanitize data poisoning attacks in Human Activity Recognition (H">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.02894v1" target="_blank">2511.02894v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> W. K. M Mithsara, Ning Yang, Ahmed Imteaj, Hussein Zangoti, Abdur R. Shahid
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.CR
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.02894v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.02894v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel framework that employs Large Language Models (LLMs) to detect and sanitize data poisoning attacks in Human Activity Recognition (HAR) systems within wearable Internet of Things (IoT) ecosystems. By leveraging zero-shot, one-shot, and few-shot learning with specialized prompting strategies, the approach overcomes the limitations of conventional methods that require extensive labeled datasets. The framework provides a robust and adaptable defense mechanism, demonstrated through comprehensive evaluation of detection accuracy, sanitization quality, latency, and communication cost.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine and healthcare as it ensures the integrity and trustworthiness of critical data collected from wearable devices for patient monitoring, diagnostics, and smart health applications. Protecting against data poisoning is vital for maintaining the reliability of HAR systems that inform clinical decisions and support personalized care.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Large Language Models (LLMs) are applied to develop a robust framework for detecting and sanitizing data poisoning attacks in Human Activity Recognition (HAR) systems used in wearable IoT devices. These devices are critical for various healthcare applications, making the LLM-based solution directly relevant to enhancing the security, reliability, and trustworthiness of AI-driven health monitoring and management systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical issue of data poisoning attacks compromising data integrity and reliability in HAR systems powered by wearable IoT devices, especially in healthcare.</li>
                    
                    <li>Proposes a novel defense framework utilizing Large Language Models (LLMs) for both detecting poisoned data and sanitizing it.</li>
                    
                    <li>Minimizes reliance on large, task-specific labeled datasets by implementing zero-shot, one-shot, and few-shot learning paradigms.</li>
                    
                    <li>Introduces two innovative prompting strategies: 'role play' (LLM acts as an expert to contextualize sensor anomalies) and 'think step-by-step' reasoning (guides LLM to infer poisoning indicators and plausible clean alternatives).</li>
                    
                    <li>Aims to provide robust, adaptable, and real-time defense mechanisms suitable for dynamic IoT environments.</li>
                    
                    <li>Undergoes extensive evaluation measuring key performance indicators including detection accuracy, sanitization quality, latency, and communication cost.</li>
                    
                    <li>Demonstrates the practicality and effectiveness of LLMs in significantly enhancing the security and reliability of wearable IoT systems.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed methodology centers on employing Large Language Models (LLMs) for poisoning detection and sanitization in HAR systems. It utilizes zero-shot, one-shot, and few-shot learning to reduce dependence on extensive labeled datasets. Core to the approach are 'role play' prompting, where the LLM assumes an expert persona to interpret sensor anomalies, and 'think step-by-step' reasoning, which guides the LLM to deduce poisoning indicators in raw sensor data and suggest clean alternatives. The framework's performance was rigorously evaluated across detection accuracy, sanitization quality, latency, and communication cost.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The extensive evaluation demonstrated that the LLM-based framework is practical and effective in improving the security and reliability of wearable IoT systems. It achieved robust performance in both detecting data poisoning attacks and sanitizing compromised data, exhibiting satisfactory detection accuracy and sanitization quality while maintaining acceptable latency and communication costs.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By ensuring the integrity of HAR data from wearable devices, this framework can significantly improve the reliability of health monitoring and medical decision-making. Clinicians can confidently use data for accurate diagnoses, personalized treatment plans, and continuous patient assessment in remote monitoring, smart home care, and rehabilitation settings, fostering greater trust in digital health technologies and potentially leading to better patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Limitations of the proposed framework itself are not explicitly mentioned in the abstract. The abstract primarily highlights limitations of conventional approaches (e.g., reliance on extensive labeled datasets, limited adaptability).</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions are not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Remote Patient Monitoring</span>
                    
                    <span class="tag">Elderly Care</span>
                    
                    <span class="tag">Rehabilitation</span>
                    
                    <span class="tag">Preventative Health</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Digital Therapeutics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Data poisoning detection</span>
                    
                    <span class="tag tag-keyword">Data sanitization</span>
                    
                    <span class="tag tag-keyword">Wearable IoT</span>
                    
                    <span class="tag tag-keyword">Human Activity Recognition (HAR)</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Zero-shot learning</span>
                    
                    <span class="tag tag-keyword">Prompt engineering</span>
                    
                    <span class="tag tag-keyword">Cybersecurity in healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>