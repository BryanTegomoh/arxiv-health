<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel Vision-Language Pretraining (VLP) framework to overcome challenges of noisy web data and complex unstructured medical texts in med">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.03445v1" target="_blank">2512.03445v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.03445v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.03445v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel Vision-Language Pretraining (VLP) framework to overcome challenges of noisy web data and complex unstructured medical texts in medical image analysis. It integrates a Multi-Agent data GENeration (MAGEN) system for enhancing data quality and an Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining module for structured learning from long texts. Validated in dermatology, the approach achieves state-of-the-art zero-shot performance in disease classification and cross-modal retrieval.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research significantly advances medical image analysis by improving the quality and interpretability of vision-language models, particularly for dermatology. It enables robust representation learning from vast medical data without costly manual annotations, crucial for developing accurate diagnostic and information retrieval tools.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research develops an AI framework (Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation) designed to improve medical image analysis. Specifically, it aims to enhance the accuracy and robustness of AI models for tasks like disease classification and cross-modal retrieval in healthcare settings, particularly demonstrated in dermatology. By improving data quality and the understanding of complex medical texts, the AI application contributes to more effective diagnostic tools and information retrieval for medical professionals.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses critical challenges in medical VLP: noise from web-collected data and the complexity of unstructured, long medical texts.</li>
                    
                    <li>Proposes MAGEN (Multi-Agent data GENeration system) to improve data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline.</li>
                    
                    <li>Introduces O-MAKE (Ontology-based Multi-Aspect Knowledge-Enhanced pretraining) to handle long medical texts by decomposing them into distinct knowledge aspects.</li>
                    
                    <li>O-MAKE facilitates fine-grained alignment between vision and text at both global and patch levels and explicitly models medical concept relationships through ontology-guided mechanisms.</li>
                    
                    <li>The framework's effectiveness is comprehensively demonstrated in the field of dermatology, confirming the utility of both MAGEN and O-MAKE components.</li>
                    
                    <li>Achieves state-of-the-art (SOTA) zero-shot performance on disease classification and cross-modal retrieval tasks across eight diverse dermatological datasets.</li>
                    
                    <li>The authors will release their code and an augmented dataset, Derm1M-AgentAug, comprising over 400,000 skin-image-text pairs.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed VLP framework consists of two main components: 1) A Multi-Agent data GENeration (MAGEN) system that uses a foundation model for captioning and a retrieval-based verification process to synthesize high-quality, knowledge-enriched image descriptions. 2) An Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining module which decomposes long medical texts into distinct knowledge aspects. This module facilitates fine-grained vision-text alignment at global and patch levels and explicitly integrates medical concept relationships using an ontology. The framework was validated using zero-shot performance metrics on disease classification and cross-modal retrieval.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The framework achieves state-of-the-art zero-shot performance in disease classification and cross-modal retrieval across eight dermatological datasets. Comprehensive experiments demonstrate the effectiveness of both the MAGEN data generation system for enhancing data quality and the O-MAKE pretraining module for robust learning from complex medical texts, leading to superior performance in medical VLP.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The improved zero-shot performance in disease classification and cross-modal retrieval has significant clinical impact by enabling more accurate and automated preliminary diagnoses, efficient retrieval of similar cases or relevant medical literature from image queries, and enhanced clinical decision support in dermatology, even for conditions with limited labeled data.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of the proposed framework.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions for this work.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Dermatology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Vision-Language Pretraining</span>
                    
                    <span class="tag tag-keyword">Medical Image Analysis</span>
                    
                    <span class="tag tag-keyword">Dermatology</span>
                    
                    <span class="tag tag-keyword">Data Augmentation</span>
                    
                    <span class="tag tag-keyword">Knowledge Enhancement</span>
                    
                    <span class="tag tag-keyword">Zero-shot Learning</span>
                    
                    <span class="tag tag-keyword">Ontology</span>
                    
                    <span class="tag tag-keyword">Foundation Models</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>10 pages. Under Review</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>