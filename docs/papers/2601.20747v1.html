<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts - Health AI Hub</title>
    <meta name="description" content="This paper analyzes 5,126 Reddit posts detailing users' experiences with AI for emotional support and mental health, revealing that engagement is primarily driv">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.20747v1" target="_blank">2601.20747v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-28
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Elham Aghakhani, Rezvaneh Rezapour
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.HC
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.20747v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.20747v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper analyzes 5,126 Reddit posts detailing users' experiences with AI for emotional support and mental health, revealing that engagement is primarily driven by narrated outcomes, trust, and response quality, not solely emotional bonds. It found positive sentiment correlates with task and goal alignment, while companionship-oriented use often leads to misaligned alliances and risks like dependence and symptom escalation.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for understanding the real-world impact and user perceptions of AI in informal mental health support, directly informing the development of ethical, effective, and safer digital mental health interventions and guiding clinicians on potential patient interactions with such technologies.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application being analyzed is the use of Large Language Models (LLMs) for emotional support, informal mental health assistance, and therapy-like interactions. The paper focuses on understanding user evaluation, adoption, and the potential risks/benefits of these AI tools in sensitive mental health contexts.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the gap in understanding how people evaluate and relate to LLMs used for emotional support and mental health outside clinical settings.</li>
                    
                    <li>Utilizes a large dataset of 5,126 Reddit posts from 47 mental health communities, focusing on experiential or exploratory AI use.</li>
                    
                    <li>Employs a theory-informed annotation framework grounded in the Technology Acceptance Model and therapeutic alliance theory, analyzed via a hybrid LLM-human pipeline.</li>
                    
                    <li>Identifies that user engagement is primarily shaped by narrated outcomes, trust in the AI, and the perceived quality of AI responses, rather than solely by emotional bonding.</li>
                    
                    <li>Reports that positive user sentiment towards AI is most strongly associated with the AI's ability to achieve task and goal alignment.</li>
                    
                    <li>Highlights that companionship-oriented use of AI for mental health frequently leads to misaligned alliances and is associated with reported risks such as user dependence and potential symptom escalation.</li>
                    
                    <li>Demonstrates the operationalization of theory-grounded constructs in large-scale discourse analysis for sensitive, real-world language technology contexts.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved analyzing 5,126 Reddit posts from 47 mental health communities, focusing on user narratives regarding the experiential or exploratory use of AI for emotional support or therapy. A theory-informed annotation framework, developed from the Technology Acceptance Model and therapeutic alliance theory, was applied using a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>User engagement with AI for mental health is predominantly driven by narrated positive outcomes, user trust in the AI, and the quality of its responses, rather than the formation of an emotional bond alone. Positive sentiment is strongly linked to task and goal alignment. Conversely, AI use aimed at companionship frequently results in misaligned therapeutic alliances and is associated with reported risks including user dependence and the escalation of mental health symptoms.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research provides critical insights for clinicians and AI developers in mental health. It suggests that AI tools should prioritize functional utility and accurate task/goal alignment over simulated emotional connection to maximize positive outcomes and minimize risks. Clinicians should be aware of the potential for dependence and symptom escalation in patients using AI for companionship and consider these factors when advising on or integrating digital mental health tools. It underscores the need for clear guidelines and careful monitoring of AI use in sensitive mental health contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly list limitations, but potential caveats include: reliance on self-reported, potentially biased data from a specific platform (Reddit); the observational nature prevents establishing causality; and the interpretation nuances inherent in a hybrid LLM-human annotation pipeline for sensitive qualitative data.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The study highlights the critical importance of further research into how users interpret and interact with language technologies in sensitive, real-world contexts. Future work should focus on understanding causal mechanisms behind observed outcomes (e.g., dependence, symptom escalation), developing and evaluating interventions for safer AI use, and exploring diverse user populations and platforms.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Psychiatry</span>
                    
                    <span class="tag">Clinical Psychology</span>
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Mental Health Informatics</span>
                    
                    <span class="tag">Public Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">AI in mental health</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">emotional support</span>
                    
                    <span class="tag tag-keyword">Reddit analysis</span>
                    
                    <span class="tag tag-keyword">technology acceptance</span>
                    
                    <span class="tag tag-keyword">therapeutic alliance</span>
                    
                    <span class="tag tag-keyword">digital mental health</span>
                    
                    <span class="tag tag-keyword">user perception</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>