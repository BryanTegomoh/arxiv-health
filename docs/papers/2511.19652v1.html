<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Navigating Gigapixel Pathology Images with Large Multimodal Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces GIANT (Gigapixel Image Agent for Navigating Tissue), a novel framework that enables Large Multimodal Models (LMMs) to iteratively navigate">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Navigating Gigapixel Pathology Images with Large Multimodal Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.19652v1" target="_blank">2511.19652v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-24
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Thomas A. Buckley, Kian R. Weihrauch, Katherine Latham, Andrew Z. Zhou, Padmini A. Manrai, Arjun K. Manrai
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.19652v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.19652v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces GIANT (Gigapixel Image Agent for Navigating Tissue), a novel framework that enables Large Multimodal Models (LMMs) to iteratively navigate and interpret gigapixel whole-slide pathology images like a human pathologist. It demonstrates that this agentic approach, coupled with the new MultiPathQA benchmark, significantly outperforms conventional LMM baselines and often surpasses specialized pathology models, revealing a viable path for LMMs in expert medical reasoning.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate interpretation of gigapixel pathology images is critical for cancer diagnosis and other disease assessments. This research offers a groundbreaking method for integrating advanced AI, specifically LMMs, into this complex process, potentially enhancing diagnostic accuracy and efficiency in clinical pathology.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper describes the development of an AI agent (GIANT) utilizing large multimodal models (LMMs) to iteratively navigate and interpret gigapixel pathology whole-slide images (WSIs). This AI application aims to assist or automate tasks traditionally performed by pathologists, such as cancer diagnosis and other clinically relevant evaluations, thereby improving diagnostic accuracy and efficiency in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Prior LMM studies in pathology were limited by using low-resolution thumbnails or random patches, likely underestimating model performance on gigapixel Whole-Slide Images (WSIs).</li>
                    
                    <li>The study introduces GIANT, the first framework allowing LMMs to iteratively navigate WSIs, mimicking a pathologist's workflow.</li>
                    
                    <li>A new benchmark, MultiPathQA, is released, comprising 934 WSI-level questions across five clinically relevant tasks, including 128 pathologist-authored questions requiring direct slide interpretation.</li>
                    
                    <li>GIANT-enabled LMMs (e.g., GPT-5) substantially outperform conventional patch- and thumbnail-based baselines in gigapixel image interpretation.</li>
                    
                    <li>The system approaches or surpasses the performance of specialized pathology models (e.g., TITAN, SlideChat) that are trained on millions of images.</li>
                    
                    <li>Specifically, GPT-5 with GIANT achieved 62.5% accuracy on pathologist-authored questions, significantly outperforming TITAN (43.8%) and SlideChat (37.5%).</li>
                    
                    <li>The findings highlight the potential and current limitations of foundation models for expert reasoning in complex medical domains like pathology.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study developed GIANT, an agentic framework enabling LMMs to iteratively navigate WSIs. It also created MultiPathQA, a benchmark dataset of 934 WSI-level questions spanning five clinical tasks, including questions authored by professional pathologists. Performance was evaluated by integrating LMMs (e.g., GPT-5) with GIANT and comparing them against conventional LMM baselines (patch- and thumbnail-based) and specialized pathology models like TITAN and SlideChat.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>GIANT-equipped LMMs demonstrate significantly superior performance in interpreting gigapixel pathology images compared to conventional methods. They achieve accuracy comparable to, or exceeding, highly specialized pathology AI models, particularly on complex, pathologist-authored diagnostic questions. The agentic navigation allows LMMs to reason more coherently and accurately on WSIs.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research provides a foundational step towards deploying general-purpose LMMs as highly effective diagnostic aids in clinical pathology. It has the potential to improve the speed and accuracy of pathology diagnoses, assist pathologists in complex cases, and potentially facilitate access to expert-level interpretation, ultimately benefiting patient care, especially in cancer diagnosis and treatment planning.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract mentions that the findings "reveal the strengths and limitations of current foundation models," implying that specific limitations were identified, though these are not detailed within the abstract itself. Future work would need to fully characterize these identified limitations.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The study aims to ground future development of LMMs for expert reasoning in pathology. This suggests ongoing research to refine the GIANT framework, improve LMM capabilities for medical tasks, and further characterize and overcome the identified limitations of current foundation models in this domain.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Medicine</span>
                    
                    <span class="tag">Computational Pathology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Gigapixel pathology</span>
                    
                    <span class="tag tag-keyword">Large Multimodal Models (LMMs)</span>
                    
                    <span class="tag tag-keyword">Whole-Slide Images (WSIs)</span>
                    
                    <span class="tag tag-keyword">Agentic systems</span>
                    
                    <span class="tag tag-keyword">Digital pathology</span>
                    
                    <span class="tag tag-keyword">Cancer diagnosis</span>
                    
                    <span class="tag tag-keyword">MultiPathQA</span>
                    
                    <span class="tag tag-keyword">GIANT framework</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Despite being widely used to support clinical care, general-purpose large multimodal models (LMMs) have generally shown poor or inconclusive performance in medical image interpretation, particularly in pathology, where gigapixel images are used. However, prior studies have used either low-resolution thumbnails or random patches, which likely underestimated model performance. Here, we ask whether LMMs can be adapted to reason coherently and accurately in the evaluation of such images. In this study, we introduce Gigapixel Image Agent for Navigating Tissue (GIANT), the first framework that allows LMMs to iteratively navigate whole-slide images (WSIs) like a pathologist. Accompanying GIANT, we release MultiPathQA, a new benchmark, which comprises 934 WSI-level questions, encompassing five clinically-relevant tasks ranging from cancer diagnosis to open-ended reasoning. MultiPathQA also includes 128 questions, authored by two professional pathologists, requiring direct slide interpretation. Using MultiPathQA, we show that our simple agentic system substantially outperforms conventional patch- and thumbnail-based baselines, approaching or surpassing the performance of specialized models trained on millions of images. For example, on pathologist-authored questions, GPT-5 with GIANT achieves 62.5% accuracy, outperforming specialist pathology models such as TITAN (43.8%) and SlideChat (37.5%). Our findings reveal the strengths and limitations of current foundation models and ground future development of LMMs for expert reasoning in pathology.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>