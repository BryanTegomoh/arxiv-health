<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects - Health AI Hub</title>
    <meta name="description" content="This study evaluated Vision Transformers (Swin v1/v2) against ResNet for diagnosing ear diseases from otoscopic videos, initially reporting exceptionally high a">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04872v1" target="_blank">2511.04872v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-06
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> James Ndubuisi, Fernando Auat, Marta Vallejo
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04872v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04872v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study evaluated Vision Transformers (Swin v1/v2) against ResNet for diagnosing ear diseases from otoscopic videos, initially reporting exceptionally high accuracies (up to 100%). However, a critical data leakage issue was subsequently identified in the preprocessing pipeline, leading to a significant drop in corrected model performance (82-83% accuracy) across all models, underscoring the paramount importance of rigorous data handling in medical AI development.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Addressing a reported 27% misdiagnosis rate for ear diseases, this research highlights the potential of AI models to significantly enhance diagnostic accuracy in otolaryngology. Critically, it also serves as a vital cautionary tale, demonstrating that flawed data handling can lead to highly misleading performance metrics, underscoring the absolute necessity of robust validation for reliable clinical AI tools.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves using Vision Transformers (Swin models) to analyze otoscopic videos for the automated or AI-assisted diagnosis of middle and external ear conditions, with the goal of improving diagnostic accuracy and reducing misdiagnosis rates in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The research aimed to improve the diagnostic accuracy of ear diseases using AI models, addressing a reported 27% misdiagnosis rate among specialist otolaryngologists.</li>
                    
                    <li>Vision Transformers (Swin v1 and Swin v2) were evaluated against a traditional Convolutional Neural Network (ResNet) for classifying various middle and external ear conditions from otoscopic video frames.</li>
                    
                    <li>A real-world dataset from the Clinical Hospital of the Universidad de Chile was used, with frames selected based on Laplacian and Shannon entropy thresholds, and blank frames removed.</li>
                    
                    <li>Initial evaluations showed outstanding performance: Swin v1 achieved 100% accuracy, Swin v2 99.1%, and ResNet 99.5%, surpassing metrics reported in related studies.</li>
                    
                    <li>A critical data leakage issue was uncovered in the preprocessing step, which affected the integrity of the results in this study and potentially others using the same raw dataset.</li>
                    
                    <li>After mitigating the data leakage, model performance significantly decreased, with corrected accuracies of 83% for both Swin v1 and Swin v2, and 82% for the ResNet model.</li>
                    
                    <li>The findings emphasize the vital importance of rigorous data handling in machine learning studies, especially in medical applications, and the need for an optimal balance between advanced model architectures and effective data preprocessing.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study evaluated Swin v1 and Swin v2 Vision Transformers and a ResNet model for classifying ear conditions from otoscopic videos. The dataset comprised real-world otoscopic videos, from which frames were selected using Laplacian and Shannon entropy thresholds, with blank frames removed. A crucial methodological step was the re-evaluation of model performance after identifying and correcting a significant data leakage issue within the initial data preprocessing pipeline.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Initial evaluations showed near-perfect accuracy for Swin v1 (100%), Swin v2 (99.1%), and ResNet (99.5%) in diagnosing ear diseases, outperforming previous benchmarks. However, the discovery and mitigation of a critical data leakage issue during data preprocessing led to a substantial drop in performance, with corrected accuracies settling at 83% for both Swin v1 and Swin v2, and 82% for ResNet.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research provides a dual clinical impact: it illustrates the promise of Vision Transformers in augmenting diagnostic capabilities for common ear diseases, potentially reducing the high misdiagnosis rates currently observed. Concurrently, it delivers a critical lesson on the paramount importance of meticulous data validation and preprocessing in medical AI development, preventing the deployment of models that might perform flawlessly in testing but fail dangerously in real-world clinical scenarios due to data integrity issues.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The primary limitation identified by the study itself was the presence of a critical data leakage issue in the initial preprocessing step. This leakage artificially inflated the reported performance metrics for all evaluated models, rendering the initial impressive results unreliable and potentially affecting related studies that used the same raw dataset.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should prioritize finding an optimal balance between leveraging the benefits of advanced model architectures, such as Vision Transformers, and ensuring the absolute integrity of data through effective and rigorous preprocessing. The goal is to develop highly reliable machine learning models for diagnosing ear diseases that generalize robustly to real-world clinical data, specifically addressing the challenges highlighted by the data leakage discovery.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Otolaryngology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Artificial Intelligence in Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Vision Transformers</span>
                    
                    <span class="tag tag-keyword">Swin Transformers</span>
                    
                    <span class="tag tag-keyword">Otoscopy</span>
                    
                    <span class="tag tag-keyword">Ear Diseases</span>
                    
                    <span class="tag tag-keyword">Diagnostic Accuracy</span>
                    
                    <span class="tag tag-keyword">Data Leakage</span>
                    
                    <span class="tag tag-keyword">Machine Learning</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">This study evaluates the efficacy of vision transformer models, specifically
Swin transformers, in enhancing the diagnostic accuracy of ear diseases
compared to traditional convolutional neural networks. With a reported 27%
misdiagnosis rate among specialist otolaryngologists, improving diagnostic
accuracy is crucial. The research utilised a real-world dataset from the
Department of Otolaryngology at the Clinical Hospital of the Universidad de
Chile, comprising otoscopic videos of ear examinations depicting various middle
and external ear conditions. Frames were selected based on the Laplacian and
Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and
Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively,
marginally outperforming the ResNet model (99.5%). These results surpassed
metrics reported in related studies. However, the evaluation uncovered a
critical data leakage issue in the preprocessing step, affecting both this
study and related research using the same raw dataset. After mitigating the
data leakage, model performance decreased significantly. Corrected accuracies
were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This
finding highlights the importance of rigorous data handling in machine learning
studies, especially in medical applications. The findings indicate that while
vision transformers show promise, it is essential to find an optimal balance
between the benefits of advanced model architectures and those derived from
effective data preprocessing. This balance is key to developing a reliable
machine learning model for diagnosing ear diseases.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>