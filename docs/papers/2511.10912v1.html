<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D - Health AI Hub</title>
    <meta name="description" content="This paper addresses the underexplored performance of Large Language Models (LLMs) in rare disease diagnosis from narrative medical cases. It introduces a novel">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.10912v1" target="_blank">2511.10912v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-14
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Arsh Gupta, Ajay Narayanan Sridhar, Bonam Mingole, Amulya Yadav
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.10912v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.10912v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper addresses the underexplored performance of Large Language Models (LLMs) in rare disease diagnosis from narrative medical cases. It introduces a novel, educationally validated dataset of 176 symptom-diagnosis pairs derived from House M.D. and evaluates four state-of-the-art LLMs, revealing low initial accuracy (16.48% to 38.64%) but a significant 2.3 times improvement in newer model generations, establishing a crucial baseline for future AI-assisted diagnosis research.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and timely diagnosis of rare diseases is often challenging for clinicians due to their low prevalence and varied presentations. Evaluating LLMs in this context is crucial for developing AI tools that can potentially assist healthcare professionals in identifying these conditions, thereby improving patient outcomes and reducing diagnostic delays.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The primary AI application described is AI-assisted diagnosis, specifically for rare diseases. It explores the use of LLMs as tools to aid clinicians or medical students in diagnostic reasoning, potentially forming the basis for advanced clinical decision support systems or medical education platforms.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical gap in evaluating LLM performance for rare disease diagnosis using narrative medical cases.</li>
                    
                    <li>Introduces a novel dataset comprising 176 symptom-diagnosis pairs extracted from the medically validated television series, House M.D.</li>
                    
                    <li>Evaluates four prominent state-of-the-art LLMs: GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro.</li>
                    
                    <li>Performance on narrative-based diagnostic reasoning tasks ranged from a low of 16.48% to 38.64% accuracy.</li>
                    
                    <li>Demonstrates a significant generational improvement, with newer LLM architectures showing a 2.3 times increase in diagnostic accuracy.</li>
                    
                    <li>Concludes that while current LLMs face substantial challenges, the observed progress indicates promising future development pathways.</li>
                    
                    <li>Establishes an educationally validated benchmark and provides a publicly accessible evaluation framework to advance AI-assisted diagnosis research.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved creating a novel dataset of 176 symptom-diagnosis pairs by extracting cases from the House M.D. television series, which is recognized for its relevance to rare disease recognition in medical education. Four state-of-the-art LLMs (GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro) were then evaluated on narrative-based diagnostic reasoning tasks using this dataset to determine their accuracy in identifying rare diseases.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The LLMs demonstrated significant variation in diagnostic accuracy for rare diseases, ranging from 16.48% to 38.64%. Despite these low overall scores, newer model generations exhibited a substantial 2.3-fold improvement in performance compared to their predecessors. All evaluated models, however, still face considerable challenges in accurately diagnosing rare conditions from narrative medical inputs.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research provides a foundational benchmark for the current capabilities and limitations of AI in a critically difficult area of clinical medicine. While current LLM performance is not yet suitable for direct clinical application in rare disease diagnosis, the observed improvements suggest a potential future role for AI as a diagnostic aid, particularly in supporting clinicians in complex, undifferentiated cases, guiding further development toward practical clinical tools.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The primary limitation noted is the universally low accuracy of all evaluated LLMs (16.48% to 38.64%), indicating they currently face substantial challenges with rare disease diagnosis. The dataset size of 176 cases, while novel, might also be considered relatively small for comprehensive LLM training or evaluation, though not explicitly stated as a limitation in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The observed improvement across LLM architectures suggests promising directions for future development in AI-assisted diagnosis. The established educationally validated benchmark and publicly accessible evaluation framework will serve as a valuable tool for advancing research in this field, guiding efforts to develop more accurate and reliable AI diagnostic aids.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Rare Diseases</span>
                    
                    <span class="tag">Clinical Diagnostics</span>
                    
                    <span class="tag">Internal Medicine</span>
                    
                    <span class="tag">Medical Education</span>
                    
                    <span class="tag">Computational Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Rare Disease Diagnosis</span>
                    
                    <span class="tag tag-keyword">Narrative Medicine</span>
                    
                    <span class="tag tag-keyword">AI-assisted Diagnosis</span>
                    
                    <span class="tag tag-keyword">Diagnostic Reasoning</span>
                    
                    <span class="tag tag-keyword">Medical Education</span>
                    
                    <span class="tag tag-keyword">Benchmarking</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>