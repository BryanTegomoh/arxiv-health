<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>On the use of graph models to achieve individual and group fairness - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel theoretical framework utilizing Sheaf Diffusion, dynamical systems, and homology to model and achieve fairness in Machine Learning">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>On the use of graph models to achieve individual and group fairness</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.08784v1" target="_blank">2601.08784v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-13
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Arturo P√©rez-Peralta, Sandra Ben√≠tez-Pe√±a, Rosa E. Lillo
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> stat.ML, cs.CY, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.08784v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.08784v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel theoretical framework utilizing Sheaf Diffusion, dynamical systems, and homology to model and achieve fairness in Machine Learning algorithms. It projects input data into a bias-free space, offering a unified method to address both individual and group bias through different network topologies, while also providing interpretable SHAP values. The proposed models demonstrate satisfactory performance on standard fairness benchmarks, analyzing accuracy-fairness trade-offs and hyper-parameter effects.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Machine Learning algorithms are increasingly used in critical healthcare decisions, such as diagnosis, treatment recommendations, and resource allocation. Ensuring fairness in these algorithms is paramount to prevent disparate impacts on different patient populations, reduce health inequities, and build trust in AI-driven healthcare systems.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research provides a foundational methodology for developing fair, unbiased, and interpretable machine learning models for various health and medical AI applications. It can be applied to create AI systems that minimize bias in predicting patient outcomes, identifying disease risks, recommending personalized treatments, allocating healthcare resources equitably, and informing public health interventions. By addressing both individual and group fairness, it directly contributes to reducing health disparities and ensuring ethical and responsible AI deployment in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the poorly understood theoretical properties of ML fairness and the relationship between group and individual fairness.</li>
                    
                    <li>Proposes a theoretical framework based on Sheaf Diffusion, leveraging tools from dynamical systems and homology to model fairness.</li>
                    
                    <li>The method projects input data into a 'bias-free space' that explicitly encodes fairness constraints, leading to fair solutions.</li>
                    
                    <li>Introduces a collection of network topologies designed to handle various fairness metrics, providing a unified approach for individual and group bias.</li>
                    
                    <li>Ensures interpretability of models through closed-form expressions for their SHAP (SHapley Additive exPlanations) values, contributing to responsible AI.</li>
                    
                    <li>Validated through a simulation study and standard fairness benchmarks, demonstrating satisfactory performance in terms of accuracy and fairness.</li>
                    
                    <li>Analyzes available trade-offs on the Pareto frontier between accuracy and fairness, studies hyper-parameter effects, and provides insights into output interpretation.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The core methodology involves a theoretical framework built on Sheaf Diffusion, which uses dynamical systems and homology to project input data into a 'bias-free space' where fairness constraints are encoded. This is achieved by employing various graph network topologies, each tailored to different fairness metrics, thereby unifying the handling of individual and group bias. The models also incorporate a mechanism for deriving closed-form expressions for SHAP values to ensure interpretability.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The proposed graph-based Sheaf Diffusion models achieve satisfactory results on simulation studies and standard fairness benchmarks, effectively reducing bias while maintaining acceptable accuracy. They successfully unify the treatment of individual and group fairness. The models provide inherent interpretability via SHAP values and allow for detailed analysis of accuracy-fairness trade-offs and the impact of hyper-parameter adjustments.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research can lead to the development of more ethically sound and equitable AI systems in healthcare, minimizing biased outcomes in patient care, diagnostics, and treatment plans. By providing interpretable models and explicitly addressing fairness, it can enhance clinician trust, improve patient acceptance of AI-driven recommendations, and help ensure that healthcare advancements benefit all demographic groups fairly.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of the proposed method or its current applicability, focusing instead on its theoretical advancements and demonstrated performance.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract mentions 'delving into the interpretation of its outputs' as part of the current study, suggesting continued work on understanding and explaining the model's decisions. No other explicit future research directions are detailed within the abstract itself.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Diagnostic AI</span>
                    
                    <span class="tag">Predictive Analytics in Healthcare</span>
                    
                    <span class="tag">Clinical Decision Support Systems</span>
                    
                    <span class="tag">Healthcare Resource Allocation</span>
                    
                    <span class="tag">Medical Imaging Analysis</span>
                    
                    <span class="tag">Personalized Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Machine Learning Fairness</span>
                    
                    <span class="tag tag-keyword">Sheaf Diffusion</span>
                    
                    <span class="tag tag-keyword">Graph Models</span>
                    
                    <span class="tag tag-keyword">Individual Fairness</span>
                    
                    <span class="tag tag-keyword">Group Fairness</span>
                    
                    <span class="tag tag-keyword">Responsible AI</span>
                    
                    <span class="tag tag-keyword">SHAP values</span>
                    
                    <span class="tag tag-keyword">Bias Mitigation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>75 pages, 46 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>