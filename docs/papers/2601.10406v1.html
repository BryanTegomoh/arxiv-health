<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics - Health AI Hub</title>
    <meta name="description" content="This paper introduces ErrEval, an Error-aware Evaluation framework designed to improve automatic Question Generation (QG) evaluation by explicitly identifying a">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.10406v1" target="_blank">2601.10406v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-15
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Weiping Fu, Bifan Wei, Jingyi Hao, Yushun Zhang, Jian Zhang, Jiaxin Wang, Bo Li, Yu He, Lingling Zhang, Jun Liu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.85 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.10406v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.10406v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces ErrEval, an Error-aware Evaluation framework designed to improve automatic Question Generation (QG) evaluation by explicitly identifying and diagnosing critical defects like factual hallucinations and answer mismatches. ErrEval employs a two-stage process involving an Error Identifier and informed LLM evaluators, demonstrating enhanced alignment with human judgments and effectively mitigating the overestimation of low-quality questions.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate Question Generation (QG) is vital in medicine for applications like patient education, clinical decision support, and medical training. Defects like factual hallucinations or answer mismatches, if propagated, could lead to severe consequences such as incorrect diagnoses, inappropriate treatments, or patient misinformation, making robust, error-aware evaluation crucial for medical AI tools.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>ErrEval enhances the reliability and safety of medical AI applications that employ Question Generation (QG). By providing an error-aware evaluation framework, it ensures that QG systems used in healthcare contexts‚Äîsuch as generating accurate assessment questions for medical students, creating patient-friendly explanations from complex medical records, or assisting clinicians in formulating relevant questions for diagnostic processes‚Äîproduce high-quality, factually grounded questions. This directly mitigates risks associated with misinformation or hallucination, which are particularly dangerous in critical healthcare scenarios, thereby improving the trustworthiness and effectiveness of AI in medicine.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Current QG evaluation methods, including LLM-based ones, are often black-box and holistic, failing to explicitly model critical defects such as factual hallucinations and answer mismatches, leading to overestimation of question quality.</li>
                    
                    <li>ErrEval reformulates QG evaluation into a two-stage process: error diagnosis followed by informed scoring, providing a more fine-grained and grounded assessment.</li>
                    
                    <li>The first stage utilizes a lightweight, plug-and-play Error Identifier to detect and categorize common errors across structural, linguistic, and content-related aspects of generated questions.</li>
                    
                    <li>Diagnostic signals from the Error Identifier serve as explicit evidence, guiding LLM evaluators toward more precise and accurate judgments of question quality.</li>
                    
                    <li>Extensive experiments across three benchmarks validate ErrEval's effectiveness, showing improved alignment with human judgments compared to existing evaluation methods.</li>
                    
                    <li>Further analysis confirms that ErrEval effectively addresses the problem of overestimating the quality of low-quality questions, providing a more realistic assessment.</li>
                    
                    <li>The framework is flexible and enhances QG evaluation by integrating explicit error diagnostics, making the evaluation process more transparent and robust.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>ErrEval operates as a two-stage evaluation framework. The first stage involves a lightweight, plug-and-play Error Identifier component responsible for detecting and categorizing common errors in generated questions (structural, linguistic, content-related). The second stage incorporates these diagnostic signals as explicit evidence to guide Large Language Model (LLM) evaluators, enabling them to make more fine-grained and grounded judgments of question quality, moving beyond a black-box holistic assessment.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>ErrEval significantly improves the alignment of automatic QG evaluation with human judgments. It demonstrably mitigates the overestimation of low-quality questions, providing a more accurate and reliable assessment of QG system performance by explicitly identifying critical errors like factual hallucinations and answer mismatches.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research can lead to more reliable and safer AI-generated content in clinical settings. By preventing the overestimation of flawed questions, ErrEval can help ensure that AI tools generating educational materials for patients or medical students, or supporting clinical decisions with Q&A, produce highly accurate and contextually relevant questions. This reduces the risk of disseminating misinformation, improves the quality of medical training, and enhances the trustworthiness of AI in healthcare, potentially saving clinicians time in verifying AI outputs.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations; however, potential limitations could include the generalization of the 'common errors' identified by the Error Identifier to highly specialized or nuanced medical contexts, the computational cost associated with the two-stage process, or the exhaustiveness of the error categories across all possible QG defects.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions. However, potential future work could involve expanding the scope of detectable error types, adapting ErrEval for domain-specific medical QG, integrating real-time feedback loops for QG model training, or exploring the framework's applicability to other text generation tasks within healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Education</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Patient Information Systems</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                    <span class="tag">Pharmacovigilance</span>
                    
                    <span class="tag">Public Health Communication</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Question Generation</span>
                    
                    <span class="tag tag-keyword">AI Evaluation</span>
                    
                    <span class="tag tag-keyword">Error Diagnostics</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Factual Hallucinations</span>
                    
                    <span class="tag tag-keyword">Answer Mismatch</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Quality Assessment</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>