<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning - Health AI Hub</title>
    <meta name="description" content="This paper introduces Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical reinforcement learning approach designed to mitigate performa">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.19893v1" target="_blank">2510.19893v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-22
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Shiqi Dai, Wei Dai, Jiaee Cheong, Paul Pu Liang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.19893v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.19893v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical reinforcement learning approach designed to mitigate performance disparities across demographic groups in medical AI systems. FairGRPO employs adaptive importance weighting and unsupervised clustering to promote equitable learning and address missing demographic labels. Through extensive experiments, FairGRPO significantly reduces predictive parity and improves diagnostic F1 scores across multiple clinical modalities, culminating in the release of a fairness-aware clinical VLLM, FairMedGemma-4B.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Ensuring equitable and unbiased medical AI systems is paramount for preventing real-world harm to underrepresented populations, improving diagnostic accuracy across all patient demographics, and advancing health equity in clinical practice by making AI tools trustworthy and inclusive.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The primary AI health application is the development of fairness-aware artificial intelligence systems, specifically Vision-Language Large Language Models (VLLMs), for equitable clinical diagnosis. These systems aim to reduce performance disparities across different demographic groups in diagnostic tasks, ensuring more accurate and just medical predictions and care for all populations, particularly the underrepresented.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical problem of performance disparities and bias amplification in medical AI, particularly in multimodal reasoning foundation models trained with reinforcement learning, which cause harm to underrepresented populations.</li>
                    
                    <li>Introduces FairGRPO, a novel hierarchical reinforcement learning approach engineered to promote equitable learning across diverse clinical populations.</li>
                    
                    <li>FairGRPO leverages adaptive importance weighting of advantages, considering crucial factors like group representation, task difficulty, and data source to ensure fairness.</li>
                    
                    <li>To circumvent the pervasive challenge of missing demographic labels in clinical datasets, FairGRPO incorporates unsupervised clustering for automatic discovery of latent demographic groups.</li>
                    
                    <li>Evaluated comprehensively across 7 clinical diagnostic datasets spanning 5 modalities (X-ray, CT scan, dermoscopy, mammography, ultrasound), demonstrating broad applicability.</li>
                    
                    <li>Achieves a substantial 27.2% reduction in predictive parity and a 12.49% improvement in F1 score compared to both vanilla and existing bias-mitigated reinforcement learning baselines.</li>
                    
                    <li>Training dynamics analysis reveals that FairGRPO consistently improves fairness throughout the optimization process, a stark contrast to baseline RL methods which show deteriorating fairness.</li>
                    
                    <li>The research culminates in the release of FairMedGemma-4B, a fairness-aware clinical Vision-Language Model (VLLM) that establishes state-of-the-art performance while exhibiting significantly reduced demographic disparities.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>FairGRPO is a hierarchical reinforcement learning (RL) approach that promotes equitable learning. It employs adaptive importance weighting of advantages based on group representation, task difficulty, and data source. To address the common issue of missing demographic labels, it integrates unsupervised clustering for automatic discovery of latent demographic groups, enabling fairness mitigation even without explicit demographic information.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>FairGRPO achieved a 27.2% reduction in predictive parity and a 12.49% improvement in F1 score across 7 diverse clinical diagnostic datasets compared to vanilla and bias-mitigated RL baselines. Its training dynamics showed progressive fairness improvement, unlike baselines where fairness deteriorated. The released FairMedGemma-4B VLLM demonstrated state-of-the-art performance with significantly reduced demographic disparities.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work directly addresses a critical challenge in medical AI by developing more equitable and trustworthy diagnostic systems. By mitigating performance disparities, FairGRPO can lead to more accurate and reliable clinical diagnoses for all patients, particularly those from underrepresented groups, thus fostering greater health equity and preventing AI-induced harm in healthcare settings. The release of FairMedGemma-4B provides a tangible, fairness-aware tool for clinical application.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract highlights the 'common issue of missing demographic labels in the clinical domain' as a practical challenge, which FairGRPO explicitly addresses with unsupervised clustering. No direct limitations of the FairGRPO method itself or its evaluation are explicitly stated in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology (X-ray, CT scan, Mammography, Ultrasound)</span>
                    
                    <span class="tag">Dermatology (Dermoscopy)</span>
                    
                    <span class="tag">General Clinical Diagnosis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Fairness-aware AI</span>
                    
                    <span class="tag tag-keyword">Reinforcement Learning</span>
                    
                    <span class="tag tag-keyword">Clinical Reasoning</span>
                    
                    <span class="tag tag-keyword">Health Equity</span>
                    
                    <span class="tag tag-keyword">Multimodal AI</span>
                    
                    <span class="tag tag-keyword">Bias Mitigation</span>
                    
                    <span class="tag tag-keyword">Demographic Disparities</span>
                    
                    <span class="tag tag-keyword">Medical Diagnostics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted as Oral on NeurIPS 2025 GenAI4Health Workshop</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>