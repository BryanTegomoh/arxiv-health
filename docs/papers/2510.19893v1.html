<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning - Health AI Hub</title>
    <meta name="description" content="This paper introduces Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical reinforcement learning approach designed to mitigate performa">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.19893v1" target="_blank">2510.19893v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-22
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Shiqi Dai, Wei Dai, Jiaee Cheong, Paul Pu Liang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.19893v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.19893v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical reinforcement learning approach designed to mitigate performance disparities of medical AI systems across diverse demographic groups. FairGRPO employs adaptive importance weighting and unsupervised clustering to promote equitable learning, demonstrating a 27.2% reduction in predictive parity and a 12.49% improvement in F1 score across 7 clinical diagnostic datasets spanning 5 modalities, culminating in the release of a fairness-aware clinical VLLM, FairMedGemma-4B.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research directly addresses the critical issue of bias in medical AI, which can lead to misdiagnosis, delayed treatment, and exacerbate health inequities in underrepresented patient populations. By developing fairer AI systems, it aims to ensure more equitable and reliable diagnostic care for all individuals.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development of fair and equitable medical artificial intelligence systems for clinical reasoning and diagnosis. Specifically, it aims to create diagnostic AI tools (such as the FairMedGemma-4B clinical VLLM) that reduce performance disparities across diverse demographic groups, ensuring that diagnostic capabilities are consistent and reliable for all patients, thereby preventing harm to underrepresented populations in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Medical AI systems, despite advanced diagnostic capabilities, exhibit significant performance disparities across demographic groups, causing harm to underrepresented populations.</li>
                    
                    <li>Reinforcement Learning (RL) training for multimodal reasoning foundation models often inherits and amplifies biases from training datasets disproportionately representing majority populations.</li>
                    
                    <li>FairGRPO (Fairness-aware Group Relative Policy Optimization) is a novel hierarchical RL approach specifically designed to promote equitable learning outcomes across heterogeneous clinical populations.</li>
                    
                    <li>The method utilizes adaptive importance weighting of advantages, which dynamically adjusts learning based on group representation, task difficulty, and data source to address bias.</li>
                    
                    <li>To tackle the common issue of missing demographic labels in clinical datasets, FairGRPO integrates unsupervised clustering to automatically discover latent demographic groups.</li>
                    
                    <li>Experimental validation across 7 clinical diagnostic datasets covering X-ray, CT scan, dermoscopy, mammography, and ultrasound modalities showed FairGRPO reduced predictive parity by 27.2% against baselines and improved the F1 score by 12.49%.</li>
                    
                    <li>Training dynamics analysis revealed that FairGRPO progressively improves fairness throughout optimization, in contrast to baseline RL methods where fairness often deteriorates during training.</li>
                    
                    <li>Based on this work, the authors released FairMedGemma-4B, a fairness-aware clinical Visual-Language Model (VLLM) that achieves state-of-the-art performance with significantly reduced demographic disparities.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>FairGRPO is a hierarchical reinforcement learning (RL) framework. It incorporates an adaptive importance weighting mechanism for advantages, which considers factors such as demographic representation, task difficulty, and data source to guide equitable learning. To mitigate the challenge of missing demographic labels in clinical data, the method integrates unsupervised clustering to automatically identify and group latent demographic populations.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>FairGRPO demonstrated a substantial 27.2% reduction in predictive parity compared to both vanilla and bias-mitigated RL baselines, alongside a 12.49% improvement in overall F1 score. Analysis of training dynamics showed that FairGRPO consistently improved fairness throughout the optimization process, a stark contrast to baseline methods where fairness deteriorated over time. The practical outcome is FairMedGemma-4B, a fairness-aware clinical VLLM that achieves state-of-the-art performance with significantly reduced demographic disparities.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The development and release of FairGRPO and FairMedGemma-4B could lead to more trustworthy and equitable AI-powered clinical decision support systems. By reducing diagnostic disparities across demographic groups, these tools can help ensure that underrepresented patients receive equally accurate and timely diagnoses, thereby mitigating existing health inequities and improving patient outcomes in diverse clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed method or experimental setup.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions for FairGRPO or its applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology (X-ray, CT scan, Mammography, Ultrasound)</span>
                    
                    <span class="tag">Dermatology (Dermoscopy)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Reinforcement Learning</span>
                    
                    <span class="tag tag-keyword">Fairness</span>
                    
                    <span class="tag tag-keyword">Clinical Reasoning</span>
                    
                    <span class="tag tag-keyword">AI Bias</span>
                    
                    <span class="tag tag-keyword">Demographic Disparity</span>
                    
                    <span class="tag tag-keyword">Multimodal AI</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Predictive Parity</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted as Oral on NeurIPS 2025 GenAI4Health Workshop</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>