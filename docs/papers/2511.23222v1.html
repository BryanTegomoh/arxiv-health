<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DAONet-YOLOv8: An Occlusion-Aware Dual-Attention Network for Tea Leaf Pest and Disease Detection - Health AI Hub</title>
    <meta name="description" content="This paper introduces DAONet-YOLOv8, a novel deep learning model designed to enhance the accurate detection of tea leaf pests and diseases in challenging real-w">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>DAONet-YOLOv8: An Occlusion-Aware Dual-Attention Network for Tea Leaf Pest and Disease Detection</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.23222v1" target="_blank">2511.23222v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-28
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yefeng Wu, Shan Wan, Ling Wu, Yecheng Zhao
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.23222v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.23222v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces DAONet-YOLOv8, a novel deep learning model designed to enhance the accurate detection of tea leaf pests and diseases in challenging real-world plantation environments. It addresses issues like complex backgrounds, variable illumination, and frequent occlusions by integrating a dual-attention fusion module, an occlusion-aware detection head, and dynamic synthesis convolutions. The model significantly outperforms the YOLOv8n baseline and other mainstream detectors on a real-world tea plantation dataset, demonstrating improved precision, recall, and mAP while reducing computational parameters.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Although focused on agriculture, the core methodological advancements‚Äîhandling occlusion, complex backgrounds, subtle feature detection, and irregular boundary recognition‚Äîare highly relevant to medical imaging for disease diagnosis. These techniques can improve the accuracy and robustness of AI systems in detecting pathologies (e.g., tumors, lesions) that are often occluded by anatomical structures, present in noisy or complex tissue environments, or exhibit irregular shapes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI (DAONet-YOLOv8) is applied to detect pests and diseases on tea leaves. While not directly a 'medical AI' application in the human health sense, it is a significant AI application for 'biosecurity'. By enabling early and accurate identification of plant health threats, it supports agricultural biosecurity efforts to protect crop yields and ensure food supply stability. This indirect contribution to public health through food security aligns with broader biosecurity objectives.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Addressed:** Challenging conditions in real tea plantations, including complex backgrounds, variable illumination, and frequent occlusions, lead to missed detections and false positives in existing pest/disease detectors.</li>
                    
                    <li>**Proposed Solution (DAONet-YOLOv8):** An enhanced YOLOv8 variant incorporating three key architectural improvements for robust detection.</li>
                    
                    <li>**Dual-Attention Fusion Module (DAFM):** Combines convolutional feature extraction with self-attention for global context modeling, enabling focus on subtle lesion regions and effective background noise suppression.</li>
                    
                    <li>**Occlusion-Aware Detection Head (Detect-OAHead):** Learns the relationship between visible and occluded parts of lesions to compensate for missing features caused by occlusion, crucial for complex real-world scenes.</li>
                    
                    <li>**C2f-DSConv Module:** Employs dynamic synthesis convolutions with multiple kernel shapes to better capture and delineate irregular lesion boundaries, improving segmentation and localization accuracy.</li>
                    
                    <li>**Performance:** Achieved 92.97% precision, 92.80% recall, 97.10% mAP@50, and 76.90% mAP@50:95 on a real-world tea plantation dataset with six categories of pests/diseases.</li>
                    
                    <li>**Superiority & Efficiency:** Outperformed YOLOv8n baseline by 2.34-4.68 percentage points across metrics and reduced parameters by 16.7%, also showing superior performance against other mainstream detection models.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study proposes DAONet-YOLOv8, an improved YOLOv8 model. It integrates a Dual-Attention Fusion Module (DAFM) for local and global feature extraction, an occlusion-aware detection head (Detect-OAHead) to address partial visibility, and a C2f-DSConv module with dynamic synthesis convolutions for irregular boundary capture. The model was trained and evaluated on a custom real-world tea plantation dataset comprising six categories of pests and diseases.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>DAONet-YOLOv8 achieved state-of-the-art performance for tea leaf pest and disease detection, with 97.10% mAP@50 and 76.90% mAP@50:95. It demonstrated a significant improvement over the YOLOv8n baseline (e.g., 4.68 percentage points increase in recall, 1.80 points in mAP@50:95) while being more computationally efficient, reducing parameters by 16.7%. The model's enhancements effectively mitigate challenges posed by complex backgrounds and occlusions.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The techniques developed, particularly the occlusion-aware detection and dual-attention mechanisms, hold significant promise for clinical applications. They could lead to more accurate and reliable detection of subtle or partially obscured pathologies in medical images (e.g., small tumors in dense tissue, lesions behind bones). This could enhance early diagnosis, reduce false negatives, and improve the overall efficiency and precision of computer-aided diagnostic systems, ultimately benefiting patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the proposed DAONet-YOLOv8 model. However, common challenges for deep learning models, especially in complex real-world scenarios, include generalization to unseen environments or variations in pest/disease manifestations not represented in the training dataset, and the computational resources required for deployment in resource-constrained settings.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions for this specific work. However, potential future work stemming from such methods could involve expanding the dataset to cover a wider range of disease variations and environmental conditions, exploring real-time deployment on edge devices, or adapting the architecture for few-shot learning scenarios.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Radiology</span>
                    
                    <span class="tag">Pathology Detection</span>
                    
                    <span class="tag">Computer-Aided Diagnosis</span>
                    
                    <span class="tag">AI in Healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">DAONet-YOLOv8</span>
                    
                    <span class="tag tag-keyword">Occlusion-Aware</span>
                    
                    <span class="tag tag-keyword">Dual-Attention</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Object Detection</span>
                    
                    <span class="tag tag-keyword">Medical Imaging Analogy</span>
                    
                    <span class="tag tag-keyword">Computer Vision</span>
                    
                    <span class="tag tag-keyword">Agricultural Diagnostics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Accurate detection of tea leaf pests and diseases in real plantations remains challenging due to complex backgrounds, variable illumination, and frequent occlusions among dense branches and leaves. Existing detectors often suffer from missed detections and false positives in such scenarios. To address these issues, we propose DAONet-YOLOv8, an enhanced YOLOv8 variant with three key improvements: (1) a Dual-Attention Fusion Module (DAFM) that combines convolutional local feature extraction with self-attention based global context modeling to focus on subtle lesion regions while suppressing background noise; (2) an occlusion-aware detection head (Detect-OAHead) that learns the relationship between visible and occluded parts to compensate for missing lesion features; and (3) a C2f-DSConv module employing dynamic synthesis convolutions with multiple kernel shapes to better capture irregular lesion boundaries. Experiments on our real-world tea plantation dataset containing six pest and disease categories demonstrate that DAONet-YOLOv8 achieves 92.97% precision, 92.80% recall, 97.10% mAP@50 and 76.90% mAP@50:95, outperforming the YOLOv8n baseline by 2.34, 4.68, 1.40 and 1.80 percentage points respectively, while reducing parameters by 16.7%. Comparative experiments further confirm that DAONet-YOLOv8 achieves superior performance over mainstream detection models.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>