<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analysis of heart failure patient trajectories using sequence modeling - Health AI Hub</title>
    <meta name="description" content="This paper conducts a systematic empirical analysis of six advanced sequence models (Transformers, Transformers++, Mambas) for three one-year clinical predictio">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Analysis of heart failure patient trajectories using sequence modeling</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.16839v1" target="_blank">2511.16839v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Falk Dippela, Yinan Yu, Annika Rosengren, Martin Lindgren, Christina E. Lundberg, Erik Aerts, Martin Adiels, Helen Sj√∂land
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.16839v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.16839v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper conducts a systematic empirical analysis of six advanced sequence models (Transformers, Transformers++, Mambas) for three one-year clinical prediction tasks in a large heart failure cohort using electronic health records (EHRs). It demonstrates that Llama-based Transformer++ and Mamba architectures achieve superior predictive discrimination, calibration, and efficiency, outperforming other Transformers even with smaller configurations or less training data.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for advancing precision medicine in heart failure by identifying highly effective and efficient AI architectures capable of accurate risk prediction from routine EHR data. Improved predictions of readmission and mortality can enable earlier, more targeted interventions, personalize patient management strategies, and optimize healthcare resource allocation for better patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper applies state-of-the-art AI sequence modeling architectures (Transformers, Mamba) to Electronic Health Records (EHRs) for predicting critical patient outcomes such as clinical instability (readmission) and mortality in heart failure patients. This falls under medical AI for predictive analytics, risk stratification, and potentially clinical decision support in cardiology and general hospital care.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Investigated six sequence models across Transformer, Transformer++ (Llama-based), and Mamba architectures for clinical prediction using EHRs.</li>
                    
                    <li>Utilized a large Swedish heart failure cohort (N=42,820) with rich in-hospital EHR data (diagnoses, vital signs, labs, medications, procedures).</li>
                    
                    <li>Evaluated model performance on three one-year prediction tasks: clinical instability (readmission) after initial HF hospitalization, mortality after initial HF hospitalization, and mortality after latest hospitalization.</li>
                    
                    <li>Llama architecture demonstrated the highest predictive discrimination, best calibration, and robustness across all tasks, closely followed by Mamba architectures.</li>
                    
                    <li>Both Llama and Mamba architectures showed efficient representation learning, with their 'tiny' configurations surpassing the performance of larger-scaled conventional Transformers.</li>
                    
                    <li>The study included a first-of-its-kind ablation study systematically evaluating modifications in EHR input sequences, architectural model configurations, and temporal preprocessing techniques.</li>
                    
                    <li>Llama and Mamba achieved superior performance using 25% less training data compared to other models of equal size, highlighting their training efficiency.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study compared six sequence models (Transformers, Llama-based Transformers++, Mambas) on a large Swedish heart failure cohort (N=42,820) using in-hospital EHR data. Models were evaluated for predictive discrimination and calibration on three one-year clinical prediction tasks: clinical instability and mortality after initial HF hospitalization, and mortality after latest hospitalization. An extensive ablation study was performed to assess the impact of input patient sequence modifications, architectural model configurations, and temporal preprocessing techniques.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The Llama-based Transformer++ architecture achieved the highest predictive discrimination, best calibration, and demonstrated strong robustness across all heart failure prediction tasks. Mamba architectures showed comparable performance, closely trailing Llama. Both Llama and Mamba exhibited superior efficiency, with their smaller configurations outperforming larger conventional Transformers. Notably, Llama and Mamba achieved better predictive performance with 25% less training data at equal model size, indicating their advanced representation learning capabilities.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Identifying Llama and Mamba as highly performing and efficient architectures for clinical prediction from EHRs can lead to the deployment of more accurate and resource-effective risk prediction tools in heart failure care. These tools could empower clinicians to better stratify patient risk for readmission or mortality, facilitating proactive, personalized interventions, optimizing resource allocation, and ultimately improving patient outcomes and reducing healthcare costs associated with heart failure.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations; however, potential implications could include the generalizability of findings given the focus on a single-country (Swedish) cohort and specific disease (heart failure). The study focuses on empirical comparison of existing architectures rather than novel model development.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future model development for clinical prediction tasks using EHRs should leverage the systematic design choices and recommendations presented in this study regarding input tokenization, model configuration, and temporal data preprocessing as a foundational starting point. Further research could explore the application of these efficient architectures to other chronic diseases and diverse healthcare systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Internal Medicine</span>
                    
                    <span class="tag">Hospital Medicine</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Public Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Heart Failure</span>
                    
                    <span class="tag tag-keyword">Clinical Prediction</span>
                    
                    <span class="tag tag-keyword">Electronic Health Records</span>
                    
                    <span class="tag tag-keyword">Sequence Modeling</span>
                    
                    <span class="tag tag-keyword">Transformers</span>
                    
                    <span class="tag tag-keyword">Mamba</span>
                    
                    <span class="tag tag-keyword">Llama</span>
                    
                    <span class="tag tag-keyword">Machine Learning</span>
                    
                    <span class="tag tag-keyword">Ablation Study</span>
                    
                    <span class="tag tag-keyword">Mortality Prediction</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Transformers have defined the state-of-the-art for clinical prediction tasks involving electronic health records (EHRs). The recently introduced Mamba architecture outperformed an advanced Transformer (Transformer++) based on Llama in handling long context lengths, while using fewer model parameters. Despite the impressive performance of these architectures, a systematic approach to empirically analyze model performance and efficiency under various settings is not well established in the medical domain. The performances of six sequence models were investigated across three architecture classes (Transformers, Transformers++, Mambas) in a large Swedish heart failure (HF) cohort (N = 42820), providing a clinically relevant case study. Patient data included diagnoses, vital signs, laboratories, medications and procedures extracted from in-hospital EHRs. The models were evaluated on three one-year prediction tasks: clinical instability (a readmission phenotype) after initial HF hospitalization, mortality after initial HF hospitalization and mortality after latest hospitalization. Ablations account for modifications of the EHR-based input patient sequence, architectural model configurations, and temporal preprocessing techniques for data collection. Llama achieves the highest predictive discrimination, best calibration, and showed robustness across all tasks, followed by Mambas. Both architectures demonstrate efficient representation learning, with tiny configurations surpassing other large-scaled Transformers. At equal model size, Llama and Mambas achieve superior performance using 25% less training data. This paper presents a first ablation study with systematic design choices for input tokenization, model configuration and temporal data preprocessing. Future model development in clinical prediction tasks using EHRs could build upon this study's recommendation as a starting point.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>