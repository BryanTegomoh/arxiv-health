<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explaining Digital Pathology Models via Clustering Activations - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel clustering-based explainability technique for convolutional neural network models used in digital pathology. Unlike traditional sa">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Explaining Digital Pathology Models via Clustering Activations</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.14558v1" target="_blank">2511.14558v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-18
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Adam Bajger, Jan Obdr≈æ√°lek, Vojtƒõch K≈Ør, Rudolf Nenutil, Petr Holub, V√≠t Musil, Tom√°≈° Br√°zdil
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.14558v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.14558v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel clustering-based explainability technique for convolutional neural network models used in digital pathology. Unlike traditional saliency map methods that offer local explanations, this approach provides insights into the global behavior of the model through visualization of activation clusters, offering both a broader understanding and fine-grained information. The authors demonstrate its utility by evaluating it on an existing prostate cancer detection model, showing it can increase confidence in model operation and facilitate clinical adoption.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for increasing the trustworthiness and adoption of AI models in clinical digital pathology by providing clear, interpretable explanations of their decisions, which is essential for accurate diagnosis and treatment planning.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research develops an explainability technique for AI models (Convolutional Neural Networks) used in digital pathology. The specific application mentioned is detecting prostate cancer from pathology slides. The aim is to make these AI models more transparent and trustworthy for clinicians, facilitating their adoption in healthcare for improved diagnosis and patient care.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Proposes a new explainability technique for digital pathology models, utilizing clustering of Convolutional Neural Network (CNN) activations.</li>
                    
                    <li>Differentiates itself from common saliency map methods (e.g., occlusion, GradCAM) by providing a global understanding of model behavior rather than just local prediction contributions for single slides.</li>
                    
                    <li>The method also offers more fine-grained information about the model's decision-making process.</li>
                    
                    <li>Visualizable result clusters are intended to enhance understanding of the model, build confidence in its operations, and accelerate its integration into clinical practice.</li>
                    
                    <li>The technique's usefulness is demonstrated through an evaluation on an existing model designed for detecting prostate cancer.</li>
                    
                    <li>Aims to address the critical need for transparent and trustworthy AI in medical imaging to overcome barriers to clinical adoption.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The core methodology involves a clustering-based explainability technique applied to the activations of convolutional neural networks. This method processes the internal representations learned by the CNNs and groups similar activation patterns into clusters, which can then be visualized to infer global model behavior and fine-grained decision criteria, contrasting with local saliency-based approaches.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key finding is the successful demonstration of the technique's usefulness when applied to an existing model for detecting prostate cancer. This suggests that the clustering-based approach effectively provides global and fine-grained insights into model behavior, potentially increasing user confidence.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By providing clear, visualizable explanations of AI model decisions, this technique can significantly increase clinician confidence in AI-powered diagnostic tools. This enhanced transparency is expected to accelerate the adoption of digital pathology models in clinical practice, leading to potentially faster and more accurate diagnoses, particularly for diseases like prostate cancer.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The provided abstract does not explicitly state any limitations or caveats of the proposed method. Further details would be needed from the full paper.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The provided abstract does not explicitly state any future research directions. Further details would be needed from the full paper.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Urology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Digital Pathology</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">XAI</span>
                    
                    <span class="tag tag-keyword">Convolutional Neural Networks</span>
                    
                    <span class="tag tag-keyword">Clustering</span>
                    
                    <span class="tag tag-keyword">Model Interpretability</span>
                    
                    <span class="tag tag-keyword">Prostate Cancer Detection</span>
                    
                    <span class="tag tag-keyword">Global Explainability</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We present a clustering-based explainability technique for digital pathology models based on convolutional neural networks. Unlike commonly used methods based on saliency maps, such as occlusion, GradCAM, or relevance propagation, which highlight regions that contribute the most to the prediction for a single slide, our method shows the global behaviour of the model under consideration, while also providing more fine-grained information. The result clusters can be visualised not only to understand the model, but also to increase confidence in its operation, leading to faster adoption in clinical practice. We also evaluate the performance of our technique on an existing model for detecting prostate cancer, demonstrating its usefulness.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>