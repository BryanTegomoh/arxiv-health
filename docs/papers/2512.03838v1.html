<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training and Evaluation of Guideline-Based Medical Reasoning in LLMs - Health AI Hub</title>
    <meta name="description" content="This paper introduces a method to teach Large Language Models (LLMs) to perform step-by-step, guideline-based medical reasoning for prediction, aiming to improv">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Training and Evaluation of Guideline-Based Medical Reasoning in LLMs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.03838v1" target="_blank">2512.03838v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Michael Staniek, Artem Sokolov, Stefan Riezler
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.03838v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.03838v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a method to teach Large Language Models (LLMs) to perform step-by-step, guideline-based medical reasoning for prediction, aiming to improve explainability and trust in medical AI. It demonstrates that small, fine-tuned LLMs, trained on verbalized medical consensus rules from EHRs, outperform larger models and achieve nearly perfect reasoning correctness, with a multimodal approach addressing the bottleneck of forecasting sparsely sampled clinical variables for early prediction.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This work is crucial for medical AI adoption as it provides a path to transparent, auditable, and guideline-based reasoning for predictions, which is essential for gaining the trust of medical practitioners, ensuring patient safety, and enabling responsible clinical decision-making.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves training and fine-tuning LLMs to perform guideline-based medical reasoning for early disease prediction (e.g., Sepsis-3). This aims to develop explainable AI systems that can deduce medical conclusions faithfully from patient data and clinical variables, following established medical consensus guidelines. It also explores multimodal AI for forecasting sparsely sampled clinical variables to improve future prediction.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical need for faithful, explainable reasoning in medical AI by enabling LLMs to follow medical consensus guidelines step-by-step.</li>
                    
                    <li>Proposes fine-tuning LLMs on verbalized instantiations of medical inference rules derived from Electronic Health Records (EHRs) and consensus guidelines, applicable across many medical areas.</li>
                    
                    <li>Introduces an automatic evaluation framework for LLM inference, assessing both 'derivation correctness' (fidelity of reasoning) and 'value correctness' (prediction accuracy).</li>
                    
                    <li>Experiments show that small, fine-tuned LLMs significantly outperform larger LLMs that use one-shot learning with explicit definitions or are trained broadly on medical texts, exemplified with the Sepsis-3 definition.</li>
                    
                    <li>Achieves nearly perfect derivation correctness for rules and exceptions on unseen patient data within the specific medical domain (e.g., Sepsis-3).</li>
                    
                    <li>Identifies the main bottleneck for early prediction as 'generalization into the future' by forecasting sparsely and irregularly sampled clinical variables, rather than out-of-distribution generalization of rules.</li>
                    
                    <li>Demonstrates improvement in addressing the forecasting bottleneck through a multimodal setup that integrates the output representations of a time series forecasting model with the LLM.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study fine-tuned Large Language Models (LLMs) using data comprising verbalized instantiations of medical consensus guidelines and their exceptions, extracted from electronic health records (EHRs). This process teaches LLMs to emulate step-by-step medical inference. An automatic evaluation system was developed based on these consensus rules, measuring 'derivation correctness' (logical fidelity of conclusions from premises) and 'value correctness' (accuracy of predicted values). To address the challenge of future variable prediction, a multimodal approach was explored, integrating output representations from a time series forecasting model with the LLM's reasoning process.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Small, fine-tuned LLMs significantly outperform considerably larger LLMs, whether prompted with explicit definitions (one-shot learning) or pre-trained on general medical texts. Fine-tuning on verbalized rule instantiations achieved nearly perfect derivation correctness for rules and exceptions on unseen patient data within the specific medical area. The primary bottleneck for accurate early prediction was identified as the 'generalization into the future' through forecasting sparsely and irregularly sampled clinical variables, rather than the LLM's ability for out-of-distribution rule generalization. This forecasting bottleneck was shown to be ameliorated by integrating a time series forecasting model's outputs with the LLM in a multimodal setup.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research paves the way for the development of highly trustworthy and transparent AI systems in clinical practice, particularly for early disease prediction and diagnosis. By grounding AI reasoning in established medical guidelines and providing explainable outputs with high derivation correctness, it can accelerate the adoption of AI tools by clinicians, improve diagnostic accuracy, and enable earlier, more effective interventions, ultimately enhancing patient care and outcomes, especially for time-sensitive conditions like sepsis.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The main limitation identified is the inherent challenge of 'generalization into the future' for early prediction, specifically concerning the accurate forecasting of sparsely and irregularly sampled clinical variables. While a multimodal approach showed improvements, the intrinsic sparsity and irregularity of real-world clinical time-series data remain a significant hurdle for robust future state prediction.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper suggests future research should focus on further improving the forecasting of sparsely and irregularly sampled clinical variables. This implies continued development of robust time series forecasting models and their seamless, effective integration within multimodal LLM architectures to enhance early prediction capabilities in medicine.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Sepsis</span>
                    
                    <span class="tag">General Medical Diagnostics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Medical Reasoning</span>
                    
                    <span class="tag tag-keyword">Consensus Guidelines</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">Sepsis Prediction</span>
                    
                    <span class="tag tag-keyword">Fine-tuning</span>
                    
                    <span class="tag tag-keyword">Multimodal AI</span>
                    
                    <span class="tag tag-keyword">Time Series Forecasting</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>