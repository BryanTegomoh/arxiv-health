<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging - Health AI Hub</title>
    <meta name="description" content="BTB3D (Better Tokens for Better 3D) introduces a novel causal convolutional encoder-decoder to advance vision-language modeling in 3D medical imaging, particula">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">← Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20639v1" target="_blank">2510.20639v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Dong Yang, Pengfei Guo, Marc Edgar, Daguang Xu, Bernhard Kainz, Bjoern Menze
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20639v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20639v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">BTB3D (Better Tokens for Better 3D) introduces a novel causal convolutional encoder-decoder to advance vision-language modeling in 3D medical imaging, particularly for high-resolution CT volumes. By generating compact, frequency-aware volumetric tokens through a three-stage training curriculum, it achieves new state-of-the-art results in automated report generation and text-to-CT synthesis, demonstrating the crucial role of precise 3D tokenization.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research significantly enhances the capabilities of AI in processing and generating medical images and reports, offering improved diagnostic accuracy and efficiency for radiologists and clinicians in interpreting complex 3D scans.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research contributes to medical AI by developing an advanced vision-language model for 3D medical imaging. Specifically, it enables: 
1. **Automated Medical Report Generation**: AI systems can automatically generate comprehensive clinical reports from 3D CT scans, assisting radiologists, improving efficiency, and potentially reducing diagnostic errors in healthcare.
2. **Text-to-3D CT Synthesis**: AI can generate anatomically consistent 3D CT images from textual descriptions. This has applications in creating synthetic training data for other medical AI models (especially for rare diseases), medical education, research, and potentially pre-surgical planning simulations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the limitations of current 3D medical VLM, which struggle with high-resolution, long-sequence volumes due to misaligned vision encoders and slice-wise tokenization blurring fine anatomy.</li>
                    
                    <li>Introduces BTB3D, a causal convolutional encoder-decoder that unifies 2D and 3D training/inference and generates compact, frequency-aware volumetric tokens.</li>
                    
                    <li>Employs a unique three-stage training curriculum: (i) local reconstruction, (ii) overlapping-window tiling, and (iii) long-context decoder refinement, enabling generalization to scans exceeding 300 slices without memory overhead.</li>
                    
                    <li>Achieves state-of-the-art performance in automated report generation, improving clinical F1 by 40% and BLEU scores over leading models (CT2Rep, CT-CHAT, Merlin).</li>
                    
                    <li>Sets new benchmarks in text-to-CT synthesis, reducing FID by 75% and halving FVD compared to GenerateCT and MedSyn, producing anatomically consistent 512x512x241 CT volumes.</li>
                    
                    <li>The core finding is that precise three-dimensional tokenization is essential for scalable vision-language modeling in 3D medical imaging, surpassing the impact of larger language backbones alone.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>BTB3D is a causal convolutional encoder-decoder architecture designed to unify 2D and 3D training and inference. It processes high-resolution 3D medical volumes to produce compact, frequency-aware volumetric tokens. The training strategy involves a three-stage curriculum: (i) local reconstruction of volume excerpts, (ii) processing via overlapping-window tiling, and (iii) long-context decoder refinement. This approach enables the model to learn efficiently from short slice excerpts and generalize to full scans without increasing memory footprint.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>BTB3D sets new state-of-the-art benchmarks in two critical tasks: (1) For automated medical report generation, it achieves a 40% increase in clinical F1 score and improved BLEU scores compared to existing models. (2) For text-to-CT synthesis, it drastically reduces Frechet Inception Distance (FID) by 75% and halves Frechet Video Distance (FVD) against prior methods, generating high-resolution (512x512x241) and anatomically consistent CT volumes. The central finding is that precise 3D tokenization, rather than merely larger language models, is the key to scalable 3D medical VLM.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact is substantial, offering enhanced AI tools for medical diagnosis and image management. It can lead to more accurate and faster automated radiology report generation, reducing radiologist workload and potentially improving patient care. Furthermore, its ability to synthesize high-fidelity, anatomically consistent CT volumes from text could aid in medical education, surgical planning, or data augmentation for training robust diagnostic models, especially for rare conditions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the limitations of *current approaches* (e.g., misalignment of vision encoders with clinical language, blurring from slice-wise tokenization) that BTB3D aims to overcome. Explicit limitations of the BTB3D model itself are not mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions are not explicitly stated in the abstract. The paper concludes by emphasizing the importance of precise three-dimensional tokenization as a foundational insight for future scalable vision-language modeling in 3D medical imaging.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical AI</span>
                    
                    <span class="tag">Computational Imaging</span>
                    
                    <span class="tag">Medical Image Analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">3D medical imaging</span>
                    
                    <span class="tag tag-keyword">vision-language modeling</span>
                    
                    <span class="tag tag-keyword">computed tomography (CT)</span>
                    
                    <span class="tag tag-keyword">report generation</span>
                    
                    <span class="tag tag-keyword">image synthesis</span>
                    
                    <span class="tag tag-keyword">volumetric tokens</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">causal convolution</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent progress in vision-language modeling for 3D medical imaging has been
fueled by large-scale computed tomography (CT) corpora with paired free-text
reports, stronger architectures, and powerful pretrained models. This has
enabled applications such as automated report generation and text-conditioned
3D image synthesis. Yet, current approaches struggle with high-resolution,
long-sequence volumes: contrastive pretraining often yields vision encoders
that are misaligned with clinical language, and slice-wise tokenization blurs
fine anatomy, reducing diagnostic performance on downstream tasks. We introduce
BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder
that unifies 2D and 3D training and inference while producing compact,
frequency-aware volumetric tokens. A three-stage training curriculum enables
(i) local reconstruction, (ii) overlapping-window tiling, and (iii)
long-context decoder refinement, during which the model learns from short slice
excerpts yet generalizes to scans exceeding 300 slices without additional
memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it
improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and
Merlin for report generation; and it reduces FID by 75% and halves FVD compared
to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically
consistent 512*512*241 volumes. These results confirm that precise
three-dimensional tokenization, rather than larger language backbones alone, is
essential for scalable vision-language modeling in 3D medical imaging. The
codebase is available at: https://github.com/ibrahimethemhamamci/BTB3D</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>NeurIPS 2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">← Back to all papers</a></p>
    </footer>
</body>
</html>