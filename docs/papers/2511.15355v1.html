<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning - Health AI Hub</title>
    <meta name="description" content="HEAD-QA v2 is an expanded and updated multilingual healthcare multiple-choice reasoning dataset, now comprising over 12,000 questions derived from a decade of S">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.15355v1" target="_blank">2511.15355v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-19
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Alexis Correa-Guill√©n, Carlos G√≥mez-Rodr√≠guez, David Vilares
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.15355v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.15355v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">HEAD-QA v2 is an expanded and updated multilingual healthcare multiple-choice reasoning dataset, now comprising over 12,000 questions derived from a decade of Spanish professional exams. This resource serves as a robust benchmark for evaluating Large Language Models (LLMs), demonstrating that model scale and intrinsic reasoning ability are the primary drivers of performance, with complex inference strategies yielding only limited additional gains.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This work is highly relevant to medicine as it provides a critical benchmark for developing and evaluating AI systems capable of robust medical reasoning. Improving LLM performance on complex healthcare questions is essential for applications such as clinical decision support, diagnostic assistance, and enhancing medical information retrieval.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves training and benchmarking Large Language Models (LLMs) to improve their performance in understanding, reasoning, and answering questions within the healthcare domain. This can lead to AI applications in medical education (e.g., automated tutoring or assessment tools), clinical decision support (e.g., providing information to clinicians), or patient information systems (e.g., answering patient queries).</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Introduces HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset.</li>
                    
                    <li>The dataset has been significantly extended to over 12,000 questions, sourced from ten years of Spanish professional healthcare exams.</li>
                    
                    <li>Addresses the critical need for high-quality datasets that capture the linguistic and conceptual complexity inherent in healthcare reasoning.</li>
                    
                    <li>Benchmarks several open-source LLMs, employing diverse inference strategies including direct prompting, Retrieval-Augmented Generation (RAG), and probability-based answer selection.</li>
                    
                    <li>Key finding: LLM performance in healthcare reasoning tasks is predominantly driven by the model's scale and its intrinsic reasoning capabilities.</li>
                    
                    <li>Observation: More complex inference strategies, such as RAG and advanced prompting, provided only limited performance gains over the models' inherent abilities.</li>
                    
                    <li>Provides additional multilingual versions to foster future research in biomedical reasoning and cross-lingual model improvement.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors expanded the HEAD-QA dataset by compiling over 12,000 multiple-choice questions from ten years of Spanish professional healthcare exams. They then benchmarked several open-source Large Language Models (LLMs) on this expanded dataset, evaluating performance using different inference strategies: direct prompting, Retrieval-Augmented Generation (RAG), and probability-based answer selection.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Performance of Large Language Models on complex healthcare reasoning tasks is primarily determined by their inherent scale and intrinsic reasoning abilities. Advanced inference strategies like RAG or specialized prompting mechanisms offered only marginal improvements, suggesting that the core reasoning capacity of the models is a more dominant factor than strategy refinement.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By providing a rigorous and expanded benchmark for medical reasoning, this research facilitates the development of more reliable and accurate AI tools for clinical practice. Improved LLMs could assist clinicians in navigating complex diagnostic pathways, accessing evidence-based information efficiently, and supporting medical education, ultimately leading to enhanced patient care and reduced cognitive burden on healthcare professionals.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract highlights that complex inference strategies (e.g., prompting, RAG, probability-based answer selection) achieved only "limited gains." This suggests that current LLM architectures or the methods of integrating these strategies may not yet fully leverage their potential for complex medical reasoning, or that inherent model capabilities currently overshadow strategic refinements.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors intend for HEAD-QA v2 to serve as a reliable resource for advancing future research in biomedical reasoning and model improvement. The provision of additional multilingual versions explicitly supports future work on cross-lingual medical reasoning and the generalization of LLM capabilities across different languages.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">General Medicine</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Medical Education</span>
                    
                    <span class="tag">Diagnostic Reasoning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">healthcare reasoning</span>
                    
                    <span class="tag tag-keyword">medical QA</span>
                    
                    <span class="tag tag-keyword">LLM benchmarking</span>
                    
                    <span class="tag tag-keyword">medical datasets</span>
                    
                    <span class="tag tag-keyword">biomedical NLP</span>
                    
                    <span class="tag tag-keyword">Spanish language</span>
                    
                    <span class="tag tag-keyword">multilingual AI</span>
                    
                    <span class="tag tag-keyword">clinical decision support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and G√≥mez-Rodr√≠guez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Preprint. 12 pages</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>