<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech - Health AI Hub</title>
    <meta name="description" content="This paper introduces ProfASR-Bench, a novel evaluation benchmark designed to assess Automatic Speech Recognition (ASR) performance in high-stakes professional ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.23686v1" target="_blank">2512.23686v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-29
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Deepak Babu Piskala
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.SD
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.23686v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.23686v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces ProfASR-Bench, a novel evaluation benchmark designed to assess Automatic Speech Recognition (ASR) performance in high-stakes professional environments, specifically addressing challenges in fields like medicine, finance, legal, and technology. The research reveals a significant "context-utilization gap" (CUG), indicating that current advanced ASR systems, even when provided with explicit contextual prompts, largely fail to leverage this information to improve accuracy, particularly for critical entity recognition.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as it directly addresses the critical need for accurate ASR in clinical settings, where misrecognition of dense medical terminology, drug names, or patient conditions can lead to severe patient harm and legal ramifications. The identified 'context-utilization gap' highlights a fundamental limitation of current ASR systems in processing medically complex speech, underscoring why they often fall short in practical healthcare applications despite seemingly high general accuracy.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research aims to improve Automatic Speech Recognition (ASR) systems for professional settings, explicitly including medicine. This directly impacts AI applications in health such as voice-activated clinical documentation tools, medical transcription services, AI assistants for healthcare professionals, and intelligent interfaces for EHR systems, making them more accurate, reliable, and capable of handling complex medical terminology and critical data with fewer errors. The focus on context-conditioned recognition is crucial for nuanced medical communication.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>ProfASR-Bench is a new, domain-specific evaluation suite targeting high-stakes professional ASR applications in fields such as medicine, finance, legal, and technology.</li>
                    
                    <li>It addresses key challenges: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors that existing benchmarks underplay.</li>
                    
                    <li>The benchmark features examples comprising a natural-language prompt (domain cue and/or speaker profile) paired with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition.</li>
                    
                    <li>Evaluation includes conventional ASR metrics (Word Error Rate - WER) alongside novel entity-aware scores and slice-wise reporting by accent and gender, providing a comprehensive assessment.</li>
                    
                    <li>Testing with representative ASR models (Whisper, Qwen-Omni) across a 'standardized context ladder' (no-context, profile, domain+profile, oracle, adversarial conditions) was conducted.</li>
                    
                    <li>A consistent 'context-utilization gap' (CUG) was identified, demonstrating that lightweight textual context, even with oracle prompts, produces little to no change in average WER, and adversarial prompts do not reliably degrade performance.</li>
                    
                    <li>ProfASR-Bench provides a reproducible testbed for comparing context fusion strategies across different ASR model families to address the identified CUG.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>ProfASR-Bench was constructed as a corpus pairing audio utterances with natural-language prompts (acting as domain cues or speaker profiles) and entity-rich target transcriptions. The evaluation suite supports conventional WER, entity-aware scoring, and slice-wise analysis by accent and gender. Testing involved two families of ASR models, Whisper (encoder-decoder) and Qwen-Omni (audio language models), under five distinct context conditions: no-context, speaker profile only, domain and speaker profile combined, oracle (ideal context), and adversarial prompts.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is the 'context-utilization gap' (CUG): current state-of-the-art ASR models (Whisper, Qwen-Omni) consistently show minimal to no improvement in average Word Error Rate (WER) even when provided with optimal, explicit textual context (oracle prompts). Furthermore, adversarial prompts failed to reliably degrade performance. This indicates a fundamental inability of these systems to effectively integrate and leverage readily available side information to enhance recognition accuracy, particularly for critical entities in specialized professional domains.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings have significant clinical impact, revealing that current ASR technologies are not yet robust enough for high-stakes medical applications requiring precise transcription of complex and critical information. This necessitates a cautious approach to deploying ASR in clinical workflows and highlights the urgent need for research into novel ASR architectures and context fusion strategies that can effectively bridge the identified 'context-utilization gap' to ensure patient safety and data integrity in medical documentation.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>While the abstract doesn't list limitations of ProfASR-Bench itself, it critically identifies a major limitation of current ASR systems: their inability to effectively utilize contextual information. This suggests that existing models, despite being nominally promptable, lack the necessary mechanisms for robust context integration crucial for high-stakes professional applications like medical transcription.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper implies future research should focus on developing and rigorously evaluating advanced context fusion strategies across different ASR model architectures. ProfASR-Bench is positioned as a reproducible testbed for this purpose, encouraging innovation to overcome the identified 'context-utilization gap' and improve the reliability of ASR for critical entity recognition in professional domains, including medicine.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Documentation</span>
                    
                    <span class="tag">Medical Diagnostics</span>
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Surgical Reporting</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Medical Legal Compliance</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Automatic Speech Recognition (ASR)</span>
                    
                    <span class="tag tag-keyword">Context-Conditioned ASR</span>
                    
                    <span class="tag tag-keyword">Medical Transcription</span>
                    
                    <span class="tag tag-keyword">Professional Speech</span>
                    
                    <span class="tag tag-keyword">Benchmarking</span>
                    
                    <span class="tag tag-keyword">Word Error Rate (WER)</span>
                    
                    <span class="tag tag-keyword">Entity Recognition</span>
                    
                    <span class="tag tag-keyword">Context-Utilization Gap (CUG)</span>
                    
                    <span class="tag tag-keyword">High-Stakes Applications</span>
                    
                    <span class="tag tag-keyword">AI in Healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.
  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench
  Code: https://github.com/prdeepakbabu/ProfASR-Bench</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Benchmark dataset and evaluation suite. Data and code available at: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench https://github.com/prdeepakbabu/ProfASR-Bench</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>