<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whisper Leak: a side-channel attack on Large Language Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces Whisper Leak, a novel side-channel attack that infers user prompt topics from encrypted Large Language Model (LLM) traffic by analyzing pa">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Whisper Leak: a side-channel attack on Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03675v1" target="_blank">2511.03675v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Geoff McDonald, Jonathan Bar Or
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CR, cs.AI, K.4.1; C.2.0; K.6.5; I.2.7
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03675v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03675v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Whisper Leak, a novel side-channel attack that infers user prompt topics from encrypted Large Language Model (LLM) traffic by analyzing packet size and timing patterns in streaming responses. Despite TLS encryption, these metadata patterns leak sufficient information, enabling near-perfect topic classification across 28 popular LLMs. The vulnerability poses significant privacy risks for users in sensitive domains like healthcare, as current mitigation strategies offer only partial protection.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>LLMs are increasingly adopted in healthcare for sensitive tasks such as medical record summarization, clinical decision support, and patient communication. Whisper Leak means that even encrypted interactions with these LLMs could expose the *topic* of medical queries or patient data to network adversaries, compromising patient privacy and potentially violating HIPAA or other health data regulations.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The attack affects the privacy and security of any Large Language Model deployed within medicine or healthcare. This includes applications such as AI assistants for clinicians, diagnostic support systems, patient information retrieval systems, administrative tools handling patient data, or personalized health advice systems where user prompts might contain sensitive medical information.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Whisper Leak is a side-channel attack that exploits metadata (packet size and timing patterns) in encrypted LLM streaming responses to infer user prompt topics.</li>
                    
                    <li>The attack successfully infers topic information even though the communication content is protected by TLS encryption.</li>
                    
                    <li>It was demonstrated against 28 popular LLMs from major providers, revealing an industry-wide vulnerability.</li>
                    
                    <li>The attack achieves high performance, with often >98% AUPRC for topic classification and 100% precision for identifying specific sensitive topics (e.g., 'money laundering') under extreme class imbalance.</li>
                    
                    <li>This vulnerability creates significant privacy risks for users under network surveillance by adversaries (ISPs, governments) in sensitive applications like healthcare and legal services.</li>
                    
                    <li>Three mitigation strategies (random padding, token batching, packet injection) were evaluated; while they reduce attack effectiveness, none provide complete protection.</li>
                    
                    <li>The research underscores the urgent need for LLM providers to develop more robust solutions to address metadata leakage, particularly as AI handles increasingly sensitive information.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The researchers developed a side-channel attack named Whisper Leak that monitors and analyzes network traffic metadata, specifically packet size and timing patterns, generated during streaming responses from LLMs. By observing these patterns, the attack infers the topic of the user's prompt, effectively classifying it despite TLS encryption of the content. They tested this method against 28 widely used LLMs and evaluated the effectiveness of various mitigation strategies.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The Whisper Leak attack can achieve near-perfect topic classification (often >98% AUPRC) of user prompts based solely on encrypted LLM traffic metadata. It demonstrates high precision, including 100% precision for identifying specific sensitive topics, even with extreme class imbalance (10,000:1 noise-to-target ratio), recovering 5-20% of target conversations. This vulnerability is widespread across major LLM providers. While evaluated mitigation strategies (random padding, token batching, packet injection) can reduce the attack's efficacy, none offer complete protection.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>In a clinical setting, this vulnerability could allow unauthorized entities (e.g., internet service providers, nation-state actors) to infer highly sensitive information, such as a patient's medical condition, a clinician's diagnostic inquiry, or a research query about a specific disease, if an LLM is used. This could lead to severe privacy breaches, stigmatization, discrimination, or targeted advertising based on health data, undermining patient trust in digital health tools and violating medical confidentiality standards.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract explicitly states that the evaluated mitigation strategies‚Äîrandom padding, token batching, and packet injection‚Äîwhile reducing attack effectiveness, do not provide complete protection against Whisper Leak, indicating a current lack of a fully robust defense.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The research highlights an urgent need for LLM providers to develop and implement more comprehensive and effective solutions to address metadata leakage in their systems. This includes exploring novel architectural designs or encryption techniques that can fully obscure information conveyed through packet sizes and timing, especially as AI systems are increasingly deployed in highly sensitive information environments.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Electronic Health Records (EHR) management</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Patient-Provider Communication</span>
                    
                    <span class="tag">Medical Research</span>
                    
                    <span class="tag">Mental Health Services</span>
                    
                    <span class="tag">Health Information Exchange</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Side-channel attack</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Privacy</span>
                    
                    <span class="tag tag-keyword">Network surveillance</span>
                    
                    <span class="tag tag-keyword">Metadata leakage</span>
                    
                    <span class="tag tag-keyword">Packet analysis</span>
                    
                    <span class="tag tag-keyword">Topic inference</span>
                    
                    <span class="tag tag-keyword">Healthcare privacy</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential communications, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted LLM traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular LLMs
from major providers, achieving near-perfect classification (often >98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like "money laundering" while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for LLM providers to address metadata leakage as AI systems handle
increasingly sensitive information.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>14 pages, 7 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>