<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whisper Leak: a side-channel attack on Large Language Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces Whisper Leak, a side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Whisper Leak: a side-channel attack on Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03675v1" target="_blank">2511.03675v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Geoff McDonald, Jonathan Bar Or
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CR, cs.AI, K.4.1; C.2.0; K.6.5; I.2.7
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03675v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03675v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Whisper Leak, a side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns in streaming responses. It achieves near-perfect classification (>98% AUPRC) across 28 popular LLMs, even identifying sensitive topics with 100% precision. The attack highlights a significant, industry-wide metadata leakage vulnerability, posing risks for users under network surveillance, with evaluated mitigation strategies proving incomplete.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Large Language Models are being deployed in sensitive healthcare domains for tasks like diagnostics, treatment planning, and patient communication. This attack directly threatens patient privacy by inferring sensitive medical conditions, diagnoses, or treatment queries from encrypted traffic, thus compromising confidentiality in critical medical applications.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is critical for medical AI applications as it addresses a fundamental security vulnerability in Large Language Models (LLMs), which are increasingly being integrated into health AI tools. These applications include clinical decision support, patient education and interaction platforms, mental health chatbots, medical research analysis, and administrative assistants. A side-channel attack like 'Whisper Leak' could compromise the privacy of sensitive medical inquiries, patient data analyses, or confidential research discussions conducted via these AI systems, undermining their utility and trustworthiness in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Whisper Leak is a novel side-channel attack specifically designed to infer user prompt topics from Large Language Model (LLM) interactions.</li>
                    
                    <li>The attack exploits metadata leakage by analyzing packet size and timing patterns in streaming LLM responses, effectively bypassing TLS encryption which protects content but not traffic characteristics.</li>
                    
                    <li>It was successfully demonstrated across 28 popular LLMs from major providers, indicating an industry-wide vulnerability.</li>
                    
                    <li>Whisper Leak achieves near-perfect classification performance (often >98% AUPRC) and high precision, even at extreme class imbalance (10,000:1 noise-to-target ratio), identifying sensitive topics with 100% precision for many models.</li>
                    
                    <li>The attack can recover 5-20% of target conversations and poses significant risks for users under network surveillance by ISPs, governments, or local adversaries.</li>
                    
                    <li>Three mitigation strategies (random padding, token batching, packet injection) were evaluated; while each reduced attack effectiveness, none provided complete protection.</li>
                    
                    <li>The authors engaged in responsible disclosure with providers, underscoring the urgent need for LLM developers to address metadata leakage as these systems handle increasingly sensitive information.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The researchers developed and implemented Whisper Leak, a side-channel attack that monitors and analyzes network traffic metadata, specifically packet size and timing patterns, during the streaming responses of LLMs. They then used these observed patterns to train machine learning classifiers to infer the topic of the user's initial prompt. The attack's effectiveness was rigorously evaluated against 28 popular LLMs using metrics such as AUPRC (Area Under the Precision-Recall Curve) and precision, including scenarios with high class imbalance.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The Whisper Leak attack successfully infers user prompt topics with near-perfect classification (often >98% AUPRC) across 28 leading LLMs from major providers. It demonstrated high precision, even in challenging scenarios (10,000:1 noise-to-target ratio), identifying sensitive topics (e.g., 'money laundering') with 100% precision for many models while recovering 5-20% of target conversations. Critically, common mitigation strategies like padding and batching were found to reduce but not eliminate the attack's effectiveness, indicating a persistent, industry-wide vulnerability.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This vulnerability poses a significant clinical threat, as it could expose highly sensitive patient data and medical inquiries even when supposedly protected by encryption. For instance, if a clinician or patient uses an LLM for advice on a rare disease, a mental health issue, or a sensitive medical condition, the topic of their query could be inferred by network observers. This metadata leakage could lead to breaches of patient confidentiality, potential discrimination, targeted advertising based on health status, or even coercion, severely eroding trust in AI-powered healthcare tools and hindering their ethical and secure adoption in clinical practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract explicitly states that while three evaluated mitigation strategies (random padding, token batching, and packet injection) reduce the effectiveness of the Whisper Leak attack, *none provides complete protection*. This indicates a current limitation in fully securing LLM traffic against such side-channel attacks, leaving a residual vulnerability despite implemented countermeasures.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper strongly emphasizes the need for LLM providers to proactively address metadata leakage. This suggests future research and development will focus on designing and implementing more robust, comprehensive countermeasures and potentially re-architecting LLM communication protocols to fully prevent side-channel inference from network traffic patterns. Continued collaboration between researchers and providers will be crucial to securing sensitive AI applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Electronic Health Records (EHR) systems</span>
                    
                    <span class="tag">Clinical decision support</span>
                    
                    <span class="tag">Patient privacy and data security</span>
                    
                    <span class="tag">Mental health support (AI chatbots)</span>
                    
                    <span class="tag">Medical research data analytics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">side-channel attack</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">privacy</span>
                    
                    <span class="tag tag-keyword">metadata leakage</span>
                    
                    <span class="tag tag-keyword">network surveillance</span>
                    
                    <span class="tag tag-keyword">TLS encryption</span>
                    
                    <span class="tag tag-keyword">packet analysis</span>
                    
                    <span class="tag tag-keyword">healthcare security</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential communications, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted LLM traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular LLMs
from major providers, achieving near-perfect classification (often >98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like "money laundering" while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for LLM providers to address metadata leakage as AI systems handle
increasingly sensitive information.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>14 pages, 7 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>