<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whisper Leak: a side-channel attack on Large Language Models - Health AI Hub</title>
    <meta name="description" content="Whisper Leak is a novel side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns in streaming ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Whisper Leak: a side-channel attack on Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03675v1" target="_blank">2511.03675v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Geoff McDonald, Jonathan Bar Or
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CR, cs.AI, K.4.1; C.2.0; K.6.5; I.2.7
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03675v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03675v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Whisper Leak is a novel side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns in streaming responses. This attack achieves near-perfect classification (often >98% AUPRC) across 28 popular LLMs, even identifying specific sensitive content with 100% precision for certain topics. The findings highlight a significant industry-wide vulnerability that poses severe privacy risks for users in sensitive domains like healthcare, despite TLS encryption.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This attack directly impacts the medical field, as LLMs are increasingly deployed in healthcare for sensitive tasks. It compromises patient and medical professional privacy by allowing inference of confidential health-related prompt topics from seemingly secure, encrypted LLM interactions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research paper directly addresses a security vulnerability (side-channel attack) in Large Language Models (LLMs), which are a core component of medical AI applications. When LLMs are used in healthcare for tasks such as patient interaction, clinical documentation, diagnostic support, or mental health assistance, this attack could leak sensitive information about patient conditions, diagnoses, treatments, or research topics through metadata analysis, despite encryption. This compromises the privacy and confidentiality essential for healthcare AI applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Whisper Leak is a side-channel attack designed to infer user prompt topics from encrypted LLM communications.</li>
                    
                    <li>The attack exploits metadata patterns, specifically packet size and timing, in streaming LLM responses, bypassing TLS content encryption.</li>
                    
                    <li>It was successfully demonstrated across 28 popular Large Language Models from major providers, showing widespread vulnerability.</li>
                    
                    <li>The attack achieves high efficacy, with near-perfect classification (often >98% AUPRC) and high precision even at extreme class imbalance (10,000:1 noise-to-target ratio).</li>
                    
                    <li>For many models, 100% precision was achieved in identifying sensitive topics (e.g., 'money laundering'), recovering 5-20% of target conversations.</li>
                    
                    <li>This industry-wide vulnerability creates significant privacy risks for users under network surveillance by adversaries like ISPs, governments, or local entities.</li>
                    
                    <li>Evaluated mitigation strategies (random padding, token batching, packet injection) reduce attack effectiveness but none provide complete protection, underscoring the urgent need for robust countermeasures.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The researchers developed and implemented the Whisper Leak attack by analyzing network traffic, specifically focusing on packet size and timing patterns observed during streaming responses from 28 diverse LLMs. They used these metadata fingerprints, which are exposed even through TLS encryption, to train classification models capable of accurately inferring the underlying topics of user prompts.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The Whisper Leak attack achieved near-perfect classification (often >98% AUPRC) of user prompt topics across numerous LLMs. It demonstrated robust performance, maintaining high precision even with significant class imbalance, and remarkably achieved 100% precision in identifying specific sensitive topics, recovering a notable percentage (5-20%) of relevant conversations. Crucially, existing and proposed mitigation strategies were found to be insufficient for complete protection.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact is substantial, as it jeopardizes the confidentiality of patient data and medical advice. If patients or clinicians use LLMs for symptom checkers, diagnosis assistance, treatment inquiries, or other sensitive health communications, this attack could allow unauthorized entities to infer specific medical conditions, personal health details, or even sensitive legal/ethical discussions, leading to severe privacy breaches, discrimination, or legal liabilities.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>While the paper evaluates several mitigation strategies (random padding, token batching, packet injection), it explicitly states that none provide complete protection against the Whisper Leak attack. This indicates a significant current limitation in securing LLM communications against metadata leakage.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper strongly emphasizes the necessity for LLM providers to proactively address metadata leakage. Future research and development should focus on creating more robust and comprehensive privacy-preserving communication protocols and architectural changes that go beyond current partial mitigations to effectively prevent side-channel attacks on sensitive AI interactions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Patient Consultation AI</span>
                    
                    <span class="tag">Medical Research</span>
                    
                    <span class="tag">Mental Health Support via AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM</span>
                    
                    <span class="tag tag-keyword">side-channel attack</span>
                    
                    <span class="tag tag-keyword">metadata leakage</span>
                    
                    <span class="tag tag-keyword">privacy</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">TLS bypass</span>
                    
                    <span class="tag tag-keyword">network surveillance</span>
                    
                    <span class="tag tag-keyword">packet analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential communications, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted LLM traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular LLMs
from major providers, achieving near-perfect classification (often >98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like "money laundering" while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for LLM providers to address metadata leakage as AI systems handle
increasingly sensitive information.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>14 pages, 7 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>