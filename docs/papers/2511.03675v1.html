<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whisper Leak: a side-channel attack on Large Language Models - Health AI Hub</title>
    <meta name="description" content="The paper introduces "Whisper Leak," a novel side-channel attack that infers user prompt topics from encrypted Large Language Model (LLM) traffic by analyzing p">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Whisper Leak: a side-channel attack on Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03675v1" target="_blank">2511.03675v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Geoff McDonald, Jonathan Bar Or
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CR, cs.AI, K.4.1; C.2.0; K.6.5; I.2.7
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03675v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03675v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">The paper introduces "Whisper Leak," a novel side-channel attack that infers user prompt topics from encrypted Large Language Model (LLM) traffic by analyzing packet size and timing patterns. Despite TLS encryption, this metadata leakage allows for highly accurate topic classification across 28 popular LLMs, posing significant privacy risks for users in sensitive domains like healthcare. While some mitigation strategies were evaluated, none offered complete protection against this industry-wide vulnerability.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>LLMs are increasingly integrated into healthcare for sensitive tasks like patient consultations, medical record analysis, and confidential communications. This attack compromises patient privacy by enabling external adversaries to infer the topics of medical queries or sensitive health information being processed by LLMs, even when the communication is otherwise encrypted, thereby exposing personal health data.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is directly applicable to ensuring the secure and private deployment of Large Language Models in healthcare settings. This includes LLMs used for tasks such as medical record summarization, patient query answering, diagnostic assistance, physician training, and confidential patient-provider communication, where the protection of sensitive health information is critical.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Whisper Leak is a side-channel attack designed to infer the topic of user prompts interacting with LLMs.</li>
                    
                    <li>The attack exploits metadata (packet size and timing patterns) in encrypted streaming LLM responses, bypassing TLS content encryption.</li>
                    
                    <li>It was successfully demonstrated across 28 popular LLMs from major providers, indicating an industry-wide vulnerability.</li>
                    
                    <li>The attack achieves high classification performance, often exceeding 98% AUPRC, and maintains high precision even at extreme class imbalance (10,000:1 noise-to-target ratio).</li>
                    
                    <li>For many models, sensitive topics (e.g., "money laundering") were identified with 100% precision, recovering 5-20% of targeted conversations.</li>
                    
                    <li>This vulnerability creates significant privacy and security risks for users under network surveillance by adversaries such as ISPs, governments, or local entities.</li>
                    
                    <li>Evaluated mitigation strategies (random padding, token batching, packet injection) partially reduce attack effectiveness but fail to provide complete protection.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors developed the Whisper Leak attack by observing and analyzing packet size and timing patterns within encrypted network traffic generated by LLM streaming responses. These patterns, which implicitly correlate with the topic and content being generated by the LLM, were used to train a classifier. The effectiveness of this classifier in inferring user prompt topics was evaluated against 28 diverse LLMs, using metrics like AUPRC and precision under varying conditions, including extreme class imbalance. They also tested the efficacy of three mitigation strategies: random padding, token batching, and packet injection.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The Whisper Leak attack consistently achieved near-perfect topic classification (often >98% AUPRC) across 28 popular LLMs, demonstrating its widespread applicability. It maintained high precision even when identifying rare target topics amidst vast noise (10,000:1 ratio), and notably, achieved 100% precision for sensitive topics in many models, recovering a significant portion of relevant conversations (5-20%). The study also found that proposed mitigation strategies, while reducing effectiveness, did not offer complete protection against the attack.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact is severe, as it undermines the confidentiality essential for trust in healthcare. If LLMs are used for sensitive tasks such as diagnosing conditions, discussing treatment options, or providing mental health support, an adversary could infer highly private patient information (e.g., a specific illness, a mental health condition, drug interactions) through network surveillance. This could lead to severe privacy breaches, stigmatization, discrimination, or even targeted exploitation of patients, eroding confidence in AI-powered health solutions and potentially deterring individuals from seeking necessary medical advice via these platforms.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract highlights that the evaluated mitigation strategies (random padding, token batching, and packet injection) were found to only reduce the attack's effectiveness and did not provide complete protection. This implies that fully robust countermeasures against Whisper Leak are currently lacking or unproven, leaving the vulnerability partially exposed even with initial attempts at remediation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings underscore an urgent need for LLM providers to prioritize and address metadata leakage comprehensively, especially as these AI systems handle increasingly sensitive information. Future research should focus on developing and implementing more robust, possibly novel, mitigation strategies that can provide complete protection against side-channel attacks targeting LLM traffic, moving beyond the partial effectiveness of current countermeasures.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Electronic Health Records (EHR)</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Mental Health Support</span>
                    
                    <span class="tag">Patient-Provider Communication</span>
                    
                    <span class="tag">Medical Research</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Side-channel attack</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">privacy</span>
                    
                    <span class="tag tag-keyword">metadata leakage</span>
                    
                    <span class="tag tag-keyword">TLS encryption</span>
                    
                    <span class="tag tag-keyword">healthcare privacy</span>
                    
                    <span class="tag tag-keyword">prompt inference</span>
                    
                    <span class="tag tag-keyword">network surveillance</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential communications, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted LLM traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular LLMs
from major providers, achieving near-perfect classification (often >98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like "money laundering" while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for LLM providers to address metadata leakage as AI systems handle
increasingly sensitive information.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>14 pages, 7 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>