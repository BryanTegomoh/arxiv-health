<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ask What Your Country Can Do For You: Towards a Public Red Teaming Model - Health AI Hub</title>
    <meta name="description" content="AI systems have the potential to produce both benefits and harms, but without
rigorous and ongoing adversarial evaluation, AI actors will struggle to assess
the">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Ask What Your Country Can Do For You: Towards a Public Red Teaming Model</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20061v1" target="_blank">2510.20061v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-22
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Wm. Matthew Kennedy, Cigdem Patlak, Jayraj Dave, Blake Chambers, Aayush Dhanotiya, Darshini Ramiah, Reva Schwartz, Jack Hagen, Akash Kundu, Mouni Pendharkar, Liam Baisley, Theodora Skeadas, Rumman Chowdhury
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CY, cs.AI, cs.CR
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.85 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20061v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20061v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">AI systems have the potential to produce both benefits and harms, but without
rigorous and ongoing adversarial evaluation, AI actors will struggle to assess
the breadth and magnitude of the AI risk surface. Researchers from the field of
systems design have developed several effective sociotechnical ...</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Medical/health related research</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research addresses the critical need for robust evaluation and red-teaming methodologies for AI systems, particularly those deployed in healthcare. It aims to identify and mitigate harms (e.g., bias, mis/disinformation, security vulnerabilities) in AI applications used across various medical domains, such as diagnostics, treatment planning, health operations, and patient care, thereby contributing to the development of responsible and secure medical AI.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>See abstract for details</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>See paper for methodology</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>See abstract</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Potential clinical applications</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not analyzed</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not analyzed</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">cs.CY</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">cs.CY</span>
                    
                    <span class="tag tag-keyword">cs.AI</span>
                    
                    <span class="tag tag-keyword">cs.CR</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">AI systems have the potential to produce both benefits and harms, but without
rigorous and ongoing adversarial evaluation, AI actors will struggle to assess
the breadth and magnitude of the AI risk surface. Researchers from the field of
systems design have developed several effective sociotechnical AI evaluation
and red teaming techniques targeting bias, hate speech, mis/disinformation, and
other documented harm classes. However, as increasingly sophisticated AI
systems are released into high-stakes sectors (such as education, healthcare,
and intelligence-gathering), our current evaluation and monitoring methods are
proving less and less capable of delivering effective oversight.
  In order to actually deliver responsible AI and to ensure AI's harms are
fully understood and its security vulnerabilities mitigated, pioneering new
approaches to close this "responsibility gap" are now more urgent than ever. In
this paper, we propose one such approach, the cooperative public AI red-teaming
exercise, and discuss early results of its prior pilot implementations. This
approach is intertwined with CAMLIS itself: the first in-person public
demonstrator exercise was held in conjunction with CAMLIS 2024. We review the
operational design and results of this exercise, the prior National Institute
of Standards and Technology (NIST)'s Assessing the Risks and Impacts of AI
(ARIA) pilot exercise, and another similar exercise conducted with the
Singapore Infocomm Media Development Authority (IMDA). Ultimately, we argue
that this approach is both capable of delivering meaningful results and is also
scalable to many AI developing jurisdictions.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>