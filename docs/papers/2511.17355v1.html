<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification - Health AI Hub</title>
    <meta name="description" content="This paper introduces UAM, a novel Unified Attention-Mamba backbone designed for cell-level tumor classification using radiomics features from H&E images, addre">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.17355v1" target="_blank">2511.17355v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-21
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Taixi Chen, Jingyun Chen, Nancy Guo
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.17355v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.17355v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces UAM, a novel Unified Attention-Mamba backbone designed for cell-level tumor classification using radiomics features from H&E images, addressing the gap in dedicated backbones for radiomics data. The multimodal UAM framework flexibly combines Attention and Mamba architectures, achieving state-of-the-art performance in both cell classification and image segmentation. It significantly improves cell classification accuracy from 74% to 78% and tumor segmentation precision from 75% to 80% on public benchmarks.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for advancing precision medicine in oncology by enabling more accurate and interpretable AI-driven diagnosis at the cellular level. By providing fine-grained insights into tumor phenotypes, it can significantly enhance diagnostic accuracy and assist pathologists in identifying diagnostically relevant cells.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research proposes a novel AI backbone (Unified Attention-Mamba, UAM) and a multimodal framework designed to enhance the accuracy of cell-level tumor classification and image segmentation in histopathological images. This application aims to improve precision in cancer diagnosis, support pathologists in identifying diagnostically relevant cells, and contribute to more interpretable AI systems in medical imaging analysis.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the underexplored area of cell-level radiomics analysis for tumor classification on H&E images, moving beyond traditional slide or patch-level approaches.</li>
                    
                    <li>Introduces UAM (Unified Attention-Mamba), a novel backbone specifically designed for radiomics data that flexibly integrates Attention and Mamba capabilities, eliminating manual ratio tuning.</li>
                    
                    <li>Proposes a multimodal UAM framework capable of jointly performing fine-grained cell-level classification and image segmentation of tumor regions.</li>
                    
                    <li>Achieves state-of-the-art performance on public benchmarks, surpassing leading image-based foundation models in both tasks.</li>
                    
                    <li>Demonstrates a significant improvement in cell classification accuracy from 74% to 78% across a large dataset of 349,882 cells.</li>
                    
                    <li>Shows enhanced tumor segmentation precision from 75% to 80% when evaluated on 406 image patches.</li>
                    
                    <li>Highlights UAM's potential as a unified, extensible, and effective multimodal foundation for AI-driven cancer diagnosis based on micro-level features.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study proposes a Unified Attention-Mamba (UAM) backbone, which is a novel deep learning architecture that flexibly combines Attention and Mamba modules within a single cohesive design, eliminating the need for manual tuning of their integration ratios. Two variants of UAM are developed. This backbone is then extended into a multimodal UAM framework, designed to perform both cell-level tumor classification and image segmentation simultaneously. The framework processes radiomics features extracted from hematoxylin and eosin (H&E) stained images and is evaluated against leading image-based foundation models on public benchmarks.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The UAM framework achieved state-of-the-art performance, significantly improving diagnostic capabilities: cell classification accuracy increased from 74% to 78% (on $n$=349,882 cells), and tumor segmentation precision improved from 75% to 80% (on $n$=406 patches). These results surpassed those of leading image-based foundation models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The UAM framework holds significant potential for clinical impact by providing pathologists with more precise and interpretable AI tools for cancer diagnosis. Its ability to accurately classify tumor cells and segment tumor regions at a fine-grained level could lead to earlier, more consistent, and more accurate diagnoses, ultimately improving patient management and outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any specific limitations of the UAM framework or the experimental setup presented in this paper.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper suggests that UAM serves as a 'unified and extensible multimodal foundation for radiomics-driven cancer diagnosis,' implying future research could involve applying it to a broader range of cancer types, integrating additional data modalities, or exploring its utility in prognostic or therapeutic response prediction.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Computational Pathology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">tumor cell classification</span>
                    
                    <span class="tag tag-keyword">radiomics</span>
                    
                    <span class="tag tag-keyword">H&E images</span>
                    
                    <span class="tag tag-keyword">Attention-Mamba</span>
                    
                    <span class="tag tag-keyword">multimodal framework</span>
                    
                    <span class="tag tag-keyword">image segmentation</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">computational pathology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>