<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making - Health AI Hub</title>
    <meta name="description" content="UCAgents is a hierarchical multi-agent framework designed to improve the reliability and efficiency of Vision-Language Models (VLMs) in medical diagnosis by add">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.02485v1" target="_blank">2512.02485v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-02
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Qianhan Feng, Zhongzhen Huang, Yakun Zhu, Xiaofan Zhang, Qi Dou
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.02485v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.02485v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">UCAgents is a hierarchical multi-agent framework designed to improve the reliability and efficiency of Vision-Language Models (VLMs) in medical diagnosis by addressing reasoning detachment and textual noise. It enforces unidirectional convergence through structured evidence auditing, formalizing a dual visual-textual noise bottleneck via information theory to suppress rhetorical drift. The framework achieves superior diagnostic accuracy (+6.0% on PathVQA) with 87.7% lower token cost, confirming its diagnostic reliability and computational efficiency critical for real-world clinical deployment.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research significantly enhances the trustworthiness and practical applicability of AI in medical diagnosis by ensuring VLM explanations are anchored to visual evidence, mitigating the critical issue of 'reasoning detachment.' This boosts diagnostic reliability and efficiency, enabling clinicians to adopt AI tools with greater confidence for improved patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application described is an enhanced multi-agent Vision-Language Model (VLM) system designed to improve the accuracy, reliability, and trustworthiness of medical diagnosis. It achieves this by enforcing a 'unidirectional convergence' of reasoning, strictly anchoring AI explanations and decisions to visual evidence, mimicking clinical workflows for evidence auditing, and mitigating reasoning detachment and textual noise. The ultimate goal is to provide a more dependable AI diagnostic aid for clinical deployment.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses 'reasoning detachment' in VLMs where explanations drift from verifiable image evidence, and mitigates textual noise/computational cost in existing multi-agent medical diagnostic frameworks.</li>
                    
                    <li>Proposes UCAgents, a hierarchical multi-agent framework that enforces 'unidirectional convergence' through structured evidence auditing to anchor reasoning to visual evidence.</li>
                    
                    <li>Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, which suppresses rhetorical drift and amplifies visual signal extraction.</li>
                    
                    <li>Introduces a one-round inquiry discussion specifically designed to uncover and mitigate potential risks of visual-textual misalignment.</li>
                    
                    <li>Formalizes the problem of joint visual ambiguity and textual noise as a 'dual-noise bottleneck' via information theory, which UCAgents' design aims to constrain.</li>
                    
                    <li>Achieves superior diagnostic accuracy, evidenced by a 71.3% accuracy on PathVQA (a 6.0% improvement over state-of-the-art methods).</li>
                    
                    <li>Demonstrates significant computational efficiency, with an 87.7% lower token cost compared to previous multi-agent approaches, while striking a balance between visual evidence extraction and avoiding textual interference.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>UCAgents employs a hierarchical multi-agent framework, mimicking clinical workflows, to achieve unidirectional convergence via structured evidence auditing. It enforces strict interaction protocols by forbidding position changes and limiting agent discussions to targeted evidence verification. A key component is a 'one-round inquiry discussion' designed to identify visual-textual misalignments. The underlying design principle to constrain visual ambiguity and textual noise (a 'dual-noise bottleneck') is formally described using information theory. The framework's performance was rigorously evaluated on four medical Visual Question Answering (VQA) benchmarks.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>UCAgents achieved superior diagnostic accuracy, reaching 71.3% on PathVQA, which represents a 6.0% improvement over existing state-of-the-art methods. Crucially, it also demonstrated significantly higher computational efficiency with an 87.7% reduction in token cost. The evaluation confirmed its ability to effectively balance the extraction of visual evidence with the avoidance of confusing textual interference, leading to enhanced diagnostic reliability.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The enhanced diagnostic accuracy, reliability, and computational efficiency of UCAgents hold substantial clinical impact. By ensuring VLM reasoning is explicitly anchored to visual evidence, it directly addresses a major barrier to AI adoption in medicine ‚Äì lack of trust due to inexplicable or detached reasoning. This fosters greater clinician confidence in AI decision support systems, potentially leading to more accurate diagnoses, especially in medical image interpretation, and more efficient clinical workflows due to reduced computational overhead.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily positions UCAgents as a solution overcoming the limitations of prior VLM and multi-agent frameworks (e.g., reasoning detachment, textual noise, computational cost). It does not explicitly state specific limitations inherent to the UCAgents framework itself.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions for UCAgents.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Medicine</span>
                    
                    <span class="tag">Clinical Decision Support Systems</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Vision-Language Models</span>
                    
                    <span class="tag tag-keyword">Multi-agent Systems</span>
                    
                    <span class="tag tag-keyword">Medical Diagnosis</span>
                    
                    <span class="tag tag-keyword">Visual Question Answering</span>
                    
                    <span class="tag tag-keyword">Reasoning Detachment</span>
                    
                    <span class="tag tag-keyword">Unidirectional Convergence</span>
                    
                    <span class="tag tag-keyword">Evidence Auditing</span>
                    
                    <span class="tag tag-keyword">Clinical Decision Support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>