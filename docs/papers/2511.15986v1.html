<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection - Health AI Hub</title>
    <meta name="description" content="This paper addresses the critical issue of fairness across demographic groups in medical image reasoning using Multimodal Large Language Models (MLLMs), proposi">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.15986v1" target="_blank">2511.15986v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Dawei Li, Zijian Gu, Peng Wang, Chuhan Song, Zhen Tan, Mohan Zhang, Tianlong Chen, Yu Tian, Song Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.CY, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.15986v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.15986v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper addresses the critical issue of fairness across demographic groups in medical image reasoning using Multimodal Large Language Models (MLLMs), proposing a novel tuning-free approach. The authors introduce Fairness-Aware Demonstration Selection (FADS), which constructs demographically balanced and semantically relevant in-context learning demonstrations using clustering-based sampling. FADS is shown to consistently reduce gender, race, and ethnicity disparities on medical imaging benchmarks while maintaining high accuracy, offering a scalable solution for equitable medical AI.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Fairness in medical AI is paramount, as algorithmic bias can lead to unequal access to care, misdiagnosis, and poorer health outcomes for underrepresented demographic groups. This research directly addresses these disparities, aiming to build more equitable diagnostic tools that perform reliably across all patients, regardless of gender, race, or ethnicity.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development of robust, fair, and equitable artificial intelligence models (Multimodal Large Language Models) for analyzing medical images and assisting in medical diagnosis, ensuring consistent and unbiased performance across diverse demographic groups (gender, race, ethnicity).</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Multimodal Large Language Models (MLLMs) demonstrate significant potential for medical image reasoning, but suffer from substantial fairness concerns across different demographic groups.</li>
                    
                    <li>Existing debiasing methods, often requiring large labeled datasets or extensive fine-tuning, are impractical for application to large-scale foundation models due to their computational and data demands.</li>
                    
                    <li>The study investigates In-Context Learning (ICL) as a lightweight, tuning-free, and practical alternative for enhancing fairness in MLLMs.</li>
                    
                    <li>Conventional demonstration selection (DS) strategies are identified as a primary cause of unfairness due to their tendency to create demographically imbalanced sets of exemplars.</li>
                    
                    <li>The paper proposes Fairness-Aware Demonstration Selection (FADS), a novel methodology that employs clustering-based sampling to create demographically balanced and semantically relevant demonstrations for ICL.</li>
                    
                    <li>Experiments on multiple medical imaging benchmarks demonstrate that FADS effectively and consistently reduces gender-, race-, and ethnicity-related disparities in MLLM predictions.</li>
                    
                    <li>Crucially, FADS achieves these fairness improvements without compromising the overall diagnostic accuracy, highlighting its efficiency and scalability for fair medical image reasoning.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The core methodology involves In-Context Learning (ICL) with a novel Fairness-Aware Demonstration Selection (FADS) strategy. FADS utilizes clustering-based sampling to construct demonstrations that are both demographically balanced (addressing gender, race, ethnicity) and semantically relevant to the target task. This approach aims to guide MLLMs to make fairer predictions without requiring model fine-tuning or large auxiliary datasets.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>FADS consistently reduces gender-, race-, and ethnicity-related disparities in MLLM performance on various medical imaging benchmarks. Importantly, these fairness improvements are achieved while maintaining, and in some cases enhancing, the overall diagnostic accuracy of the MLLMs, demonstrating its effectiveness as a debiasing strategy.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research offers a practical and scalable solution for deploying more equitable AI models in clinical settings. By mitigating demographic biases in medical image reasoning, FADS can help ensure that MLLM-powered diagnostic tools provide reliable and fair assessments for all patients, potentially reducing misdiagnosis rates and improving healthcare access and outcomes for historically underserved populations. Its tuning-free nature makes it feasible for adoption with large foundation models already in use.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail specific limitations of the proposed FADS method itself. It highlights the limitations of *existing* debiasing methods (reliance on large labeled datasets or fine-tuning), which FADS aims to overcome.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The results suggest FADS provides an efficient and scalable path toward fair medical image reasoning, indicating future work could focus on broader application across diverse medical imaging tasks, validation on larger and more varied real-world clinical datasets, and exploring its generalizability to other types of multimodal medical data beyond images.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Radiology</span>
                    
                    <span class="tag">Computational Pathology</span>
                    
                    <span class="tag">Healthcare Equity</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Fairness</span>
                    
                    <span class="tag tag-keyword">Multimodal LLMs</span>
                    
                    <span class="tag tag-keyword">In-Context Learning</span>
                    
                    <span class="tag tag-keyword">Medical Diagnosis</span>
                    
                    <span class="tag tag-keyword">Bias Mitigation</span>
                    
                    <span class="tag tag-keyword">Demonstration Selection</span>
                    
                    <span class="tag tag-keyword">Demographic Disparity</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>10 pages (including 2 pages of references), 4 figures. This work explores fairness in multi-modal medical image reasoning using in-context learning</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>