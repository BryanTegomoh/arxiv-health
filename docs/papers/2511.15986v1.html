<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection - Health AI Hub</title>
    <meta name="description" content="Multimodal large language models (MLLMs) show promise in medical image reasoning, but demographic fairness remains a critical concern. This paper introduces Fai">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.15986v1" target="_blank">2511.15986v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Dawei Li, Zijian Gu, Peng Wang, Chuhan Song, Zhen Tan, Mohan Zhang, Tianlong Chen, Yu Tian, Song Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.CY, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.15986v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.15986v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Multimodal large language models (MLLMs) show promise in medical image reasoning, but demographic fairness remains a critical concern. This paper introduces Fairness-Aware Demonstration Selection (FADS), a lightweight, tuning-free In-Context Learning (ICL) strategy that employs clustering-based sampling to create demographically balanced and semantically relevant demonstrations. FADS consistently reduces gender, race, and ethnicity disparities across medical imaging benchmarks while maintaining strong diagnostic accuracy, presenting an efficient path towards equitable medical AI.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Ensuring fairness in AI-powered medical diagnosis is paramount to prevent exacerbating existing healthcare disparities and guaranteeing equitable, high-quality care for all demographic groups, particularly when interpreting sensitive data like medical images.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development and debiasing of multimodal large language models (MLLMs) to perform more fair and accurate medical image reasoning and diagnosis. Specifically, it proposes a method (FADS) to ensure equitable performance of AI diagnostics across different demographic groups (gender, race, ethnicity) by creating balanced demonstration sets for in-context learning, thereby improving the reliability and fairness of AI-powered diagnostic tools in healthcare settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Multimodal large language models (MLLMs) for medical image reasoning are prone to fairness issues across demographic groups.</li>
                    
                    <li>Existing debiasing methods are often impractical for foundation-scale MLLMs due to their reliance on extensive labeled datasets or computationally expensive fine-tuning.</li>
                    
                    <li>The paper explores In-Context Learning (ICL) as a lightweight, tuning-free alternative to improve fairness in MLLMs.</li>
                    
                    <li>Conventional demonstration selection (DS) strategies in ICL are identified as failing to ensure fairness due to demographic imbalance in selected exemplars.</li>
                    
                    <li>Fairness-Aware Demonstration Selection (FADS) is proposed, which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling.</li>
                    
                    <li>Experiments on multiple medical imaging benchmarks demonstrate that FADS consistently reduces gender-, race-, and ethnicity-related disparities.</li>
                    
                    <li>Crucially, FADS achieves these fairness improvements without compromising the strong diagnostic accuracy of the MLLMs.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed method, Fairness-Aware Demonstration Selection (FADS), is integrated into an In-Context Learning (ICL) framework for Multimodal Large Language Models (MLLMs). FADS addresses demographic imbalance in conventional demonstration selection by using clustering-based sampling. This technique ensures that the demonstrations provided to the MLLM are both demographically balanced (e.g., across gender, race, ethnicity) and semantically relevant to the target task, thereby fostering fair reasoning without requiring model fine-tuning or large new datasets.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>['Conventional demonstration selection strategies lead to demographic imbalance in exemplars, contributing to unfairness in MLLM-based medical diagnosis.', 'FADS effectively and consistently reduces gender-, race-, and ethnicity-related disparities in medical image reasoning tasks.', 'The fairness improvements achieved by FADS are accomplished while simultaneously maintaining the high diagnostic accuracy of the MLLMs on multiple medical imaging benchmarks.']</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research offers an efficient, scalable, and data-efficient solution for deploying more equitable AI models in clinical practice. By mitigating biases related to gender, race, and ethnicity in medical image diagnosis, FADS can help ensure that AI diagnostic tools perform reliably for all patient populations, reducing the risk of misdiagnosis or delayed treatment for underserved groups and fostering greater trust in AI-assisted healthcare decisions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The paper identifies existing debiasing methods' reliance on large labeled datasets or extensive fine-tuning as impractical limitations for foundation-scale models. While FADS aims to overcome these, the abstract does not explicitly detail potential inherent limitations of the FADS methodology itself, such as challenges with extremely rare demographic groups or the generalizability of 'semantic relevance' across highly diverse medical pathologies.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper suggests FADS as a 'scalable and data-efficient solution for equitable medical image reasoning,' implying further research into expanding its application to a wider range of medical imaging tasks and diverse MLLMs. Future work could also explore its integration into broader clinical workflows, its efficacy in addressing other dimensions of bias (e.g., socioeconomic status, age-related disparities), and its performance in complex, intersectional bias scenarios.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">General Medicine</span>
                    
                    <span class="tag">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Fairness</span>
                    
                    <span class="tag tag-keyword">In-Context Learning</span>
                    
                    <span class="tag tag-keyword">Multimodal Large Language Models</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Debiasing</span>
                    
                    <span class="tag tag-keyword">Healthcare Disparities</span>
                    
                    <span class="tag tag-keyword">AI Ethics</span>
                    
                    <span class="tag tag-keyword">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>10 pages (including 2 pages of references), 4 figures. This work explores fairness in multi-modal medical image reasoning using in-context learning</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>