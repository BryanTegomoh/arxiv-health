<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation - Health AI Hub</title>
    <meta name="description" content="This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture, specifically designed to address the suboptimality of existing backbones in large-">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.17774v1" target="_blank">2512.17774v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-19
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> eess.IV, cs.AI, cs.CV, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.17774v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.17774v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture, specifically designed to address the suboptimality of existing backbones in large-scale supervised representation learning for 3D medical image segmentation. By leveraging an improved micro-architecture and data scaling, MedNeXt-v2 achieves state-of-the-art performance across multiple CT and MR benchmarks, demonstrating the critical importance of effective backbone design beyond just dataset size.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This work significantly advances automated medical image segmentation by providing a highly effective deep learning backbone, MedNeXt-v2, which can improve the accuracy and efficiency of delineating anatomical structures and pathologies in 3D CT and MR scans. Enhanced segmentation can lead to more precise diagnoses, better surgical and radiotherapy planning, and more effective treatment monitoring in various clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This paper develops MedNeXt-v2, an advanced AI model (a scaled 3D ConvNeXt architecture) specifically designed for large-scale supervised representation learning in medical image segmentation. Its application is to accurately identify and delineate anatomical structures and pathological findings within 3D medical scans (CT and MR), which is critical for clinical decision-making, diagnosis, surgical planning, radiation therapy, and monitoring various health conditions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>MedNeXt-v2 is a compound-scaled 3D ConvNeXt architecture, integrating a 3D Global Response Normalization (GRN) module and optimized through depth, width, and context scaling.</li>
                    
                    <li>The research highlights that routinely used backbones in large-scale pretraining pipelines for 3D medical image segmentation are often suboptimal, underscoring the need for architectural improvements.</li>
                    
                    <li>A key finding is that stronger 'from scratch' backbone performance reliably predicts stronger downstream performance after large-scale pretraining.</li>
                    
                    <li>MedNeXt-v2 was pretrained on a vast dataset of 18,000 CT volumes and subsequently achieved state-of-the-art performance when fine-tuned across six challenging CT and MR benchmarks, encompassing 144 anatomical structures.</li>
                    
                    <li>Benchmarking results indicate that representation scaling disproportionately benefits the segmentation of pathological structures.</li>
                    
                    <li>The study also reveals that once full fine-tuning is applied, modality-specific pretraining offers negligible additional benefit compared to general pretraining.</li>
                    
                    <li>The code and pretrained models for MedNeXt-v2 are made publicly available with the official nnUNet repository to facilitate further research and application.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study commenced with comprehensive benchmarking of existing backbone networks for 3D volumetric segmentation to identify suboptimal architectures. It then introduced MedNeXt-v2, a 3D ConvNeXt enhanced with a 3D Global Response Normalization module and optimized via compound scaling (depth, width, and context). MedNeXt-v2 was pretrained on 18,000 CT volumes and subsequently fine-tuned and rigorously evaluated across six diverse CT and MR benchmarks, covering 144 structures. Its performance was compared against seven established, publicly released pretrained models.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The research found that current backbones often underperform in large-scale pretraining for medical image segmentation. Crucially, a backbone's 'from scratch' performance is a strong predictor of its ultimate effectiveness after pretraining. MedNeXt-v2 achieved state-of-the-art results across a wide array of CT and MR segmentation tasks. The study also highlighted that representation scaling offers particular advantages for pathological segmentation and that modality-specific pretraining provides no significant benefit over general pretraining when full fine-tuning is applied.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>MedNeXt-v2's superior segmentation capabilities have the potential to directly enhance clinical decision-making. Its improved accuracy for delineating tumors, organs, and other structures can lead to more reliable diagnostic assessments, highly precise surgical interventions, optimized radiation therapy planning to minimize damage to healthy tissue, and more objective monitoring of disease progression and treatment response, ultimately improving patient outcomes and streamlining clinical workflows.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations or caveats of the MedNeXt-v2 architecture or the study methodology itself. It primarily focuses on presenting the strengths and contributions of the proposed model.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions. However, the public release of the code and pretrained models implies an encouragement for broader adoption, validation, and further development by the scientific community, potentially including applications to new modalities or diseases.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Diagnostic Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Radiotherapy Planning</span>
                    
                    <span class="tag">Surgical Planning</span>
                    
                    <span class="tag">Medical Image Analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">3D Medical Image Segmentation</span>
                    
                    <span class="tag tag-keyword">ConvNeXt</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Representation Learning</span>
                    
                    <span class="tag tag-keyword">Pretraining</span>
                    
                    <span class="tag tag-keyword">CT Imaging</span>
                    
                    <span class="tag tag-keyword">MR Imaging</span>
                    
                    <span class="tag tag-keyword">Global Response Normalization</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>