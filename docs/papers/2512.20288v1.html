<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UbiQVision: Quantifying Uncertainty in XAI for Image Recognition - Health AI Hub</title>
    <meta name="description" content="This paper introduces UbiQVision, a novel framework designed to quantify uncertainty in SHAP-based eXplainable AI (XAI) visualizations, particularly in medical ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>UbiQVision: Quantifying Uncertainty in XAI for Image Recognition</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.20288v1" target="_blank">2512.20288v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Akshat Dubey, Aleksandar An≈æel, Bahar ƒ∞lgen, Georges Hattab
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.20288v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.20288v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces UbiQVision, a novel framework designed to quantify uncertainty in SHAP-based eXplainable AI (XAI) visualizations, particularly in medical imaging applications. It addresses the instability and unreliability of SHAP explanations, which stem from epistemic and aleatoric uncertainty in complex deep learning models, by leveraging Dirichlet posterior sampling and Dempster-Shafer theory. The framework provides uncertainty quantification through belief, plausible, and fusion maps, validated across diverse medical imaging datasets from pathology, ophthalmology, and radiology.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for increasing the trustworthiness and clinical utility of deep learning models in medical diagnostics and treatment planning by providing a method to quantify the reliability of their explanations. Understanding the uncertainty in AI-generated explanations helps clinicians make more informed decisions, especially where model complexity obscures reasoning.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research develops a framework to quantify and manage uncertainty in Explainable AI (XAI) methods, specifically SHAP, when applied to deep learning models used in medical imaging analysis. By improving the reliability and interpretability of AI explanations in areas like pathology, ophthalmology, and radiology, it enhances the trustworthiness and utility of AI systems for diagnosis and clinical support in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of unstable and unreliable SHAP explanations in deep learning models due to inherent epistemic and aleatoric uncertainty, which compromises interpretability.</li>
                    
                    <li>Proposes UbiQVision, a framework that integrates Dirichlet posterior sampling and Dempster-Shafer theory to robustly quantify uncertainty in XAI outputs.</li>
                    
                    <li>Generates interpretable 'belief, plausible, and fusion maps' to visualize and present the quantified uncertainty associated with SHAP explanations.</li>
                    
                    <li>Incorporates statistical quantitative analysis alongside the map-based approach to provide a comprehensive measure of uncertainty in SHAP outputs.</li>
                    
                    <li>Evaluated on three distinct medical imaging datasets, specifically chosen for their varying class distributions, image qualities, resolutions, and modality types, representing real-world challenges.</li>
                    
                    <li>The chosen medical imaging datasets cover critical domains including pathology, ophthalmology, and radiology, known for introducing significant epistemic uncertainty in diagnostic tasks.</li>
                    
                    <li>The ultimate goal is to enhance the reliability and trustworthiness of XAI methods, making model explanations more dependable for domain experts in high-stakes medical applications.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The UbiQVision framework quantifies uncertainty in SHAP explanations by integrating advanced probabilistic and evidential reasoning techniques: Dirichlet posterior sampling and Dempster-Shafer theory. It employs a multi-faceted approach involving the generation of 'belief maps,' 'plausible maps,' and 'fusion maps' to visualize and combine evidence of uncertainty. This is complemented by statistical quantitative analysis to provide a robust assessment of the instability and unreliability inherent in SHAP explanations.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The paper developed and evaluated UbiQVision, a framework that successfully quantifies uncertainty in SHAP explanations for complex deep learning models, particularly in medical imaging. This quantification, expressed through belief, plausible, and fusion maps, effectively addresses the instability arising from epistemic and aleatoric uncertainty, as demonstrated across diverse medical imaging datasets from pathology, ophthalmology, and radiology.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By providing a quantifiable measure of uncertainty in AI explanations, this work can significantly improve the clinical adoption and safety of AI by offering critical transparency to healthcare professionals. Clinicians can better assess the reliability of a model's 'reasoning' for a specific diagnosis, prognosis, or treatment recommendation, reducing the risk of misinterpretation of AI outputs and fostering greater confidence and trust in AI-assisted decision-making in high-stakes medical scenarios.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly list limitations of the proposed UbiQVision framework itself. It primarily focuses on addressing the limitations (instability and unreliability) of existing SHAP explanations.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions are not explicitly mentioned in the provided abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Ophthalmology</span>
                    
                    <span class="tag">Radiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Explainable AI (XAI)</span>
                    
                    <span class="tag tag-keyword">SHAP</span>
                    
                    <span class="tag tag-keyword">Uncertainty Quantification</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Dirichlet Posterior Sampling</span>
                    
                    <span class="tag tag-keyword">Dempster-Shafer Theory</span>
                    
                    <span class="tag tag-keyword">Epistemic Uncertainty</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>