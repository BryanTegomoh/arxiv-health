<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models - Health AI Hub</title>
    <meta name="description" content="This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data for robust Parkinson's disease (PD) gait recognition under re">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04425v1" target="_blank">2512.04425v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Manar Alnaasan, Md Selim Sarowar, Sungho Kim
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04425v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04425v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data for robust Parkinson's disease (PD) gait recognition under realistic conditions. It leverages advanced deep learning (YOLOv11, MLGE, fusion mechanisms) to capture subtle gait abnormalities and incorporates a Large Language Model (LLM) to translate visual findings into clinically meaningful textual explanations, thereby improving both accuracy and interpretability.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and interpretable early detection of Parkinson's disease through objective gait analysis is vital for timely diagnosis and intervention. This framework offers a robust, clinically transparent tool that could improve diagnostic confidence and lead to better patient management and outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an explainable diagnostic and monitoring tool for Parkinson's disease. It uses multimodal computer vision (RGB-D fusion) to analyze gait patterns, identifying subtle signs of the disease, and then leverages a Large Language Model to translate these visual findings into clinically interpretable textual explanations, aiding clinicians in early detection and patient management.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses critical limitations of existing PD gait analysis methods, including reliance on single-modality inputs, low robustness in varied environments, and a lack of clinical transparency or interpretability.</li>
                    
                    <li>Proposes an explainable multimodal RGB-D fusion framework designed for robust Parkinsonian gait recognition, even in challenging real-world scenarios like low lighting or partial occlusion.</li>
                    
                    <li>Employs dual YOLOv11-based encoders for efficient, modality-specific feature extraction from both RGB and Depth data streams.</li>
                    
                    <li>Utilizes a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to synthesize comprehensive spatio-temporal representations, capturing both fine-grained limb movements (e.g., reduced arm swing) and macroscopic gait dynamics (e.g., short stride, turning difficulty).</li>
                    
                    <li>Incorporates a frozen Large Language Model (LLM) to translate the fused visual embeddings and structured metadata into coherent, clinically meaningful textual explanations, enhancing the interpretability for medical professionals.</li>
                    
                    <li>Experimental evaluations on multimodal gait datasets demonstrate superior performance in recognition accuracy and robustness to environmental variations compared to single-input baseline approaches.</li>
                    
                    <li>Aims to bridge the gap between advanced visual recognition capabilities and practical clinical understanding, establishing a novel vision-language paradigm for reliable and transparent PD gait analysis.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The system employs dual YOLOv11-based encoders for parallel feature extraction from RGB and Depth video streams. These modality-specific features are then processed by a Multi-Scale Local-Global Extraction (MLGE) module and subsequently fused via a Cross-Spatial Neck Fusion mechanism to create a comprehensive spatio-temporal representation. A frozen Large Language Model (LLM) is then used to translate these fused visual embeddings, combined with structured metadata, into clinically meaningful, textual explanations.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The proposed RGB-D fusion framework achieved higher recognition accuracy for Parkinsonian gait patterns and demonstrated improved robustness to challenging environmental conditions (e.g., low lighting, occlusion) compared to single-input baselines. Crucially, it provided clear visual-linguistic reasoning, translating complex visual cues into understandable clinical explanations.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology has the potential to significantly enhance the early and objective diagnosis of Parkinson's disease by providing clinicians with a highly accurate, robust, and most importantly, interpretable diagnostic aid. The ability to generate clinically meaningful explanations could build trust in AI-driven diagnostics, facilitating earlier interventions and more personalized patient care pathways.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations or caveats for the proposed system itself, but rather highlights how this framework overcomes the limitations (single-modality, low robustness, lack of transparency) of prior existing approaches.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract, though the paper's conclusion about offering a "novel vision-language paradigm" implies continued research and development in integrating multimodal vision with advanced language models for clinical applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Movement Disorders</span>
                    
                    <span class="tag">Geriatrics</span>
                    
                    <span class="tag">Rehabilitation Medicine</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Artificial Intelligence in Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Parkinson's disease</span>
                    
                    <span class="tag tag-keyword">gait analysis</span>
                    
                    <span class="tag tag-keyword">RGB-D fusion</span>
                    
                    <span class="tag tag-keyword">multimodal learning</span>
                    
                    <span class="tag tag-keyword">explainable AI</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">clinical interpretability</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>