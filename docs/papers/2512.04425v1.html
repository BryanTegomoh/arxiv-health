<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces an explainable multimodal RGB-D fusion framework for Parkinson's Disease (PD) gait recognition, addressing limitations of single-modality ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04425v1" target="_blank">2512.04425v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Manar Alnaasan, Md Selim Sarowar, Sungho Kim
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04425v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04425v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces an explainable multimodal RGB-D fusion framework for Parkinson's Disease (PD) gait recognition, addressing limitations of single-modality inputs, low robustness, and lack of clinical interpretability in existing methods. The system leverages advanced deep learning and a Large Language Model (LLM) to achieve higher accuracy, improved robustness in challenging conditions, and provide clinically meaningful textual explanations of gait patterns. It offers a novel vision-language paradigm that bridges the gap between visual analysis and clinical understanding for PD diagnosis.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research provides a crucial advancement for the early and accurate detection of Parkinson's Disease by offering a robust, interpretable, and comprehensive gait analysis tool. Its ability to generate clinically meaningful explanations can significantly aid neurologists in diagnosis, monitoring disease progression, and tailoring treatment plans.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an explainable multimodal system for Parkinson's disease gait recognition. It utilizes computer vision (RGB-D fusion, YOLOv11) and Large Language Models to analyze gait patterns, identify Parkinsonian symptoms, and translate these visual findings into clinically meaningful textual explanations, thereby assisting in early detection, diagnosis, and potentially monitoring of Parkinson's disease patients.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Addressed**: Existing Parkinson's Disease (PD) gait analysis suffers from single-modality limitations, low robustness, and a critical lack of clinical transparency or interpretability.</li>
                    
                    <li>**Multimodal Framework**: Proposes a novel explainable multimodal framework that integrates both RGB (visual) and Depth data (RGB-D) for robust Parkinsonian gait pattern recognition under realistic, challenging conditions.</li>
                    
                    <li>**Advanced Feature Extraction**: Employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation, capturing both fine-grained limb movements (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride).</li>
                    
                    <li>**Enhanced Robustness**: The system is designed to maintain performance even in challenging environmental scenarios, such as low lighting or partial occlusion caused by clothing, improving real-world applicability.</li>
                    
                    <li>**Explainability via LLM**: A frozen Large Language Model (LLM) is incorporated to translate the fused visual embeddings and structured metadata into clinically meaningful and transparent textual explanations, addressing the interpretability gap.</li>
                    
                    <li>**Superior Performance**: Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, significantly improved robustness to environmental variations, and clear visual-linguistic reasoning compared to single-input baseline approaches.</li>
                    
                    <li>**Clinical Bridge**: The study effectively bridges the gap between complex visual recognition capabilities and practical clinical understanding, establishing a novel vision-language paradigm for reliable and explainable PD gait analysis.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed system employs dual YOLOv11-based encoders to extract distinct features from RGB and Depth data streams. These modality-specific features are then processed through a Multi-Scale Local-Global Extraction (MLGE) module to capture varying granularities of motion, followed by a Cross-Spatial Neck Fusion mechanism for comprehensive spatial-temporal representation. Crucially, a frozen Large Language Model (LLM) is integrated to convert the fused visual embeddings and associated structured metadata into human-readable, clinically meaningful textual explanations.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Experimental evaluations on multimodal gait datasets demonstrated that the proposed RGB-D fusion framework achieved superior recognition accuracy and significantly improved robustness when compared to single-input baselines. Furthermore, the system successfully provided clear visual-linguistic reasoning, translating complex gait patterns into actionable clinical insights.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This framework holds substantial potential to revolutionize the early diagnosis and ongoing management of Parkinson's Disease. By offering highly accurate, robust, and interpretable gait analysis, clinicians can gain deeper insights into specific gait anomalies, enabling earlier intervention, more precise monitoring of disease progression, and the development of personalized therapeutic strategies, ultimately improving patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the limitations of existing single-modality approaches (low robustness, lack of transparency) as the motivation for this work. It does not explicitly state specific limitations or caveats pertaining to the proposed RGB-D fusion framework itself.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While the abstract doesn't explicitly state future research directions, it positions the work as "offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis," implying its foundational nature for future advancements in AI-driven clinical decision support and multimodal interpretable medical diagnostics.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Movement Disorders</span>
                    
                    <span class="tag">Geriatrics</span>
                    
                    <span class="tag">Rehabilitation Medicine</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Parkinson's Disease</span>
                    
                    <span class="tag tag-keyword">Gait Analysis</span>
                    
                    <span class="tag tag-keyword">RGB-D Fusion</span>
                    
                    <span class="tag tag-keyword">Multimodal Learning</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">Clinical Interpretability</span>
                    
                    <span class="tag tag-keyword">Movement Disorders</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>