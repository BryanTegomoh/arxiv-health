<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces an explainable multimodal framework that integrates RGB and Depth (RGB-D) data for robust Parkinson's disease (PD) gait recognition under ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04425v1" target="_blank">2512.04425v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Manar Alnaasan, Md Selim Sarowar, Sungho Kim
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04425v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04425v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces an explainable multimodal framework that integrates RGB and Depth (RGB-D) data for robust Parkinson's disease (PD) gait recognition under realistic conditions. It leverages advanced deep learning for feature extraction and fusion, coupled with a Large Language Model (LLM) to provide clinically meaningful textual explanations, thus bridging the gap between visual analysis and medical interpretability.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research significantly enhances the early detection and ongoing monitoring of Parkinson's disease by providing an objective, robust, and, crucially, *interpretable* gait analysis tool. It allows clinicians to not only identify gait abnormalities but also understand the specific visual cues driving those findings, fostering greater trust and utility in AI-assisted diagnostics.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an explainable multimodal system that uses RGB and Depth (RGB-D) data fusion and Large Language Models (LLMs) to recognize Parkinsonian gait patterns. It aims to achieve higher recognition accuracy and provide clinically meaningful textual explanations for the early detection and monitoring of Parkinson's disease, enhancing robustness and interpretability for healthcare professionals.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the limitations of existing PD gait analysis methods, including reliance on single-modality inputs, low robustness, and a lack of clinical transparency.</li>
                    
                    <li>Proposes a multimodal RGB-D fusion framework to enhance the recognition of Parkinsonian gait patterns, improving robustness in challenging environments (e.g., low lighting, occlusion).</li>
                    
                    <li>Employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to capture comprehensive spatial-temporal gait dynamics.</li>
                    
                    <li>Specifically designed to identify both fine-grained limb movements (e.g., reduced arm swing) and overall gait patterns (e.g., short stride, turning difficulty).</li>
                    
                    <li>Integrates a frozen Large Language Model (LLM) to translate fused visual embeddings and structured metadata into clinically interpretable textual explanations, ensuring transparency.</li>
                    
                    <li>Experimental evaluations demonstrate superior recognition accuracy and improved robustness to environmental variations compared to single-input baseline approaches on multimodal gait datasets.</li>
                    
                    <li>Establishes a novel vision-language paradigm for reliable and explainable Parkinson's disease gait analysis, directly translating visual findings into actionable clinical understanding.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed system integrates RGB and Depth data, using dual YOLOv11-based encoders for modality-specific feature extraction. These features are then processed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to create enhanced spatial-temporal representations. A frozen Large Language Model (LLM) then takes these fused visual embeddings along with structured metadata to generate clinically meaningful textual explanations of the observed gait patterns.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The RGB-D fusion framework achieved higher recognition accuracy and improved robustness to environmental variations (e.g., low lighting, occlusion) compared to single-input baselines. It successfully provides clear visual-linguistic reasoning, translating complex visual gait patterns into interpretable clinical explanations.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This framework offers a significant step towards more accurate, robust, and clinically transparent diagnostic tools for Parkinson's disease. It can aid neurologists in early detection, differential diagnosis, and monitoring disease progression and treatment efficacy by providing objective, explainable gait assessments that are easily understood by medical professionals.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail limitations or caveats pertaining specifically to the proposed RGB-D fusion framework itself, beyond addressing the shortcomings of existing single-modality approaches.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The study suggests a novel vision-language paradigm for Parkinson's disease gait analysis, implying future exploration into this integrated approach to further bridge visual recognition with clinical understanding. This opens avenues for developing more comprehensive and integrated diagnostic and monitoring systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Movement Disorders</span>
                    
                    <span class="tag">Geriatrics</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Rehabilitation Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Parkinson's Disease</span>
                    
                    <span class="tag tag-keyword">Gait Analysis</span>
                    
                    <span class="tag tag-keyword">RGB-D Fusion</span>
                    
                    <span class="tag tag-keyword">Multimodal Learning</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Clinical Transparency</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>