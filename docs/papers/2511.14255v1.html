<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR - Health AI Hub</title>
    <meta name="description" content="AfriSpeech-MultiBench is presented as the first domain-specific evaluation suite for over 100 African English accents across 10+ countries, spanning seven appli">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.14255v1" target="_blank">2511.14255v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-18
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Gabrial Zencha Ashungafac, Mardhiyah Sanni, Busayo Awobade, Alex Gichamba, Tobi Olatunji
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.14255v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.14255v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">AfriSpeech-MultiBench is presented as the first domain-specific evaluation suite for over 100 African English accents across 10+ countries, spanning seven application domains including Medical. The benchmark rigorously evaluates diverse ASR and LLM-based speech recognition systems, revealing nuanced performance variations and highlighting critical challenges such as domain-specific named entity recognition and hallucinations.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This benchmark directly addresses the critical need for accurate and reliable speech recognition in medical contexts for African English accents. It enables the development of robust voice interfaces for healthcare, identifies crucial weaknesses like poor recognition of domain-specific medical terms and hallucinations, which are vital for patient safety and clinical accuracy.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research aims to improve the accuracy and robustness of Automatic Speech Recognition (ASR) for African accented English within medical applications. This directly supports the development of AI tools for healthcare such as accurate voice-to-text for clinical note-taking and dictation, improved communication in telemedicine platforms, more effective AI-powered assistants for both patients and healthcare professionals, and enhanced accessibility of health information through voice interfaces for underserved communities in Africa.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Introduction of AfriSpeech-MultiBench, the first domain-specific benchmark for over 100 African English accents across 10+ countries and seven application domains.</li>
                    
                    <li>The application domains include Finance, Legal, Medical, General dialogue, Call Center, Named Entities, and Hallucination Robustness.</li>
                    
                    <li>Benchmarking performed on a diverse range of open, closed, unimodal ASR, and multimodal LLM-based speech recognition systems.</li>
                    
                    <li>Evaluation utilizes both spontaneous and non-spontaneous speech data drawn from various open African accented English datasets.</li>
                    
                    <li>Key findings indicate open-source ASR excels in spontaneous speech but degrades on noisy non-native dialogue, while multimodal LLMs are accent-robust but struggle with domain-specific named entities.</li>
                    
                    <li>Proprietary models show high accuracy on clean speech but performance varies significantly by country and domain; fine-tuned models offer competitive accuracy with lower latency.</li>
                    
                    <li>A significant and persistent challenge across most state-of-the-art models is the generation of hallucinations.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors developed AfriSpeech-MultiBench, a benchmark suite for over 100 African English accents from 10+ countries, incorporating seven application-specific domains. They evaluated a diverse set of open and closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversations collected from various open African accented English speech datasets.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Empirical analysis revealed that open-source ASR models perform well in spontaneous speech but degrade significantly with noisy, non-native dialogue. Multimodal LLMs demonstrated greater accent robustness but struggled specifically with domain-specific named entities. Proprietary models achieved high accuracy on clean speech, but their performance varied considerably across countries and domains. Models fine-tuned on African English data achieved competitive accuracy with the added benefit of lower latency, yet hallucinations remained a pervasive problem for most state-of-the-art models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The benchmark provides essential insights for selecting and deploying appropriate voice technologies in African healthcare settings, improving the accuracy of clinical documentation, enhancing telemedicine consultations, and facilitating patient communication. By highlighting issues like poor domain-specific named entity recognition and hallucinations, it directly informs the development of safer and more effective AI tools crucial for patient care and data integrity in medicine.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract explicitly notes that multimodal LLMs struggle with domain-specific named entities, and that hallucinations remain a significant problem for most state-of-the-art models. It also points out that proprietary models' accuracy can vary significantly by country and domain.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings implicitly suggest a strong need for future research focusing on improving ASR and LLM robustness to domain-specific terminology, enhancing hallucination robustness, and developing models that perform consistently well across diverse African accents and noisy speech conditions. Further development and fine-tuning of models specifically on African English data are highlighted as a practical advantage for deployment.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Clinical Documentation</span>
                    
                    <span class="tag">Medical Transcription</span>
                    
                    <span class="tag">Patient-Provider Communication</span>
                    
                    <span class="tag">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">African English accents</span>
                    
                    <span class="tag tag-keyword">ASR</span>
                    
                    <span class="tag tag-keyword">Speech Recognition</span>
                    
                    <span class="tag tag-keyword">Multidomain Benchmark</span>
                    
                    <span class="tag tag-keyword">LLM</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Hallucinations</span>
                    
                    <span class="tag tag-keyword">Voice Interfaces</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted As a Conference Paper IJCNLP-AACL 2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>