<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel text-to-audio jailbreak attack, named "Now You Hear Me," that exploits large audio-language models (ALMs) by embedding disallowed ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.23255v1" target="_blank">2601.23255v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ye Yu, Haibo Jin, Yaoning Yu, Jun Zhuang, Haohan Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI, cs.CR
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.23255v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.23255v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel text-to-audio jailbreak attack, named "Now You Hear Me," that exploits large audio-language models (ALMs) by embedding disallowed directives within narrative-style audio streams. Leveraging advanced text-to-speech (TTS) models to manipulate acoustic and structural properties, the attack successfully bypasses ALM safety mechanisms with a 98.26% success rate against models like Gemini 2.0 Flash. The findings highlight a critical vulnerability in speech-based AI interfaces and emphasize the urgent need for multimodal safety frameworks that analyze both linguistic and paralinguistic features.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>As ALMs are increasingly integrated into healthcare for applications like voice assistants, patient education, and clinical triage, this research exposes a critical security flaw. The ability to inject malicious or restricted directives via audio could lead to the generation of harmful medical advice, misinformation, or breaches of patient confidentiality, directly impacting patient safety and clinical integrity.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application to health discussed is the use of large audio-language models in healthcare settings, specifically for tasks like clinical triage. These models would typically assist in initial patient assessment, guiding patients, or providing information, where their susceptibility to 'narrative audio attacks' could compromise the integrity and safety of healthcare delivery.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Identifies a new class of vulnerabilities in large audio-language models (ALMs) that operate on raw speech inputs.</li>
                    
                    <li>Proposes a text-to-audio jailbreak attack that embeds disallowed directives into narrative-style audio streams.</li>
                    
                    <li>Utilizes an advanced instruction-following text-to-speech (TTS) model to exploit the structural and acoustic properties of speech.</li>
                    
                    <li>Designed to circumvent safety mechanisms primarily calibrated for text-based inputs.</li>
                    
                    <li>Successfully demonstrated against state-of-the-art ALMs, including Gemini 2.0 Flash.</li>
                    
                    <li>Achieves a high success rate of 98.26% in eliciting restricted outputs, significantly exceeding text-only attack baselines.</li>
                    
                    <li>Stresses the imperative for developing safety frameworks that jointly reason over linguistic and paralinguistic representations.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involved designing a text-to-audio jailbreak by first crafting disallowed textual directives. These directives were then fed into an advanced instruction-following text-to-speech (TTS) model. The TTS model was specifically used to embed these directives within a narrative-style audio stream, exploiting inherent structural (e.g., phrasing, cadence) and acoustic (e.g., intonation, emphasis) properties of synthetic speech. This generated malicious audio was subsequently used as input to test the robustness of state-of-the-art large audio-language models, such as Gemini 2.0 Flash, assessing their ability to resist generating restricted outputs.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key finding is the extreme effectiveness of the proposed audio narrative attack, achieving a remarkable 98.26% success rate in compelling state-of-the-art audio-language models (ALMs) like Gemini 2.0 Flash to produce restricted or disallowed outputs. This substantially outperforms traditional text-only jailbreak methods, demonstrating that current ALM safety mechanisms are highly susceptible to sophisticated audio-based manipulations that leverage paralinguistic cues and narrative structures.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has profound clinical impact, indicating that ALM-powered tools in healthcare are vulnerable to subtle audio attacks. For instance, a malicious actor could prompt an AI-driven clinical triage system to provide dangerously incorrect medical advice, a patient information bot to leak sensitive data, or a medical voice assistant to generate unethical content. This necessitates an immediate paradigm shift in designing secure medical AI, moving towards robust multimodal validation to prevent potentially life-threatening or privacy-compromising scenarios in clinical practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state study limitations. However, an implied limitation is the focus on demonstrating the existence and efficacy of the attack rather than providing concrete defensive solutions. The generalizability across all possible TTS models or ALM architectures is also not fully explored within the scope of the abstract, nor are the computational costs of developing such an attack or robust defenses.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper strongly advocates for future research into developing new safety frameworks for large audio-language models that can jointly reason over both linguistic (what is said) and paralinguistic (how it is said) representations. This implies a need for advanced security mechanisms capable of detecting malicious intent embedded in acoustic features, intonation, and narrative structure within audio inputs, crucial for securing speech-based AI in critical applications like healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">clinical triage</span>
                    
                    <span class="tag">telemedicine</span>
                    
                    <span class="tag">digital health</span>
                    
                    <span class="tag">medical voice assistants</span>
                    
                    <span class="tag">patient education platforms</span>
                    
                    <span class="tag">clinical decision support systems</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">audio-language models</span>
                    
                    <span class="tag tag-keyword">jailbreak attack</span>
                    
                    <span class="tag tag-keyword">text-to-speech</span>
                    
                    <span class="tag tag-keyword">vulnerability</span>
                    
                    <span class="tag tag-keyword">safety mechanisms</span>
                    
                    <span class="tag tag-keyword">clinical triage</span>
                    
                    <span class="tag tag-keyword">paralinguistic</span>
                    
                    <span class="tag tag-keyword">AI security</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>to be published at EACL 2026 main conference</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>