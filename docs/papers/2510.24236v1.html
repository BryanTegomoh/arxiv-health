<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models? - Health AI Hub</title>
    <meta name="description" content="This paper investigates factors influencing the faithfulness of Large Language Model (LLM) explanations, a critical concern in healthcare where unfaithful reaso">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.24236v1" target="_blank">2510.24236v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-28
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Teague McMillan, Gabriele Dominici, Martin Gjoreski, Marc Langheinrich
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.24236v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.24236v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper investigates factors influencing the faithfulness of Large Language Model (LLM) explanations, a critical concern in healthcare where unfaithful reasoning can lead to unsafe decision support. The research evaluates three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on social bias (BBQ) and medical (MedQA) datasets, manipulating few-shot examples, prompting strategies, and training procedures. Key findings demonstrate that both the quantity and quality of few-shot examples, prompting design, and instruction-tuning significantly impact explanation faithfulness.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Unfaithful LLM explanations in healthcare can omit critical clinical cues or mask spurious shortcuts, thereby undermining clinician trust and leading to potentially unsafe or incorrect decision support, making this research crucial for developing reliable and trustworthy AI tools in medicine.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research focuses on improving the faithfulness and interpretability of Large Language Models (LLMs) specifically for use in healthcare settings. This directly applies to developing more trustworthy and safe AI systems for clinical decision support, medical education, and other health-related applications where understanding the AI's reasoning is critical for clinicians and patient safety.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLMs often produce explanations that lack faithfulness, which is particularly problematic in healthcare settings as it can undermine clinician trust and lead to unsafe decision support by omitting salient cues or masking spurious shortcuts.</li>
                    
                    <li>The study's primary objective was to understand how inference-time (e.g., few-shot examples, prompting) and training-time (e.g., instruction-tuning) choices, controllable by practitioners, influence explanation faithfulness.</li>
                    
                    <li>Three distinct LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) were evaluated using two datasets: BBQ (social bias) and MedQA (medical licensing questions).</li>
                    
                    <li>Key experimental variables manipulated included the number and type of few-shot examples provided, various prompting strategies, and the impact of the instruction-tuning phase of model training.</li>
                    
                    <li>Results showed that both the quantity and, crucially, the quality of few-shot examples significantly impact the measured faithfulness of LLM explanations.</li>
                    
                    <li>The faithfulness of LLM explanations was found to be sensitive to the specific design and implementation of prompting strategies.</li>
                    
                    <li>The instruction-tuning phase of the LLM training procedure positively influenced measured faithfulness, particularly when the models were evaluated on the MedQA medical licensing question dataset.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employed three distinct LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) and evaluated their explanation faithfulness on two datasets: BBQ (social bias) and MedQA (medical licensing questions). Researchers systematically manipulated three key factors: the quantity and type of few-shot examples provided during inference, various prompting strategies, and the impact of the instruction-tuning phase of the model's training procedure.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>1. Both the quantity and the quality of few-shot examples significantly impact the faithfulness of explanations produced by LLMs. 2. Explanation faithfulness is sensitive to the specific design and implementation of prompting strategies. 3. The instruction-tuning phase during model training was observed to improve measured faithfulness, particularly when evaluated on the MedQA dataset, which comprises medical licensing questions.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>These findings offer practical strategies for clinicians and developers to enhance the interpretability and trustworthiness of LLMs used in clinical decision support. By carefully selecting high-quality and appropriate few-shot examples, designing robust prompting strategies, and leveraging instruction-tuned models, the reliability and safety of AI-assisted diagnoses and treatment recommendations can be improved, ensuring the AI's reasoning aligns with actual clinical factors.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state the limitations of this specific study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly detailing specific future research paths, the abstract suggests that the findings offer insights for future work aimed at further enhancing the interpretability and trustworthiness of LLMs in sensitive domains like healthcare through the continued refinement of prompting, few-shot learning, and training methodologies.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">clinical decision support</span>
                    
                    <span class="tag">medical education</span>
                    
                    <span class="tag">general medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">faithfulness</span>
                    
                    <span class="tag tag-keyword">explainability</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">prompting strategies</span>
                    
                    <span class="tag tag-keyword">few-shot examples</span>
                    
                    <span class="tag tag-keyword">instruction-tuning</span>
                    
                    <span class="tag tag-keyword">MedQA</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) often produce explanations that do not
faithfully reflect the factors driving their predictions. In healthcare
settings, such unfaithfulness is especially problematic: explanations that omit
salient clinical cues or mask spurious shortcuts can undermine clinician trust
and lead to unsafe decision support. We study how inference and training-time
choices shape explanation faithfulness, focusing on factors practitioners can
control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA
8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),
and manipulate the number and type of few-shot examples, prompting strategies,
and training procedure. Our results show: (i) both the quantity and quality of
few-shot examples significantly impact model faithfulness; (ii) faithfulness is
sensitive to prompting design; (iii) the instruction-tuning phase improves
measured faithfulness on MedQA. These findings offer insights into strategies
for enhancing the interpretability and trustworthiness of LLMs in sensitive
domains.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>39th Conference on Neural Information Processing Systems (NeurIPS
  2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM
  Lifecycle: Benchmarks, Emergent Abilities, and Scaling</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>