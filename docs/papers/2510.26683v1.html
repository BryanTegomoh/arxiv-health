<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models - Health AI Hub</title>
    <meta name="description" content="Evontree proposes a novel framework for domain adaptation of Large Language Models (LLMs) in data-sensitive fields like healthcare by leveraging a small set of ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26683v1" target="_blank">2510.26683v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mingchen Tu, Zhiqiang Liu, Juan Li, Liangyurui Liu, Junjie Wang, Lei Liang, Wen Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26683v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26683v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Evontree proposes a novel framework for domain adaptation of Large Language Models (LLMs) in data-sensitive fields like healthcare by leveraging a small set of high-quality ontology rules. It systematically extracts, validates, and enhances domain knowledge within LLMs through a three-step process: ontology extraction, inconsistency detection, and self-distilled fine-tuning. This approach significantly improves LLM performance on medical QA benchmarks (up to 3.7% accuracy) without requiring extensive external datasets, proving effective for low-resource adaptation.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine and health by providing a method to adapt powerful LLMs for specialized healthcare applications, where data sensitivity and the need for accurate, consistent information are paramount, especially when domain-specific training data is scarce.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research proposes a framework (Evontree) to enhance the accuracy and domain adaptation of Large Language Models (LLMs) for healthcare applications. By leveraging ontology rules to extract, validate, and reinforce domain knowledge, it aims to create more reliable and accurate AI models for tasks such as medical question answering, clinical decision support, and knowledge extraction from medical texts, particularly in low-resource environments where extensive training data is scarce.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of adapting LLMs to data-sensitive domains like healthcare due to a scarcity of high-quality, domain-specific training data.</li>
                    
                    <li>Introduces Evontree, a framework that utilizes a small set of high-quality, expert-distilled ontology rules for LLM domain adaptation.</li>
                    
                    <li>Evontree operates by first extracting implicit domain ontology from raw LLMs.</li>
                    
                    <li>It then detects inconsistencies in the extracted knowledge using two core ontology rules, ensuring knowledge integrity.</li>
                    
                    <li>The refined and validated knowledge is subsequently reinforced back into the LLM via self-distilled fine-tuning.</li>
                    
                    <li>Achieves up to a 3.7% improvement in accuracy on medical QA benchmarks, outperforming unmodified models and leading supervised baselines.</li>
                    
                    <li>Demonstrates an effective, efficient, and robust method for low-resource domain adaptation of LLMs without reliance on extensive external datasets.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>Evontree employs a three-stage methodology: (1) Domain ontology extraction from raw LLMs, where the model's implicit knowledge is formalized. (2) Inconsistency detection using two core, high-quality ontology rules to validate and refine the extracted knowledge. (3) Knowledge reinforcement via self-distilled fine-tuning, where the refined, consistent knowledge is distilled back into the LLM, enhancing its domain-specific capabilities.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The Evontree framework consistently outperformed both unmodified LLMs (Llama3-8B-Instruct, Med42-v2) and leading supervised baselines on medical QA benchmarks. It achieved a significant improvement in accuracy, up to 3.7%, confirming its effectiveness, efficiency, and robustness for domain adaptation in low-resource settings.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Evontree has the potential to significantly improve the accuracy and reliability of LLM-powered applications in healthcare, such as clinical decision support systems, medical diagnosis aids, patient education tools, and automated medical question answering, especially in specialties where extensive training data is not readily available. By ensuring knowledge integrity through ontology rules, it can reduce the risk of inaccurate or inconsistent information in clinical contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the Evontree framework, but typical limitations might include the scalability of ontology rule definition for extremely complex domains or the potential for bias inherited from the initial LLM despite rule-based refinement.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions. However, potential future work could involve exploring the application of Evontree with a broader range of ontology rule types, testing its efficacy across more diverse medical tasks (e.g., electronic health record analysis, clinical note summarization), or investigating methods to semi-automate the generation of high-quality ontology rules.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Healthcare</span>
                    
                    <span class="tag">Medical Question Answering</span>
                    
                    <span class="tag">Clinical Decision Support (implied)</span>
                    
                    <span class="tag">Knowledge Management in Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Ontology Rules</span>
                    
                    <span class="tag tag-keyword">Domain Adaptation</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Medical QA</span>
                    
                    <span class="tag tag-keyword">Self-Evolution</span>
                    
                    <span class="tag tag-keyword">Knowledge Distillation</span>
                    
                    <span class="tag tag-keyword">Low-Resource Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>