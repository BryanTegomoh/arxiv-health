<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models - Health AI Hub</title>
    <meta name="description" content="Evontree is a novel framework that enables Large Language Models (LLMs) to adapt to data-sensitive domains like healthcare by leveraging a small set of high-qua">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26683v1" target="_blank">2510.26683v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mingchen Tu, Zhiqiang Liu, Juan Li, Liangyurui Liu, Junjie Wang, Lei Liang, Wen Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26683v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26683v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Evontree is a novel framework that enables Large Language Models (LLMs) to adapt to data-sensitive domains like healthcare by leveraging a small set of high-quality ontology rules. It systematically extracts, validates, and enhances domain knowledge within LLMs without extensive external datasets, addressing inconsistencies through self-distilled fine-tuning. This approach significantly outperforms existing baselines, achieving up to a 3.7% accuracy improvement on medical QA benchmarks.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine and health as it offers a robust method to adapt powerful LLMs for specialized healthcare applications, such as medical question answering and potentially clinical decision support, overcoming the common hurdle of limited high-quality, domain-specific training data. By integrating expert-defined ontology rules, it aims to enhance the factual accuracy and reliability of LLM outputs in critical medical contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research aims to improve the accuracy, reliability, and domain adaptation capabilities of Large Language Models (LLMs) for medical and healthcare applications. By leveraging ontology rules to extract, validate, and enhance domain-specific knowledge within LLMs, Evontree facilitates the development of more robust and accurate AI tools for tasks like medical question answering, especially in settings with limited domain-specific data. This directly contributes to the advancement of medical AI applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of adapting LLMs to data-sensitive fields like healthcare due to the scarcity of high-quality, domain-specific training data.</li>
                    
                    <li>Proposes Evontree, a framework that utilizes a small set of high-quality ontology rules, which formalize domain wisdom, to guide LLM self-evolution.</li>
                    
                    <li>The core methodology involves extracting implicit domain ontology from raw LLMs, detecting inconsistencies using two specific ontology rules, and reinforcing refined knowledge via self-distilled fine-tuning.</li>
                    
                    <li>Operates efficiently by not requiring extensive external datasets, making it highly suitable for low-resource domain adaptation scenarios.</li>
                    
                    <li>Extensively evaluated on medical Question Answering (QA) benchmarks using Llama3-8B-Instruct and Med42-v2 models.</li>
                    
                    <li>Demonstrates consistent outperformance over both unmodified LLMs and leading supervised baselines, achieving a substantial accuracy improvement of up to 3.7%.</li>
                    
                    <li>Confirms the effectiveness, efficiency, and robustness of the approach for domain adaptation, particularly in contexts where data is limited.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>Evontree's methodology is a three-step process: 1) **Ontology Extraction**: Implicit domain ontology is extracted from the raw LLM's internal representations. 2) **Inconsistency Detection**: Two core ontology rules are applied to the extracted knowledge to identify factual inconsistencies or contradictions. 3) **Knowledge Reinforcement**: The refined and validated knowledge, free of detected inconsistencies, is reinforced into the LLM through a self-distilled fine-tuning mechanism. This entire process is designed to be independent of extensive external datasets.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key findings indicate that Evontree consistently outperforms both the original Llama3-8B-Instruct and Med42-v2 models, as well as leading supervised baselines, on medical QA benchmarks. The approach achieved a significant accuracy improvement of up to 3.7%, demonstrating its effectiveness, efficiency, and robustness for domain adaptation in low-resource settings.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Evontree has the potential to significantly enhance the reliability and accuracy of LLM-powered applications in clinical settings, such as aiding in diagnostics, providing evidence-based answers to medical queries, or supporting complex clinical decision-making. By ensuring that LLMs adhere to established medical ontology rules, it can lead to more consistent and medically sound information, potentially reducing errors and improving patient care, even when extensive curated datasets are unavailable.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the Evontree framework.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention any future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Question Answering</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Healthcare Informatics</span>
                    
                    <span class="tag">Medical Knowledge Management</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Ontology Rules</span>
                    
                    <span class="tag tag-keyword">Domain Adaptation</span>
                    
                    <span class="tag tag-keyword">Self-Evolution</span>
                    
                    <span class="tag tag-keyword">Medical QA</span>
                    
                    <span class="tag tag-keyword">Knowledge Extraction</span>
                    
                    <span class="tag tag-keyword">Fine-tuning</span>
                    
                    <span class="tag tag-keyword">Low-Resource Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>