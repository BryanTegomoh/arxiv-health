<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation - Health AI Hub</title>
    <meta name="description" content="MaskMed proposes a novel approach to medical image segmentation by introducing a unified decoupled segmentation head that separates class-agnostic mask predicti">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.15603v1" target="_blank">2511.15603v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-19
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Bin Xie, Gady Agam
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.15603v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.15603v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">MaskMed proposes a novel approach to medical image segmentation by introducing a unified decoupled segmentation head that separates class-agnostic mask prediction from class label prediction using shared object queries. It also features a Full-Scale Aware Deformable Transformer module for memory-efficient and spatially aligned full-scale feature fusion. This method achieves state-of-the-art performance, significantly outperforming nnUNet on AMOS 2022 and BTCV datasets.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for medical imaging as it enhances the accuracy and generalization capabilities of automated segmentation tools, which are vital for precise diagnosis, treatment planning (e.g., surgery, radiotherapy), and quantitative disease assessment in clinical practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This AI model aims to improve the accuracy and efficiency of medical image segmentation, a critical task for identifying and delineating anatomical structures, tumors, or pathologies in medical scans (e.g., CT, MRI). This has direct applications in disease diagnosis, staging, treatment planning (e.g., radiation therapy, surgery), surgical guidance, and quantitative analysis of medical images to monitor disease progression or treatment response.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the limitations of traditional point-wise convolutional segmentation heads, which rigidly tie output channels to specific classes, hindering feature sharing and semantic generalization.</li>
                    
                    <li>Introduces a unified **decoupled segmentation head** that separates multi-class prediction into two distinct tasks: class-agnostic mask prediction and class label prediction.</li>
                    
                    <li>Employs **shared object queries** to facilitate the decoupled prediction process, enabling better feature sharing across classes.</li>
                    
                    <li>Proposes a **Full-Scale Aware Deformable Transformer module (FSADT)** to enhance feature representation.</li>
                    
                    <li>The FSADT allows low-resolution encoder features to attend across full-resolution encoder features via deformable attention, ensuring memory-efficient and spatially aligned full-scale fusion.</li>
                    
                    <li>Achieves state-of-the-art performance, surpassing the nnUNet baseline by +2.0% Dice score on the AMOS 2022 benchmark.</li>
                    
                    <li>Demonstrates superior accuracy with a +6.9% Dice score improvement over nnUNet on the BTCV dataset.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The MaskMed method integrates a unified decoupled segmentation head that uses shared object queries to perform class-agnostic mask prediction and class label prediction separately. Additionally, it incorporates a Full-Scale Aware Deformable Transformer module, which leverages deformable attention to allow low-resolution encoder features to interact with full-resolution features, achieving memory-efficient and spatially aligned feature fusion.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>MaskMed achieves state-of-the-art segmentation performance, demonstrating significant improvements over the strong nnUNet baseline. Specifically, it recorded a +2.0% increase in Dice score on the AMOS 2022 dataset and a +6.9% increase in Dice score on the BTCV dataset, highlighting its superior accuracy and ability to generalize across different medical imaging challenges.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The enhanced accuracy and generalization of MaskMed can lead to more reliable automated segmentation in various clinical applications. This includes improved precision in delineating organs-at-risk and tumors for radiotherapy planning, more accurate quantification of disease burden for prognosis, better anatomical mapping for surgical navigation, and a reduction in manual annotation time, ultimately supporting clinicians in delivering higher quality patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Limitations are not explicitly mentioned in the provided abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions are not explicitly mentioned in the provided abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic imaging</span>
                    
                    <span class="tag">Surgical planning</span>
                    
                    <span class="tag">Oncology (radiotherapy planning)</span>
                    
                    <span class="tag">Anatomical segmentation (e.g., abdominal organs)</span>
                    
                    <span class="tag">Quantitative image analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medical image segmentation</span>
                    
                    <span class="tag tag-keyword">Decoupled prediction</span>
                    
                    <span class="tag tag-keyword">Transformer networks</span>
                    
                    <span class="tag tag-keyword">Deformable attention</span>
                    
                    <span class="tag tag-keyword">Deep learning</span>
                    
                    <span class="tag tag-keyword">Multi-class segmentation</span>
                    
                    <span class="tag tag-keyword">Object queries</span>
                    
                    <span class="tag tag-keyword">State-of-the-art</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>