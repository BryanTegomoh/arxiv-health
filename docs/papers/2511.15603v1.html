<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation - Health AI Hub</title>
    <meta name="description" content="MaskMed introduces a novel architecture for medical image segmentation, addressing limitations of traditional point-wise convolutional heads by proposing a unif">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.15603v1" target="_blank">2511.15603v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-19
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Bin Xie, Gady Agam
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.15603v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.15603v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">MaskMed introduces a novel architecture for medical image segmentation, addressing limitations of traditional point-wise convolutional heads by proposing a unified decoupled segmentation head and a Full-Scale Aware Deformable Transformer module. This approach separates class-agnostic mask prediction from class label prediction using shared object queries and enables memory-efficient, spatially aligned full-scale feature fusion. The method achieves state-of-the-art performance on challenging medical benchmarks, significantly outperforming nnUNet.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research significantly advances the precision and efficiency of automated medical image segmentation, a critical process for accurate diagnosis, treatment planning, and surgical guidance in various clinical applications. Improved segmentation directly translates to better patient outcomes by enabling more reliable analysis of anatomical structures and pathologies.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This paper presents an AI application (a deep learning model called MaskMed) designed to automate and improve the accuracy of medical image segmentation. This technology assists healthcare professionals in tasks such as identifying and quantifying organs, tumors, and other anatomical structures from scans (e.g., CT, MRI), which is crucial for diagnosis, disease monitoring, treatment planning (e.g., radiation therapy, surgery), and medical research.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the rigidity of traditional point-wise convolutional segmentation heads, which limit feature sharing and semantic generalization in medical imaging.</li>
                    
                    <li>Proposes a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction.</li>
                    
                    <li>Utilizes shared object queries for both mask and class label predictions, enhancing feature sharing across different classes.</li>
                    
                    <li>Introduces a Full-Scale Aware Deformable Transformer (FSDTM) module for efficient and spatially aligned full-scale feature fusion.</li>
                    
                    <li>The FSDTM employs deformable attention, allowing low-resolution encoder features to attend across full-resolution encoder features, optimizing memory usage.</li>
                    
                    <li>Achieves state-of-the-art performance, outperforming the strong baseline nnUNet by +2.0% Dice score on the AMOS 2022 multi-organ segmentation dataset.</li>
                    
                    <li>Demonstrates even more significant improvement, surpassing nnUNet by +6.9% Dice score on the BTCV (Beyond The Cranial Vault) abdominal CT segmentation dataset.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>MaskMed introduces two core architectural innovations. First, a **unified decoupled segmentation head** that moves away from conventional channel-wise class prediction. It uses shared object queries to separately predict class-agnostic masks and their corresponding class labels. Second, a **Full-Scale Aware Deformable Transformer module** is integrated. This module leverages deformable attention to allow features from lower-resolution encoder stages to attend to and fuse information from full-resolution encoder features, thereby achieving memory-efficient and spatially coherent information integration across scales.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The MaskMed method achieves state-of-the-art performance in medical image segmentation. It significantly outperforms the nnUNet baseline, showing a +2.0% increase in Dice score on the AMOS 2022 dataset for multi-organ abdominal segmentation. A more substantial improvement of +6.9% Dice score was observed on the BTCV abdominal CT segmentation dataset.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The enhanced accuracy and efficiency offered by MaskMed have several practical clinical impacts. It can lead to more precise delineation of organs, tumors, and other anatomical structures, improving the accuracy of disease diagnosis, staging, and progression monitoring. This precision is crucial for highly accurate treatment planning in radiation therapy and surgical procedures, minimizing damage to healthy tissues. The memory-efficient design could also facilitate faster processing times, accelerating clinical workflows and supporting real-time applications where rapid image analysis is critical for patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly mention any limitations of the proposed method.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions for MaskMed.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Anatomy</span>
                    
                    <span class="tag">Surgery Planning</span>
                    
                    <span class="tag">Gastroenterology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">medical image segmentation</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">decoupled segmentation</span>
                    
                    <span class="tag tag-keyword">transformer</span>
                    
                    <span class="tag tag-keyword">deformable attention</span>
                    
                    <span class="tag tag-keyword">AMOS 2022</span>
                    
                    <span class="tag tag-keyword">BTCV</span>
                    
                    <span class="tag tag-keyword">Dice score</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>