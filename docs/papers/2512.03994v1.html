<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel training-free method for robust policy violation detection in LLMs by framing it as an out-of-distribution problem. It leverages a">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.03994v1" target="_blank">2512.03994v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Oren Rachmil, Roy Betser, Itay Gershon, Omer Hofman, Nitay Yakoby, Yuval Meron, Idan Yankelev, Asaf Shabtai, Yuval Elovici, Roman Vainshtein
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.03994v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.03994v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel training-free method for robust policy violation detection in LLMs by framing it as an out-of-distribution problem. It leverages activation-space whitening to decorrelate and standardize hidden LLM activations, using the Euclidean norm in this transformed space as an efficient and interpretable compliance score. The approach achieves state-of-the-art results on policy benchmarks, outperforming existing guardrails and fine-tuned models.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This method is highly relevant for the safe and compliant deployment of LLMs in medical services, where adherence to strict regulations (e.g., HIPAA, clinical protocols) and ethical guidelines is paramount, thus mitigating legal, reputational, and patient safety risks.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research enables the development and deployment of Large Language Models (LLMs) in healthcare settings while ensuring they adhere to specific organizational policies, ethical guidelines, and regulatory frameworks. For example, it could be used to detect if an LLM providing clinical decision support generates advice that violates established medical protocols, if a patient-facing chatbot shares sensitive information inappropriately, or if an LLM assisting with medical research misinterprets data privacy policies. It provides a mechanism for robust policy-aware oversight, which is crucial for the safe, ethical, and effective integration of AI into medicine.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical need for aligning proprietary LLMs with nuanced internal organizational policies in sensitive domains like legal, finance, and medical services, where breaches pose significant risks.</li>
                    
                    <li>Proposes a training-free and efficient method for policy violation detection, overcoming limitations of existing approaches such as generic safety guardrails (lack of robustness for nuance) and LLM-as-a-judge/fine-tuning (high latency, low interpretability).</li>
                    
                    <li>Treats policy violation detection as an out-of-distribution (OOD) problem within the LLM's internal activation space.</li>
                    
                    <li>Employs 'activation-space whitening,' a linear transformation that decorrelates the model's hidden activations and standardizes them to zero mean and unit variance, yielding a near-identity covariance matrix.</li>
                    
                    <li>Utilizes the Euclidean norm of these whitened activations as a statistically grounded compliance score to identify content that deviates from policy, indicating a violation.</li>
                    
                    <li>The method is lightweight and easily deployable, requiring only the policy text and a small number of illustrative in-policy samples, rather than extensive training data or fine-tuning.</li>
                    
                    <li>Achieves state-of-the-art performance on a challenging policy benchmark, significantly surpassing both conventional content moderation guardrails and more resource-intensive fine-tuned reasoning models.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The approach frames policy violation detection as an out-of-distribution (OOD) problem. It applies a linear transformation, termed 'activation-space whitening,' to the hidden activations of a large language model. This transformation decorrelates the activations and standardizes them to zero mean and unit variance. In this decorrelated space, the Euclidean norm of the whitened activations serves as a compliance score; a higher norm indicates a greater deviation from 'in-policy' behavior and thus a potential violation. The method is training-free, requiring only the policy text and a small set of representative policy-compliant samples to establish the baseline activation distribution.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The proposed training-free activation-space whitening method achieved state-of-the-art performance in detecting policy violations on a challenging benchmark. It significantly outperformed existing content moderation guardrails and even complex fine-tuned reasoning models, demonstrating superior robustness and accuracy in identifying nuanced policy breaches.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This framework offers a practical and statistically grounded mechanism for real-time oversight of LLMs in healthcare settings. It can ensure that AI-generated clinical notes, patient advice, or diagnostic aids strictly adhere to institutional guidelines, patient privacy laws (like HIPAA), and medical best practices. This reduces the risk of generating medically inappropriate, non-compliant, or unethical content, thereby enhancing patient safety, fostering trust in AI tools, and protecting healthcare organizations from legal liabilities.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail limitations of the proposed method itself. However, it implicitly relies on the quality and representativeness of the 'small number of illustrative samples' used to define in-policy behavior. Its effectiveness might vary with the complexity or ambiguity of policy texts, and the generality of using Euclidean norm as a compliance score across all types of highly nuanced policy violations might warrant further investigation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper contributes to the 'broader goal of deployable AI governance,' suggesting ongoing research into practical, robust, and policy-aware oversight mechanisms for LLMs in sensitive applications. While specific future research steps for this exact method are not outlined, the implication is continuous development towards making AI systems safer and more trustworthy for real-world enterprise deployment.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical decision support</span>
                    
                    <span class="tag">Medical record summarization</span>
                    
                    <span class="tag">Patient communication</span>
                    
                    <span class="tag">Medical legal compliance</span>
                    
                    <span class="tag">Healthcare administration</span>
                    
                    <span class="tag">Telehealth platforms</span>
                    
                    <span class="tag">Drug interaction screening</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM alignment</span>
                    
                    <span class="tag tag-keyword">policy violation detection</span>
                    
                    <span class="tag tag-keyword">out-of-distribution detection</span>
                    
                    <span class="tag tag-keyword">activation-space whitening</span>
                    
                    <span class="tag tag-keyword">AI governance</span>
                    
                    <span class="tag tag-keyword">medical AI</span>
                    
                    <span class="tag tag-keyword">compliance</span>
                    
                    <span class="tag tag-keyword">training-free</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to the AAAI 2026 Deployable AI (DAI) Workshop</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>