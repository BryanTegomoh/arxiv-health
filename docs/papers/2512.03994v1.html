<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel training-free and efficient method for detecting policy violations in large language models (LLMs) by treating it as an out-of-dis">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.03994v1" target="_blank">2512.03994v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Oren Rachmil, Roy Betser, Itay Gershon, Omer Hofman, Nitay Yakoby, Yuval Meron, Idan Yankelev, Asaf Shabtai, Yuval Elovici, Roman Vainshtein
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.03994v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.03994v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel training-free and efficient method for detecting policy violations in large language models (LLMs) by treating it as an out-of-distribution (OOD) detection problem. The approach applies activation-space whitening to decorrelate and standardize LLM hidden activations, using the Euclidean norm in this transformed space as a compliance score. It achieves state-of-the-art results on a challenging policy benchmark, surpassing existing guardrails and fine-tuned models, providing a practical framework for policy-aware AI governance.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This method is highly relevant for medical/health services as it enables robust detection of LLM policy violations, crucial for ensuring adherence to sensitive medical guidelines, ethical frameworks, patient privacy regulations (e.g., HIPAA), and specific treatment protocols. It provides a mechanism to mitigate legal and reputational risks associated with misaligned LLM outputs in clinical and administrative healthcare settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is to ensure that Large Language Models (LLMs) used in healthcare settings (e.g., for patient interaction, medical information retrieval, administrative tasks, or clinical support) comply with specific organizational, ethical, and regulatory policies. This technology helps prevent policy violations such as inappropriate disclosure of sensitive patient information, generation of inaccurate or non-compliant medical advice, or deviations from established protocols, thereby enabling safe, ethical, and responsible deployment of AI in medicine.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical need for LLM alignment with internal organizational policies in sensitive domains like legal, finance, and medical services, beyond generic safety filters.</li>
                    
                    <li>Identifies limitations of existing content moderation (guardrails) as lacking robustness for nuanced policies, and LLM-as-a-judge/fine-tuning approaches as introducing latency and lacking interpretability.</li>
                    
                    <li>Proposes a training-free and efficient methodology that conceptualizes policy violation detection as an out-of-distribution detection task.</li>
                    
                    <li>Leverages activation-space whitening: a linear transformation applied to LLM hidden activations to decorrelate them and standardize to zero mean and unit variance, yielding a near-identity covariance matrix.</li>
                    
                    <li>Utilizes the Euclidean norm in the whitened activation space as a 'compliance score' to identify policy violations.</li>
                    
                    <li>Requires only the policy text and a small number of illustrative examples, making it lightweight and easily deployable.</li>
                    
                    <li>Achieves state-of-the-art performance on a challenging policy benchmark, outperforming conventional guardrails and more complex fine-tuned reasoning models.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The method treats policy violation detection as an out-of-distribution (OOD) problem. It applies a linear transformation, specifically activation-space whitening, to the hidden activations of an LLM. This transformation decorrelates the activations and standardizes them to have a zero mean and unit variance, effectively creating a near-identity covariance matrix. In this whitened space, the Euclidean norm of the activations is used as a 'compliance score'; a higher norm indicates a greater deviation from 'compliant' behavior and thus a potential policy violation. The approach is training-free, only requiring the policy text and a small set of compliant example samples.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is that the proposed training-free activation-space whitening method achieves state-of-the-art results on a challenging policy benchmark for LLM policy violation detection. It significantly outperforms both traditional content moderation guardrails and more computationally intensive fine-tuned reasoning models, demonstrating its efficiency and effectiveness as a statistically grounded framework for LLM oversight.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has significant clinical impact by providing a practical and robust solution for deploying LLMs more safely and responsibly in healthcare. It can ensure LLMs adhere to critical medical policies, such as providing evidence-based information, respecting patient confidentiality, avoiding biased or inappropriate medical advice, and complying with regulatory standards. This enhances trust in AI applications within healthcare, reduces the risk of adverse events or legal liabilities, and facilitates the ethical integration of LLMs into clinical workflows and administrative tasks.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights limitations of *existing* methods (e.g., latency and lack of interpretability in LLM-as-a-judge/fine-tuning, limited robustness of guardrails). It does not explicitly detail specific limitations or caveats of the proposed activation-space whitening method itself. However, implicit limitations might include reliance on the quality of illustrative samples and the assumption that policy violations consistently manifest as OOD behavior in the whitened activation space.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract broadly mentions 'advancing the broader goal of deployable AI governance.' While not detailing specific future research for *this method*, this implies continued work on making AI systems, including LLMs, more trustworthy and controllable in real-world, sensitive applications. Future directions could involve exploring the method's robustness across diverse policy types, its applicability to multilingual contexts, or integrating it with other explainability techniques to enhance interpretability of detected violations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Clinical Decision Support Systems</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                    <span class="tag">Patient Privacy & Security</span>
                    
                    <span class="tag">Healthcare Regulatory Compliance</span>
                    
                    <span class="tag">Medical Ethics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Policy Violation Detection</span>
                    
                    <span class="tag tag-keyword">Out-of-Distribution Detection</span>
                    
                    <span class="tag tag-keyword">Activation-Space Whitening</span>
                    
                    <span class="tag tag-keyword">AI Governance</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Regulatory Compliance</span>
                    
                    <span class="tag tag-keyword">Alignment</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to the AAAI 2026 Deployable AI (DAI) Workshop</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>