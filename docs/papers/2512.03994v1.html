<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel, training-free method for detecting policy violations in large language models (LLMs), crucial for sensitive domains like medical ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.03994v1" target="_blank">2512.03994v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Oren Rachmil, Roy Betser, Itay Gershon, Omer Hofman, Nitay Yakoby, Yuval Meron, Idan Yankelev, Asaf Shabtai, Yuval Elovici, Roman Vainshtein
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.03994v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.03994v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel, training-free method for detecting policy violations in large language models (LLMs), crucial for sensitive domains like medical services. It re-frames the problem as out-of-distribution detection by applying activation-space whitening to transform the LLM's hidden activations, using the Euclidean norm as a compliance score. The approach is efficient, requires minimal data, and achieves state-of-the-art results on policy benchmarks, offering a practical framework for LLM governance.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This method directly addresses a critical challenge for deploying LLMs in healthcare: ensuring strict adherence to medical ethics, patient privacy (HIPAA), clinical guidelines, and hospital-specific protocols. It offers a way to detect subtle deviations from established medical policies, which is essential for safe and responsible AI integration in patient care and health information management.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research provides a framework for ensuring that Large Language Models (LLMs) used in healthcare adhere to specific organizational policies, ethical guidelines, and regulatory frameworks (e.g., patient privacy, diagnostic protocols, information disclaimers). By detecting policy violations in a training-free and efficient manner, it enables the safer and more reliable deployment of AI in medical services, such as clinical assistance, patient communication, medical record analysis, and administrative tasks, thereby reducing risks associated with misinformation, privacy breaches, or non-compliance with clinical standards.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical need for robust policy alignment in LLMs deployed in sensitive domains (e.g., legal, finance, medical services) beyond generic safety filters.</li>
                    
                    <li>Proposes a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem.</li>
                    
                    <li>Utilizes activation-space whitening: a linear transformation that decorrelates and standardizes LLM's hidden activations to zero mean and unit variance, yielding a near-identity covariance matrix.</li>
                    
                    <li>Detects policy violations by using the Euclidean norm of activations in this transformed space as a compliance score.</li>
                    
                    <li>Requires minimal inputs: only the policy text and a small number of illustrative samples, making it light-weight and easily deployable.</li>
                    
                    <li>Achieves state-of-the-art performance on a challenging policy benchmark, surpassing existing guardrails and fine-tuned reasoning models.</li>
                    
                    <li>Provides a practical and statistically grounded framework for policy-aware oversight, advancing deployable AI governance for LLMs.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The method re-conceptualizes policy violation detection as an out-of-distribution (OOD) problem. It involves applying a linear transformation, inspired by whitening techniques, to the hidden activations of an LLM. This transformation decorrelates the activations and standardizes them to have zero mean and unit variance, effectively yielding a near-identity covariance matrix. In this decorrelated, standardized activation space, the Euclidean norm is then used as a compliance score, where a higher norm indicates a greater likelihood of policy violation (i.e., being 'out-of-distribution' from compliant responses). The approach is training-free, requiring only the policy text and a few illustrative examples.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The proposed activation-space whitening method achieves state-of-the-art results on a challenging policy benchmark. It significantly surpasses the performance of conventional content moderation guardrails and even more flexible fine-tuned reasoning models in detecting policy violations, demonstrating its superior robustness and efficacy in capturing nuanced organizational policies.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology has significant potential to enhance the safety and trustworthiness of LLM deployments in healthcare. It can be used to automatically flag or prevent LLMs from generating responses that violate patient privacy (e.g., HIPAA), deviate from established clinical guidelines, or offer unethical/misleading medical advice. By providing a rapid, efficient, and interpretable mechanism, it enables healthcare organizations to tailor LLM oversight to specific institutional policies and accelerate the safe deployment of AI in critical medical applications.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the limitations of *existing* content moderation frameworks (e.g., lack of robustness of guardrails, latency and interpretability issues with LLM-as-a-judge/fine-tuning). It does not explicitly state any specific limitations or caveats of the proposed activation-space whitening method itself.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While the abstract does not detail specific future research directions, it positions this work as 'advancing the broader goal of deployable AI governance.' This implies future work could involve extending its application to a wider range of complex policies and domains, further enhancing its interpretability, or integrating it into comprehensive AI governance platforms for enterprise-level LLM oversight.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical decision support</span>
                    
                    <span class="tag">Patient information management</span>
                    
                    <span class="tag">Medical ethics and compliance</span>
                    
                    <span class="tag">Health informatics</span>
                    
                    <span class="tag">Drug safety and pharmacovigilance</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM policy alignment</span>
                    
                    <span class="tag tag-keyword">Out-of-distribution detection</span>
                    
                    <span class="tag tag-keyword">Activation-space whitening</span>
                    
                    <span class="tag tag-keyword">AI governance</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Training-free</span>
                    
                    <span class="tag tag-keyword">Euclidean norm</span>
                    
                    <span class="tag tag-keyword">Compliance</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to the AAAI 2026 Deployable AI (DAI) Workshop</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>