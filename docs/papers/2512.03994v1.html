<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel, training-free method for detecting policy violations in large language models (LLMs) by framing it as an out-of-distribution (OOD">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.03994v1" target="_blank">2512.03994v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Oren Rachmil, Roy Betser, Itay Gershon, Omer Hofman, Nitay Yakoby, Yuval Meron, Idan Yankelev, Asaf Shabtai, Yuval Elovici, Roman Vainshtein
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.03994v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.03994v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel, training-free method for detecting policy violations in large language models (LLMs) by framing it as an out-of-distribution (OOD) detection problem. It employs activation-space whitening to decorrelate and standardize LLM hidden activations, using the Euclidean norm as a compliance score. The approach achieves state-of-the-art results on a challenging policy benchmark, offering an efficient and interpretable solution for robust LLM governance.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critically relevant to medicine and healthcare, as it offers a robust, training-free mechanism to ensure LLMs deployed in medical services adhere strictly to ethical guidelines, regulatory frameworks (e.g., HIPAA for patient privacy), and organizational protocols, thereby preventing the generation of unsafe advice, misinformation, or non-compliant interactions, which are paramount for patient safety and trust.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research provides a practical and statistically grounded framework to ensure that Large Language Models (LLMs) used in healthcare adhere to internal organizational policies, regulatory frameworks (e.g., HIPAA for patient privacy, specific hospital protocols), and ethical guidelines. By enabling efficient and training-free detection of policy violations, it helps to mitigate risks such as providing inappropriate medical advice, mishandling sensitive patient data, or non-compliance with institutional policies. This contributes to safer, more reliable, and trustworthy deployment of AI in medical services.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Addressed:** The urgent need for LLMs, especially in sensitive domains like legal, finance, and medical services, to align with internal organizational policies to mitigate significant legal and reputational risks.</li>
                    
                    <li>**Limitations of Existing Solutions:** Current guardrail frameworks are confined to generic safety and lack robustness for nuanced organizational policies, while LLM-as-a-judge and fine-tuning approaches introduce high latency and lack interpretability.</li>
                    
                    <li>**Proposed Method:** A training-free and efficient technique that re-frames policy violation detection as an out-of-distribution (OOD) detection task.</li>
                    
                    <li>**Core Mechanism:** Applies activation-space whitening ‚Äì a linear transformation ‚Äì to decorrelate the LLM's hidden activations and standardize them to zero mean and unit variance, yielding a near-identity covariance matrix.</li>
                    
                    <li>**Compliance Scoring:** In this transformed (whitened) activation space, the Euclidean norm is utilized as a direct compliance score, where deviations indicate potential policy violations.</li>
                    
                    <li>**Resource Efficiency:** The method is lightweight and easily deployable, requiring only the policy text itself and a small number of illustrative policy-compliant samples, without extensive fine-tuning or large datasets.</li>
                    
                    <li>**Performance:** Achieves state-of-the-art results on a challenging policy benchmark, outperforming both traditional content moderation guardrails and more complex fine-tuned reasoning models.</li>
                    
                    <li>**Broader Impact:** Provides organizations with a practical, statistically grounded framework for robust policy-aware oversight of LLMs, advancing the field of deployable AI governance.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed method treats policy violation detection as an out-of-distribution (OOD) problem. It leverages 'activation-space whitening,' a technique where a linear transformation is applied to the hidden activations of an LLM. This transformation aims to decorrelate the activations and standardize them to have a zero mean and unit variance, effectively yielding a near-identity covariance matrix. In this newly transformed, whitened activation space, the Euclidean norm of the activations is calculated. This norm serves as a 'compliance score,' where a higher norm indicates a greater deviation from the expected distribution of policy-compliant responses, thus flagging a potential policy violation. The method is training-free and only requires the policy text and a minimal set of illustrative, compliant samples to establish the baseline for the whitened space.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study demonstrates that the training-free activation-space whitening approach achieves state-of-the-art performance in detecting LLM policy violations on a challenging benchmark. It significantly surpasses the efficacy of existing guardrail systems and even more flexible fine-tuned reasoning models. The method's key strengths include its efficiency, interpretability (due to its statistical foundation), and its lightweight nature, requiring minimal input (policy text and a few samples) for deployment.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology has substantial potential to enhance patient safety and regulatory compliance when integrating LLMs into clinical and administrative healthcare workflows. It can proactively identify and mitigate risks such as an LLM providing inaccurate medical advice, breaching patient confidentiality (e.g., PHI violations), generating unethical recommendations, or failing to adhere to specific clinical protocols. By enabling real-time, robust policy enforcement, it allows healthcare providers to deploy AI with greater confidence, improving the reliability and trustworthiness of LLM-powered applications in sensitive medical contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail specific limitations of the proposed activation-space whitening method. However, it implicitly suggests a practical requirement for a 'small number of illustrative samples,' which, while minimal, is still necessary to define the compliant baseline. The generalizability of its performance on the 'challenging policy benchmark' to the full spectrum of complex, evolving, and highly nuanced real-world medical policies is not extensively discussed within the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While specific future research directions for the method are not explicitly detailed in the abstract, the work contributes to the broader goal of 'deployable AI governance.' This implies ongoing efforts to enhance the robustness, scalability, and adaptability of such policy-aware LLM oversight mechanisms for increasingly complex applications, particularly within highly regulated and sensitive sectors like healthcare, where policies are continuously evolving and the stakes are exceptionally high.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Patient Engagement Platforms</span>
                    
                    <span class="tag">Medical Information Retrieval</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Health Administration</span>
                    
                    <span class="tag">Medical Education</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">policy violation detection</span>
                    
                    <span class="tag tag-keyword">out-of-distribution detection</span>
                    
                    <span class="tag tag-keyword">activation-space whitening</span>
                    
                    <span class="tag tag-keyword">AI governance</span>
                    
                    <span class="tag tag-keyword">medical services</span>
                    
                    <span class="tag tag-keyword">regulatory compliance</span>
                    
                    <span class="tag tag-keyword">computational ethics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to the AAAI 2026 Deployable AI (DAI) Workshop</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>