<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformers in Medicine: Improving Vision-Language Alignment for Medical Image Captioning - Health AI Hub</title>
    <meta name="description" content="This paper introduces a transformer-based multimodal framework designed to generate clinically relevant captions for MRI scans, integrating a DEiT-Small vision ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Transformers in Medicine: Improving Vision-Language Alignment for Medical Image Captioning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.25164v1" target="_blank">2510.25164v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-29
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yogesh Thakku Suresh, Vishwajeet Shivaji Hogale, Luca-Alexandru Zamfira, Anandavardhana Hegde
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> eess.IV, cs.AI, cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.25164v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.25164v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a transformer-based multimodal framework designed to generate clinically relevant captions for MRI scans, integrating a DEiT-Small vision transformer, MediCareBERT, and an LSTM decoder. The system is engineered to semantically align image and textual embeddings using a hybrid cosine-MSE loss and contrastive inference. Benchmarking on the MultiCaRe dataset demonstrates that focusing on domain-specific data, specifically brain-only MRIs, significantly improves caption accuracy and semantic alignment compared to general MRI images.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Automating the generation of accurate and clinically relevant captions for MRI scans can significantly reduce the workload on radiologists, improve the consistency and standardization of reports, and expedite the diagnostic process in clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI system is designed to automatically generate descriptive, clinically relevant captions for MRI scans. This application aims to improve and automate medical image reporting, potentially assisting radiologists in drafting reports, enhancing reporting consistency, and improving the efficiency and accuracy of medical diagnoses and patient care.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Proposes a transformer-based multimodal framework for automated medical image captioning, specifically for MRI scans.</li>
                    
                    <li>The architecture combines a DEiT-Small vision transformer for image encoding, MediCareBERT for caption embedding, and a custom LSTM-based decoder for text generation.</li>
                    
                    <li>Emphasizes semantic alignment between image and textual embeddings, achieved through a hybrid loss function combining cosine-MSE loss and contrastive inference via vector similarity.</li>
                    
                    <li>Evaluated on the MultiCaRe dataset, with performance comparisons made between filtered brain-only MRIs and general MRI images.</li>
                    
                    <li>Benchmarked against state-of-the-art medical image captioning methods, including BLIP, R2GenGPT, and other recent transformer-based approaches.</li>
                    
                    <li>Key finding reveals that leveraging domain-specific data (e.g., brain-only MRI scans) substantially enhances caption accuracy and semantic alignment.</li>
                    
                    <li>Presents the framework as a scalable and interpretable solution for improving automated medical image reporting workflows.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The framework utilizes a DEiT-Small vision transformer as an image encoder to extract visual features from MRI scans. For textual understanding and embedding, MediCareBERT is employed. These encoded representations are then fed into a custom LSTM-based decoder responsible for generating the final captions. The system is trained to achieve semantic alignment between image and text embeddings using a hybrid loss function comprising cosine-MSE loss and contrastive inference, leveraging vector similarity.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The main finding is that the framework achieves superior performance, both in caption accuracy and semantic alignment, when trained on domain-specific data (e.g., filtered brain-only MRI scans) compared to training on general MRI images. This indicates the crucial role of specialized datasets in enhancing the clinical relevance and precision of automated medical image captioning, outperforming existing state-of-the-art methods in this specific context.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology holds significant potential to streamline clinical workflows by providing automated preliminary reports for MRI scans, particularly in specialized areas like neuroradiology. It can help reduce reporting fatigue for radiologists, ensure greater consistency in descriptive language, minimize transcription errors, and accelerate the overall diagnostic turnaround time, ultimately benefiting patient care through faster and more standardized assessments.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>None explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>None explicitly mentioned in the abstract, beyond stating the proposed solution's attributes as 'scalable' and 'interpretable' for automated medical image reporting, implying a trajectory towards practical clinical deployment.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Neuroradiology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Clinical Reporting</span>
                    
                    <span class="tag">Neurology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medical Image Captioning</span>
                    
                    <span class="tag tag-keyword">Transformers</span>
                    
                    <span class="tag tag-keyword">MRI</span>
                    
                    <span class="tag tag-keyword">Vision-Language Alignment</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">DEiT</span>
                    
                    <span class="tag tag-keyword">MediCareBERT</span>
                    
                    <span class="tag tag-keyword">MultiCaRe dataset</span>
                    
                    <span class="tag tag-keyword">Radiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We present a transformer-based multimodal framework for generating clinically
relevant captions for MRI scans. Our system combines a DEiT-Small vision
transformer as an image encoder, MediCareBERT for caption embedding, and a
custom LSTM-based decoder. The architecture is designed to semantically align
image and textual embeddings, using hybrid cosine-MSE loss and contrastive
inference via vector similarity. We benchmark our method on the MultiCaRe
dataset, comparing performance on filtered brain-only MRIs versus general MRI
images against state-of-the-art medical image captioning methods including
BLIP, R2GenGPT, and recent transformer-based approaches. Results show that
focusing on domain-specific data improves caption accuracy and semantic
alignment. Our work proposes a scalable, interpretable solution for automated
medical image reporting.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>This work is to appear in the Proceedings of MICAD 2025, the 6th
  International Conference on Medical Imaging and Computer-Aided Diagnosis</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>