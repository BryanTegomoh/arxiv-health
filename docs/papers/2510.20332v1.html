<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems - Health AI Hub</title>
    <meta name="description" content="This paper highlights how biased data collection practices are a major barrier to integrating AI into real-world clinical practice, despite AI's promise for hea">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">← Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20332v1" target="_blank">2510.20332v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Anna Arias-Duart, Maria Eugenia Cardello, Atia Cortés
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20332v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20332v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper highlights how biased data collection practices are a major barrier to integrating AI into real-world clinical practice, despite AI's promise for healthcare transformation. Drawing insights from Spain's AI4HealthyAging project, the authors identify historical, representation, and measurement biases affecting critical variables, offering practical recommendations to improve fairness and robustness in clinical problem design and data collection for future AI healthcare systems.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Addressing biases in clinical data is crucial for developing AI healthcare systems that are effective, equitable, and safe for all patient populations. Unfair AI can lead to misdiagnoses, suboptimal treatments, and exacerbation of existing health disparities, underscoring the necessity of robust, bias-aware data practices.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper's core application is to improve the design and data collection practices for AI systems intended for use in healthcare, ensuring they are fairer and more robust. This directly contributes to the ethical and effective deployment of AI for various medical tasks, such as diagnostics, prognostics, treatment planning, and health management for aging populations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>AI's potential in healthcare is hindered by limited integration into clinical practice, primarily due to issues with training data quality and fairness.</li>
                    
                    <li>Biased data collection practices are identified as the root cause of compromised training data, impacting the reliability and equity of AI solutions.</li>
                    
                    <li>The research draws on real-world experience from the AI4HealthyAging project, a national R&D initiative in Spain, specifically focusing on detecting biases during clinical data collection.</li>
                    
                    <li>Specific types of bias identified include historical, representation, and measurement biases, which can perpetuate or exacerbate existing inequalities.</li>
                    
                    <li>These biases manifest across crucial patient and operational variables such as sex, gender, age, habitat, socioeconomic status (SES), equipment used, and data labeling practices.</li>
                    
                    <li>The paper concludes by providing practical, actionable recommendations aimed at improving the fairness and robustness of clinical problem design and the data collection methodologies.</li>
                    
                    <li>The findings and experiences are intended to serve as guidance for future projects striving to develop more equitable and reliable AI systems within the healthcare sector.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves drawing on observational insights and experiential learning from the AI4HealthyAging project, a real-world national R&D initiative. The authors' task within this project was to detect biases specifically during the clinical data collection phase, implying a qualitative analysis and identification of bias types and their manifestations in practical healthcare data pipelines.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary findings are the identification of specific bias types—historical, representation, and measurement biases—that compromise clinical data quality. These biases were observed to affect key variables including sex, gender, age, habitat, socioeconomic status, equipment utilization, and data labeling, originating from the data collection practices within a national healthcare AI project.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By providing practical recommendations for mitigating bias in data collection and problem design, this research can lead to the development of more robust, fair, and reliable AI systems. This will translate into more accurate diagnoses, more equitable treatment plans, and ultimately improved health outcomes across diverse patient populations, preventing AI from perpetuating or amplifying health inequities.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The findings are derived from the specific experiences and insights gained during one particular national R&D project (AI4HealthyAging). While valuable, these insights may not encompass the full spectrum of biases or data collection challenges faced across all diverse healthcare systems or AI applications globally.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors express the hope that their findings and experience will contribute to guiding future projects in the development of fairer and more robust AI systems in healthcare, suggesting a need for broader adoption of bias-mitigation strategies.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Health Informatics</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Geriatrics (due to HealthyAging project context)</span>
                    
                    <span class="tag">Precision Medicine</span>
                    
                    <span class="tag">Medical Ethics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">AI healthcare</span>
                    
                    <span class="tag tag-keyword">data bias</span>
                    
                    <span class="tag tag-keyword">clinical data collection</span>
                    
                    <span class="tag tag-keyword">fairness in AI</span>
                    
                    <span class="tag tag-keyword">representation bias</span>
                    
                    <span class="tag tag-keyword">measurement bias</span>
                    
                    <span class="tag tag-keyword">AI ethics</span>
                    
                    <span class="tag tag-keyword">health disparities</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>8 pages, 3 tables, accepted in AEQUITAS 2025 (not in proceedings)</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">← Back to all papers</a></p>
    </footer>
</body>
</html>