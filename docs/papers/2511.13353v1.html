<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images - Health AI Hub</title>
    <meta name="description" content="This paper introduces a hybrid semi-supervised multi-task learning approach to enhance retinal image quality assessment (RIQA) by providing interpretable feedba">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.13353v1" target="_blank">2511.13353v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-17
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Lucas Gabriel Telesco, Danila Nejamkin, Estefan√≠a Mata, Francisco Filizzola, Kevin Wignall, Luc√≠a Franco Troilo, Mar√≠a de los Angeles Cenoz, Melissa Thompson, Mercedes Legu√≠a, Ignacio Larrabide, Jos√© Ignacio Orlando
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.13353v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.13353v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a hybrid semi-supervised multi-task learning approach to enhance retinal image quality assessment (RIQA) by providing interpretable feedback on specific acquisition defects without extensive manual labeling. The method combines manual overall quality labels with pseudo-labels for detailed quality attributes, generated by a Teacher model, to improve overall quality assessment and offer actionable insights into issues like illumination, clarity, and contrast. This approach aims to make RIQA models more useful in clinical settings by guiding image recapture processes.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant for ophthalmology and diagnostic imaging as it provides a practical solution to improve the quality of fundus images used for computer-aided diagnosis. By giving precise feedback on image defects, it directly supports more accurate disease detection and management, while streamlining clinical workflows and reducing costs associated with poor image acquisition.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves using semi-supervised multi-task learning to perform interpretable quality assessment of fundus images. This AI model not only classifies overall image quality but also identifies specific acquisition defects (illumination, clarity, contrast). This provides actionable feedback for clinicians to guide image recapture, thus improving the quality of medical images used for diagnosing eye diseases and enhancing the efficiency of healthcare workflows.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Addressed:** Existing RIQA tools typically classify only overall image quality, lacking specific defect indications (e.g., illumination, clarity, contrast) necessary to guide image recapture, primarily due to the high cost of detailed manual annotations.</li>
                    
                    <li>**Proposed Solution:** A hybrid semi-supervised multi-task learning framework is introduced, combining manual labels for overall quality with pseudo-labels for detailed quality attributes (like illumination, clarity, contrast).</li>
                    
                    <li>**Methodology:** Pseudo-labels for detailed quality defects are generated by a 'Teacher model' trained on a small, limited dataset. These pseudo-labels, alongside manual overall quality labels, are then used to fine-tune a pre-trained ResNet-18 backbone model within a multi-task learning setting.</li>
                    
                    <li>**Improved Performance:** The semi-supervised multi-task model significantly improved overall quality assessment, achieving F1 scores of 0.875 on EyeQ and 0.778 on DeepDRiD datasets, outperforming single-task baselines and matching or surpassing existing methods.</li>
                    
                    <li>**Enhanced Interpretability:** The multi-task model delivered performance for specific detail prediction tasks (e.g., illumination, clarity, contrast) statistically comparable to the Teacher model, providing crucial interpretable feedback.</li>
                    
                    <li>**Expert Alignment:** On a newly annotated EyeQ subset, the model's performance was similar to human experts, suggesting that the inherent noise in pseudo-labels aligns with typical expert variability.</li>
                    
                    <li>**Clinical Actionability:** The model provides clinically actionable outputs, offering specific, interpretable feedback on capture conditions to guide image recapture, thereby improving the efficiency and quality of diagnostic imaging workflows.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors developed a hybrid semi-supervised multi-task learning framework. This involves training a 'Teacher model' on a small, manually annotated dataset to generate pseudo-labels for specific image quality defects (e.g., illumination, clarity, contrast). These pseudo-labels are then used in conjunction with a small set of manual labels for overall image quality to fine-tune a pre-trained ResNet-18 backbone model. This multi-task setup simultaneously trains the model to assess overall quality and predict specific defect categories, leveraging the weakly supervised pseudo-labels for interpretability.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The proposed semi-supervised approach significantly improved overall quality assessment, achieving F1 scores of 0.875 on EyeQ and 0.778 on DeepDRiD, surpassing single-task baselines. Crucially, the multi-task model provided interpretable feedback on capture conditions (illumination, clarity, contrast) with performance statistically comparable to a fully supervised Teacher model for most detail prediction tasks. Furthermore, the model's performance on a newly annotated EyeQ subset aligned closely with expert judgments, demonstrating the clinical utility and robustness of the pseudo-labeling strategy.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research offers a significant clinical impact by transforming RIQA from a simple pass/fail system into a diagnostic support tool that provides actionable guidance. Clinicians and technicians can receive specific feedback on why an image is poor (e.g., 'too dark', 'blurry'), enabling immediate corrective action during image acquisition. This leads to higher quality images for diagnosis, reduces the need for costly repeat visits, optimizes screening programs, and ultimately facilitates more accurate and efficient computer-aided diagnosis of eye diseases.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the proposed method itself. It highlights the initial high cost of detailed annotations as the problem the research aims to mitigate. The finding that 'pseudo-label noise aligns with expert variability' is presented as a positive aspect, not a limitation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions for this work.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Ophthalmology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Preventive Medicine (Eye Health Screening)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Retinal Image Quality Assessment</span>
                    
                    <span class="tag tag-keyword">Semi-Supervised Learning</span>
                    
                    <span class="tag tag-keyword">Multi-Task Learning</span>
                    
                    <span class="tag tag-keyword">Pseudo-Labeling</span>
                    
                    <span class="tag tag-keyword">Fundus Images</span>
                    
                    <span class="tag tag-keyword">Interpretable AI</span>
                    
                    <span class="tag tag-keyword">Ophthalmology</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.</p>
            </section>

            

            
            <section class="paper-section">
                <h2>Journal Reference</h2>
                <p>Biomedical Signal Processing and Control 113 (2026) 109167</p>
            </section>
            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>