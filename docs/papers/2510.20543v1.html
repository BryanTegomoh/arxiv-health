<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts - Health AI Hub</title>
    <meta name="description" content="This paper introduces CenterBench, a novel dataset designed to evaluate when language models (LMs) abandon structural syntactic analysis in favor of semantic pa">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20543v1" target="_blank">2510.20543v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Sangmitra Madhusudan, Kaige Chen, Ali Emami
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.80 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20543v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20543v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces CenterBench, a novel dataset designed to evaluate when language models (LMs) abandon structural syntactic analysis in favor of semantic pattern matching. It demonstrates that LMs systematically prioritize semantic plausibility over precise syntactic understanding as sentence complexity increases, revealing a fundamental difference in how models and humans process complex language structures.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Language models are increasingly deployed in healthcare for tasks like clinical decision support and EHR analysis. This research is crucial because it highlights that these models might misinterpret complex medical texts, prioritizing 'common sense' semantic associations over precise, syntactically nuanced clinical information, potentially leading to diagnostic errors or suboptimal treatment recommendations.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper's insights are vital for ensuring the reliability and safety of AI systems in healthcare. Specifically, for LMs used in clinical settings, these findings inform the need for better architectural designs and evaluation metrics that prioritize accurate structural and causal reasoning over superficial semantic plausibility. Developers of medical AI applications must account for these identified failure modes to prevent misinterpretations of complex medical histories, diagnostic criteria, or treatment protocols. This research can guide the development of more robust AI for tasks such as summarizing patient records, answering complex clinical questions, or supporting diagnostic processes where precise understanding of nested dependencies and causal relationships is paramount for patient care.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Introduced CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences, varying in recursive nesting depth.</li>
                    
                    <li>Each sentence has a syntactically identical plausible and semantically implausible counterpart (e.g., 'mailmen prescribe medicine' vs. 'doctors deliver mail') to isolate the effect of semantics.</li>
                    
                    <li>Six types of comprehension questions were designed to test surface understanding, syntactic dependencies, and causal reasoning.</li>
                    
                    <li>Testing six diverse language models revealed that performance gaps between plausible and implausible sentences systematically widen with complexity, showing median gaps up to 26.8 percentage points, quantifying the shift to semantic shortcuts.</li>
                    
                    <li>Paradoxically, semantic plausibility *harms* model performance on questions regarding resulting actions, indicating an over-reliance on common semantic associations even when it contradicts causal logic implied by syntax.</li>
                    
                    <li>Reasoning-enhanced models improved accuracy but their internal traces still indicated semantic shortcuts, 'overthinking,' and instances of answer refusal, suggesting inherent limitations in their reasoning processes.</li>
                    
                    <li>Unlike models, human performance showed variable semantic effects, indicating a more nuanced and less systematically biased approach to integrating syntax and semantics.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved creating CenterBench, a dataset comprising 9,720 comprehension questions. This dataset features center-embedded sentences with varying depths of recursion, each presented in a semantically plausible version and a syntactically identical, semantically implausible version. Six types of questions were developed to assess different aspects of comprehension (surface, syntactic dependencies, causal reasoning). Six different language models were then evaluated on this dataset, and their performance was analyzed, comparing accuracy on plausible vs. implausible sentences and across different complexity levels. Human performance was also used as a benchmark for comparison.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Language models exhibit a significant and systematic increase in reliance on semantic plausibility, rather than structural understanding, as syntactic complexity (nesting depth) of sentences grows. This results in performance gaps up to 26.8 percentage points between plausible and implausible sentences. Intriguingly, semantic plausibility negatively impacts performance on questions requiring causal reasoning. While reasoning models improve overall accuracy, they still display evidence of semantic shortcuts, overthinking, and answer refusal, indicating persistent challenges in robust structural analysis.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>In clinical settings, misinterpreting complex medical language (e.g., in patient histories, surgical notes, or research papers) can have severe consequences. If LMs prioritize common semantic patterns over precise syntax, they could misinterpret drug interactions, patient symptoms, or treatment protocols. For instance, an LM might wrongly infer a common condition based on keywords, overlooking a rare, complex diagnosis described with intricate sentence structure. This study provides a framework to identify and potentially mitigate such biases, which is critical for the safe and reliable deployment of AI in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily focuses on center-embedded sentences, which represent a specific type of syntactic complexity, and the findings might not generalize perfectly to all forms of complex grammatical structures. The study also evaluated a specific set of six language models, and results may vary with other or future models. The abstract does not explicitly state limitations regarding dataset size or scope beyond the specific sentence type.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should focus on developing language models that can maintain robust structural understanding irrespective of semantic plausibility, especially for complex and nuanced text. Applying the CenterBench framework to evaluate domain-specific medical LMs on medical-specific syntactically complex sentences could reveal further insights. Research into architectural changes or training methodologies to reduce semantic shortcutting and improve genuine causal and syntactic reasoning in LMs is warranted.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Electronic Health Records (EHR) Analysis</span>
                    
                    <span class="tag">Medical Education</span>
                    
                    <span class="tag">Patient Communication (Chatbots)</span>
                    
                    <span class="tag">Medical Research Interpretation</span>
                    
                    <span class="tag">Diagnostic Imaging Report Analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Language Models</span>
                    
                    <span class="tag tag-keyword">Syntactic Understanding</span>
                    
                    <span class="tag tag-keyword">Semantic Plausibility</span>
                    
                    <span class="tag tag-keyword">Center-Embedded Sentences</span>
                    
                    <span class="tag tag-keyword">AI Evaluation</span>
                    
                    <span class="tag tag-keyword">Natural Language Processing</span>
                    
                    <span class="tag tag-keyword">Cognitive Biases (in AI)</span>
                    
                    <span class="tag tag-keyword">Medical NLP</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">When language models correctly parse "The cat that the dog chased meowed,"
are they analyzing syntax or simply familiar with dogs chasing cats? Despite
extensive benchmarking, we lack methods to distinguish structural understanding
from semantic pattern matching. We introduce CenterBench, a dataset of 9,720
comprehension questions on center-embedded sentences (like "The cat [that the
dog chased] meowed") where relative clauses nest recursively, creating
processing demands from simple to deeply nested structures. Each sentence has a
syntactically identical but semantically implausible counterpart (e.g., mailmen
prescribe medicine, doctors deliver mail) and six comprehension questions
testing surface understanding, syntactic dependencies, and causal reasoning.
Testing six models reveals that performance gaps between plausible and
implausible sentences widen systematically with complexity, with models showing
median gaps up to 26.8 percentage points, quantifying when they abandon
structural analysis for semantic associations. Notably, semantic plausibility
harms performance on questions about resulting actions, where following causal
relationships matters more than semantic coherence. Reasoning models improve
accuracy but their traces show semantic shortcuts, overthinking, and answer
refusal. Unlike models whose plausibility advantage systematically widens with
complexity, humans shows variable semantic effects. CenterBench provides the
first framework to identify when models shift from structural analysis to
pattern matching.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>