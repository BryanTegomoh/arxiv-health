<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Improving action classification with brain-inspired deep networks - Health AI Hub</title>
    <meta name="description" content="This paper investigates the reliance of deep neural networks (DNNs) on body versus background information for action recognition, finding that conventional DNNs">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Improving action classification with brain-inspired deep networks</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.07729v1" target="_blank">2512.07729v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-08
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Aidas Aglinskas, Stefano Anzellotti
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.85 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.07729v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.07729v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper investigates the reliance of deep neural networks (DNNs) on body versus background information for action recognition, finding that conventional DNNs predominantly use background cues. It proposes a novel brain-inspired DNN architecture with separate processing streams for body and scene data, demonstrating improved action recognition performance and a more human-like accuracy profile across varying stimulus conditions.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Improving the robustness and accuracy of action recognition, particularly under varying environmental conditions or occlusions, is vital for advanced healthcare monitoring applications such as detecting patient falls, tracking rehabilitation progress, and enabling safer and more precise assistive robotics in clinical and home settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The improved action classification capabilities can be applied to develop more accurate and reliable AI systems for remote patient monitoring (tracking daily activities, sleep patterns), fall detection in vulnerable populations, automated assessment of rehabilitation exercise adherence and form, early detection of abnormal movements indicative of neurological conditions, and monitoring patient behavior in clinical settings for safety and care quality.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Conventional deep neural networks (DNNs) trained on the HAA500 dataset exhibited a strong reliance on background information for action recognition, performing at chance-level when only body information was available.</li>
                    
                    <li>Human participants (N=28) demonstrated robust action recognition across stimuli with both body and background, body-only, and background-only versions, performing significantly better with body-only stimuli than with background-only stimuli.</li>
                    
                    <li>The study identified a critical disparity: original DNNs were almost as accurate on full scenes and background-only scenes, but failed on body-only scenes, unlike humans who performed well on all versions.</li>
                    
                    <li>A novel brain-inspired DNN architecture was implemented, featuring separate, domain-specific streams for processing body and background information, mirroring human brain modularity.</li>
                    
                    <li>This brain-inspired architecture significantly improved overall action recognition performance compared to conventional DNNs.</li>
                    
                    <li>The accuracy pattern of the new architecture across the three stimulus types (full, body-only, background-only) more closely matched the performance profile observed in human participants.</li>
                    
                    <li>The findings suggest that incorporating brain-inspired modularity for processing different visual cues can enhance DNN robustness and lead to more human-like AI performance in complex tasks.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study utilized the HAA500 dataset to compare action recognition performance between conventional deep neural networks (DNNs) and human participants (N=28). Stimuli were presented in three formats: full scenes (body + background), body-removed (background only), and background-removed (body only). Subsequently, a novel brain-inspired DNN architecture, featuring distinct processing streams for body and background information, was developed and tested using the same stimulus variations to evaluate its performance and alignment with human accuracy patterns.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Conventional DNNs showed a strong reliance on background information, performing near chance-level on body-only stimuli while maintaining accuracy on background-only stimuli. Conversely, human participants demonstrated accurate action recognition across all three stimulus types, with significantly higher performance on body-only stimuli than on background-only stimuli. The novel brain-inspired architecture not only improved overall action recognition performance but also exhibited an accuracy pattern across different stimulus versions that closely mirrored that observed in human participants.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The development of DNNs that more robustly leverage distinct visual cues for action recognition has significant clinical impact. It enables the creation of more reliable AI systems for patient safety, such as accurate real-time fall detection systems in homes or hospitals, especially in cluttered or partially obscured environments. It also supports better automated assessment of patient movement and progress in physical therapy and rehabilitation, and can lead to more sophisticated and context-aware assistive robotics for elderly care or surgical support, ultimately enhancing patient outcomes and reducing caregiver burden.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Healthcare Monitoring</span>
                    
                    <span class="tag">Geriatric Care</span>
                    
                    <span class="tag">Rehabilitation</span>
                    
                    <span class="tag">Assistive Robotics</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Action Recognition</span>
                    
                    <span class="tag tag-keyword">Deep Neural Networks</span>
                    
                    <span class="tag tag-keyword">Brain-Inspired AI</span>
                    
                    <span class="tag tag-keyword">Body Perception</span>
                    
                    <span class="tag tag-keyword">Scene Perception</span>
                    
                    <span class="tag tag-keyword">Healthcare Monitoring</span>
                    
                    <span class="tag tag-keyword">Computer Vision</span>
                    
                    <span class="tag tag-keyword">Human-like Performance</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>