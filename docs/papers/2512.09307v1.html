<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation - Health AI Hub</title>
    <meta name="description" content="The paper introduces Polyp-DiFoM, a novel distillation framework that transfers rich representations from large vision foundation models (like SAM, DINOv2) into">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.09307v1" target="_blank">2512.09307v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Shivanshu Agnihotri, Snehashis Majhi, Deepak Ranjan Nayak, Debesh Jha
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.09307v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.09307v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">The paper introduces Polyp-DiFoM, a novel distillation framework that transfers rich representations from large vision foundation models (like SAM, DINOv2) into lightweight segmentation baselines (U-Net, U-Net++) for generalized polyp segmentation. This approach effectively bridges the gap between powerful foundation models and practical medical imaging applications, demonstrating significant performance improvements over baselines and state-of-the-art models with nearly 9 times reduced computational overhead across five benchmark datasets.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and early detection of polyps during colonoscopy is crucial for preventing colorectal cancer, one of the leading causes of cancer-related deaths. This research significantly improves the reliability and efficiency of automated polyp segmentation, directly contributing to enhanced diagnostic capabilities and potentially better patient outcomes by allowing precise and timely intervention.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>AI-assisted detection and segmentation of polyps in colonoscopy images/videos, aiding clinicians in the early diagnosis and screening of colorectal cancer, thereby improving diagnostic accuracy and efficiency in clinical settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of accurate polyp segmentation, which is difficult due to variations and camouflage, and the limitations of both lightweight models (poor performance) and foundation models (hard to transfer to medical data).</li>
                    
                    <li>Proposes Polyp-DiFoM, a novel knowledge distillation framework, to leverage the impressive generalization capabilities of large-scale vision foundation models.</li>
                    
                    <li>Polyp-DiFoM transfers semantic priors and rich representations from foundation models (SAM, DINOv2, OneFormer, Mask2Former) into more efficient, canonical architectures like U-Net and U-Net++.</li>
                    
                    <li>Enhances the distillation process by incorporating frequency domain encoding for improved transfer of knowledge and generalization capability.</li>
                    
                    <li>Extensively evaluated across five diverse benchmark datasets: Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300, demonstrating broad applicability.</li>
                    
                    <li>Achieves consistent and significant outperformance compared to respective baseline models and the state-of-the-art, while drastically reducing computational overhead by approximately 9 times.</li>
                    
                    <li>Facilitates the efficient and accurate deployment of advanced segmentation models in real-world clinical settings, addressing computational constraints and data scarcity in medical imaging.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors propose Polyp-DiFoM, a knowledge distillation framework. This involves using large-scale vision foundation models (e.g., SAM, DINOv2, OneFormer, Mask2Former) as 'teachers' to infuse 'semantic priors' and 'rich representations' into lightweight 'student' segmentation baselines (e.g., U-Net, U-Net++). The distillation process is further enhanced by incorporating frequency domain encoding. The framework was extensively evaluated on five benchmark datasets: Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The Polyp-DiFoM framework consistently and significantly outperforms both lightweight baseline models (U-Net, U-Net++) and current state-of-the-art models for polyp segmentation across all evaluated datasets. A crucial finding is its ability to achieve this superior performance with a substantial reduction in computational overhead, approximately 9 times less than existing methods, indicating high efficiency and practical deployability.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The proposed Polyp-DiFoM model can significantly improve the accuracy and efficiency of automated polyp detection during colonoscopies, leading to earlier and more reliable diagnosis of colorectal cancer and facilitating more effective treatment planning. Its reduced computational requirements make it highly suitable for deployment in various clinical environments, including those with limited computational resources, thus democratizing access to advanced AI-driven diagnostic tools for critical cancer screening.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Gastroenterology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical Artificial Intelligence</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Polyp segmentation</span>
                    
                    <span class="tag tag-keyword">Colorectal cancer</span>
                    
                    <span class="tag tag-keyword">Foundation models</span>
                    
                    <span class="tag tag-keyword">Knowledge distillation</span>
                    
                    <span class="tag tag-keyword">Deep learning</span>
                    
                    <span class="tag tag-keyword">U-Net</span>
                    
                    <span class="tag tag-keyword">SAM</span>
                    
                    <span class="tag tag-keyword">DINOv2</span>
                    
                    <span class="tag tag-keyword">Medical imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>