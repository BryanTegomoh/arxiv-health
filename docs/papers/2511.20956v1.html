<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model - Health AI Hub</title>
    <meta name="description" content="BUSTR proposes a novel multitask vision-language framework for automated breast ultrasound (BUS) report generation that overcomes the limitations of scarce pair">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20956v1" target="_blank">2511.20956v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Rawa Mohammed, Mina Attin, Bryar Shareef
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20956v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20956v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">BUSTR proposes a novel multitask vision-language framework for automated breast ultrasound (BUS) report generation that overcomes the limitations of scarce paired image-report datasets and large language model hallucinations. It achieves this by generating reports from structured descriptors and radiomics features, learning descriptor-aware visual representations without direct paired supervision, and demonstrating improved natural language generation and crucial clinical efficacy metrics.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant for medical imaging and diagnostics, offering a novel approach to enhance the efficiency, consistency, and diagnostic accuracy of breast ultrasound reporting, potentially aiding earlier and more reliable breast cancer detection.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an automated vision-language model (BUSTR) designed to generate structured and comprehensive radiology reports from breast ultrasound images. It integrates medical descriptors (e.g., BI-RADS, pathology) and radiomics features to improve the accuracy and clinical utility of diagnostic reporting, thereby aiding radiologists and enhancing patient care.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical limitations in automated breast ultrasound (BUS) report generation: the scarcity of paired image-report datasets and the risk of hallucinations from large language models (LLMs).</li>
                    
                    <li>Introduces BUSTR, a multitask vision-language framework capable of generating BUS reports without requiring explicit paired image-report supervision.</li>
                    
                    <li>Reports are constructed from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, rather than directly from free-text reports.</li>
                    
                    <li>Employs a multi-head Swin encoder to learn descriptor-aware visual representations, trained with a multitask loss over dataset-specific descriptor sets.</li>
                    
                    <li>Utilizes a dual-level objective function combining token-level cross-entropy for textual generation with a cosine-similarity alignment loss for aligning visual and textual token representations.</li>
                    
                    <li>Evaluated on two public BUS datasets, BrEaST and BUS-BRA, which vary in size and available descriptor information.</li>
                    
                    <li>Consistently demonstrates improvements in both standard natural language generation (NLG) metrics and clinically relevant efficacy metrics, particularly for key diagnostic targets such as BI-RADS category and pathology findings.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>BUSTR is a multitask vision-language framework that generates reports by first extracting structured descriptors and radiomics features from BUS images. It uses a multi-head Swin encoder to learn visual representations, which are specifically trained to be descriptor-aware through a multitask loss applied over dataset-specific descriptor sets. Report generation is then guided by these representations. A dual-level objective function aligns visual and textual tokens: a token-level cross-entropy loss for text generation and a cosine-similarity alignment loss between input and output representations, crucially operating without direct paired image-report supervision.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study found that BUSTR consistently improved both standard natural language generation (NLG) metrics and clinical efficacy metrics across two distinct public BUS datasets (BrEaST and BUS-BRA). Significant improvements were particularly noted for critical diagnostic targets such as the BI-RADS category and pathology reports, validating the model's ability to generate clinically relevant and accurate information without relying on paired image-report training data.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>BUSTR has the potential to significantly reduce radiologist workload by automating and standardizing breast ultrasound reporting, leading to increased reporting consistency and potentially reducing diagnostic errors and inter-reader variability. By providing accurate, descriptor-aware reports, it could accelerate diagnostic pathways for breast cancer and improve the overall quality and efficiency of care in breast imaging departments.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the BUSTR model itself. However, it addresses two major limitations prevalent in the field of automated radiology report generation: the lack of sufficiently large paired image-report datasets and the risk of hallucination (generating factually incorrect information) associated with general large language models.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Breast Imaging</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Medical AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Breast ultrasound</span>
                    
                    <span class="tag tag-keyword">Radiology report generation</span>
                    
                    <span class="tag tag-keyword">Vision-language model</span>
                    
                    <span class="tag tag-keyword">Descriptor-aware</span>
                    
                    <span class="tag tag-keyword">Multitask learning</span>
                    
                    <span class="tag tag-keyword">Swin encoder</span>
                    
                    <span class="tag tag-keyword">BI-RADS</span>
                    
                    <span class="tag tag-keyword">Pathology</span>
                    
                    <span class="tag tag-keyword">Medical imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>13 pages, 2 figures, 6 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>