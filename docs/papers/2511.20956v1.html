<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model - Health AI Hub</title>
    <meta name="description" content="BUSTR introduces a novel multitask vision-language framework for automated breast ultrasound (BUS) report generation (RRG), addressing the critical challenges o">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20956v1" target="_blank">2511.20956v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Rawa Mohammed, Mina Attin, Bryar Shareef
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20956v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20956v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">BUSTR introduces a novel multitask vision-language framework for automated breast ultrasound (BUS) report generation (RRG), addressing the critical challenges of scarce paired image-report datasets and potential large language model (LLM) hallucinations. It constructs reports using structured descriptors and radiomics features, learning descriptor-aware visual representations and aligning visual-textual tokens via a dual-level loss. This approach significantly improves both standard natural language generation and clinical efficacy metrics, particularly for key diagnostic targets like BI-RADS and pathology, without requiring paired image-report supervision.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medical imaging and oncology, as it offers a robust solution for automating breast ultrasound reporting, potentially streamlining radiologists' workflows, reducing reporting variability, and enhancing the consistency and accuracy of breast cancer detection and management.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the automated generation of structured and accurate radiology reports for breast ultrasound images. This aims to assist radiologists, improve the efficiency and consistency of medical reporting, potentially reduce diagnostic errors, and address challenges related to data scarcity in medical AI development by not requiring paired image-report supervision.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the core limitations in automated BUS report generation: lack of paired image-report datasets and hallucination risks from LLMs.</li>
                    
                    <li>Proposes BUSTR, a multitask vision-language framework designed to generate BUS reports without requiring explicit paired image-report supervision.</li>
                    
                    <li>Reports are constructed by leveraging structured clinical descriptors (e.g., BI-RADS, pathology, histology) and radiomics features extracted from the images.</li>
                    
                    <li>Employs a multi-head Swin encoder to learn descriptor-aware visual representations, trained using a multitask loss over dataset-specific descriptor sets.</li>
                    
                    <li>Utilizes a novel dual-level objective for visual and textual token alignment, combining token-level cross-entropy with a cosine-similarity alignment loss between input and output representations.</li>
                    
                    <li>Evaluated on two public BUS datasets (BrEaST and BUS-BRA), demonstrating consistent improvements across standard natural language generation metrics.</li>
                    
                    <li>Achieves significant advancements in clinical efficacy metrics, with notable improvements in predicting crucial targets such as BI-RADS category and pathology findings.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>BUSTR is a multitask vision-language framework. It employs a multi-head Swin encoder to learn descriptor-aware visual representations from breast ultrasound images, trained with a multitask loss over a set of structured clinical descriptors (e.g., BI-RADS, pathology, histology) and radiomics features. A dual-level objective function, comprising token-level cross-entropy and a cosine-similarity alignment loss between input and output representations, is used to align these visual features with textual tokens for report generation, crucially operating without direct paired image-report supervision.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary findings indicate that BUSTR effectively generates comprehensive breast ultrasound reports without needing paired image-report data. It consistently outperforms existing methods on standard natural language generation metrics and, more importantly, demonstrates significant improvements in clinical efficacy metrics, particularly in accurately identifying and reporting BI-RADS categories and pathology findings.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology has the potential to substantially impact clinical practice by automating a significant portion of breast ultrasound reporting, thereby improving radiologists' efficiency and reducing their workload. By standardizing report generation based on structured descriptors, it can minimize inter-radiologist variability, leading to more consistent diagnoses and treatment recommendations for breast lesions. The enhanced accuracy in predicting BI-RADS categories and pathology could also facilitate earlier and more precise patient management.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations. However, potential implicit limitations could include reliance on the quality and completeness of the extracted structured descriptors and radiomics features, as well as the generalizability of the model to clinical datasets with different populations, scanner types, or reporting conventions not represented in the BrEaST and BUS-BRA datasets.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions. However, implied future work could involve evaluating BUSTR on a broader range of diverse BUS datasets, exploring its performance with additional descriptor types, or investigating its integration into larger clinical decision support systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Breast Cancer Screening</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">breast ultrasound</span>
                    
                    <span class="tag tag-keyword">radiology report generation</span>
                    
                    <span class="tag tag-keyword">vision-language model</span>
                    
                    <span class="tag tag-keyword">multitask learning</span>
                    
                    <span class="tag tag-keyword">BI-RADS</span>
                    
                    <span class="tag tag-keyword">medical imaging</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">clinical efficacy</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>13 pages, 2 figures, 6 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>