<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model - Health AI Hub</title>
    <meta name="description" content="BUSTR proposes a novel multitask vision-language framework for automated breast ultrasound (BUS) radiology report generation (RRG) that overcomes the critical l">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20956v1" target="_blank">2511.20956v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Rawa Mohammed, Mina Attin, Bryar Shareef
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20956v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20956v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">BUSTR proposes a novel multitask vision-language framework for automated breast ultrasound (BUS) radiology report generation (RRG) that overcomes the critical limitation of requiring paired image-report datasets. By generating reports from structured descriptors and radiomics features, utilizing a descriptor-aware Swin encoder and a dual-level vision-language alignment loss, BUSTR significantly improves both natural language generation and clinical efficacy metrics, particularly for key diagnostic targets like BI-RADS classification and pathology.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant as it addresses a significant challenge in medical artificial intelligence: the automated, accurate, and reliable generation of radiology reports for breast ultrasound. By bypassing the need for extensive paired datasets and improving clinical efficacy, BUSTR has the potential to standardize reporting, reduce radiologist workload, and enhance the consistency and speed of breast cancer diagnosis and management.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an automated system for generating structured and comprehensive radiology reports for breast ultrasound images. This system, BUSTR, uses a vision-language model to interpret ultrasound images and produce textual reports, aiding clinicians in breast cancer diagnosis, improving reporting consistency, and potentially streamlining workflow in healthcare settings by reducing manual reporting time and risks of human error or omissions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Unsupervised Report Generation:** BUSTR uniquely generates BUS reports without requiring large, paired image-report datasets, addressing a major bottleneck in medical RRG development.</li>
                    
                    <li>**Descriptor-Aware Architecture:** The framework constructs reports from structured clinical descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, ensuring clinically relevant content generation.</li>
                    
                    <li>**Multi-head Swin Encoder:** A multi-head Swin encoder is employed to learn robust, descriptor-aware visual representations, trained using a multitask loss over dataset-specific descriptor sets.</li>
                    
                    <li>**Dual-Level Alignment Loss:** Vision and language tokens are aligned via a sophisticated dual-level objective function that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations.</li>
                    
                    <li>**Robust Evaluation:** The model was comprehensively evaluated on two distinct public BUS datasets, BrEaST and BUS-BRA, demonstrating its adaptability and performance across varying data characteristics.</li>
                    
                    <li>**Superior Clinical Efficacy:** BUSTR consistently improved standard natural language generation metrics and, critically, clinical efficacy metrics, with notable gains for key diagnostic elements such as BI-RADS category and pathology findings.</li>
                    
                    <li>**Addressing Hallucinations:** By relying on structured descriptors rather than purely generative language models, BUSTR inherently mitigates the risk of clinically inaccurate "hallucinations."</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>BUSTR is a multitask vision-language framework that constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features. It utilizes a multi-head Swin encoder to learn descriptor-aware visual representations, trained with a multitask loss over dataset-specific descriptor sets. Visual and textual tokens are aligned through a dual-level objective function, combining token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. This approach enables report generation without requiring paired image-report supervision.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>BUSTR consistently demonstrated improvements in both standard natural language generation (NLG) metrics and critical clinical efficacy metrics when evaluated on the BrEaST and BUS-BRA datasets. Significant enhancements were observed specifically for key clinical targets such as BI-RADS category and pathology. These results confirm that the descriptor-aware vision model, trained with a combined token-level and alignment loss, effectively generates accurate and clinically relevant reports without relying on paired image-report data.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This framework has substantial potential to enhance the efficiency and quality of breast ultrasound diagnostics. By automating the generation of comprehensive and accurate reports, it can significantly reduce radiologist workload, decrease reporting times, and improve the consistency of diagnostic conclusions. Its improved performance on critical clinical indicators like BI-RADS classification can lead to more precise risk stratification and timely patient management in breast cancer screening and diagnosis.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the BUSTR model itself. However, the work inherently addresses critical limitations of prior radiology report generation models, such as their dependence on large and often scarce paired image-report datasets, and the susceptibility of large language models to generating inaccurate or 'hallucinated' information.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Breast Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Breast Ultrasound</span>
                    
                    <span class="tag tag-keyword">Radiology Report Generation</span>
                    
                    <span class="tag tag-keyword">Vision-Language Model</span>
                    
                    <span class="tag tag-keyword">Descriptor-Aware</span>
                    
                    <span class="tag tag-keyword">BI-RADS</span>
                    
                    <span class="tag tag-keyword">Multitask Learning</span>
                    
                    <span class="tag tag-keyword">Swin Transformer</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">AI in Radiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>13 pages, 2 figures, 6 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>