<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model - Health AI Hub</title>
    <meta name="description" content="This paper introduces BUSTR, a novel multitask vision-language framework designed for automated breast ultrasound (BUS) report generation (RRG). BUSTR overcomes">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20956v1" target="_blank">2511.20956v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Rawa Mohammed, Mina Attin, Bryar Shareef
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20956v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20956v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces BUSTR, a novel multitask vision-language framework designed for automated breast ultrasound (BUS) report generation (RRG). BUSTR overcomes limitations of prior RRG methods, such as the scarcity of paired image-report datasets and large language model hallucinations, by generating reports from structured descriptors and achieving improved natural language generation and clinical efficacy metrics without requiring paired supervision.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Improving the automation and accuracy of breast ultrasound reporting is paramount for efficient and reliable diagnosis of breast conditions, including cancer, leading to more consistent patient management and potentially reducing diagnostic delays and errors in clinical practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Automated generation of radiology reports for breast ultrasound examinations, which can assist radiologists in diagnosis, documentation, and potentially improve efficiency and accuracy in breast cancer screening and follow-up.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical limitations in automated BUS report generation, specifically the lack of paired image-report datasets and the risk of hallucinations from large language models.</li>
                    
                    <li>Proposes BUSTR, a multitask vision-language framework that generates BUS reports without requiring explicit paired image-report supervision.</li>
                    
                    <li>Reports are constructed by leveraging structured clinical descriptors (e.g., BI-RADS, pathology, histology) and radiomics features.</li>
                    
                    <li>Utilizes a multi-head Swin encoder to learn descriptor-aware visual representations, which is trained using a multitask loss over dataset-specific descriptor sets.</li>
                    
                    <li>Aligns visual and textual tokens through a novel dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations.</li>
                    
                    <li>Evaluated on two public BUS datasets, BrEaST and BUS-BRA, demonstrating consistent improvements across standard natural language generation (NLG) metrics and clinical efficacy metrics.</li>
                    
                    <li>Shows particular improvement in accuracy for key clinical targets such as BI-RADS category and pathology, indicating strong clinical relevance.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>BUSTR is a multitask vision-language framework. It learns descriptor-aware visual representations from breast ultrasound images using a multi-head Swin encoder, trained with a multitask loss over structured descriptor sets (BI-RADS, pathology, histology) and radiomics features. A dual-level objective function is employed to align visual and textual tokens, combining token-level cross-entropy for generation with a cosine-similarity alignment loss between input and output representations. This approach specifically avoids the need for paired image-report supervision.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>BUSTR consistently improved both standard natural language generation (NLG) metrics and clinical efficacy metrics across the BrEaST and BUS-BRA datasets. Notably, it achieved significant improvements for crucial clinical targets such as BI-RADS category and pathology prediction, demonstrating the effectiveness of its descriptor-aware vision model and combined loss function in generating high-quality reports without requiring paired image-report data.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to significantly streamline and standardize the process of breast ultrasound reporting. By automating report generation with increased accuracy, especially for critical diagnostic information like BI-RADS categories and pathology findings, BUSTR can assist radiologists in making faster and more consistent diagnoses, reduce inter-observer variability, and ultimately contribute to more effective patient care and management pathways for breast diseases.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the limitations of prior automated radiology report generation methods (lack of paired datasets, LLM hallucinations) which BUSTR aims to solve. It does not explicitly state any limitations or caveats specific to the BUSTR model itself.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Breast ultrasound</span>
                    
                    <span class="tag tag-keyword">Radiology report generation</span>
                    
                    <span class="tag tag-keyword">Vision-language model</span>
                    
                    <span class="tag tag-keyword">Deep learning</span>
                    
                    <span class="tag tag-keyword">BI-RADS</span>
                    
                    <span class="tag tag-keyword">Medical imaging</span>
                    
                    <span class="tag tag-keyword">Artificial intelligence</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>13 pages, 2 figures, 6 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>