<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features - Health AI Hub</title>
    <meta name="description" content="This paper introduces FusionDP, a novel two-step framework designed to enhance model utility in differentially private learning when only a subset of features a">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03806v1" target="_blank">2511.03806v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Linghui Zeng, Ruixuan Liu, Atiquer Rahman Sarkar, Xiaoqian Jiang, Joyce C. Ho, Li Xiong
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03806v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03806v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces FusionDP, a novel two-step framework designed to enhance model utility in differentially private learning when only a subset of features are sensitive. It leverages large foundation models to impute sensitive features as external priors, followed by a modified DP-SGD algorithm that trains models on both original and imputed features while formally preserving privacy. FusionDP significantly improves model performance and the privacy-utility trade-off on tasks like sepsis prediction and clinical note classification compared to existing privacy-preserving baselines.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for medicine and health as it provides a method to build accurate AI models using highly sensitive patient data (e.g., ICU demographics, clinical notes) while rigorously protecting individual privacy, which is a major barrier to deploying AI in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper introduces FusionDP, an AI framework utilizing large foundation models for imputing sensitive features, combined with a modified differentially private SGD algorithm. This allows for training more accurate machine learning models on partially sensitive health data (e.g., ICU patient demographics, clinical notes) while rigorously preserving patient privacy. This AI application directly addresses a critical challenge in developing trustworthy and deployable AI systems for medical tasks like disease prediction (e.g., sepsis) and automated clinical note analysis.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of differentially private learning for partially sensitive features, where traditional DP-SGD injects excessive noise by over-protecting all features.</li>
                    
                    <li>Proposes FusionDP, a two-step framework, to improve model utility under feature-level differential privacy.</li>
                    
                    <li>The first step involves utilizing large foundation models to impute sensitive features based on non-sensitive features, treating these imputations as high-quality external priors without direct access to true sensitive values during training.</li>
                    
                    <li>The second step introduces a modified DP-SGD algorithm that trains models on a combination of original non-sensitive features and the generated imputed sensitive features.</li>
                    
                    <li>This modified DP-SGD formally preserves the privacy of the original sensitive features, ensuring rigorous privacy guarantees.</li>
                    
                    <li>FusionDP was evaluated on two distinct medical modalities: a sepsis prediction task using tabular data from PhysioNet and a clinical note classification task using data from MIMIC-III.</li>
                    
                    <li>Results demonstrate that FusionDP significantly outperforms privacy-preserving baselines, achieving enhanced model performance and a better privacy-utility trade-off across both evaluated tasks.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>FusionDP employs a two-step framework. First, large foundation models are leveraged to impute sensitive features based on available non-sensitive features, effectively creating high-quality synthetic estimates as external priors. Second, a modified Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm is applied to train the predictive model using both the original non-sensitive features and the imputed sensitive features, thereby formally preserving the differential privacy of the true sensitive attributes.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>FusionDP significantly improves model performance and utility in medical tasks such as sepsis prediction (on PhysioNet tabular data) and clinical note classification (on MIMIC-III data) compared to privacy-preserving baselines, while maintaining rigorous feature-level differential privacy. This demonstrates its potential to enhance the privacy-utility trade-off across various data modalities.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By enabling the development of more accurate and robust AI models under strict privacy constraints, FusionDP can accelerate the safe and ethical deployment of machine learning in clinical settings. This could lead to improved early detection systems for conditions like sepsis, better clinical decision support from medical records, and enhanced patient outcomes, all while safeguarding sensitive patient information.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Intensive Care Unit (ICU)</span>
                    
                    <span class="tag">Sepsis</span>
                    
                    <span class="tag">Clinical Natural Language Processing (NLP)</span>
                    
                    <span class="tag">Biomedical Informatics</span>
                    
                    <span class="tag">Patient Privacy</span>
                    
                    <span class="tag">Medical Machine Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Differential Privacy</span>
                    
                    <span class="tag tag-keyword">Foundation Models</span>
                    
                    <span class="tag tag-keyword">Feature Imputation</span>
                    
                    <span class="tag tag-keyword">Privacy-Preserving Machine Learning</span>
                    
                    <span class="tag tag-keyword">Sepsis Prediction</span>
                    
                    <span class="tag tag-keyword">Clinical Notes</span>
                    
                    <span class="tag tag-keyword">Privacy-Utility Trade-off</span>
                    
                    <span class="tag tag-keyword">Medical Data Privacy</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>