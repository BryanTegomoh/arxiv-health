<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Investigating Associational Biases in Inter-Model Communication of Large Generative Models - Health AI Hub</title>
    <meta name="description" content="This paper investigates how social associational biases propagate and amplify in inter-model communication pipelines of large generative AI, focusing on human a">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Investigating Associational Biases in Inter-Model Communication of Large Generative Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.22093v1" target="_blank">2601.22093v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-29
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Fethiye Irmak Dogan, Yuval Weiss, Kajal Patel, Jiaee Cheong, Hatice Gunes
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CY, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.22093v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.22093v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper investigates how social associational biases propagate and amplify in inter-model communication pipelines of large generative AI, focusing on human activity recognition and affective expression. It reveals systematic demographic drifts towards younger representations for both actions and emotions, and more female-presenting representations primarily for emotions, often supported by spurious visual cues. The study highlights the critical need for safeguards and mitigation strategies in human-centered AI systems to prevent unequal treatment stemming from these biases.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>In healthcare, AI systems performing human-centered perception tasks like patient activity monitoring, fall detection, or affect prediction could develop and propagate biases. This could lead to misinterpretations of patient states, inappropriate care recommendations, or unequal treatment for specific demographic groups, such as elderly patients or individuals of particular genders or ethnicities.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is crucial for developing fair and equitable AI systems in healthcare, particularly for applications involving patient monitoring (e.g., activity levels, fall detection), mental health assessment (e.g., emotion analysis), personalized rehabilitation programs, and virtual care platforms. It highlights the need to mitigate biases in generative models used to interpret human behavior and affect, ensuring that these systems do not perpetuate or amplify stereotypes, leading to misdiagnosis or unequal treatment for diverse patient populations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Generative AI models can manifest associational bias, reproducing stereotypical links between concepts and demographic groups (e.g., doctors associated with men) even without explicit demographic information.</li>
                    
                    <li>These biases are shown to propagate and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output serves as another's input.</li>
                    
                    <li>The research focuses on human activity and affective expression, utilizing an inter-model pipeline that alternates between image generation and image description, applied to RAF-DB and PHASE datasets.</li>
                    
                    <li>Quantifiable demographic distribution drifts were observed: specifically, a shift towards younger representations for both actions and emotions, and towards more female-presenting representations primarily for emotions.</li>
                    
                    <li>An explainability pipeline revealed that some biased predictions were supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face).</li>
                    
                    <li>The identified demographic drifts translate into measurable differences in downstream behavior, impacting the prediction of activity and emotion labels.</li>
                    
                    <li>The paper emphasizes the necessity for careful safeguards and outlines mitigation strategies, including data-centric, training, and deployment interventions, for interconnected human-centered AI systems.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study utilized an inter-model communication pipeline that iteratively alternates between image generation and image description. It employed the RAF-DB dataset for facial expressions and the PHASE dataset for human activities. The methodology involved quantifying demographic distribution drift induced by model-to-model information exchange and assessing its systematic nature using an explainability pipeline. The researchers also examined whether these demographic drifts resulted in measurable differences in downstream activity and emotion label predictions.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The research revealed systematic demographic drifts within the inter-model communication pipeline: representations shifted towards younger individuals for both actions and emotions, and towards more female-presenting individuals primarily for emotions. A significant finding was that some predictions were supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). These demographic drifts were found to translate into measurable differences in the downstream prediction accuracy and characteristics of activity and emotion labels.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings imply that generative AI models used in clinical practice for tasks like patient monitoring, symptom assessment, or behavioral analysis could inadvertently encode and amplify biases. For instance, an AI system biased towards younger representations might misinterpret the activity levels or emotional states of older patients, potentially leading to incorrect diagnoses or delayed interventions. Similarly, gender-biased affect prediction could misrepresent mental health indicators. This necessitates rigorous bias detection, mitigation, and validation strategies for AI deployment in healthcare to ensure equitable and accurate patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract, but implied by the specific focus on RAF-DB and PHASE datasets and the tasks of activity/affect prediction, the findings might not universally generalize to all types of generative models, medical imaging datasets, or other human-centered AI tasks. The abstract does not detail the specific architectures of the generative models employed.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper outlines a strong emphasis on developing and implementing mitigation strategies across data-centric, training, and deployment phases. Future research should focus on operationalizing these interventions, such as creating demographically balanced datasets, developing bias-aware training algorithms, and establishing robust safeguards during model deployment. This aims to prevent the propagation and amplification of associational biases in real-world human-centered AI systems, especially within sensitive domains like healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Remote Patient Monitoring</span>
                    
                    <span class="tag">Geriatrics</span>
                    
                    <span class="tag">Mental Health</span>
                    
                    <span class="tag">Assistive Technology</span>
                    
                    <span class="tag">Diagnostic Bias</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Generative AI</span>
                    
                    <span class="tag tag-keyword">Associational Bias</span>
                    
                    <span class="tag tag-keyword">Inter-Model Communication</span>
                    
                    <span class="tag tag-keyword">Demographic Drift</span>
                    
                    <span class="tag tag-keyword">Human Activity Recognition</span>
                    
                    <span class="tag tag-keyword">Affect Prediction</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">Social Bias</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>