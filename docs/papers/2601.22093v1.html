<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Investigating Associational Biases in Inter-Model Communication of Large Generative Models - Health AI Hub</title>
    <meta name="description" content="This paper investigates how associational biases, such as stereotypical links between concepts and demographic groups, evolve and propagate within inter-model c">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Investigating Associational Biases in Inter-Model Communication of Large Generative Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.22093v1" target="_blank">2601.22093v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-29
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Fethiye Irmak Dogan, Yuval Weiss, Kajal Patel, Jiaee Cheong, Hatice Gunes
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CY, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.85 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.22093v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.22093v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper investigates how associational biases, such as stereotypical links between concepts and demographic groups, evolve and propagate within inter-model communication pipelines of large generative AI models. Focusing on human activity recognition and affect prediction, the study reveals demographic drifts towards younger representations for actions and emotions, and towards female-presenting representations primarily for emotions, potentially driven by spurious visual cues. It highlights the critical need for safeguards to prevent the amplification of biases in human-centred AI systems.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine and health as AI systems are increasingly used for critical human-centred perception tasks such as patient activity monitoring, emotion assessment for mental health, and diagnostic support. Biases in these systems can lead to misdiagnosis, inequitable treatment, or flawed care plans, particularly if they misinterpret or misrepresent demographic groups based on learned stereotypes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research investigates how biases in generative AI models, specifically for tasks like human activity recognition and affect prediction, can propagate and amplify. In healthcare, such AI systems are increasingly used for patient monitoring (e.g., tracking activity levels for rehabilitation or fall detection), assessing mental well-being, pain detection (via affect analysis), and personalized care. Understanding and mitigating these biases is crucial to develop medical AI applications that are accurate, fair, and do not lead to 'unequal treatment' or perpetuate stereotypes across diverse patient populations. The findings and proposed mitigation strategies are directly applicable to ensuring the ethical and effective deployment of AI in various health-related tools and systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Generative AI models can manifest 'associational bias,' reproducing stereotypical links between concepts and demographic groups (e.g., associating doctors with men), even without explicit demographic input.</li>
                    
                    <li>These biases can propagate and amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input, particularly in human-centred perception tasks.</li>
                    
                    <li>The study utilized an inter-model communication pipeline alternating between image generation and description, employing RAF-DB and PHASE datasets to quantify demographic distribution drift in human activity and affective expression.</li>
                    
                    <li>Results revealed systematic demographic drifts: towards younger representations for both actions and emotions, and towards more female-presenting representations primarily for emotions.</li>
                    
                    <li>An explainability pipeline found that some model predictions were supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face), indicating a lack of robust feature learning.</li>
                    
                    <li>The research also examined if these demographic drifts translate into measurable differences in downstream prediction tasks (activity and emotion labels), implying potential impact on real-world applications.</li>
                    
                    <li>The paper emphasizes the necessity for careful safeguards across data-centric, training, and deployment interventions to mitigate bias when deploying interconnected generative models in human-centred AI systems.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employed an inter-model communication pipeline that alternated between image generation and image description. This pipeline was used to process data from the RAF-DB (affective expression) and PHASE (human activity) datasets. Demographic distribution drift in the generated representations was quantified. An explainability pipeline was then utilized to assess whether these drifts were systematic and if predictions were supported by spurious visual regions rather than concept-relevant cues. The impact of these drifts on downstream activity and emotion label prediction was also examined.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key findings include systematic demographic drifts in generated representations: a consistent shift towards younger representations for both human actions and emotional expressions. Additionally, a drift towards more female-presenting representations was observed, primarily for emotions. Critically, it was found that some predictions were based on spurious visual features (e.g., background, hair) rather than semantically relevant body or face regions, suggesting a fundamental issue in feature learning that contributes to bias propagation.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact is significant: AI systems used for patient monitoring (e.g., detecting falls or activity levels in elderly patients), mental health assessment (e.g., interpreting emotional states), or rehabilitation progress tracking could inadvertently amplify biases. This could lead to inaccurate or disproportionate assessments for certain demographic groups, such as misinterpreting emotions in older adults or specific genders, potentially resulting in suboptimal treatment plans, misdiagnosis, or exacerbating existing health disparities if robust mitigation strategies are not implemented.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the conducted study. However, the identified prevalence and propagation of associational biases, particularly from spurious visual cues, inherently represent significant limitations in the current development and deployment of generative AI for human-centred tasks, necessitating the mitigation strategies proposed. The scope of the investigation focused on image-based inter-model communication for human activity and affective expression.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper outlines a critical future direction: the development and implementation of mitigation strategies across data-centric (e.g., creating debiased datasets), training (e.g., developing bias-aware algorithms), and deployment interventions (e.g., continuous monitoring and bias auditing). This emphasizes the need for ongoing research and engineering efforts to ensure fairness, robustness, and ethical deployment of interconnected generative AI systems in sensitive applications like healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Digital health</span>
                    
                    <span class="tag">remote patient monitoring</span>
                    
                    <span class="tag">mental health assessment</span>
                    
                    <span class="tag">geriatric care</span>
                    
                    <span class="tag">rehabilitation medicine</span>
                    
                    <span class="tag">medical ethics</span>
                    
                    <span class="tag">personalized medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Generative AI</span>
                    
                    <span class="tag tag-keyword">social bias</span>
                    
                    <span class="tag tag-keyword">associational bias</span>
                    
                    <span class="tag tag-keyword">inter-model communication</span>
                    
                    <span class="tag tag-keyword">demographic drift</span>
                    
                    <span class="tag tag-keyword">human activity recognition</span>
                    
                    <span class="tag tag-keyword">affect prediction</span>
                    
                    <span class="tag tag-keyword">fairness in AI</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>