<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>From ACR O-RADS 2022 to Explainable Deep Learning: Comparative Performance of Expert Radiologists, Convolutional Neural Networks, Vision Transformers, and Fusion Models in Ovarian Masses - Health AI Hub</title>
    <meta name="description" content="This study comprehensively compares the diagnostic performance of expert radiologists applying O-RADS v2022 with various deep learning (DL) models, including Co">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>From ACR O-RADS 2022 to Explainable Deep Learning: Comparative Performance of Expert Radiologists, Convolutional Neural Networks, Vision Transformers, and Fusion Models in Ovarian Masses</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.06282v1" target="_blank">2511.06282v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-09
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ali Abbasian Ardakani, Afshin Mohammadi, Alisa Mohebbi, Anushya Vijayananthan, Sook Sam Leong, Lim Yi Ting, Mohd Kamil Bin Mohamad Fabell, U Rajendra Acharya, Sepideh Hatamikia
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.06282v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.06282v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study comprehensively compares the diagnostic performance of expert radiologists applying O-RADS v2022 with various deep learning (DL) models, including Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), along with hybrid human-AI frameworks, for classifying ovarian masses. The findings indicate that ViT models significantly outperform radiologists and other CNNs, achieving the highest individual performance, and that integrating expert scores with AI yields superior diagnostic accuracy, particularly enhancing CNNs.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant as it proposes a method to significantly improve the accuracy and standardization of ovarian mass characterization, potentially leading to earlier and more precise diagnosis of ovarian cancer, reduced false positives, and optimization of patient management pathways by minimizing unnecessary invasive procedures.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves using various deep learning models (Convolutional Neural Networks and Vision Transformers) to analyze ultrasound images of adnexal masses for the characterization and risk stratification of ovarian lesions. The AI aims to assist or outperform human radiologists in identifying malignant cysts and improving diagnostic accuracy, potentially leading to earlier and more precise diagnoses in women's health.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study evaluated radiologist performance using the O-RADS v2022 update, comparing it with 16 distinct deep learning models and hybrid human-AI frameworks.</li>
                    
                    <li>A single-center, retrospective cohort of 512 adnexal mass images from 227 patients (110 with malignant cysts) was utilized for model training, validation, and radiologist assessment.</li>
                    
                    <li>Radiologist-only O-RADS v2022 assessment achieved an Area Under the Curve (AUC) of 0.683 and an overall accuracy of 68.0%.</li>
                    
                    <li>Deep learning models, especially the Vision Transformer (ViT16-384), demonstrated superior performance, with ViT16-384 reaching an AUC of 0.941 and an accuracy of 87.4%.</li>
                    
                    <li>CNN models showed a wide range of performance (AUCs 0.620-0.908, accuracies 59.2%-86.4%), with some outperforming radiologists.</li>
                    
                    <li>Hybrid human-AI frameworks significantly enhanced the performance of CNN models, although the improvement for ViT models was not statistically significant (P-value > 0.05).</li>
                    
                    <li>The study concludes that DL models markedly outperform radiologist-only O-RADS, and hybrid integration of expert scores with AI offers the highest diagnostic accuracy and discrimination.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>A single-center, retrospective cohort study involved 512 adnexal mass ultrasound images from 227 patients. Sixteen deep learning models, encompassing various CNN architectures (DenseNets, EfficientNets, ResNets, VGGs, Xception) and Vision Transformers (ViTs), were trained and validated. Expert radiologists' O-RADS v2022 assessments were used as a baseline for comparison. Additionally, hybrid models were constructed by integrating radiologist O-RADS scores with the probabilities predicted by each DL model.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Expert radiologists achieved an AUC of 0.683 and 68.0% accuracy with O-RADS v2022. CNN models showed a broad performance range (AUCs 0.620-0.908, accuracies 59.2%-86.4%). The Vision Transformer (ViT16-384) model demonstrated the best individual performance with an AUC of 0.941 and 87.4% accuracy. Hybrid human-AI frameworks significantly improved the performance of CNN models, though not statistically for ViT models (P > 0.05). Overall, deep learning models markedly outperformed radiologist-only O-RADS v2022 assessment.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The implementation of advanced deep learning models, particularly Vision Transformers, or hybrid human-AI frameworks in clinical practice could standardize pelvic ultrasound interpretation for adnexal masses. This could lead to a significant reduction in inter-observer variability, lower rates of false positives (thereby decreasing unnecessary biopsies and surgeries), and a substantial improvement in the early and accurate detection of high-risk malignant lesions, ultimately optimizing patient care and resource allocation.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study's single-center, retrospective design inherently limits the generalizability of its findings, as performance may vary across different institutions, patient populations, and ultrasound equipment. The abstract does not detail external validation on an independent dataset, which is crucial for assessing real-world applicability.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly stated, the findings suggest future research should focus on prospective, multi-center validation studies to confirm the generalizability and robustness of these DL models and hybrid frameworks. Further exploration into the explainability of these complex AI models and their seamless integration into existing clinical workflows would also be critical for widespread adoption.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Gynecology</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Ovarian-Adnexal Reporting and Data System</span>
                    
                    <span class="tag tag-keyword">O-RADS v2022</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Convolutional Neural Networks</span>
                    
                    <span class="tag tag-keyword">Vision Transformers</span>
                    
                    <span class="tag tag-keyword">Hybrid AI</span>
                    
                    <span class="tag tag-keyword">Adnexal Mass Characterization</span>
                    
                    <span class="tag tag-keyword">Pelvic Ultrasound</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Background: The 2022 update of the Ovarian-Adnexal Reporting and Data System
(O-RADS) ultrasound classification refines risk stratification for adnexal
lesions, yet human interpretation remains subject to variability and
conservative thresholds. Concurrently, deep learning (DL) models have
demonstrated promise in image-based ovarian lesion characterization. This study
evaluates radiologist performance applying O-RADS v2022, compares it to leading
convolutional neural network (CNN) and Vision Transformer (ViT) models, and
investigates the diagnostic gains achieved by hybrid human-AI frameworks.
Methods: In this single-center, retrospective cohort study, a total of 512
adnexal mass images from 227 patients (110 with at least one malignant cyst)
were included. Sixteen DL models, including DenseNets, EfficientNets, ResNets,
VGGs, Xception, and ViTs, were trained and validated. A hybrid model
integrating radiologist O-RADS scores with DL-predicted probabilities was also
built for each scheme. Results: Radiologist-only O-RADS assessment achieved an
AUC of 0.683 and an overall accuracy of 68.0%. CNN models yielded AUCs of 0.620
to 0.908 and accuracies of 59.2% to 86.4%, while ViT16-384 reached the best
performance, with an AUC of 0.941 and an accuracy of 87.4%. Hybrid human-AI
frameworks further significantly enhanced the performance of CNN models;
however, the improvement for ViT models was not statistically significant
(P-value >0.05). Conclusions: DL models markedly outperform radiologist-only
O-RADS v2022 assessment, and the integration of expert scores with AI yields
the highest diagnostic accuracy and discrimination. Hybrid human-AI paradigms
hold substantial potential to standardize pelvic ultrasound interpretation,
reduce false positives, and improve detection of high-risk lesions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>18 pages, 4 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>