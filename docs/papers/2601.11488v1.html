<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation - Health AI Hub</title>
    <meta name="description" content="This paper introduces CTest-Metric, a novel unified framework designed to assess the clinical validity and robustness of metrics used for automated radiology re">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.11488v1" target="_blank">2601.11488v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-16
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Vanshali Sharma, Andrea Mia Bejar, Gorkem Durak, Ulas Bagci
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.11488v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.11488v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces CTest-Metric, a novel unified framework designed to assess the clinical validity and robustness of metrics used for automated radiology report generation (RRG) in CT imaging. By evaluating eight common metrics across three modules focusing on writing style generalizability, synthetic error injection, and expert correlation, the study found that lexical metrics are sensitive to style, GREEN Score best correlates with expert judgments, and BERTScore-F1 is robust to factual errors. The framework aims to address the current reliance on suboptimal metrics by providing a systematic assessment tool.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and reliable automated radiology report generation can significantly improve diagnostic efficiency and reduce radiologist workload. This framework directly contributes to patient safety and diagnostic quality by ensuring that the metrics used to evaluate these AI systems are clinically valid and robust, leading to more trustworthy medical reports.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the evaluation of models for automated radiology report generation (RRG) from CT scans. This research provides a framework to determine the clinical robustness and applicability of metrics used to assess the quality of AI-generated medical reports, which is essential for the safe and effective deployment of AI in clinical diagnostics.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The paper addresses the critical need for a unified, clinically validated framework to assess the robustness and applicability of metrics for automated radiology report generation (RRG).</li>
                    
                    <li>It introduces CTest-Metric, a novel three-module framework comprising: (i) Writing Style Generalizability (WSG) using LLM-based rephrasing, (ii) Synthetic Error Injection (SEI) at graded severities, and (iii) Metrics-vs-Expert correlation (MvE) utilizing clinician ratings on 175 disagreement cases.</li>
                    
                    <li>Eight widely used RRG metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) were evaluated, with generative models built on a CT-CLIP encoder for report generation and rephrasing.</li>
                    
                    <li>A key finding indicates that traditional lexical NLG metrics (e.g., BLEU, ROUGE, METEOR) are highly sensitive to stylistic variations in CT reports.</li>
                    
                    <li>The GREEN Score demonstrated the highest alignment with expert clinical judgments, achieving a Spearman correlation of approximately 0.70, while CRG showed a negative correlation.</li>
                    
                    <li>BERTScore-F1 was identified as the least sensitive metric to the injection of synthetic factual errors, suggesting better robustness in assessing factual accuracy.</li>
                    
                    <li>The authors commit to releasing the framework, code, and anonymized evaluation data to facilitate reproducible benchmarking and future metric development in the field.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>CTest-Metric is a unified framework with three modules. The **Writing Style Generalizability (WSG)** module assesses metric robustness to stylistic changes by rephrasing CT reports using LLM-based techniques. The **Synthetic Error Injection (SEI)** module evaluates metric sensitivity to factual errors by introducing errors of graded severities into reports. The **Metrics-vs-Expert correlation (MvE)** module quantifies the agreement between metric scores and clinician ratings on 175 'disagreement' cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) were studied, utilizing seven LLMs built on a CT-CLIP encoder for report processing.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Lexical Natural Language Generation (NLG) metrics demonstrate high sensitivity to stylistic variations in CT reports. The GREEN Score exhibits the strongest positive correlation (~0.70 Spearman) with expert clinical judgments, making it a reliable indicator of report quality. Conversely, CRG shows a negative correlation with expert assessments. BERTScore-F1 is identified as the least sensitive metric to synthetic factual error injection, indicating its robustness in identifying factual inaccuracies.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This framework provides a crucial tool for developers and clinicians to rigorously evaluate the quality assessment metrics for automated radiology report generation. By identifying metrics that align well with clinical judgment and are robust to errors and style variations, it will guide the creation of more clinically valid and reliable AI systems. This will lead to more accurate and trustworthy radiology reports, enhance diagnostic confidence, streamline radiologist workflows, and ultimately improve patient care and safety by reducing potential misinterpretations or errors in automated medical documentation.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the CTest-Metric framework or the presented study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors plan to release the CTest-Metric framework, code, and allowable anonymized evaluation data (including rephrased and error-injected CT reports). This will facilitate reproducible benchmarking of existing metrics and foster the development of more clinically robust and effective metrics for automated radiology report generation.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Medical AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">radiology report generation</span>
                    
                    <span class="tag tag-keyword">CT reports</span>
                    
                    <span class="tag tag-keyword">clinical validity</span>
                    
                    <span class="tag tag-keyword">metric assessment</span>
                    
                    <span class="tag tag-keyword">generative AI</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">BERTScore</span>
                    
                    <span class="tag tag-keyword">GREEN Score</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 "disagreement" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted at ISBI 2026</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>