<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation - Health AI Hub</title>
    <meta name="description" content="This paper introduces CTest-Metric, a novel unified framework designed to assess the clinical validity and feasibility of metrics used for CT radiology report g">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.11488v1" target="_blank">2601.11488v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-16
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Vanshali Sharma, Andrea Mia Bejar, Gorkem Durak, Ulas Bagci
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.11488v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.11488v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces CTest-Metric, a novel unified framework designed to assess the clinical validity and feasibility of metrics used for CT radiology report generation (RRG). By evaluating eight widely used metrics across three modules (Writing Style Generalizability, Synthetic Error Injection, Metrics-vs-Expert correlation), the study identifies strengths and weaknesses, highlighting GREEN Score's strong alignment with expert judgment and BERTScore-F1's robustness to factual errors.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Ensuring the accuracy, reliability, and clinical validity of metrics used to evaluate AI-generated radiology reports is paramount for patient safety, diagnostic precision, and the trustworthiness of automated medical tasks. This framework provides a crucial tool for objectively assessing these metrics, thereby guiding the development of clinically sound RRG systems.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The primary AI application is the evaluation and improvement of AI models (specifically Large Language Models) for generating radiology reports from CT scans. This falls under medical NLP and AI-assisted diagnostics, aiming to ensure AI-generated reports are accurate, clinically valid, and reliable for use by medical professionals in patient care.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Identifies the critical need for a unified, well-defined framework to assess the clinical validity and robustness of metrics used in automated radiology report generation (RRG).</li>
                    
                    <li>Presents CTest-Metric, the first unified framework comprising three modules: (i) Writing Style Generalizability (WSG) using LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) with clinician ratings on 175 disagreement cases.</li>
                    
                    <li>Evaluates eight prominent metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) utilizing seven LLMs built on a CT-CLIP encoder.</li>
                    
                    <li>Key findings indicate that lexical NLG metrics are highly sensitive to stylistic variations in reports.</li>
                    
                    <li>GREEN Score demonstrated the strongest alignment with expert clinician judgments, achieving a Spearman correlation of approximately 0.70, while CRG showed a negative correlation.</li>
                    
                    <li>BERTScore-F1 was found to be the least sensitive metric to the injection of synthetic factual errors, suggesting better robustness in detecting factual inaccuracies.</li>
                    
                    <li>The authors commit to releasing the framework, code, and anonymized evaluation data to promote reproducible benchmarking and future advancements in RRG metric development.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study developed CTest-Metric, a three-module framework for metric assessment. The modules include: (i) Writing Style Generalizability (WSG), evaluated by rephrasing CT reports using LLMs; (ii) Synthetic Error Injection (SEI), where errors of varying severities are introduced into reports; and (iii) Metrics-vs-Expert correlation (MvE), comparing metric scores against clinician ratings on 175 'disagreement' cases. Eight established metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) were tested across seven LLMs integrated with a CT-CLIP encoder.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Lexical NLG metrics exhibit high sensitivity to stylistic variations. GREEN Score shows the best alignment with expert clinical judgments (Spearman correlation ~0.70), whereas CRG demonstrates a negative correlation. BERTScore-F1 is identified as the least sensitive metric to factual error injection, indicating superior robustness in detecting factual inaccuracies.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This framework offers a standardized and robust method for evaluating the clinical utility of metrics designed for AI-generated CT reports. It empowers developers and clinicians to select and refine metrics that genuinely reflect report quality and factual correctness, thereby fostering the creation of more reliable and clinically acceptable automated RRG systems and ultimately enhancing diagnostic accuracy and patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Limitations were not explicitly detailed within the provided abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors plan to release the CTest-Metric framework, associated code, and an allowable portion of the anonymized evaluation data (including rephrased and error-injected CT reports). This release aims to facilitate reproducible benchmarking and stimulate further research and development in the field of radiology report generation metrics.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Radiology Report Generation (RRG)</span>
                    
                    <span class="tag tag-keyword">CT Report Generation</span>
                    
                    <span class="tag tag-keyword">Clinical Validity</span>
                    
                    <span class="tag tag-keyword">Metric Assessment</span>
                    
                    <span class="tag tag-keyword">Generative AI</span>
                    
                    <span class="tag tag-keyword">Natural Language Generation (NLG)</span>
                    
                    <span class="tag tag-keyword">LLM</span>
                    
                    <span class="tag tag-keyword">CT-CLIP</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 "disagreement" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted at ISBI 2026</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>