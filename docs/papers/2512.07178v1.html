<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation - Health AI Hub</title>
    <meta name="description" content="This paper introduces ContextualSHAP, a Python package that extends the SHAP XAI method by integrating it with Large Language Models (LLMs), specifically OpenAI">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.07178v1" target="_blank">2512.07178v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-08
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Latifa Dwiyanti, Sergio Ryan Wibisono, Hidetaka Nambo
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI, cs.HC, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.07178v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.07178v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces ContextualSHAP, a Python package that extends the SHAP XAI method by integrating it with Large Language Models (LLMs), specifically OpenAI's GPT, to generate contextualized textual explanations. Addressing SHAP's limitation in providing meaningful context for non-technical users, the tool uses user-defined parameters to tailor explanations. Preliminary user evaluations in a healthcare case study suggest that these combined visual-textual explanations are perceived as more understandable and contextually appropriate than visual-only outputs.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as it aims to make complex AI model decisions more interpretable and understandable for healthcare professionals. By providing contextual explanations, it can enhance trust and facilitate the responsible deployment of AI in high-stakes medical domains like diagnostics and treatment planning.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research enhances the explainability (XAI) of AI models used in healthcare by providing contextualized, natural language explanations alongside traditional visualizations. This is critical for medical AI applications where clinicians, patients, or administrators need to understand why an AI model made a particular prediction or recommendation (e.g., diagnosis, treatment plan, risk assessment). By making AI explanations more accessible and trustworthy, it facilitates safer and more effective integration of AI into clinical practice and healthcare decision-making, improving user acceptance and supporting more informed actions based on AI outputs.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>SHAP, while prominent for feature importance visualization, often lacks contextual explanations that are meaningful for non-technical end-users.</li>
                    
                    <li>The proposed solution, ContextualSHAP, is a Python package that integrates SHAP with an LLM (OpenAI's GPT) to generate rich, contextualized textual explanations.</li>
                    
                    <li>Explanations are customized using user-defined parameters, including feature aliases, descriptions, and additional background, allowing tailoring to model context and user perspective.</li>
                    
                    <li>The effectiveness was evaluated through a healthcare-related case study, involving real end-users.</li>
                    
                    <li>User evaluations, utilizing Likert-scale surveys and follow-up interviews, indicated improved perceived understandability and contextual appropriateness of the generated explanations.</li>
                    
                    <li>The findings suggest that combining SHAP's visual outputs with LLM-generated contextual text can lead to more user-friendly and trustworthy model explanations.</li>
                    
                    <li>The results are considered preliminary, highlighting the need for further validation.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involved developing a Python package (ContextualSHAP) that integrates the SHAP explanation framework with OpenAI's GPT LLM. This integration enables the generation of natural language explanations, guided by user-defined parameters (e.g., feature aliases, descriptions, background information). The package's effectiveness was evaluated through a healthcare-related case study, employing user evaluations via Likert-scale surveys and follow-up interviews with real end-users to assess perceived understandability and contextual appropriateness.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary findings indicate that the LLM-generated contextual explanations, when combined with SHAP visualizations, were perceived by end-users as significantly more understandable and contextually appropriate compared to visual-only SHAP outputs. This suggests that the proposed integration enhances the interpretability of AI model predictions for non-technical audiences.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This approach has the potential to significantly improve the adoption and trustworthiness of AI systems in clinical settings. By translating complex AI decisions into understandable, context-rich narratives, ContextualSHAP can empower clinicians, who often lack a technical background, to better comprehend model outputs for diagnoses, prognoses, or treatment recommendations, thereby supporting more informed and confident clinical decision-making.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The authors explicitly state that the findings are preliminary. This suggests that further, more extensive validation studies are needed to confirm the generalizability and robustness of the results across diverse AI models, medical domains, and user groups.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly detailed as 'future directions' in the abstract, the preliminary nature of the findings implicitly calls for more comprehensive evaluations to solidify the reported benefits. Further research could focus on validating the long-term impact on user trust, decision-making accuracy, and efficiency in real-world clinical workflows, as well as exploring its applicability across a wider range of medical AI tasks and different LLM architectures.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Medical Diagnostics</span>
                    
                    <span class="tag">Predictive Analytics in Healthcare</span>
                    
                    <span class="tag">Medical Imaging Interpretation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">SHAP</span>
                    
                    <span class="tag tag-keyword">Large Language Model</span>
                    
                    <span class="tag tag-keyword">Contextual Explanations</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Model Interpretability</span>
                    
                    <span class="tag tag-keyword">User Evaluation</span>
                    
                    <span class="tag tag-keyword">GPT</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>This paper was accepted and presented at the 7th World Symposium on Software Engineering (WSSE) 2025 on 25 October 2025 in Okayama, Japan, and is currently awaiting publication</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>