<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning - Health AI Hub</title>
    <meta name="description" content="This paper introduces OctoMed, a novel approach utilizing carefully curated data recipes and supervised fine-tuning (SFT) to develop a robust multimodal medical">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.23269v1" target="_blank">2511.23269v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-28
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Timothy Ossowski, Sheng Zhang, Qianchu Liu, Guanghui Qin, Reuben Tan, Tristan Naumann, Junjie Hu, Hoifung Poon
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.23269v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.23269v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces OctoMed, a novel approach utilizing carefully curated data recipes and supervised fine-tuning (SFT) to develop a robust multimodal medical reasoning model. By leveraging structured reasoning traces and scaling experiments to a vast dataset, the model achieves state-of-the-art performance among open-source models on diverse out-of-distribution medical benchmark tasks. A key finding is the model's ability to self-calibrate its reasoning trajectory lengths based on task requirements, without explicit supervision.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research significantly advances the development of AI capable of complex medical reasoning by improving model robustness and generalization, which is crucial for reliable clinical decision support, diagnostics, and medical education across various specialties.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research focuses on advancing the core methodology for training and curating data for multimodal AI models specifically designed for medical reasoning. These models are intended to assist healthcare professionals in diverse clinical tasks, potentially leading to improved diagnostic accuracy, enhanced decision support, and more robust interpretation of complex medical data, including medical images combined with textual information.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>High-quality, curated data is critical for training robust medical large language models (LLMs) to enhance generalization and robustness to unseen clinical tasks.</li>
                    
                    <li>The methodology focuses on Supervised Fine-Tuning (SFT) and the development of specific 'data recipes' that integrate structured reasoning traces.</li>
                    
                    <li>Experiments were conducted on a large scale, using a dataset comprising over 8 million examples and 6.8 billion response tokens.</li>
                    
                    <li>OctoMed achieved state-of-the-art performance among open-source models across a variety of out-of-distribution medical benchmark tasks.</li>
                    
                    <li>The model demonstrates an ability to self-calibrate the length of its reasoning trajectories based on the specific downstream task, which is facilitated by diverse training data with varying trace lengths.</li>
                    
                    <li>The research provides insights into data curation strategies for multimodal medical reasoning systems.</li>
                    
                    <li>Future work aims at developing a fully robust medical vision-language reasoning system.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employs supervised fine-tuning (SFT) with a focus on creating 'data recipes' that explicitly leverage structured reasoning traces. These traces are incorporated into a large and diverse training dataset (over 8 million examples, 6.8 billion response tokens) to teach the model to generate logical and structured reasoning paths.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The model achieved state-of-the-art performance among open-source models on diverse out-of-distribution medical benchmarks. A crucial discovery is that training with varied structured reasoning trace lengths enables the model to intelligently self-calibrate its reasoning trajectory lengths based on the specific requirements of a downstream task, without explicit task-specific supervision.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This advancement could lead to more accurate and reliable AI assistants for clinicians, enhancing diagnostic capabilities, treatment planning, and patient management by providing robust, explainable reasoning. It could also support medical education by generating detailed reasoning paths, and potentially reduce diagnostic errors by offering comprehensive, AI-driven insights, particularly in complex or multimodal clinical scenarios.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the current work, focusing instead on the achieved state-of-the-art performance and promising future directions.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors aim to further develop robust medical vision-language reasoning systems, suggesting continued work on integrating visual information with language understanding for even more comprehensive medical AI applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">General Diagnostics</span>
                    
                    <span class="tag">Medical Imaging (e.g., Radiology, Pathology)</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Medical Education</span>
                    
                    <span class="tag">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Multimodal Medical Reasoning</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Supervised Fine-Tuning (SFT)</span>
                    
                    <span class="tag tag-keyword">Data Curation</span>
                    
                    <span class="tag tag-keyword">Reasoning Traces</span>
                    
                    <span class="tag tag-keyword">Out-of-Distribution Generalization</span>
                    
                    <span class="tag tag-keyword">Clinical Tasks</span>
                    
                    <span class="tag tag-keyword">Vision-Language Models</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>