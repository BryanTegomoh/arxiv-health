<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting - Health AI Hub</title>
    <meta name="description" content="This paper introduces PETAR-4B, a 3D mask-aware vision-language model designed for automated generation of localized findings in PET/CT radiology reports. It ad">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27680v1" target="_blank">2510.27680v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Danyal Maqbool, Changhee Lee, Zachary Huemann, Samuel D. Church, Matthew E. Larson, Scott B. Perlman, Tomas A. Romero, Joshua D. Warner, Meghan Lubner, Xin Tie, Jameson Merkow, Junjie Hu, Steve Y. Cho, Tyler J. Bradshaw
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27680v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27680v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces PETAR-4B, a 3D mask-aware vision-language model designed for automated generation of localized findings in PET/CT radiology reports. It addresses the challenge of applying VLMs to complex 3D medical imaging by creating a large-scale dataset of over 11,000 lesion-level descriptions with 3D segmentations and integrating PET, CT, and lesion contours into a unified model, demonstrating significant improvements in report generation quality.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This work is highly relevant for medical imaging and diagnostics, as it enables more accurate, consistent, and automated interpretation of complex 3D PET/CT scans, which are critical for diagnosing and monitoring diseases like cancer.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the automated generation of spatially grounded and clinically coherent radiology reports for PET/CT scans, integrating multimodal medical imaging data (PET, CT) and lesion contours with textual descriptions to assist radiologists in diagnosis and streamline reporting workflows.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the limitation of most medical VLMs to 2D imaging, extending them to the complex 3D volumetric data of PET/CT, which features small, dispersed lesions and lengthy reports.</li>
                    
                    <li>Introduces a novel, large-scale dataset comprising over 11,000 lesion-level descriptions paired with 3D segmentations, derived from more than 5,000 PET/CT exams.</li>
                    
                    <li>The dataset creation leverages an innovative hybrid pipeline combining rule-based methods and Large Language Models (LLMs) for extracting structured information.</li>
                    
                    <li>Proposes PETAR-4B, a 3D mask-aware vision-language model that integrates multi-modal inputs: PET images, CT images, and 3D lesion contours.</li>
                    
                    <li>PETAR-4B is designed to provide spatially grounded report generation, effectively bridging global contextual reasoning with fine-grained, lesion-specific awareness.</li>
                    
                    <li>The model produces clinically coherent and localized findings, moving beyond global summaries to detailed, segment-specific descriptions.</li>
                    
                    <li>Comprehensive automated and human evaluations validate that PETAR substantially improves the quality of PET/CT report generation, advancing 3D medical vision-language understanding.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involved two core components: 1) **Dataset Construction**: A large-scale dataset was assembled from over 5,000 PET/CT exams, generating >11,000 lesion-level descriptions and corresponding 3D segmentations using a hybrid pipeline combining rule-based methods and Large Language Models (LLMs). 2) **Model Development (PETAR-4B)**: A 3D mask-aware vision-language model was proposed. This model integrates volumetric PET and CT images along with 3D lesion contours as input to generate spatially grounded and localized findings for radiology reports, utilizing both global contextual reasoning and fine-grained lesion awareness.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key findings include the successful creation of a novel, large-scale, and annotated 3D PET/CT dataset through an innovative hybrid rule-based/LLM pipeline. More importantly, PETAR-4B, a 3D mask-aware VLM, demonstrated the ability to effectively integrate multi-modal 3D imaging data (PET, CT, lesion contours) to generate clinically coherent and localized findings. Evaluations, both automated and human, confirmed that PETAR substantially improves the quality of automated PET/CT report generation, significantly advancing the field of 3D medical vision-language understanding.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has significant potential to revolutionize PET/CT reporting by automating the generation of localized and clinically coherent findings. This could reduce the cognitive load and dictation time for radiologists, increase report consistency and standardization, and potentially improve diagnostic accuracy and efficiency in busy clinical settings, particularly for conditions requiring detailed lesion analysis such as oncology. It could also serve as a foundational tool for training, quality control, and large-scale research in medical imaging.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Radiology</span>
                    
                    <span class="tag">Nuclear Medicine</span>
                    
                    <span class="tag">Medical Imaging Informatics</span>
                    
                    <span class="tag">AI in Healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">PET/CT</span>
                    
                    <span class="tag tag-keyword">Vision-Language Model</span>
                    
                    <span class="tag tag-keyword">3D Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Automated Reporting</span>
                    
                    <span class="tag tag-keyword">Lesion Segmentation</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Multimodal AI</span>
                    
                    <span class="tag tag-keyword">Radiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in vision-language models (VLMs) have enabled impressive
multimodal reasoning, yet most medical applications remain limited to 2D
imaging. In this work, we extend VLMs to 3D positron emission tomography and
computed tomography (PET/CT), a domain characterized by large volumetric data,
small and dispersed lesions, and lengthy radiology reports. We introduce a
large-scale dataset comprising over 11,000 lesion-level descriptions paired
with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid
rule-based and large language model (LLM) pipeline. Building upon this dataset,
we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,
CT, and lesion contours for spatially grounded report generation. PETAR bridges
global contextual reasoning with fine-grained lesion awareness, producing
clinically coherent and localized findings. Comprehensive automated and human
evaluations demonstrate that PETAR substantially improves PET/CT report
generation quality, advancing 3D medical vision-language understanding.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>