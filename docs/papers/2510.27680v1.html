<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting - Health AI Hub</title>
    <meta name="description" content="This paper introduces PETAR-4B, a 3D mask-aware vision-language model designed for automated generation of localized findings in PET/CT radiology reports. The a">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27680v1" target="_blank">2510.27680v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Danyal Maqbool, Changhee Lee, Zachary Huemann, Samuel D. Church, Matthew E. Larson, Scott B. Perlman, Tomas A. Romero, Joshua D. Warner, Meghan Lubner, Xin Tie, Jameson Merkow, Junjie Hu, Steve Y. Cho, Tyler J. Bradshaw
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27680v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27680v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces PETAR-4B, a 3D mask-aware vision-language model designed for automated generation of localized findings in PET/CT radiology reports. The authors address the limitation of current medical VLMs to 2D imaging by extending them to complex 3D PET/CT data, leveraging a newly created large-scale dataset of over 11,000 lesion descriptions. PETAR-4B integrates PET, CT, and lesion contour information, significantly improving the quality of report generation through comprehensive automated and human evaluations.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This work is highly relevant to medical imaging and radiology as it offers a significant advancement in automating the generation of detailed and localized findings for PET/CT reports, potentially reducing radiologist workload, improving report consistency, and accelerating diagnosis for complex conditions like cancer.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an advanced vision-language model (PETAR-4B) designed to automate and improve the generation of clinical radiology reports from 3D PET/CT scans. It integrates PET, CT, and lesion contours to provide spatially grounded, clinically coherent, and localized findings, thereby assisting radiologists, enhancing diagnostic accuracy, and streamlining the reporting process in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the gap in medical vision-language models (VLMs) by extending them from 2D to 3D Positron Emission Tomography/Computed Tomography (PET/CT) imaging.</li>
                    
                    <li>Introduces a novel, large-scale dataset comprising over 11,000 lesion-level descriptions with 3D segmentations, extracted from more than 5,000 PET/CT exams using a hybrid rule-based and LLM pipeline.</li>
                    
                    <li>Proposes PETAR-4B, a 3D mask-aware VLM specifically designed to integrate PET, CT, and lesion contours for spatially grounded and localized report generation.</li>
                    
                    <li>PETAR-4B bridges global contextual reasoning with fine-grained lesion awareness, enabling the production of clinically coherent and detailed findings.</li>
                    
                    <li>Demonstrates substantial improvements in PET/CT report generation quality through rigorous automated and human evaluations.</li>
                    
                    <li>Aims to advance 3D medical vision-language understanding and automated reporting capabilities for complex volumetric data.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves two main components: 1) **Dataset Creation**: A large-scale dataset with over 11,000 lesion-level descriptions and corresponding 3D segmentations from more than 5,000 PET/CT exams was curated using a hybrid pipeline combining rule-based methods and Large Language Models (LLMs). 2) **Model Development**: PETAR-4B, a 3D mask-aware Vision-Language Model, was developed. This model is designed to integrate volumetric PET and CT images along with 3D lesion contours to generate spatially grounded medical reports. The model's performance was evaluated using both automated metrics and human expert review.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The PETAR-4B model substantially improves the quality of PET/CT report generation, producing clinically coherent and localized findings. Comprehensive automated and human evaluations consistently demonstrated its superior performance in generating spatially grounded descriptions of lesions compared to existing methods, thereby significantly advancing 3D medical vision-language understanding.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The potential clinical impact is significant, including the acceleration of PET/CT reporting, reduction of cognitive burden on radiologists, and improved standardization and consistency of reports. By generating precise, localized findings automatically, PETAR-4B could assist in earlier and more accurate diagnosis, treatment planning, and monitoring of diseases, especially in oncology.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations. Potential implicit limitations might include generalizability to diverse scanner protocols, disease types beyond those represented in the training data, and the need for robust validation in real-world clinical workflows.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions. Potential future work could involve expanding the dataset to include a wider variety of pathologies and annotations, exploring real-time application in clinical settings, and investigating the integration of longitudinal study data for disease progression analysis.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Nuclear Medicine</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">PET/CT</span>
                    
                    <span class="tag tag-keyword">Vision-Language Model</span>
                    
                    <span class="tag tag-keyword">3D Imaging</span>
                    
                    <span class="tag tag-keyword">Automated Reporting</span>
                    
                    <span class="tag tag-keyword">Lesion Segmentation</span>
                    
                    <span class="tag tag-keyword">Mask-Aware Modeling</span>
                    
                    <span class="tag tag-keyword">Radiology</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in vision-language models (VLMs) have enabled impressive
multimodal reasoning, yet most medical applications remain limited to 2D
imaging. In this work, we extend VLMs to 3D positron emission tomography and
computed tomography (PET/CT), a domain characterized by large volumetric data,
small and dispersed lesions, and lengthy radiology reports. We introduce a
large-scale dataset comprising over 11,000 lesion-level descriptions paired
with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid
rule-based and large language model (LLM) pipeline. Building upon this dataset,
we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,
CT, and lesion contours for spatially grounded report generation. PETAR bridges
global contextual reasoning with fine-grained lesion awareness, producing
clinically coherent and localized findings. Comprehensive automated and human
evaluations demonstrate that PETAR substantially improves PET/CT report
generation quality, advancing 3D medical vision-language understanding.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>