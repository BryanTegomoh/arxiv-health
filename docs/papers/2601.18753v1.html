<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs - Health AI Hub</title>
    <meta name="description" content="This paper introduces HalluGuard, a novel theoretical framework and practical method to address critical LLM hallucinations stemming from both data-driven and r">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.18753v1" target="_blank">2601.18753v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinyue Zeng, Junhong Lin, Yujun Yan, Feng Guo, Liang Shi, Jun Wu, Dawei Zhou
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.18753v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.18753v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces HalluGuard, a novel theoretical framework and practical method to address critical LLM hallucinations stemming from both data-driven and reasoning-driven sources. It formalizes hallucination risk using the Hallucination Risk Bound and proposes an NTK-based score to jointly detect these issues. HalluGuard consistently achieves state-of-the-art performance across diverse LLMs and benchmarks, significantly enhancing LLM reliability.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Hallucinations in LLMs pose significant risks in healthcare, potentially leading to incorrect diagnoses, flawed treatment recommendations, or erroneous scientific discoveries. HalluGuard's ability to reliably detect and characterize these hallucinations is crucial for deploying safer and more trustworthy AI systems in clinical practice and medical research.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research directly contributes to making AI models, specifically LLMs, more reliable and trustworthy for deployment in healthcare. By detecting and demystifying data-driven and reasoning-driven hallucinations, HalluGuard aims to improve the safety and accuracy of LLMs when used in medical contexts, such as assisting clinicians, processing patient data, or supporting scientific discovery in health-related fields. It enables safer integration of LLMs into critical healthcare workflows.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLM reliability in high-stakes domains like healthcare is compromised by two main types of hallucinations: data-driven (from training-time mismatches) and reasoning-driven (from inference-time instabilities).</li>
                    
                    <li>Existing hallucination detection methods are limited, often addressing only one source and relying on task-specific heuristics, which restricts their generalizability.</li>
                    
                    <li>The Hallucination Risk Bound is introduced as a unified theoretical framework that formally decomposes hallucination risk into its data-driven and reasoning-driven components, providing a principled foundation for analysis.</li>
                    
                    <li>HalluGuard is proposed as an NTK-based (Neural Tangent Kernel) score that leverages the induced geometry and captured representations of the NTK to jointly identify both types of hallucinations.</li>
                    
                    <li>The method was extensively evaluated across 10 diverse benchmarks, against 11 competitive baselines, and with 9 popular LLM backbones.</li>
                    
                    <li>HalluGuard consistently demonstrated state-of-the-art performance in detecting a wide variety of LLM hallucinations.</li>
                    
                    <li>This work provides both a theoretical foundation and a practical tool for improving the trustworthiness and safety of LLMs by robustly identifying their failure modes.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves a two-pronged approach: (1) A theoretical framework, the Hallucination Risk Bound, which formally decomposes hallucination risk into components attributed to training-time data mismatches and inference-time instabilities. (2) A practical detection score, HalluGuard, which is NTK-based and utilizes the induced geometry and captured representations of the Neural Tangent Kernel to jointly identify both data-driven and reasoning-driven hallucinations. This method was empirically validated through extensive experimentation on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The paper successfully established a unified theoretical framework (Hallucination Risk Bound) for understanding the emergence and evolution of LLM hallucinations. More critically, the proposed HalluGuard, an NTK-based score, consistently achieved state-of-the-art performance in detecting both data-driven and reasoning-driven hallucinations across a broad range of LLMs and evaluation scenarios.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By providing a robust mechanism for detecting LLM hallucinations, HalluGuard can significantly enhance the safety and reliability of AI applications in healthcare. This will reduce the risk of clinical errors stemming from AI-generated misinformation in diagnostic aids, treatment planning, and information retrieval for medical professionals. It also supports more trustworthy LLM-driven medical research by ensuring higher fidelity in data analysis and hypothesis generation.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly stated in the provided abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly stated in the provided abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Medical Diagnostics</span>
                    
                    <span class="tag">Drug Discovery</span>
                    
                    <span class="tag">Medical Research</span>
                    
                    <span class="tag">Personalized Medicine</span>
                    
                    <span class="tag">Public Health Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM</span>
                    
                    <span class="tag tag-keyword">hallucinations</span>
                    
                    <span class="tag tag-keyword">data-driven</span>
                    
                    <span class="tag tag-keyword">reasoning-driven</span>
                    
                    <span class="tag tag-keyword">Neural Tangent Kernel (NTK)</span>
                    
                    <span class="tag tag-keyword">reliability</span>
                    
                    <span class="tag tag-keyword">AI safety</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">hallucination detection</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Have been accepted by ICLR'26</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>