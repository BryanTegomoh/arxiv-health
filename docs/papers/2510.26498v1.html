<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool - Health AI Hub</title>
    <meta name="description" content="This study demonstrates that an ensemble of multiple large language models (LLMs) can provide a more reliable and consistent method for retrospectively assessin">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26498v1" target="_blank">2510.26498v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Adam E. Flanders, Yifan Peng, Luciano Prevedello, Robyn Ball, Errol Colak, Prahlad Menon, George Shih, Hui-Ming Lin, Paras Lakhani
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26498v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26498v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study demonstrates that an ensemble of multiple large language models (LLMs) can provide a more reliable and consistent method for retrospectively assessing the performance of a clinical AI triage tool, specifically for intracranial hemorrhage (ICH) detection, compared to using a single LLM. Analyzing a large dataset of head CT reports, the researchers found that LLM ensembles achieved superior performance metrics in deriving ground truth evaluations.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant for medical AI validation and continuous monitoring, offering a scalable and automated approach to reliably assess the performance of clinical AI tools, particularly for critical and time-sensitive diagnoses like intracranial hemorrhage, thereby reducing the dependency on extensive manual review.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The primary AI health application described is the use of a multi-agent Large Language Model (LLM) framework to automatically and reliably assess the performance and derive ground truth for a 'clinical AI triage tool' designed for detecting intracranial hemorrhage (ICH) from CT head exams. This is a meta-AI application, focusing on the validation and quality assurance of other medical AI tools.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study aimed to determine if an ensemble of LLMs offers a more reliable assessment of a pixel-based AI triage tool than a single LLM.</li>
                    
                    <li>A large dataset of 29,766 non-contrast CT head exams from fourteen hospitals was used, processed by a commercial ICH AI detection tool.</li>
                    
                    <li>Radiology reports were analyzed by an ensemble of eight open-source LLM models (e.g., llama3.3:70b) and a HIPAA-compliant internal GPT-4o using a multi-shot prompt for ICH detection.</li>
                    
                    <li>Individual LLMs like llama3.3:70b and GPT-4o showed strong performance (AUC=0.78, AP=0.75-0.76), with llama3.3:70b achieving the highest F1 score (0.81), recall (0.85), precision (0.78), specificity (0.72), and MCC (0.57).</li>
                    
                    <li>Ensemble methods, including a Full-9 ensemble (MCC=0.571), Top-3 ensemble (MCC=0.558), and Consensus (MCC=0.556), significantly outperformed a single GPT-4o (MCC=0.522).</li>
                    
                    <li>No statistically significant differences were observed between the Full-9, Top-3, and Consensus ensembles (p > 0.05), suggesting flexibility in ensemble size.</li>
                    
                    <li>The conclusion highlights that LLM ensembles provide a more consistent and reliable method for retrospective ground truth evaluation of clinical AI triage tools over a single LLM alone.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study utilized 29,766 non-contrast CT head exams with corresponding radiology reports, which had been processed by a commercial intracranial hemorrhage (ICH) AI detection tool. These reports were then analyzed using a multi-agent LLM framework, consisting of an ensemble of eight open-source LLM models (e.g., Llama3.3:70b) and a HIPAA-compliant internal version of GPT-4o. A single multi-shot prompt was used to assess for the presence of ICH from the reports. Performance characteristics, including AUC, Average Precision (AP), F1 score, recall, precision, specificity, and Matthews Correlation Coefficient (MCC), were computed and compared for individual LLMs and three ideal consensus LLM ensembles (Full-9, Top-3, Consensus). A subset of 1,726 examples was manually reviewed for validation.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The highest individual AUC (0.78) was achieved by llama3.3:70b and GPT-4o, with similar high average precision (0.75-0.76). Llama3.3:70b demonstrated superior overall individual performance with an F1 score of 0.81, recall of 0.85, precision of 0.78, specificity of 0.72, and an MCC of 0.57. When comparing ensembles, the Full-9 Ensemble achieved the highest MCC of 0.571 (95% CI: 0.552-0.591), significantly outperforming a single GPT-4o (MCC=0.522). Importantly, no statistically significant differences were observed between the Full-9, Top-3, and Consensus ensembles (p > 0.05), indicating robust performance across different ensemble sizes.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This multi-agent LLM framework offers a robust and potentially automated solution for consistently and reliably evaluating the performance of clinical AI triage tools in real-world scenarios. This can accelerate the validation and deployment of new AI applications, facilitate continuous monitoring of AI performance in a clinical setting, reduce the workload for human experts in retrospective chart review, and ultimately contribute to safer and more efficient patient care, particularly for time-critical conditions like ICH.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations. Potential limitations not mentioned in the abstract might include the reliance on the quality and completeness of radiology reports, the generalizability of the findings to other AI tools or different clinical contexts/pathologies, and the specific prompt engineering used.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions. However, based on the findings, future work could explore applying this framework to a wider range of clinical AI tools and pathologies, investigating optimal LLM ensemble architectures and weighting schemes, and exploring real-time or prospective AI performance monitoring using similar LLM-based approaches.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">Neurological Imaging</span>
                    
                    <span class="tag">Clinical AI Development</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Clinical AI</span>
                    
                    <span class="tag tag-keyword">AI triage</span>
                    
                    <span class="tag tag-keyword">Intracranial Hemorrhage</span>
                    
                    <span class="tag tag-keyword">CT head</span>
                    
                    <span class="tag tag-keyword">Performance Assessment</span>
                    
                    <span class="tag tag-keyword">Ensemble Learning</span>
                    
                    <span class="tag tag-keyword">Radiology</span>
                    
                    <span class="tag tag-keyword">Ground Truth</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>29 pages, 3 figures, 4 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>