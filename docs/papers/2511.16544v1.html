<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue - Health AI Hub</title>
    <meta name="description" content="This paper challenges the conventional reliance on Word Error Rate (WER) for evaluating Automatic Speech Recognition (ASR) systems in clinical dialogue, demonst">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.16544v1" target="_blank">2511.16544v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zachary Ellis, Jared Joselowitz, Yash Deo, Yajie He, Anna Kalygina, Aisling Higham, Mana Rahimzadeh, Yan Jia, Ibrahim Habli, Ernest Lim
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.16544v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.16544v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper challenges the conventional reliance on Word Error Rate (WER) for evaluating Automatic Speech Recognition (ASR) systems in clinical dialogue, demonstrating its poor correlation with the actual clinical impact of transcription errors. To address this, the authors introduce and validate an LLM-as-a-Judge, optimized with GEPA, which effectively replicates expert clinical assessments of ASR transcription discrepancies, offering a scalable solution for safety-focused evaluation.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate medical documentation and communication are vital for patient safety and quality of care. ASR systems widely deployed in clinical settings must be evaluated based on their impact on clinical understanding, not just textual fidelity, to prevent misdiagnoses, medication errors, and other adverse events resulting from transcription errors.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper addresses the application of Automatic Speech Recognition (ASR) technology in healthcare for transcribing 'clinical dialogue' and 'patient-facing dialogue.' Furthermore, it introduces an 'LLM-as-a-Judge' (a Large Language Model acting as an evaluator) specifically optimized to 'replicate expert clinical assessment' of ASR transcription errors. This LLM serves as an AI tool for evaluating the 'clinical impact' and 'safety' of other AI systems (ASR) within healthcare, effectively an AI-driven quality assurance system for medical AI.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Traditional ASR evaluation metrics, including WER, were found to correlate poorly with the clinical impact (No, Minimal, or Significant) of transcription errors in patient-facing dialogue.</li>
                    
                    <li>A gold-standard benchmark was established by having expert clinicians manually label the clinical impact of ASR errors by comparing ground-truth utterances with ASR-generated transcripts across two distinct doctor-patient dialogue datasets.</li>
                    
                    <li>The study introduces an LLM-as-a-Judge, specifically Gemini-2.5-Pro, which was programmatically optimized using GEPA (Genetic Programming for Evaluation Automation) to mimic expert clinical assessment.</li>
                    
                    <li>The optimized LLM-as-a-Judge achieved human-comparable performance, demonstrating 90% accuracy and a strong Cohen's Kappa (Œ∫) of 0.816 in replicating clinician-assigned risk labels.</li>
                    
                    <li>This work provides a validated, automated framework that enables ASR evaluation to move beyond mere textual fidelity towards a critical assessment of patient safety in clinical communication.</li>
                    
                    <li>The findings highlight a significant gap in current ASR evaluation practices for medical applications, where semantic and clinical context are paramount over simple word-level accuracy.</li>
                    
                    <li>The proposed framework offers a scalable solution for continuous monitoring and improvement of ASR systems in healthcare settings, ensuring that deployment prioritizes patient safety and accurate clinical understanding.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involved two primary phases: 1) Gold-standard creation: Expert clinicians compared ground-truth patient-doctor dialogue utterances with ASR-generated counterparts from two distinct datasets, meticulously labeling the clinical impact (No, Minimal, or Significant) of any transcription discrepancies. 2) LLM-as-a-Judge development: An LLM (Gemini-2.5-Pro) was programmatically optimized using GEPA (a method likely for prompt engineering or fine-tuning) to replicate these expert clinical assessments. The performance of this optimized LLM was then validated against the human-labeled gold standard using accuracy and Cohen's Kappa.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study conclusively found that Word Error Rate (WER) and other common ASR evaluation metrics correlate poorly with the expert clinician-assigned labels for clinical impact. Conversely, the developed LLM-as-a-Judge, optimized via GEPA, achieved human-comparable performance with 90% accuracy and a strong Cohen's Œ∫ of 0.816 in assessing the clinical impact of ASR errors.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work has significant clinical impact by providing a robust, automated, and scalable framework to assess the safety and clinical utility of ASR systems in healthcare. It moves ASR evaluation beyond superficial textual accuracy to a deeper understanding of how transcription errors might distort clinical understanding, thereby directly influencing patient safety, diagnostic accuracy, and treatment efficacy in real-world clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail limitations. However, potential limitations for such work could include: generalizability across a wider range of medical specialties or accents, the specific ASR systems evaluated, the size and diversity of the clinical dialogue datasets, and the potential biases inherent in the LLM model used or the GEPA optimization process.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper implies a future direction of broadly applying this validated, automated framework to continually assess and improve ASR systems in clinical dialogue. This includes integrating it into the development pipelines of ASR technologies to ensure they meet stringent safety standards for healthcare deployment, thereby enabling a necessary, scalable assessment of safety over simple textual fidelity.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Medical Communication</span>
                    
                    <span class="tag">Patient Safety</span>
                    
                    <span class="tag">Primary Care</span>
                    
                    <span class="tag">General Practice</span>
                    
                    <span class="tag">Health Systems Engineering</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">ASR</span>
                    
                    <span class="tag tag-keyword">WER</span>
                    
                    <span class="tag tag-keyword">Clinical Impact</span>
                    
                    <span class="tag tag-keyword">LLM-as-a-Judge</span>
                    
                    <span class="tag tag-keyword">Patient Safety</span>
                    
                    <span class="tag tag-keyword">Medical Transcription</span>
                    
                    <span class="tag tag-keyword">Clinical Dialogue</span>
                    
                    <span class="tag tag-keyword">Evaluation Metrics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $Œ∫$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>