<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction - Health AI Hub</title>
    <meta name="description" content="This paper audits state-of-the-art next location prediction models, revealing significant demographic disparities in predictive performance across racial and et">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26940v1" target="_blank">2510.26940v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ashwin Kumar, Hanyu Zhang, David A. Schweidel, William Yeoh
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26940v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26940v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper audits state-of-the-art next location prediction models, revealing significant demographic disparities in predictive performance across racial and ethnic user groups. To address this, the authors propose Fairness-Guided Incremental Sampling (FGIS), which utilizes a novel Size-Aware K-Means (SAKM) clustering method to infer proxy demographic labels, enabling the construction of fairer training datasets. Their approach reduces total inter-group disparity by up to 40% with minimal accuracy trade-offs, demonstrating the efficacy of data-centric interventions for algorithmic fairness.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Mobility prediction is a critical component of numerous public health applications, including infectious disease modeling, resource allocation for health services, and emergency response planning. Disparities in these models can lead to inequitable outcomes, where health interventions or resource deployments are less effective or systematically disadvantage specific racial and ethnic groups due to biased predictions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is crucial for ensuring fairness and reducing demographic bias in AI models used for mobility prediction within public health applications. This includes, but is not limited to, models for tracking disease spread (e.g., contact tracing, epidemic modeling), optimizing the distribution of health resources (e.g., mobile clinics, vaccine distribution), urban planning impacting health access (e.g., access to healthcare facilities, healthy food), and understanding population movement patterns for targeted health interventions. The proposed methods contribute to building more equitable and trustworthy AI systems for health.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>State-of-the-art next location prediction models, underpinning public health applications, exhibit hidden disparities in predictive performance based on user demographics.</li>
                    
                    <li>Systematic accuracy differences for racial and ethnic user groups are attributed to biases inherent in the underlying training datasets.</li>
                    
                    <li>Fairness-Guided Incremental Sampling (FGIS) is introduced as a group-aware sampling strategy for incremental data collection designed to mitigate these disparities.</li>
                    
                    <li>Size-Aware K-Means (SAKM) is developed to infer proxy racial labels (Asian, Black, Hispanic, White) by partitioning users in latent mobility space while enforcing census-derived group proportions, circumventing the unavailability of individual demographic data.</li>
                    
                    <li>FGIS prioritizes users for inclusion in training datasets based on expected performance gains and current group representation, aiming to reduce demographic performance gaps while preserving overall accuracy.</li>
                    
                    <li>The proposed method successfully reduces total disparity between groups by up to 40% with minimal accuracy trade-offs, showing significant improvements in early sampling stages.</li>
                    
                    <li>The findings emphasize that lightweight, data-centric interventions can effectively improve fairness in AI pipelines with little added complexity, especially beneficial for low-data applications.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors audited existing state-of-the-art mobility prediction models (MetaPath2Vec, transformer-encoder) using a large-scale dataset, computing performance differences across racial and ethnic groups derived from aggregate census data. They then proposed Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy. A core component of FGIS is Size-Aware K-Means (SAKM), a clustering method that partitions users in a latent mobility space to infer proxy racial labels by enforcing census-derived group proportions. FGIS then incrementally constructs training datasets by prioritizing users based on expected performance gains and current group representation to reduce demographic performance gaps.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study exposed systematic demographic disparities in next location prediction model accuracy, showing large performance differences across racial and ethnic groups stemming from inherent dataset biases. The proposed FGIS method effectively reduced total disparity between groups by up to 40% while maintaining overall accuracy, with the most pronounced improvements observed during early stages of data sampling.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Improving the fairness of mobility prediction models has direct clinical and public health implications. Equitable and accurate predictions ensure that public health interventions‚Äîsuch as targeted health campaigns, equitable distribution of medical resources (e.g., vaccines, mobile clinics), outbreak response strategies, and efficient emergency service deployment‚Äîare not biased against or under-serving specific communities. This can lead to more effective, inclusive, and equitable health outcomes, particularly for vulnerable populations whose mobility patterns might be less accurately captured by biased models.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract implies that while the method achieves 'minimal accuracy trade-offs,' there might still be some performance sacrifice. The effectiveness of the proxy demographic labels inferred by SAKM relies on aggregate census data and latent mobility space, which may not perfectly represent individual-level demographics. Additionally, the improvements are 'most significant in early sampling stages,' suggesting potentially diminishing returns for very large datasets or after substantial sampling.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper highlights the potential for lightweight, data-centric interventions to improve fairness with minimal added complexity, particularly for low-data applications. This suggests future research could focus on generalizing and adapting such fairness-aware sampling and data augmentation strategies to a wider array of AI applications beyond mobility prediction, especially in resource-constrained or data-scarce medical and health settings. Further exploration into refining proxy label inference techniques (like SAKM) for different demographic attributes or contexts where individual-level data is unavailable would also be beneficial.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Epidemiology</span>
                    
                    <span class="tag">Health Disparities Research</span>
                    
                    <span class="tag">Emergency Medical Services (EMS) planning</span>
                    
                    <span class="tag">Healthcare Resource Allocation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Mobility Prediction</span>
                    
                    <span class="tag tag-keyword">Algorithmic Fairness</span>
                    
                    <span class="tag tag-keyword">Group Inequity</span>
                    
                    <span class="tag tag-keyword">Demographic Disparity</span>
                    
                    <span class="tag tag-keyword">Incremental Sampling</span>
                    
                    <span class="tag tag-keyword">Public Health</span>
                    
                    <span class="tag tag-keyword">Machine Learning</span>
                    
                    <span class="tag tag-keyword">Location-based Services</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Next location prediction underpins a growing number of mobility, retail, and
public-health applications, yet its societal impacts remain largely unexplored.
In this paper, we audit state-of-the-art mobility prediction models trained on
a large-scale dataset, highlighting hidden disparities based on user
demographics. Drawing from aggregate census data, we compute the difference in
predictive performance on racial and ethnic user groups and show a systematic
disparity resulting from the underlying dataset, resulting in large differences
in accuracy based on location and user groups. To address this, we propose
Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy
designed for incremental data collection settings. Because individual-level
demographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a
clustering method that partitions users in latent mobility space while
enforcing census-derived group proportions. This yields proxy racial labels for
the four largest groups in the state: Asian, Black, Hispanic, and White. Built
on these labels, our sampling algorithm prioritizes users based on expected
performance gains and current group representation. This method incrementally
constructs training datasets that reduce demographic performance gaps while
preserving overall accuracy. Our method reduces total disparity between groups
by up to 40\% with minimal accuracy trade-offs, as evaluated on a state-of-art
MetaPath2Vec model and a transformer-encoder model. Improvements are most
significant in early sampling stages, highlighting the potential for
fairness-aware strategies to deliver meaningful gains even in low-resource
settings. Our findings expose structural inequities in mobility prediction
pipelines and demonstrate how lightweight, data-centric interventions can
improve fairness with little added complexity, especially for low-data
applications.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>