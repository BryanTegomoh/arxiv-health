<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction - Health AI Hub</title>
    <meta name="description" content="This paper reveals significant demographic disparities in state-of-the-art mobility prediction models, where predictive performance varies widely across racial ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26940v1" target="_blank">2510.26940v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ashwin Kumar, Hanyu Zhang, David A. Schweidel, William Yeoh
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26940v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26940v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper reveals significant demographic disparities in state-of-the-art mobility prediction models, where predictive performance varies widely across racial and ethnic user groups due to underlying dataset biases. To counteract this, the authors propose Fairness-Guided Incremental Sampling (FGIS), which, alongside a novel Size-Aware K-Means (SAKM) clustering method for inferring proxy demographic labels, incrementally builds fairer training datasets. FGIS successfully reduces inter-group performance disparities by up to 40% with minimal overall accuracy loss, demonstrating impactful fairness gains, especially in low-resource settings.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Disparities in mobility prediction directly translate to inequities in public health outcomes, as many public health applications (e.g., disease surveillance, resource allocation, emergency response planning) rely heavily on accurate and equitable understanding of population movement. Ensuring fair predictions across all demographic groups is critical for effective, just, and non-discriminatory public health interventions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper focuses on improving the fairness and reducing demographic bias in AI models designed for mobility prediction. When these mobility prediction models are applied in public health (e.g., for tracking disease spread, allocating medical resources, or planning emergency responses), the methods proposed in this paper ensure that the predictions are equitable across different racial and ethnic groups. This prevents AI-driven public health initiatives from inadvertently exacerbating existing health disparities due to biased models.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Audited state-of-the-art mobility prediction models show systematic demographic disparities in predictive performance across racial and ethnic user groups.</li>
                    
                    <li>These disparities are attributed to biases in the underlying large-scale mobility datasets, resulting in significant accuracy differences based on location and user group.</li>
                    
                    <li>Proposed Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy designed for incremental data collection to mitigate observed inequities.</li>
                    
                    <li>Developed Size-Aware K-Means (SAKM), a novel clustering method that partitions users in latent mobility space while enforcing census-derived group proportions to generate proxy racial labels (Asian, Black, Hispanic, White).</li>
                    
                    <li>FGIS prioritizes users for sampling based on expected performance gains and current group representation to reduce demographic performance gaps while preserving overall accuracy.</li>
                    
                    <li>Evaluated on MetaPath2Vec and transformer-encoder models, the method reduced total disparity between groups by up to 40% with minimal accuracy trade-offs.</li>
                    
                    <li>Improvements in fairness were most significant in early sampling stages, underscoring the potential for fairness-aware strategies to deliver meaningful gains in low-resource and low-data applications.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study began by auditing state-of-the-art mobility prediction models (e.g., MetaPath2Vec, transformer-encoder) on a large-scale dataset to identify systematic performance disparities across racial and ethnic user groups, leveraging aggregate census data to define these groups. To address this, they introduced Fairness-Guided Incremental Sampling (FGIS), an active learning strategy. A core component of FGIS is Size-Aware K-Means (SAKM), a custom clustering method that partitions users within a latent mobility space while incorporating and enforcing census-derived group proportions to generate proxy racial labels. FGIS then uses these proxy labels to incrementally select training data, prioritizing users based on expected performance gains and current group representation to optimize for both fairness and overall accuracy.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>A systematic and significant disparity in predictive performance was identified across racial and ethnic user groups, directly stemming from biases in the underlying large-scale mobility datasets. The proposed FGIS method, leveraging SAKM-derived proxy labels, effectively reduced the total disparity between groups by up to 40% while incurring only minimal trade-offs in overall prediction accuracy. Notably, the most substantial improvements in fairness were observed during the early stages of incremental sampling, indicating the method's strong efficacy even in data-scarce environments and its potential to expose and mitigate structural inequities.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research enables the development of more equitable and reliable public health and medical applications that depend on mobility data. By reducing demographic performance gaps, models for infectious disease spread prediction, targeted health campaign planning (e.g., vaccination drives), or optimization of emergency service deployment can become fairer, ensuring that interventions are equally effective and accessible across all racial and ethnic communities. This minimizes the risk of inadvertently widening existing health disparities and enhances the overall impact and trustworthiness of data-driven health initiatives.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The primary limitation noted is the unavailability of individual-level demographic labels, which necessitated the development and reliance on proxy labeling methods (SAKM) based on aggregate census data. This introduces a degree of approximation in group assignments. Additionally, the focus on the four largest racial/ethnic groups in the state might not capture disparities affecting smaller or more intersectional demographic groups.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly detailed as a dedicated section, the paper implicitly suggests future research avenues by highlighting the potential of lightweight, data-centric interventions to improve fairness with minimal added complexity, especially for low-data applications. This encourages further exploration and application of similar fairness-aware sampling, clustering, and data imputation techniques across a broader range of demographic factors, diverse geographic contexts, and other medical/public health prediction tasks.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Public health</span>
                    
                    <span class="tag">Epidemiology</span>
                    
                    <span class="tag">Health informatics</span>
                    
                    <span class="tag">Disease surveillance</span>
                    
                    <span class="tag">Emergency medical services (EMS)</span>
                    
                    <span class="tag">Health equity</span>
                    
                    <span class="tag">Geographic information systems (GIS) in health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Mobility prediction</span>
                    
                    <span class="tag tag-keyword">Algorithmic fairness</span>
                    
                    <span class="tag tag-keyword">Demographic disparity</span>
                    
                    <span class="tag tag-keyword">Incremental sampling</span>
                    
                    <span class="tag tag-keyword">Clustering</span>
                    
                    <span class="tag tag-keyword">Public health</span>
                    
                    <span class="tag tag-keyword">Data equity</span>
                    
                    <span class="tag tag-keyword">Machine learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Next location prediction underpins a growing number of mobility, retail, and
public-health applications, yet its societal impacts remain largely unexplored.
In this paper, we audit state-of-the-art mobility prediction models trained on
a large-scale dataset, highlighting hidden disparities based on user
demographics. Drawing from aggregate census data, we compute the difference in
predictive performance on racial and ethnic user groups and show a systematic
disparity resulting from the underlying dataset, resulting in large differences
in accuracy based on location and user groups. To address this, we propose
Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy
designed for incremental data collection settings. Because individual-level
demographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a
clustering method that partitions users in latent mobility space while
enforcing census-derived group proportions. This yields proxy racial labels for
the four largest groups in the state: Asian, Black, Hispanic, and White. Built
on these labels, our sampling algorithm prioritizes users based on expected
performance gains and current group representation. This method incrementally
constructs training datasets that reduce demographic performance gaps while
preserving overall accuracy. Our method reduces total disparity between groups
by up to 40\% with minimal accuracy trade-offs, as evaluated on a state-of-art
MetaPath2Vec model and a transformer-encoder model. Improvements are most
significant in early sampling stages, highlighting the potential for
fairness-aware strategies to deliver meaningful gains even in low-resource
settings. Our findings expose structural inequities in mobility prediction
pipelines and demonstrate how lightweight, data-centric interventions can
improve fairness with little added complexity, especially for low-data
applications.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>