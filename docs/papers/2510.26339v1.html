<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model? - Health AI Hub</title>
    <meta name="description" content="This paper introduces GLYPH-SR, a novel VLM-guided latent diffusion framework designed for image super-resolution that simultaneously optimizes for high visual ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26339v1" target="_blank">2510.26339v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.75 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26339v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26339v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces GLYPH-SR, a novel VLM-guided latent diffusion framework designed for image super-resolution that simultaneously optimizes for high visual quality and high-fidelity scene-text recovery. Addressing the common failure of previous SR models to accurately reconstruct text within natural images, GLYPH-SR leverages a Text-SR Fusion ControlNet and a ping-pong scheduler to achieve significant improvements in OCR F1 scores while maintaining competitive perceptual quality.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Many medical contexts rely heavily on legible text embedded within images, such as patient identifiers on scans, labels on pathology slides, dosage instructions on pharmaceutical packaging, or critical information in low-resolution telemedicine imagery. GLYPH-SR's ability to accurately recover and enhance such scene-text is paramount for reliable information extraction, preventing errors, and enabling robust automated analysis in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The GLYPH-SR model, by significantly improving text recovery from low-resolution images, acts as a critical enabler for various AI applications in health. For instance, it can enhance the accuracy of Optical Character Recognition (OCR) systems used by medical AI to extract data from scanned patient charts, lab reports, or historical medical documents. It can improve the reliability of AI models that process medical images containing text overlays (e.g., patient demographics, anatomical labels, measurement values) by ensuring these textual elements are clearly understood. This foundational AI capability would lead to more robust AI systems for automated diagnosis, clinical decision support, data analytics, and administrative tasks in healthcare by providing higher quality textual input.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Addressed:** Previous Super-Resolution (SR) models often treat scene-text as generic texture, resulting in blurred or hallucinated characters and subsequent failures in Optical Character Recognition (OCR), despite good overall image perceptual quality. Traditional SR metrics are largely insensitive to character-level errors.</li>
                    
                    <li>**Dual Objective:** GLYPH-SR is specifically designed to achieve both high-quality image SR and high-fidelity scene-text recovery, explicitly optimizing for text legibility and overall perceptual quality.</li>
                    
                    <li>**VLM-Guided Latent Diffusion Framework:** The core architecture is a Vision-Language Model (VLM)-guided latent diffusion model, representing a state-of-the-art generative approach for image enhancement.</li>
                    
                    <li>**Text-SR Fusion ControlNet (TS-ControlNet):** A key architectural component dedicated to targeted text restoration, guided by OCR data to ensure precise character-level reconstruction.</li>
                    
                    <li>**Ping-Pong Scheduler:** A novel scheduler that alternates between text-centric and scene-centric guidance, dynamically balancing the optimization of text fidelity and overall image realism during the super-resolution process.</li>
                    
                    <li>**Targeted Training Strategy:** The text-specific components (TS-ControlNet and ping-pong scheduler) are trained on a synthetic corpus while the main SR branch is kept frozen, enabling efficient and focused learning for text restoration without affecting general image reconstruction capabilities.</li>
                    
                    <li>**Significant Performance Improvement:** GLYPH-SR demonstrated an improvement in OCR F1 scores by up to +15.18 percentage points over diffusion/GAN baselines (SVT x8, OpenOCR) while maintaining competitive scores on established perceptual metrics (MANIQA, CLIP-IQA, MUSIQ) across SVT, SCUT-CTW1500, and CUTE80 datasets at x4 and x8 upscaling factors.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>GLYPH-SR utilizes a VLM-guided latent diffusion model framework. It incorporates a Text-SR Fusion ControlNet (TS-ControlNet) specifically guided by OCR data to target and restore textual details. A novel ping-pong scheduler dynamically alternates between text-centric and scene-centric guidance to achieve a balance between text legibility and overall image realism. These text-specific components are trained on a synthetic corpus, while the main SR branch of the diffusion model is kept frozen.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is GLYPH-SR's substantial improvement in scene-text super-resolution accuracy, demonstrated by an OCR F1 score increase of up to +15.18 percentage points on the SVT x8 benchmark using OpenOCR, significantly outperforming existing diffusion/GAN baselines. Critically, these gains in text fidelity were achieved while maintaining competitive performance on widely accepted perceptual quality metrics (MANIQA, CLIP-IQA, MUSIQ), affirming its ability to deliver both high-readability text and high visual realism across multiple challenging scene-text datasets (SVT, SCUT-CTW1500, CUTE80) at both x4 and x8 upscaling factors.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology holds significant potential to enhance the accuracy and reliability of automated systems within healthcare that process image-based data containing critical text. This could lead to improved OCR performance on scanned medical records, accurate reading of patient identifiers on diagnostic images, enhanced legibility of drug packaging or medical device labels (reducing medication errors), and more precise data extraction from telemedicine images. Ultimately, it contributes to better data quality for electronic health records, increased patient safety, and more efficient healthcare operations by reducing manual data verification and potential human transcription errors.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract regarding GLYPH-SR itself. However, the abstract implicitly highlights the limitations of prior SR research, which often focused on distortion or perceptual metrics insensitive to character-level errors and simplified benchmarks with isolated characters, overlooking complex natural scenes.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract. Potential future work could involve evaluating GLYPH-SR on real-world medical image datasets with diverse text characteristics and degradation patterns, exploring its real-time application in clinical workflows, or integrating domain-specific medical Vision-Language Models for further tailored guidance and enhanced performance in highly specialized medical contexts.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Digital Pathology</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Pharmaceutical Information Systems</span>
                    
                    <span class="tag">Health Informatics</span>
                    
                    <span class="tag">Medical Device Labeling</span>
                    
                    <span class="tag">Clinical Documentation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Super-resolution</span>
                    
                    <span class="tag tag-keyword">Scene-text</span>
                    
                    <span class="tag tag-keyword">Optical Character Recognition (OCR)</span>
                    
                    <span class="tag tag-keyword">Diffusion Models</span>
                    
                    <span class="tag tag-keyword">Vision-Language Models (VLM)</span>
                    
                    <span class="tag tag-keyword">Latent Diffusion</span>
                    
                    <span class="tag tag-keyword">Image Enhancement</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>11 pages, 6 figures. Includes supplementary material. Under review as
  a conference paper at ICLR 2026</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>