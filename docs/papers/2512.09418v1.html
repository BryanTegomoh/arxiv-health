<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis - Health AI Hub</title>
    <meta name="description" content="This paper introduces the Motion Conditioned Diffusion Model (MCDM), a novel label-free latent diffusion framework for synthesizing realistic echocardiography v">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.09418v1" target="_blank">2512.09418v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zhe Li, Hadrien Reynaud, Johanna P M√ºller, Bernhard Kainz
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.09418v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.09418v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces the Motion Conditioned Diffusion Model (MCDM), a novel label-free latent diffusion framework for synthesizing realistic echocardiography videos. It leverages self-supervised motion features, extracted by a custom Motion and Appearance Feature Extractor (MAFE) with auxiliary re-identification and optical flow losses, to condition video generation. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive and clinically realistic video synthesis without requiring manual labels.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant for medical AI as it provides a scalable solution to the pervasive problem of limited labeled medical imaging data, particularly in cardiac ultrasound, by synthesizing realistic training data to accelerate deep learning model development for improved cardiac diagnostics.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI model (Motion Conditioned Diffusion Model) synthesizes realistic cardiac ultrasound videos. This directly addresses the critical issue of data scarcity in medical AI, allowing for the training of more robust and accurate deep learning models for cardiac function assessment. This synthetic data can also be used for medical education, simulation, and data augmentation to improve existing AI diagnostic tools in clinical settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of scarce labeled data in cardiac ultrasound for deep learning, driven by privacy and annotation complexity.</li>
                    
                    <li>Proposes the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework specifically for echocardiography video synthesis.</li>
                    
                    <li>MCDM conditions the video generation process on self-supervised motion features, eliminating the need for manual annotations.</li>
                    
                    <li>A novel Motion and Appearance Feature Extractor (MAFE) is designed to disentangle motion and appearance representations from videos, forming the basis for conditioning.</li>
                    
                    <li>Feature learning within MAFE is enhanced by two auxiliary objectives: a re-identification loss (using pseudo appearance features) and an optical flow loss (using pseudo flow fields).</li>
                    
                    <li>The model achieves competitive video generation performance on the EchoNet-Dynamic dataset, producing temporally coherent and clinically realistic sequences.</li>
                    
                    <li>The work demonstrates the significant potential of self-supervised conditioning for scalable echocardiography synthesis, overcoming a major data bottleneck.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The paper presents the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework for generating echocardiography videos. Its core is a self-supervised conditioning mechanism: a dedicated Motion and Appearance Feature Extractor (MAFE) is trained to disentangle motion and appearance representations from video sequences. To boost feature learning, MAFE incorporates two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. The extracted self-supervised motion features then serve as the conditioning input for the diffusion model, guiding the synthesis of temporally coherent and realistic echocardiography videos.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The MCDM successfully achieves competitive video generation performance when evaluated on the EchoNet-Dynamic dataset. It is capable of producing temporally coherent and clinically realistic echocardiography sequences. A crucial finding is that this high-quality synthesis is achieved entirely without reliance on manual labels, thereby validating the efficacy and potential of the self-supervised motion conditioning paradigm.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research holds significant clinical impact by addressing the critical data bottleneck in developing AI tools for cardiology. By enabling the scalable, label-free synthesis of realistic echocardiography videos, it can accelerate the training and development of more robust deep learning models for automated cardiac function assessment, disease detection, and clinical decision support. This could lead to more efficient diagnostic workflows, improved accuracy, and potentially enhance medical education and training by providing diverse, synthetic case studies.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed method or its evaluation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract implies future work in further exploring and expanding the potential of self-supervised conditioning for scalable echocardiography synthesis and potentially other medical imaging modalities, although specific future research directions are not explicitly detailed.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical AI</span>
                    
                    <span class="tag">Radiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Ultrasound echocardiography</span>
                    
                    <span class="tag tag-keyword">Deep learning</span>
                    
                    <span class="tag tag-keyword">Diffusion models</span>
                    
                    <span class="tag tag-keyword">Self-supervised learning</span>
                    
                    <span class="tag tag-keyword">Medical image synthesis</span>
                    
                    <span class="tag tag-keyword">Cardiac function assessment</span>
                    
                    <span class="tag tag-keyword">Label-free learning</span>
                    
                    <span class="tag tag-keyword">Generative AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted at MICAD 2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>