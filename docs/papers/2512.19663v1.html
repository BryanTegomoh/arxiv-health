<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel knowledge-enhanced multimodal transformer framework designed to overcome the limitations of general-domain vision-language models ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.19663v1" target="_blank">2512.19663v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-22
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.19663v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.19663v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel knowledge-enhanced multimodal transformer framework designed to overcome the limitations of general-domain vision-language models like CLIP in medical applications, specifically for diabetic retinopathy (DR) diagnosis. By integrating retinal images, clinical text, and structured patient data through specialized encoders and a joint transformer, the model achieves superior cross-modal alignment and state-of-the-art DR classification performance. The framework dramatically improves medical image-text retrieval accuracy and exhibits strong zero-shot generalization.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medical AI by providing a crucial advancement in accurate automated diagnostic systems for diabetic retinopathy, a leading cause of preventable blindness. It addresses the specificity needed for medical data by integrating diverse patient information, enabling more precise interpretation and retrieval of multimodal clinical records.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This paper proposes an AI system (knowledge-enhanced multimodal transformer) designed to diagnose and grade the severity of diabetic retinopathy. It serves as an automated diagnostic tool for ophthalmology, capable of cross-modal retrieval (e.g., finding relevant images from text descriptions) and classification based on medical standards (ICDR, SDRG). This application aims to improve the accuracy and efficiency of diagnosing a significant medical condition.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical gap in medical image-text alignment where general-domain VLM like CLIP perform poorly, particularly for ophthalmological cross-modal retrieval.</li>
                    
                    <li>Proposes a novel knowledge-enhanced joint embedding framework utilizing a multimodal transformer architecture to integrate retinal fundus images, clinical text, and structured patient data.</li>
                    
                    <li>Employs modality-specific encoders: a Vision Transformer (ViT-B/16) for images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured patient features.</li>
                    
                    <li>Trains the model using a comprehensive set of objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading (ICDR and SDRG schemes).</li>
                    
                    <li>Achieves near-perfect text-to-image retrieval performance (99.94% Recall@1 on BRSET) and strong zero-shot generalization (93.95% Recall@1 on DeepEyeNet), significantly outperforming fine-tuned CLIP (1.29% and 0.22% Recall@1 respectively).</li>
                    
                    <li>Maintains state-of-the-art classification accuracy for DR severity grading (97.05% for SDRG and 97.97% for ICDR).</li>
                    
                    <li>Demonstrates that the multimodal training effectively captures complex cross-modal relationships unique to the medical domain, leading to robust diagnostic capabilities.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed framework integrates three modalities via a multimodal transformer architecture. Retinal fundus images are processed by a Vision Transformer (ViT-B/16), clinical narratives by Bio-ClinicalBERT, and structured demographic/clinical features by a multilayer perceptron. These modality-specific embeddings are then fused and jointly processed by a transformer with modality-specific embeddings. The model is trained using multiple objectives: contrastive losses to align representations across modality pairs, reconstruction losses for images and text, and classification losses to predict DR severity grades according to ICDR and SDRG schemes. Performance was evaluated on the BRSET dataset and validated with zero-shot evaluation on DeepEyeNet.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The framework achieved near-perfect text-to-image retrieval with a Recall@1 of 99.94% on the BRSET dataset, vastly outperforming fine-tuned CLIP (1.29%). It also demonstrated strong zero-shot generalizability on the unseen DeepEyeNet dataset, achieving 93.95% Recall@1 compared to 0.22% for fine-tuned CLIP. Concurrently, it maintained state-of-the-art classification accuracy for DR severity grading, with 97.05% for SDRG and 97.97% for ICDR schemes.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has significant potential to revolutionize the diagnosis and management of diabetic retinopathy by providing highly accurate and reliable automated diagnostic tools. The superior cross-modal retrieval capabilities could empower clinicians to quickly find relevant patient cases or educational materials based on complex multimodal queries, improve personalized treatment planning, and streamline medical research by facilitating efficient data exploration.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed method or the study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention specific future research directions, but the demonstrated generalizability suggests potential for broader application across other medical imaging and text domains.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Ophthalmology</span>
                    
                    <span class="tag">Diabetology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Preventive Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Diabetic Retinopathy</span>
                    
                    <span class="tag tag-keyword">Multimodal Transformers</span>
                    
                    <span class="tag tag-keyword">Cross-Modal Alignment</span>
                    
                    <span class="tag tag-keyword">Vision-Language Models</span>
                    
                    <span class="tag tag-keyword">Clinical Text</span>
                    
                    <span class="tag tag-keyword">Fundus Images</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Ophthalmology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>14 pages, 14 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>