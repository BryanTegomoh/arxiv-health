<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation - Health AI Hub</title>
    <meta name="description" content="This paper introduces TranSamba, a novel hybrid Transformer-Mamba architecture for weakly supervised volumetric medical image segmentation. It addresses the lim">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.10353v1" target="_blank">2512.10353v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-11
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yiheng Lyu, Lian Xu, Mohammed Bennamoun, Farid Boussaid, Coen Arrow, Girish Dwivedi
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.10353v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.10353v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces TranSamba, a novel hybrid Transformer-Mamba architecture for weakly supervised volumetric medical image segmentation. It addresses the limitation of 2D encoders by effectively capturing 3D context through a combination of within-slice Transformer attention and efficient cross-plane Mamba-based information exchange. TranSamba achieves state-of-the-art performance across diverse datasets and modalities while maintaining linear time complexity and constant memory usage.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research provides a more accurate and label-efficient method for segmenting anatomical structures and pathologies in 3D medical images. By leveraging inherent 3D context, it can improve diagnostic precision, surgical planning, and the development of AI tools for healthcare, reducing the significant time and cost associated with manual volumetric annotation.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is to enhance weakly supervised semantic segmentation of 3D volumetric medical images (e.g., CT, MRI). This improves the efficiency and accuracy of identifying and delineating anatomical structures, lesions, or tumors, which is crucial for diagnosis, disease monitoring, surgical planning, and personalized treatment in various medical conditions and pathologies.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of weakly supervised volumetric medical segmentation where existing 2D approaches neglect inherent 3D context.</li>
                    
                    <li>Proposes TranSamba, a hybrid architecture combining Vision Transformer for in-slice processing with novel Cross-Plane Mamba blocks for inter-slice context.</li>
                    
                    <li>Cross-Plane Mamba blocks leverage the linear complexity of state space models for efficient information exchange across neighboring slices, enhancing 3D context capture.</li>
                    
                    <li>The inter-slice information exchange directly improves the attention maps generated by Transformer blocks, crucial for accurate object localization.</li>
                    
                    <li>Achieves effective volumetric modeling with a time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing.</li>
                    
                    <li>Establishes new state-of-the-art performance on three distinct datasets, consistently outperforming existing methods across various modalities and pathologies.</li>
                    
                    <li>Offers a label-efficient solution, significantly reducing the annotation burden for training segmentation models in medical imaging.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>TranSamba augments a standard Vision Transformer backbone. While the Transformer handles pairwise self-attention within individual slices, Cross-Plane Mamba blocks are integrated to facilitate efficient information exchange across adjacent slices in the 3D volume. These Mamba blocks utilize the linear complexity of state space models to effectively capture inter-slice dependencies and contribute to the generation of more accurate attention maps for object localization, thus enabling robust 3D context modeling.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>TranSamba consistently achieved new state-of-the-art performance across three distinct medical imaging datasets, outperforming existing weakly supervised segmentation methods. It demonstrated superior accuracy across diverse modalities and pathologies, while also exhibiting efficient computational scaling with linear time complexity relative to volume depth and constant memory usage.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The proposed TranSamba architecture has the potential to significantly accelerate the development and deployment of AI-powered tools for medical diagnosis and treatment planning. By reducing the need for extensive manual annotation of 3D medical images, it lowers training costs and time, making accurate volumetric segmentation more accessible and scalable for clinical applications such as tumor delineation, organ segmentation, and disease quantification, ultimately leading to improved patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology Analysis</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Computational Pathology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Weakly supervised learning</span>
                    
                    <span class="tag tag-keyword">Volumetric medical segmentation</span>
                    
                    <span class="tag tag-keyword">Transformer</span>
                    
                    <span class="tag tag-keyword">Mamba</span>
                    
                    <span class="tag tag-keyword">Hybrid architecture</span>
                    
                    <span class="tag tag-keyword">State space models</span>
                    
                    <span class="tag tag-keyword">3D context</span>
                    
                    <span class="tag tag-keyword">Label efficiency</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>