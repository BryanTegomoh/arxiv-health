<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation - Health AI Hub</title>
    <meta name="description" content="This paper introduces UKAST, a novel U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer enc">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04084v1" target="_blank">2511.04084v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-06
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Nishchal Sapkota, Haoyan Shi, Yejia Zhang, Xianshi Ma, Bofang Zheng, Danny Z. Chen
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04084v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04084v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces UKAST, a novel U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders for medical image segmentation. UKAST achieves state-of-the-art performance across four diverse medical imaging benchmarks, particularly excelling in data-scarce environments while addressing the data-hungry and computational limitations of traditional Transformers.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate medical image segmentation is fundamental for precise diagnostics, disease staging, and effective treatment planning. This research offers a more robust and data-efficient method, which is crucial given the challenges of obtaining extensive annotated datasets in clinical practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development of an improved deep learning model (UKAST, a KAN-enhanced Swin Transformer) for medical image segmentation. This model aims to provide more accurate and data-efficient segmentation of medical images, thereby directly assisting clinicians in making more precise diagnoses and developing more effective treatment plans. Its ability to perform well in data-scarce environments is particularly valuable for real-world clinical deployment.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>UKAST is a U-Net inspired architecture that enhances Swin Transformer encoders by incorporating rational-function based Kolmogorov-Arnold Networks (KANs).</li>
                    
                    <li>It specifically utilizes Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT) to improve upon the inefficiencies of vanilla spline-based KANs.</li>
                    
                    <li>The proposed architecture offers increased expressivity and data efficiency with reduced FLOPs, incurring only a marginal increase in parameter count compared to SwinUNETR.</li>
                    
                    <li>UKAST achieves state-of-the-art performance on four distinct 2D and 3D medical image segmentation benchmarks.</li>
                    
                    <li>A critical finding is its superior accuracy in settings with limited annotated training data, effectively mitigating the data-hungry nature of standard Vision Transformers.</li>
                    
                    <li>It consistently outperforms both CNN-based and conventional Transformer-based segmentation baselines.</li>
                    
                    <li>The work highlights the significant potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation methods.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors propose UKAST, a U-Net inspired architecture. It integrates Group Rational KANs (GR-KANs), derived from the Kolmogorov-Arnold Transformer (KAT) framework and based on rational activation functions, into the encoder blocks of a Swin Transformer. This integration is designed to leverage the KANs' expressive power and overcome the limitations of spline-based KANs, optimizing for data efficiency, computational cost (FLOPs), and parameter count.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>UKAST achieved state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks. Crucially, it demonstrated superior accuracy in data-scarce scenarios, thereby alleviating the known data-hungry limitations of standard Vision Transformers and consistently surpassing both CNN and Transformer baselines.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This improved architecture could lead to more accurate and reliable automated segmentation of medical images, potentially enhancing diagnostic precision and treatment planning in various clinical specialties. Its data-efficient nature is particularly valuable, as it can reduce the burden of extensive manual annotation typically required for training deep learning models in medical settings, accelerating the deployment of AI in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the UKAST model itself, but rather positions UKAST as a solution to existing challenges: the struggle of CNNs with long-range dependencies and the data-hungry/computationally expensive nature of vanilla Transformers. The paper aims to mitigate these inherent limitations of previous approaches.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The results suggest the potential for KAN-enhanced Transformers to further advance data-efficient medical image segmentation. This implies future research could explore broader applications, different KAN variants, or integration with other transformer architectures to continuously improve performance and efficiency in clinical settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology (image analysis)</span>
                    
                    <span class="tag">Anatomy (segmentation)</span>
                    
                    <span class="tag">Oncology (tumor segmentation)</span>
                    
                    <span class="tag">Neurology (brain segmentation)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medical Image Segmentation</span>
                    
                    <span class="tag tag-keyword">Swin Transformer</span>
                    
                    <span class="tag tag-keyword">Kolmogorov-Arnold Networks (KANs)</span>
                    
                    <span class="tag tag-keyword">U-Net</span>
                    
                    <span class="tag tag-keyword">Data-efficient</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Computer Vision</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>