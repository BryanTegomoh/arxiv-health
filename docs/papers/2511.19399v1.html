<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research - Health AI Hub</title>
    <meta name="description" content="Existing deep research models struggle with multi-step, long-form tasks due to training on easily verifiable short-form QA. This paper introduces Reinforcement ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.19399v1" target="_blank">2511.19399v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-24
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Rulin Shao, Akari Asai, Shannon Zejiang Shen, Hamish Ivison, Varsha Kishore, Jingming Zhuo, Xinran Zhao, Molly Park, Samuel G. Finlayson, David Sontag, Tyler Murray, Sewon Min, Pradeep Dasigi, Luca Soldaini, Faeze Brahman, Wen-tau Yih, Tongshuang Wu, Luke Zettlemoyer, Yoon Kim, Hannaneh Hajishirzi, Pang Wei Koh
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.19399v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.19399v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Existing deep research models struggle with multi-step, long-form tasks due to training on easily verifiable short-form QA. This paper introduces Reinforcement Learning with Evolving Rubrics (RLER), a novel training methodology where evaluation rubrics dynamically co-evolve with the policy model to provide discriminative, on-policy feedback. Utilizing RLER, they developed DR Tulu-8B, the first open model for open-ended, long-form deep research, which significantly outperforms other open models and matches proprietary systems across science, healthcare, and general domains.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as it explicitly targets the 'healthcare' domain for its long-form deep research capabilities. It enables AI models to perform complex, multi-step information synthesis from vast and evolving medical literature, which is crucial for advanced clinical decision support, medical research acceleration, drug discovery, and generating comprehensive health information.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI model is designed to perform multi-step deep research to produce long-form, well-attributed answers. In a health context, this can be applied to provide comprehensive medical information, answer complex clinical queries for healthcare professionals, assist with literature reviews and synthesis for medical research, generate detailed and reliable patient education materials, or support evidence-based decision-making in various healthcare settings by summarizing and attributing findings from vast amounts of medical literature.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the limitation of current open deep research models in performing realistic long-form, multi-step research tasks, which are typically trained on short-form QA.</li>
                    
                    <li>Introduces Reinforcement Learning with Evolving Rubrics (RLER), a novel reinforcement learning paradigm for training deep research models.</li>
                    
                    <li>RLER constructs and maintains evaluation rubrics that dynamically co-evolve with the policy model during training, providing discriminative, on-policy feedback based on newly explored information.</li>
                    
                    <li>Developed DR Tulu-8B, an 8-billion parameter model, as the first open model explicitly trained for open-ended, long-form deep research using the RLER methodology.</li>
                    
                    <li>DR Tulu-8B substantially outperforms existing open deep research models across four diverse long-form benchmarks in science, healthcare, and general domains.</li>
                    
                    <li>The model matches or exceeds the performance of proprietary deep research systems, while being significantly smaller and more cost-effective per query.</li>
                    
                    <li>All data, models, code, and a new MCP-based agent infrastructure are publicly released to facilitate future research and development in this area.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The core methodology is Reinforcement Learning with Evolving Rubrics (RLER). This approach involves training a policy model (DR Tulu-8B) where the evaluation rubrics are not static but dynamically constructed and maintained, co-evolving with the policy during the training process. This co-evolution allows the rubrics to incorporate information that the model has newly explored, thereby providing discriminative and on-policy feedback. The trained model was evaluated on four long-form deep research benchmarks covering science, healthcare, and general domains.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>DR Tulu-8B, trained using the RLER method, demonstrated substantial performance improvements over existing open deep research models. It successfully matched or exceeded the capabilities of proprietary deep research systems in generating long-form, well-attributed answers across scientific, healthcare, and general benchmarks. Notably, DR Tulu-8B achieves this while being a significantly smaller (8B parameters) and more cost-effective model per query, indicating high efficiency and generalizability of the RLER approach for complex research tasks.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The ability of DR Tulu-8B to perform open-ended, long-form deep research, particularly within healthcare, has transformative clinical impact. It can empower clinicians with advanced tools for rapid synthesis of complex medical evidence for diagnosis and treatment planning, support researchers in identifying novel insights from vast biomedical datasets, and enhance the development of comprehensive and personalized patient education materials. Its open-source nature and cost-efficiency can foster broader adoption and innovation in medical AI applications, potentially accelerating medical advancements and improving patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of DR Tulu-8B or the RLER methodology. While the model is 8B parameters, the abstract highlights its competitive performance against larger proprietary systems, suggesting that scale is not a current limitation for the evaluated benchmarks.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors explicitly aim to facilitate future research by releasing all data, models, code, and their new MCP-based agent infrastructure. This encourages subsequent work in leveraging these resources to further develop and refine deep research models, explore new applications of RLER, and potentially apply the methodology to more specialized or complex domains within and beyond healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Medical Literature Analysis</span>
                    
                    <span class="tag">Drug Discovery and Development</span>
                    
                    <span class="tag">Biomedical Research</span>
                    
                    <span class="tag">Health Informatics</span>
                    
                    <span class="tag">Patient Education</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Reinforcement Learning</span>
                    
                    <span class="tag tag-keyword">Evolving Rubrics</span>
                    
                    <span class="tag tag-keyword">Deep Research</span>
                    
                    <span class="tag tag-keyword">Long-form QA</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">RLER</span>
                    
                    <span class="tag tag-keyword">DR Tulu</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>