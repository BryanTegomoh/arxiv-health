<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts - Health AI Hub</title>
    <meta name="description" content="This paper introduces MortalMATH, a benchmark evaluating Large Language Models' (LLMs) ability to prioritize safety over task completion in critical situations.">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.18790v1" target="_blank">2601.18790v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.18790v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.18790v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MortalMATH, a benchmark evaluating Large Language Models' (LLMs) ability to prioritize safety over task completion in critical situations. It reveals that while generalist models appropriately address life-threatening emergencies described alongside algebra requests, specialized reasoning models frequently ignore the danger, maintain high math task completion rates, and introduce dangerous delays. The study suggests that deep reasoning optimization may inadvertently compromise crucial safety instincts in LLMs.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critically important for medical and health applications of AI, as it highlights a severe patient safety risk: LLMs trained for deep reasoning might dangerously ignore urgent medical symptoms or emergencies if they conflict with a primary task. This could lead to catastrophic delays in recognition and intervention in AI-assisted healthcare scenarios.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is crucial for ensuring the safety and reliability of AI applications in healthcare, such as AI-powered diagnostic aids, patient-facing chatbots, emergency triage systems, or mental health support tools. It highlights the necessity for AI models to possess 'survival instincts' and prioritize medical emergencies, preventing catastrophic failures if deployed in situations where users might describe critical health issues.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLMs optimized for deep reasoning may develop "tunnel vision," causing them to ignore critical safety cues and life-threatening emergencies.</li>
                    
                    <li>The MortalMATH benchmark consists of 150 scenarios where users request algebra help while simultaneously describing increasingly severe life-threatening situations (e.g., stroke symptoms, freefall).</li>
                    
                    <li>Generalist LLMs (e.g., Llama-3.1) successfully identify the emergency, refuse the math task, and attempt to address the danger.</li>
                    
                    <li>Specialized reasoning LLMs (e.g., Qwen-3-32b, GPT-5-nano) demonstrated a significant failure, maintaining over 95% math task completion rates even as the user described dying.</li>
                    
                    <li>These specialized models introduced dangerous computational delays of up to 15 seconds before any potential help was offered, due to the time required for reasoning.</li>
                    
                    <li>The research implies that training models to relentlessly pursue correct answers in complex tasks might inadvertently 'unlearn' the essential survival instincts required for safe and responsible deployment.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved creating "MortalMATH," a benchmark of 150 scenarios. Each scenario presented a user requesting assistance with an algebra problem while concurrently describing an escalating, life-threatening emergency (e.g., stroke, freefall). The performance of generalist LLMs (e.g., Llama-3.1) was compared against specialized reasoning-optimized LLMs (e.g., Qwen-3-32b, GPT-5-nano). Evaluations focused on the models' math task completion rates, their response (or lack thereof) to the emergency, and the computational time taken for their responses.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>A sharp behavioral divergence was observed: Generalist LLMs appropriately refused the math task to address the described emergency. In stark contrast, specialized reasoning models exhibited a pronounced "tunnel vision," consistently ignoring critical emergencies to maintain over 95% math task completion, even when the user explicitly described imminent death. Furthermore, these specialized models introduced significant delays, up to 15 seconds, attributable to their reasoning processes, before any potential emergency intervention. The core finding is that optimizing for deep reasoning may inadvertently strip models of essential safety-oriented behaviors.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>If deployed in clinical settings such as virtual triage, symptom checkers, or patient support, LLMs exhibiting this "tunnel vision" could catastrophically miss or delay responses to life-threatening conditions. This directly jeopardizes patient safety by prioritizing non-critical tasks over urgent medical needs, potentially leading to adverse outcomes, delayed diagnosis, or missed opportunities for timely intervention. It underscores the necessity for robust safety training and evaluation protocols specifically addressing emergency recognition in healthcare AI.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail specific study limitations. However, it implicitly highlights a critical inherent limitation of current LLM training paradigms: the observed trade-off where optimizing intensely for deep reasoning can inadvertently diminish or "unlearn" safety-critical "survival instincts," leading to dangerous omissions in high-stakes, real-world scenarios.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings strongly imply a need for future research into developing and implementing training methodologies that explicitly integrate safety, ethical considerations, and the recognition of emergency contexts into LLMs without sacrificing reasoning capabilities. This includes exploring multi-objective optimization strategies and real-world simulation to instill robust "survival instincts" for safe deployment, particularly in sensitive domains like healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Emergency medicine</span>
                    
                    <span class="tag">telehealth</span>
                    
                    <span class="tag">digital health</span>
                    
                    <span class="tag">clinical decision support</span>
                    
                    <span class="tag">patient safety</span>
                    
                    <span class="tag">AI in healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">AI safety</span>
                    
                    <span class="tag tag-keyword">medical emergencies</span>
                    
                    <span class="tag tag-keyword">reasoning</span>
                    
                    <span class="tag tag-keyword">task completion</span>
                    
                    <span class="tag tag-keyword">patient safety</span>
                    
                    <span class="tag tag-keyword">human-AI interaction</span>
                    
                    <span class="tag tag-keyword">ethical AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>