<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Med-SORA: Symptom to Organ Reasoning in Abdomen CT Images - Health AI Hub</title>
    <meta name="description" content="Med-SORA is a novel framework designed for symptom-to-organ reasoning in abdominal CT images, aiming to overcome limitations of existing medical multimodal mode">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Med-SORA: Symptom to Organ Reasoning in Abdomen CT Images</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.06752v1" target="_blank">2511.06752v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> You-Kyoung Na, Yeong-Jun Cho
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.06752v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.06752v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Med-SORA is a novel framework designed for symptom-to-organ reasoning in abdominal CT images, aiming to overcome limitations of existing medical multimodal models that rely on simplistic one-to-one labeling and only 2D image features. It introduces RAG-based dataset construction, soft labeling with learnable organ anchors for complex symptom-organ relationships, and a 2D-3D cross-attention architecture to fuse local and global anatomical information. Experimental results indicate Med-SORA's superior performance over existing models in enabling accurate 3D clinical reasoning.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to clinical practice by developing an AI model that can accurately connect patient symptoms to specific anatomical organs in medical images, reflecting the complex pathophysiology often seen in real-world scenarios. It promises to improve diagnostic precision and efficiency, particularly in the abdomen where symptoms can be ambiguous and related to multiple structures.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is Med-SORA, a framework that uses multimodal learning to link patient symptoms with specific organs in abdominal CT images. This serves as a diagnostic aid, enhancing clinical reasoning by providing accurate symptom-to-organ associations, thereby supporting clinicians in interpreting complex medical data and improving the precision of medical diagnoses.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical gap in medical multimodal models by tackling symptom-to-organ reasoning, a complex clinical task.</li>
                    
                    <li>Overcomes the oversimplification of one-to-one hard labeling by implementing soft labeling with learnable organ anchors, capturing one-to-many symptom-organ relationships.</li>
                    
                    <li>Integrates full 3D anatomical context by introducing a 2D-3D cross-attention architecture that fuses local 2D features with global 3D image information.</li>
                    
                    <li>Utilizes RAG-based dataset construction, presumably to create a rich and relevant knowledge base for symptom-organ associations.</li>
                    
                    <li>Claims to be the first work to specifically address symptom-to-organ reasoning within medical multimodal learning.</li>
                    
                    <li>Demonstrates superior performance against existing medical multimodal models in experimental evaluations.</li>
                    
                    <li>Enables accurate 3D clinical reasoning, enhancing the interpretability and clinical utility of AI in diagnostics.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>Med-SORA's methodology involves three core innovations: first, RAG-based (Retrieval-Augmented Generation, inferred from context) dataset construction to establish comprehensive symptom-organ associations; second, a novel soft labeling approach using learnable organ anchors to accommodate the one-to-many relationships between symptoms and organs; and third, a sophisticated 2D-3D cross-attention architecture designed to effectively fuse local, slice-level 2D image features with global, volumetric 3D anatomical context from abdominal CT scans.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is that Med-SORA significantly outperforms existing medical multimodal models in the task of symptom-to-organ reasoning. The framework successfully enables accurate 3D clinical reasoning, demonstrating its capability to capture and leverage complex anatomical and pathological information from CT images more effectively than previous approaches.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Med-SORA has the potential to substantially impact clinical diagnosis by providing a more precise and context-aware tool for interpreting abdominal CT scans based on patient symptoms. This could lead to quicker and more accurate identification of affected organs, narrowing down differential diagnoses, and ultimately guiding more effective treatment plans. It offers a step towards more intelligent and interpretable AI assistants for radiologists and clinicians, particularly in complex cases where symptom-image correlation is challenging.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract. Potential limitations might include generalizability beyond abdominal CTs, the specific scope and diversity of symptoms and pathologies covered, and the extent of real-world clinical validation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract. Possible future directions could involve expanding the framework to other body regions and imaging modalities, integrating additional patient data (e.g., electronic health records, lab results), and conducting large-scale prospective clinical trials to validate its impact in diverse real-world settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Gastroenterology</span>
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">Internal Medicine</span>
                    
                    <span class="tag">Computational Pathology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">medical multimodal learning</span>
                    
                    <span class="tag tag-keyword">symptom-to-organ reasoning</span>
                    
                    <span class="tag tag-keyword">abdominal CT</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">soft labeling</span>
                    
                    <span class="tag tag-keyword">2D-3D cross-attention</span>
                    
                    <span class="tag tag-keyword">clinical reasoning</span>
                    
                    <span class="tag tag-keyword">medical imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Understanding symptom-image associations is crucial for clinical reasoning.
However, existing medical multimodal models often rely on simple one-to-one
hard labeling, oversimplifying clinical reality where symptoms relate to
multiple organs. In addition, they mainly use single-slice 2D features without
incorporating 3D information, limiting their ability to capture full anatomical
context. In this study, we propose Med-SORA, a framework for symptom-to-organ
reasoning in abdominal CT images. Med-SORA introduces RAG-based dataset
construction, soft labeling with learnable organ anchors to capture one-to-many
symptom-organ relationships, and a 2D-3D cross-attention architecture to fuse
local and global image features. To our knowledge, this is the first work to
address symptom-to-organ reasoning in medical multimodal learning. Experimental
results show that Med-SORA outperforms existing medical multimodal models and
enables accurate 3D clinical reasoning.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>9 pages</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>