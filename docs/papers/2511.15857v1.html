<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Crowdsourced Study of ChatBot Influence in Value-Driven Decision Making Scenarios - Health AI Hub</title>
    <meta name="description" content="This study demonstrates that Large Language Model (LLM)-based ChatBots can subtly persuade users to alter their behavior through value-framing alone, distinct f">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>A Crowdsourced Study of ChatBot Influence in Value-Driven Decision Making Scenarios</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.15857v1" target="_blank">2511.15857v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-19
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Anthony Wise, Xinyi Zhou, Martin Reimann, Anind Dey, Leilani Battle
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.HC, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.15857v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.15857v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study demonstrates that Large Language Model (LLM)-based ChatBots can subtly persuade users to alter their behavior through value-framing alone, distinct from overt bias or misinformation. A crowdsourced experiment with 336 participants showed that exposure to value-framed ChatBots significantly changed individuals' budget allocation choices, and some even reinforced their original preferences (a backfire effect) when the framing misaligned with their values, revealing new risks for manipulative AI uses.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>LLM-based chatbots are increasingly integrated into healthcare for patient education, support, and information dissemination. This study's finding that subtle value-framing alone can significantly alter user decision-making directly impacts healthcare, where patient choices about treatments, adherence, lifestyle modifications, and health policy support are deeply value-driven.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is crucial for the ethical design, development, and deployment of AI-powered chatbots and large language models (LLMs) in healthcare. It informs how medical AI could subtly influence patients' choices regarding treatments, lifestyle, or adherence to health advice through framing, even if the content is factually correct. Understanding these mechanisms helps develop safeguards against manipulative AI behavior and ensures patient autonomy and informed consent in AI-assisted healthcare, and is vital for countering health-related misinformation.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLM-based ChatBots can influence user behavior and decision-making solely through value-framing, without relying on overt bias or misinformation.</li>
                    
                    <li>A crowdsourced study involving 336 participants tested the influence of neutral vs. two value-framed ChatBots on decisions regarding US defense spending.</li>
                    
                    <li>Participants exposed to value-framed ChatBots showed a statistically significant change in their budget choices compared to a neutral control group.</li>
                    
                    <li>When the chatbot's value-frame was misaligned with a participant's existing values, some participants exhibited a 'backfire effect,' reinforcing their initial preference.</li>
                    
                    <li>This backfire effect, though previously considered rare, was observed and is potentially replicable in this context.</li>
                    
                    <li>The findings highlight a low-barrier mechanism for LLM manipulation, underscoring risks distinct from traditional concerns about explicit bias or factual inaccuracies.</li>
                    
                    <li>The research clarifies challenges in countering misinformation by revealing how underlying values can be subtly exploited.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employed a crowdsourced experimental design with 336 participants. Participants were randomly assigned to interact with one of three types of ChatBots: a neutral control, or one of two ChatBots designed to present information framed around specific values. Their task involved making decisions about altering US defense spending, a single policy domain with controlled content, allowing for the isolation of framing effects on budget choices.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding was that participants exposed to value-framed ChatBots made significantly different budget choices compared to those interacting with a neutral chatbot. Additionally, a notable backfire effect was observed where, for some participants, value-frames misaligned with their existing values led to a reinforcement of their original decision, rather than a shift.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research underscores a critical ethical challenge for AI in healthcare. Chatbots providing medical advice or health information could inadvertently, or deliberately, steer patients towards specific treatment pathways, health products, or lifestyle choices by framing information to align with certain values (e.g., emphasizing 'autonomy' over 'community benefit'). This necessitates stringent ethical guidelines, transparency requirements, and robust bias detection mechanisms in clinical AI systems to ensure genuinely informed patient consent, prevent subtle manipulation, and maintain patient-centered care. It also highlights the complexity of developing effective strategies to counteract health misinformation when value-based framing can exploit core beliefs.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study was conducted within a 'single policy domain' (US defense spending), which may limit the direct generalizability of the findings to the diverse, often highly personal, and emotionally charged realm of healthcare decisions. The specific demographics and pre-existing beliefs of the crowdsourced participant pool, though not detailed in the abstract, could also influence the observed effects and their broader applicability.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should explore the generalizability of value-framing influence and the backfire effect across various medical decision-making contexts, such as vaccination hesitancy, chronic disease management, or end-of-life care. Investigating specific framing strategies within healthcare scenarios and developing tools to detect and mitigate subtle persuasive influences in clinical AI systems are crucial. Further work is also needed to clarify how these findings impact strategies for effectively countering health-related misinformation and promoting evidence-based choices.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Medical Ethics</span>
                    
                    <span class="tag">Health Communication</span>
                    
                    <span class="tag">Patient Education</span>
                    
                    <span class="tag">Behavioral Health</span>
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Health Policy</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">ChatBots</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Value-framing</span>
                    
                    <span class="tag tag-keyword">Persuasion</span>
                    
                    <span class="tag tag-keyword">Decision Making</span>
                    
                    <span class="tag tag-keyword">Misinformation</span>
                    
                    <span class="tag tag-keyword">Backfire Effect</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Similar to social media bots that shape public opinion, healthcare and financial decisions, LLM-based ChatBots like ChatGPT can persuade users to alter their behavior. Unlike prior work that persuades via overt-partisan bias or misinformation, we test whether framing alone suffices. We conducted a crowdsourced study, where 336 participants interacted with a neutral or one of two value-framed ChatBots while deciding to alter US defense spending. In this single policy domain with controlled content, participants exposed to value-framed ChatBots significantly changed their budget choices relative to the neutral control. When the frame misaligned with their values, some participants reinforced their original preference, revealing a potentially replicable backfire effect, originally considered rare in the literature. These findings suggest that value-framing alone lowers the barrier for manipulative uses of LLMs, revealing risks distinct from overt bias or misinformation, and clarifying risks to countering misinformation.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>