<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ConSurv: Multimodal Continual Learning for Survival Analysis - Health AI Hub</title>
    <meta name="description" content="ConSurv addresses the critical limitations of static models and unimodal continual learning in cancer survival prediction by introducing the first multimodal co">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>ConSurv: Multimodal Continual Learning for Survival Analysis</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.09853v1" target="_blank">2511.09853v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-13
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Dianzhi Yu, Conghao Xiong, Yankai Chen, Wenqian Cui, Xinni Zhang, Yifei Zhang, Hao Chen, Joseph J. Y. Sung, Irwin King
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.09853v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.09853v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">ConSurv addresses the critical limitations of static models and unimodal continual learning in cancer survival prediction by introducing the first multimodal continual learning (MMCL) method. It effectively mitigates catastrophic forgetting and leverages complex inter-modal interactions between gigapixel whole slide images and genomics, demonstrating superior performance on a newly proposed benchmark.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to clinical oncology as it offers a robust method for more accurate and adaptive cancer survival predictions. Such dynamic models are crucial for informing mortality risks, personalizing treatment plans, and ensuring clinical decisions are based on the most current and comprehensive patient data.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>ConSurv is an AI application designed to provide more accurate and adaptable predictions of cancer patient survival. By leveraging multimodal data (whole slide images and genomic data) and continually learning from new data streams without forgetting previous knowledge, it aims to assist clinicians in assessing mortality risks, personalizing treatment plans, and improving patient outcomes in a dynamic clinical environment.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Addressed:** Static cancer survival models fail to adapt to dynamic clinical environments and continuous data streams, while existing unimodal continual learning (CL) methods suffer severe catastrophic forgetting and neglect crucial inter-modal correlations in multimodal data.</li>
                    
                    <li>**Proposed Solution:** ConSurv is introduced as the first Multimodal Continual Learning (MMCL) method specifically designed for survival analysis, capable of handling complex multimodal inputs like whole slide images (WSI) and genomics.</li>
                    
                    <li>**Multi-staged Mixture of Experts (MS-MoE):** This core component captures both task-shared and task-specific knowledge at different network learning stages (modality encoders and fusion component), enabling the learning of intricate inter-modal relationships.</li>
                    
                    <li>**Feature Constrained Replay (FCR):** FCR is employed to enhance learned knowledge and significantly mitigate catastrophic forgetting by restricting the feature deviation of previous data at multiple granularities: encoder-level features for each modality and fusion-level representations.</li>
                    
                    <li>**New Benchmark Introduction:** The paper establishes a novel benchmark, Multimodal Survival Analysis Incremental Learning (MSAIL), by integrating four distinct datasets to facilitate comprehensive evaluation in a continual learning setting.</li>
                    
                    <li>**Superior Performance:** Extensive experiments demonstrate that ConSurv consistently outperforms existing competing methods across a range of multiple metrics, validating its effectiveness in the MMCL survival analysis task.</li>
                    
                    <li>**Clinical Adaptability:** The method enables survival prediction models to adapt dynamically to evolving clinical environments and continuously integrate new multimodal data streams, significantly enhancing their practical utility.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>ConSurv employs a multimodal continual learning (MMCL) framework incorporating two key components: a Multi-staged Mixture of Experts (MS-MoE) to learn task-shared/specific knowledge and inter-modal relationships across modality encoders and fusion, and Feature Constrained Replay (FCR) to prevent catastrophic forgetting by limiting feature deviation from prior tasks at both encoder and fusion levels. The method is evaluated on a new benchmark, MSAIL, integrating four cancer datasets in an incremental learning setup.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>ConSurv successfully overcomes the challenges of catastrophic forgetting and complex inter-modal interactions in survival prediction, demonstrating superior performance over existing methods across various metrics. It effectively leverages multimodal inputs, such as whole slide images and genomics, in a continuous learning paradigm.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to significantly improve clinical decision-making in oncology by providing dynamic and adaptive survival prediction models. These models can continuously learn from new patient data, leading to more precise, up-to-date risk assessments and more personalized treatment strategies for cancer patients, ultimately enhancing patient care and outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of the ConSurv method itself or its current implementation. It primarily focuses on the limitations of existing approaches that ConSurv aims to address.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention specific future research directions for the ConSurv method or its broader application.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Cancer Research</span>
                    
                    <span class="tag">Digital Pathology</span>
                    
                    <span class="tag">Computational Genomics</span>
                    
                    <span class="tag">Precision Medicine</span>
                    
                    <span class="tag">Prognostics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Continual Learning</span>
                    
                    <span class="tag tag-keyword">Multimodal Learning</span>
                    
                    <span class="tag tag-keyword">Survival Analysis</span>
                    
                    <span class="tag tag-keyword">Cancer Prognosis</span>
                    
                    <span class="tag tag-keyword">Whole Slide Images</span>
                    
                    <span class="tag tag-keyword">Genomics</span>
                    
                    <span class="tag tag-keyword">Catastrophic Forgetting</span>
                    
                    <span class="tag tag-keyword">Mixture of Experts</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Survival prediction of cancers is crucial for clinical practice, as it informs mortality risks and influences treatment plans. However, a static model trained on a single dataset fails to adapt to the dynamically evolving clinical environment and continuous data streams, limiting its practical utility. While continual learning (CL) offers a solution to learn dynamically from new datasets, existing CL methods primarily focus on unimodal inputs and suffer from severe catastrophic forgetting in survival prediction. In real-world scenarios, multimodal inputs often provide comprehensive and complementary information, such as whole slide images and genomics; and neglecting inter-modal correlations negatively impacts the performance. To address the two challenges of catastrophic forgetting and complex inter-modal interactions between gigapixel whole slide images and genomics, we propose ConSurv, the first multimodal continual learning (MMCL) method for survival analysis. ConSurv incorporates two key components: Multi-staged Mixture of Experts (MS-MoE) and Feature Constrained Replay (FCR). MS-MoE captures both task-shared and task-specific knowledge at different learning stages of the network, including two modality encoders and the modality fusion component, learning inter-modal relationships. FCR further enhances learned knowledge and mitigates forgetting by restricting feature deviation of previous data at different levels, including encoder-level features of two modalities and the fusion-level representations. Additionally, we introduce a new benchmark integrating four datasets, Multimodal Survival Analysis Incremental Learning (MSAIL), for comprehensive evaluation in the CL setting. Extensive experiments demonstrate that ConSurv outperforms competing methods across multiple metrics.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>14 pages, 4 figures. This is the extended version of the paper accepted at AAAI 2026, which includes all technical appendices and additional experimental details</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>