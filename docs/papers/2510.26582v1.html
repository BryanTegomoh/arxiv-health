<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CATCH: A Modular Cross-domain Adaptive Template with Hook - Health AI Hub</title>
    <meta name="description" content="CATCH addresses the significant performance degradation of Visual Question Answering (VQA) models, like LLaVA, when transferred to out-of-domain scenarios such ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>CATCH: A Modular Cross-domain Adaptive Template with Hook</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26582v1" target="_blank">2510.26582v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinjin Li, Yulie Lu, Jinghan Cao, Yu Ma, Zhenglin Li, Yeyang Zhou
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26582v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26582v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">CATCH addresses the significant performance degradation of Visual Question Answering (VQA) models, like LLaVA, when transferred to out-of-domain scenarios such as medical imaging. It introduces a plug-and-play framework that decouples visual and linguistic adaptation via a domain classifier and a dual adapter mechanism (Prompt and Visual Adapters) dynamically injected without backbone retraining. This approach achieves consistent performance gains across diverse VQA benchmarks, including a notable +2.6 VQA score on MedVQA-RAD.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework directly addresses the critical challenge of adapting advanced VQA models for medical imaging analysis, potentially enabling more accurate, automated interpretation of medical visuals and diagnostic support without the need for extensive, domain-specific retraining.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application to health involves enhancing the capability of VQA models to accurately answer questions based on medical images (e.g., radiology scans). By improving cross-domain adaptation, CATCH enables AI systems to be more reliable and scalable for interpreting diverse medical imaging data, potentially assisting clinicians with diagnostic support or information retrieval from medical images without extensive model retraining per specific sub-domain.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Traditional VQA models demonstrate poor generalization to out-of-domain scenarios (e.g., medical imaging, remote sensing) due to distributional shifts and lack of effective domain adaptation.</li>
                    
                    <li>Existing domain adaptation methods for VQA are often costly, inflexible, and not scalable across diverse tasks, relying on per-domain fine-tuning or bespoke pipelines.</li>
                    
                    <li>CATCH (Cross-domain Adaptive Template with Hook) is proposed as a plug-and-play framework designed to improve VQA generalization with minimal changes to the core backbone architecture.</li>
                    
                    <li>The framework decouples visual and linguistic adaptation through two lightweight modules: a domain classifier to identify image types and a dual adapter mechanism.</li>
                    
                    <li>The dual adapter mechanism comprises a Prompt Adapter for language modulation and a Visual Adapter for vision feature adjustment.</li>
                    
                    <li>Both the domain classifier and dual adapters are dynamically injected via a unified hook interface, crucially without requiring any retraining of the backbone VQA model.</li>
                    
                    <li>Experimental results demonstrate consistent performance gains across four domain-specific VQA benchmarks, including +2.6 VQA on MedVQA-RAD, +2.3 BLEU on MathVQA, and +3.1 ROUGE on ChartQA.</li>
                    
                    <li>CATCH provides a scalable and extensible approach to multi-domain VQA, facilitating practical deployment across diverse application domains.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>CATCH employs a modular, plug-and-play framework that integrates a lightweight domain classifier to identify the input image's type. It also incorporates a dual adapter mechanism, consisting of a Prompt Adapter for language modulation and a Visual Adapter for vision feature adjustment. These modules are dynamically injected into pre-trained VQA backbone models via a unified hook interface, effectively decoupling visual and linguistic adaptation while eliminating the need for retraining the core backbone architecture.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The CATCH framework consistently improves VQA model performance across out-of-domain benchmarks without retraining the backbone model. Key results include a +2.6 VQA score increase on MedVQA-RAD, a +2.3 BLEU score improvement on MathVQA, and a +3.1 ROUGE score gain on ChartQA.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology holds significant potential to accelerate the development and deployment of AI-powered diagnostic and assistive tools in healthcare. By enabling existing VQA models to robustly interpret diverse medical images (e.g., X-rays, MRI, CT scans) without costly and time-consuming per-domain fine-tuning, CATCH could lead to more accessible, scalable, and versatile AI applications for clinical decision support, medical image analysis, and potentially improved patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations or caveats of the CATCH framework.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly detailing 'future research directions,' the paper concludes that CATCH provides a 'scalable and extensible approach to multi-domain VQA' and enables 'practical deployment across diverse application domains,' suggesting its potential for broad applicability and continued expansion into new and varied domains.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Radiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Visual Question Answering</span>
                    
                    <span class="tag tag-keyword">Domain Adaptation</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Plug-and-Play</span>
                    
                    <span class="tag tag-keyword">Cross-domain</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">VQA</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>