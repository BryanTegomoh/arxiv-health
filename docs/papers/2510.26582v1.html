<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CATCH: A Modular Cross-domain Adaptive Template with Hook - Health AI Hub</title>
    <meta name="description" content="This paper introduces CATCH, a modular, plug-and-play framework designed to improve the generalization of Visual Question Answering (VQA) models, like LLaVA, wh">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>CATCH: A Modular Cross-domain Adaptive Template with Hook</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26582v1" target="_blank">2510.26582v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinjin Li, Yulie Lu, Jinghan Cao, Yu Ma, Zhenglin Li, Yeyang Zhou
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26582v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26582v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces CATCH, a modular, plug-and-play framework designed to improve the generalization of Visual Question Answering (VQA) models, like LLaVA, when applied to out-of-domain scenarios such as medical imaging. CATCH achieves this by decoupling visual and linguistic adaptation through a domain classifier and a dual adapter mechanism (Prompt and Visual Adapters) dynamically injected via a hook interface, critically requiring no retraining of the backbone VQA model. Experimental results demonstrate consistent performance gains across diverse benchmarks, including a significant +2.6 VQA score increase on the MedVQA-RAD dataset, showcasing its scalability for multi-domain VQA deployment.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as it directly addresses the challenge of deploying advanced VQA models for analyzing medical images, where current models often fail due to domain shifts. By significantly improving performance on medical imaging datasets like MedVQA-RAD without needing to retrain large foundational models, CATCH enables more accurate and robust AI-driven medical image interpretation and diagnostic assistance.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The CATCH framework aims to make Visual Question Answering (VQA) models more robust and adaptable to medical imaging data. This has direct applications in healthcare by enabling AI systems to accurately interpret medical images (e.g., X-rays, MRI scans), answer specific clinical questions about patient conditions derived from these images, assist in generating diagnostic reports, or support medical education. By improving generalization across diverse domains without extensive re-training, it facilitates the practical deployment of AI in various medical specialties.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Traditional VQA models demonstrate significant performance degradation when transferred to out-of-domain scenarios like medical imaging due to large distributional shifts.</li>
                    
                    <li>CATCH proposes a novel plug-and-play framework for cross-domain adaptation that enhances VQA model generalization with minimal changes to core architectures.</li>
                    
                    <li>The framework's core idea is to decouple visual and linguistic adaptation using two lightweight modules: a domain classifier to identify input image type and a dual adapter mechanism.</li>
                    
                    <li>The dual adapter mechanism comprises a Prompt Adapter for language modulation and a Visual Adapter for vision feature adjustment, both injected dynamically via a unified hook interface.</li>
                    
                    <li>A key advantage of CATCH is its ability to achieve consistent performance gains across diverse domains without requiring the expensive retraining of the underlying backbone VQA model.</li>
                    
                    <li>Experimental validation included four domain-specific VQA benchmarks, notably yielding a +2.6 VQA score improvement on the MedVQA-RAD dataset.</li>
                    
                    <li>CATCH offers a scalable and extensible approach to multi-domain VQA, enabling practical and flexible deployment across various specialized application domains, including healthcare.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>CATCH is a plug-and-play framework that incorporates a domain classifier to automatically identify the input image type. It then utilizes a dual adapter mechanism, consisting of a Prompt Adapter for linguistic modulation and a Visual Adapter for adjusting vision features. These two modules are dynamically injected into the VQA backbone via a unified hook interface, facilitating adaptation to new domains without requiring any retraining of the original large language or vision models.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>CATCH consistently improves the generalization performance of VQA models across out-of-domain benchmarks. Notably, it achieved a +2.6 VQA score increase on the MedVQA-RAD dataset, demonstrating enhanced accuracy in medical imaging contexts. Furthermore, these performance gains were realized without the need for retraining the computationally expensive backbone VQA models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The improved generalization of VQA models to medical imaging, as demonstrated by CATCH's performance on MedVQA-RAD, has the potential for significant clinical impact. It could lead to more reliable AI tools for assisting radiologists in interpreting complex medical images, answering clinical questions directly from imaging data, and providing decision support, thereby potentially enhancing diagnostic accuracy, efficiency, and ultimately, patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Diagnostics</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Medical Image Analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Visual Question Answering (VQA)</span>
                    
                    <span class="tag tag-keyword">Cross-domain Adaptation</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Domain Generalization</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Plug-and-Play</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">MedVQA-RAD</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>