<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CATCH: A Modular Cross-domain Adaptive Template with Hook - Health AI Hub</title>
    <meta name="description" content="CATCH is a novel plug-and-play framework designed to overcome the significant performance degradation of Visual Question Answering (VQA) models, such as LLaVA, ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>CATCH: A Modular Cross-domain Adaptive Template with Hook</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26582v1" target="_blank">2510.26582v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinjin Li, Yulie Lu, Jinghan Cao, Yu Ma, Zhenglin Li, Yeyang Zhou
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26582v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26582v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">CATCH is a novel plug-and-play framework designed to overcome the significant performance degradation of Visual Question Answering (VQA) models, such as LLaVA, when applied to out-of-domain data like medical imaging. It achieves this by decoupling visual and linguistic adaptation through a lightweight domain classifier and a dual adapter mechanism (Prompt and Visual Adapters), dynamically injected via a hook interface. The framework demonstrates consistent and substantial performance gains across diverse domain-specific VQA benchmarks, notably +2.6 VQA on MedVQA-RAD, without requiring retraining of the backbone VQA model.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework directly addresses the challenge of deploying advanced AI models, specifically VQA, to specialized medical imaging without costly per-domain fine-tuning, enabling more accurate and scalable diagnostic or analytical support in healthcare. It allows powerful general-purpose models to interpret complex medical data effectively.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research aims to improve the robustness and generalization of Visual Question Answering (VQA) models when applied to medical images, such as radiology scans. By enabling AI models to accurately answer natural language questions about medical images even with domain shifts, it can facilitate applications in medical diagnosis support, clinical decision making, and medical education by providing more reliable and adaptable AI tools for interpreting complex medical visual data.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>VQA models, despite impressive performance on natural images, suffer significant generalization degradation on out-of-domain scenarios like medical imaging due to distributional shifts.</li>
                    
                    <li>CATCH (Cross-domain Adaptive Template with Hook) is proposed as a plug-and-play framework for cross-domain adaptation, improving VQA generalization with minimal changes to core architecture.</li>
                    
                    <li>The core innovation is the decoupling of visual and linguistic adaptation through two lightweight, dynamically injected modules.</li>
                    
                    <li>A domain classifier identifies the input image type, enabling targeted adaptation.</li>
                    
                    <li>A dual adapter mechanism comprises a Prompt Adapter for linguistic modulation and a Visual Adapter for visual feature adjustment.</li>
                    
                    <li>Both modules are injected via a unified hook interface, critically requiring no retraining of the backbone VQA model, enhancing flexibility and scalability.</li>
                    
                    <li>Experimental results demonstrate consistent performance improvements across various benchmarks, including a significant +2.6 VQA score increase on the MedVQA-RAD dataset, validating its efficacy in medical applications.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>CATCH employs a modular, plug-and-play architecture that decouples visual and linguistic adaptation processes. It integrates a lightweight domain classifier to identify the input image's type. Subsequently, a dual adapter mechanism is utilized, consisting of a Prompt Adapter responsible for language modulation and a Visual Adapter for adjusting vision features. Both these adaptation modules are dynamically injected into the pre-trained VQA backbone via a unified hook interface, critically eliminating the need for retraining the entire backbone model.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The CATCH framework consistently achieved significant performance gains across various domain-specific VQA benchmarks without requiring retraining of the backbone model. Key results include:
*   An improvement of +2.6 VQA score on the MedVQA-RAD dataset.
*   An improvement of +2.3 BLEU score on the MathVQA dataset.
*   An improvement of +3.1 ROUGE score on the ChartQA dataset.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>CATCH has the potential to significantly accelerate the adoption of advanced VQA models in clinical practice. By enabling existing, powerful VQA systems to effectively interpret and reason over diverse medical images (e.g., X-rays, MRI, CT scans) without extensive retraining for each specific medical domain or dataset, it reduces development costs and deployment time. This facilitates the creation of more versatile, adaptable, and accessible AI-powered diagnostic and analytical tools, potentially aiding radiologists and other clinicians in complex image interpretation and question answering tasks, ultimately improving patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the CATCH framework.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Diagnosis (potential for aiding)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">VQA</span>
                    
                    <span class="tag tag-keyword">Cross-domain Adaptation</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Plug-and-Play</span>
                    
                    <span class="tag tag-keyword">Domain Adaptation</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Hook Interface</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>