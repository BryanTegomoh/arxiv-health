<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge - Health AI Hub</title>
    <meta name="description" content="This paper introduces the first benchmark to evaluate Large Language Models' (LLMs) ability to internalize real-world probability distributions, specifically ob">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03070v1" target="_blank">2511.03070v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Drago Plecko, Patrik Okanovic, Torsten Hoefler, Elias Bareinboim
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI, cs.LG, stat.ML
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03070v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03070v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces the first benchmark to evaluate Large Language Models' (LLMs) ability to internalize real-world probability distributions, specifically observational knowledge, across various domains including health. The study finds that LLMs perform poorly in this regard, indicating they do not naturally acquire or accurately represent such statistical understanding from their vast training data.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly pertinent to medicine and public health as it highlights LLMs' fundamental inability to accurately grasp real-world population statistics. Such statistical understanding is crucial for epidemiological analysis, risk assessment, clinical decision-making, and formulating evidence-based health policies.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is crucial for assessing the foundational capabilities and inherent limitations of LLMs when applied to health-related tasks. It implies that current LLMs cannot reliably be assumed to 'know' or accurately reflect real-world health statistics. This has direct implications for AI applications in areas such as patient risk assessment, prevalence estimation, generating summaries of medical literature that include statistical data, public health messaging, or any application requiring robust reasoning based on observational health distributions, as their underlying statistical knowledge is shown to be weak.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The research distinguishes between factual knowledge and probabilistic knowledge, focusing on LLMs' capacity for the latter, which describes real-world probability distributions.</li>
                    
                    <li>It hypothesizes that LLMs, due to their training on immense text corpora, might internalize aspects of real-world distributions, despite the inherent statistical challenge of the 'curse of dimensionality'.</li>
                    
                    <li>A novel benchmark was developed, constituting the first direct test of LLMs' access to empirical observational distributions of real-world populations.</li>
                    
                    <li>The benchmark evaluates LLMs across critical domains such as economics, education, social behavior, and notably, health, making its findings directly relevant to medical and public health applications.</li>
                    
                    <li>Results unequivocally demonstrate that LLMs perform poorly overall, failing to internalize or accurately represent real-world statistics naturally.</li>
                    
                    <li>Interpreted through Pearl's Causal Hierarchy (PCH), this lack of Layer 1 (observational) knowledge implies severe limitations in LLMs' capabilities for Layer 2 (interventional) and Layer 3 (counterfactual) reasoning, as per the Causal Hierarchy Theorem.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors developed a novel benchmark specifically designed to assess LLMs' knowledge of empirical probability distributions describing real-world populations. This involved systematically querying LLMs on probabilistic properties across diverse domains, including health, and evaluating their responses against true statistical data. The benchmark explicitly targets the evaluation of Layer 1 (observational) knowledge within Pearl's Causal Hierarchy.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is that Large Language Models perform poorly when tested on their knowledge of real-world observational probability distributions. This indicates a significant limitation in their ability to naturally internalize or accurately represent fundamental statistical properties from their training data. Consequently, this deficit in Layer 1 knowledge severely restricts their potential for higher-level causal reasoning (interventional and counterfactual knowledge) according to the Causal Hierarchy Theorem.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The demonstrated inability of LLMs to accurately grasp basic observational distributions has substantial clinical implications. It means LLMs cannot be reliably trusted for tasks that require accurate population-level statistical reasoning in healthcare, such as predicting disease prevalence, calculating individual patient risks based on epidemiological data, or informing treatment guidelines derived from population studies. Clinical applications relying on LLMs for these functions without robust external statistical grounding risk providing inaccurate information, potentially leading to suboptimal patient care, misdiagnosis, or ineffective public health interventions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>While the abstract doesn't explicitly list study-specific limitations, it points to the 'curse of dimensionality' as a fundamental statistical challenge for any model attempting to learn high-dimensional distributions, implicitly limiting LLM capabilities. The primary limitation highlighted by the study is therefore the inherent inability of current LLMs to accurately internalize real-world observational distributions.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Although not explicitly detailed in the abstract, the findings strongly suggest future research should focus on improving LLMs' capacity for probabilistic reasoning. This could involve exploring novel architectural designs, training objectives, or data augmentation strategies tailored to statistical understanding. Another critical direction is to develop robust methods for integrating verified external statistical databases and epidemiological knowledge into LLM-powered applications for high-stakes domains like medicine, rather than relying solely on their internal 'knowledge'.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Epidemiology</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                    <span class="tag">Population Health Management</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Probability Distributions</span>
                    
                    <span class="tag tag-keyword">Observational Knowledge</span>
                    
                    <span class="tag tag-keyword">Causal Hierarchy</span>
                    
                    <span class="tag tag-keyword">Epidemiology</span>
                    
                    <span class="tag tag-keyword">Health Statistics</span>
                    
                    <span class="tag tag-keyword">AI Benchmarking</span>
                    
                    <span class="tag tag-keyword">Probabilistic Reasoning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>