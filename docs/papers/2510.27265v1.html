<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging Analysis - Health AI Hub</title>
    <meta name="description" content="This paper introduces T^3 (Test-Time Task adaptive merging) and its batch-wise variant T^3_B, a novel framework designed to dynamically merge vision-language mo">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging Analysis</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27265v1" target="_blank">2510.27265v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Raza Imam, Hu Wang, Dwarikanath Mahapatra, Mohammad Yaqub
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27265v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27265v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces T^3 (Test-Time Task adaptive merging) and its batch-wise variant T^3_B, a novel framework designed to dynamically merge vision-language models for medical imaging analysis. By computing per-sample (or batch-wise) interpolation coefficients based on Jensen-Shannon divergence between model outputs, T^3 effectively combines the strengths of robust generalist models and precise expert models, achieving state-of-the-art accuracy and error reduction across diverse medical modalities while maintaining computational efficiency.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for medical AI as it provides a method to create more reliable and adaptive diagnostic tools. By dynamically combining generalist robustness with expert precision, it ensures that AI models perform consistently and accurately across the diverse and often unpredictable data found in clinical medical imaging, reducing misdiagnoses due to modality shifts or rare conditions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is to enhance the robustness and accuracy of Vision-Language Models (VLMs) for analyzing medical images across different modalities (e.g., X-ray, MRI, CT, ultrasound). This improvement can lead to more reliable AI assistance in diagnosis, prognosis, and treatment planning within clinical settings, ultimately aiding healthcare professionals in interpreting complex medical visual data.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical duality in medical VLMs: generalist models offer robustness but lack specificity, while expert models provide high in-distribution accuracy but falter under modality shift.</li>
                    
                    <li>Introduces T^3, a backpropagation-free framework that computes per-sample interpolation coefficients via Jensen-Shannon divergence between the generalist and expert models' output distributions.</li>
                    
                    <li>T^3 enables dynamic adaptation, preserving local precision when models agree and deferring to generalist robustness under modality drift, enhancing reliability in varied clinical tasks.</li>
                    
                    <li>Proposes T^3_B, a batch-wise extension that significantly reduces inference costs and computational bottleneck by calculating a single merging coefficient across a batch of samples.</li>
                    
                    <li>Establishes a rigorous cross-evaluation protocol for medical model merging, spanning in-domain, base-to-novel, and corruptions across four diverse medical modalities, addressing a current gap in standardized benchmarks.</li>
                    
                    <li>Empirically, T^3 achieves new state-of-the-art in Top-1 accuracy and error reduction, outperforming strong baselines.</li>
                    
                    <li>The framework promises more reliable and adaptive deployment of medical vision-language models (MVLM) in clinical settings.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The core methodology involves **Test-Time Task adaptive merging (T^3)**, a backpropagation-free approach that dynamically computes per-sample interpolation coefficients. These coefficients are derived by calculating the Jensen-Shannon divergence (JSD) between the output probability distributions of a generalist model and a fine-tuned expert model. To enhance efficiency and address inference costs, a **batch-wise extension (T^3_B)** is introduced, which computes a single merging coefficient for an entire batch of samples. The method's performance was rigorously evaluated using a newly established cross-evaluation protocol, covering in-domain, base-to-novel, and corrupted datasets across four distinct medical modalities.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>T^3 achieved new state-of-the-art performance in Top-1 accuracy and error reduction for medical imaging analysis. It demonstrated superior reliability and adaptability across diverse medical modalities and challenging data shifts, consistently outperforming strong baseline model-merging techniques while maintaining computational efficiency, especially with its T^3_B variant.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The dynamic merging capabilities of T^3 and T^3_B offer a significant leap towards deploying more robust and reliable vision-language models in real-world clinical settings. This directly translates to more accurate and consistent medical imaging diagnostics, better assisting clinicians in decision-making across various medical tasks, and mitigating performance degradation caused by modality shifts, diverse patient populations, or variations in image acquisition protocols.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The paper identifies a general limitation in the field: the 'lack of a standardized medical-merging benchmark,' which their work actively addresses by proposing a rigorous cross-evaluation protocol. No specific limitations of the T^3/T^3_B method itself are detailed in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The research explicitly 'paves the way for adaptive MVLM deployment in clinical settings,' suggesting future work could focus on broader implementation and validation of T^3/T^3_B across an even wider array of medical modalities, patient demographics, and clinical workflows. Further research may also explore fine-tuning the JSD-based coefficient computation or integrating the framework into real-time diagnostic systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Medical Imaging Analysis</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostics</span>
                    
                    <span class="tag">Pathology (implied by diverse modalities)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Vision-Language Models</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Model Merging</span>
                    
                    <span class="tag tag-keyword">Test-Time Adaptation</span>
                    
                    <span class="tag tag-keyword">Jensen-Shannon Divergence</span>
                    
                    <span class="tag tag-keyword">Zero-Shot Learning</span>
                    
                    <span class="tag tag-keyword">Clinical Deployment</span>
                    
                    <span class="tag tag-keyword">Modality Shift</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">In medical imaging, vision-language models face a critical duality:
pretrained networks offer broad robustness but lack subtle, modality-specific
characteristics, while fine-tuned expert models achieve high in-distribution
accuracy yet falter under modality shift. Existing model-merging techniques,
designed for natural-image benchmarks, are simple and efficient but fail to
deliver consistent gains across diverse medical modalities; their static
interpolation limits reliability in varied clinical tasks. To address this, we
introduce Test-Time Task adaptive merging (T^3), a backpropagation-free
framework that computes per-sample interpolation coefficients via the
Jensen-Shannon divergence between the two models' output distributions. T^3
dynamically preserves local precision when models agree and defers to
generalist robustness under drift. To overcome the inference costs of
sample-wise merging, we further propose a batch-wise extension, T^3_B, that
computes a merging coefficient across a batch of samples, dramatically reducing
computational bottleneck. Recognizing the lack of a standardized
medical-merging benchmark, we present a rigorous cross-evaluation protocol
spanning in-domain, base-to-novel, and corruptions across four modalities.
Empirically, T^3 sets new state-of-the-art in Top-1 accuracy and error
reduction, outperforming strong baselines while maintaining efficiency, paving
the way for adaptive MVLM deployment in clinical settings. Our code is
available at https://github.com/Razaimam45/TCube.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Main: 11 pages, Supplementary: 9 pages 10 tables, 10 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>