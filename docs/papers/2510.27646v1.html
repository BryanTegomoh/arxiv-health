<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VessShape: Few-shot 2D blood vessel segmentation by leveraging shape priors from synthetic images - Health AI Hub</title>
    <meta name="description" content="This paper introduces VessShape, a novel methodology for generating large-scale synthetic 2D datasets designed to instill a strong shape bias in blood vessel se">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>VessShape: Few-shot 2D blood vessel segmentation by leveraging shape priors from synthetic images</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27646v1" target="_blank">2510.27646v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Cesar H. Comin, Wesley N. Galv√£o
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27646v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27646v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces VessShape, a novel methodology for generating large-scale synthetic 2D datasets designed to instill a strong shape bias in blood vessel segmentation models. By leveraging geometric priors like tubular and branching structures while varying textures, VessShape aims to overcome data scarcity and improve model generalization across different imaging modalities. The pre-trained models demonstrate robust few-shot performance, requiring minimal real-world annotations (4-10 samples) for fine-tuning, and exhibit notable zero-shot capabilities in unseen domains.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate blood vessel segmentation is crucial for diagnosing and monitoring various medical conditions, from retinal diseases to cardiovascular issues. This research addresses the significant challenges of data scarcity and model generalization in medical imaging, offering a path to develop more robust and accessible diagnostic tools faster.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper describes an AI methodology (few-shot learning, synthetic data generation, shape-biased pre-training) to create more robust and data-efficient models for automated blood vessel segmentation in medical images. This directly contributes to medical AI applications by enabling more accurate and generalizable tools for disease diagnosis, monitoring, and surgical planning across various medical specialties where vessel analysis is critical.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Semantic segmentation of blood vessels is critical but often hampered by limited annotated data and poor generalization of models across imaging modalities.</li>
                    
                    <li>Existing Convolutional Neural Networks (CNNs) tend to learn texture-based features, leading to poor performance when visual characteristics change.</li>
                    
                    <li>The authors hypothesize that leveraging geometric priors of vessel shapes (tubular, branching) can lead to more robust and data-efficient models.</li>
                    
                    <li>VessShape is introduced as a methodology to generate large-scale 2D synthetic datasets, combining procedurally generated tubular geometries with diverse textures to foster a shape bias.</li>
                    
                    <li>A model pre-trained on VessShape images achieves strong few-shot segmentation performance on two real-world datasets, requiring only 4-10 samples for fine-tuning.</li>
                    
                    <li>The pre-trained model exhibits notable zero-shot capabilities, effectively segmenting vessels in unseen domains without any target-specific training.</li>
                    
                    <li>The findings indicate that pre-training with a strong shape bias is an effective strategy to overcome data scarcity and enhance model generalization in blood vessel segmentation.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>VessShape is a methodology for generating large-scale 2D synthetic datasets. It procedurally generates tubular vessel geometries, mimicking their natural branching structure, and combines these geometries with a wide variety of synthetic foreground and background textures. This approach is designed to force segmentation models (specifically CNNs) to learn inherent shape cues rather than relying on superficial texture-based features, thus instilling a strong shape bias during pre-training.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Models pre-trained using VessShape's synthetic datasets demonstrated strong few-shot segmentation performance, requiring a mere 4 to 10 real-world samples for fine-tuning to achieve effective segmentation on two distinct real-world datasets. Furthermore, these models exhibited significant zero-shot capabilities, successfully segmenting vessels in entirely unseen domains without any domain-specific training. This highlights the effectiveness of shape-biased pre-training for data-efficient and generalizable vessel segmentation.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research could significantly accelerate the development and deployment of automated blood vessel segmentation tools in clinical practice. By drastically reducing the need for extensive, time-consuming manual annotation, it enables quicker adaptation of models to new imaging modalities or rare diseases. This could lead to improved diagnostic accuracy, earlier disease detection, and more personalized treatment planning across various medical specialties reliant on vascular analysis.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the VessShape methodology itself. However, it addresses and aims to overcome the inherent limitations of conventional CNNs in medical imaging, namely their tendency to learn texture-based features, poor generalization across domains, and reliance on large annotated datasets. The extent of 'different imaging modalities' and 'unseen domains' tested for zero-shot capabilities is not detailed in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions. However, the successful demonstration of few-shot and zero-shot learning with shape priors implicitly suggests exploring the application of VessShape to 3D vessel segmentation, other anatomical structures with strong geometric priors, or further investigation into the robustness across an even wider range of complex imaging artifacts and pathologies.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Ophthalmology</span>
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Medical Image Analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">blood vessel segmentation</span>
                    
                    <span class="tag tag-keyword">semantic segmentation</span>
                    
                    <span class="tag tag-keyword">few-shot learning</span>
                    
                    <span class="tag tag-keyword">zero-shot learning</span>
                    
                    <span class="tag tag-keyword">shape priors</span>
                    
                    <span class="tag tag-keyword">synthetic data</span>
                    
                    <span class="tag tag-keyword">generalization</span>
                    
                    <span class="tag tag-keyword">Convolutional Neural Networks</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Semantic segmentation of blood vessels is an important task in medical image
analysis, but its progress is often hindered by the scarcity of large annotated
datasets and the poor generalization of models across different imaging
modalities. A key aspect is the tendency of Convolutional Neural Networks
(CNNs) to learn texture-based features, which limits their performance when
applied to new domains with different visual characteristics. We hypothesize
that leveraging geometric priors of vessel shapes, such as their tubular and
branching nature, can lead to more robust and data-efficient models. To
investigate this, we introduce VessShape, a methodology for generating
large-scale 2D synthetic datasets designed to instill a shape bias in
segmentation models. VessShape images contain procedurally generated tubular
geometries combined with a wide variety of foreground and background textures,
encouraging models to learn shape cues rather than textures. We demonstrate
that a model pre-trained on VessShape images achieves strong few-shot
segmentation performance on two real-world datasets from different domains,
requiring only four to ten samples for fine-tuning. Furthermore, the model
exhibits notable zero-shot capabilities, effectively segmenting vessels in
unseen domains without any target-specific training. Our results indicate that
pre-training with a strong shape bias can be an effective strategy to overcome
data scarcity and improve model generalization in blood vessel segmentation.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>