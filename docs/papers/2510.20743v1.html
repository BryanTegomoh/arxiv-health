<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations - Health AI Hub</title>
    <meta name="description" content="Empathic Prompting is a novel multimodal framework that enriches Large Language Model (LLM) conversations by unobtrusively integrating implicit non-verbal emoti">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20743v1" target="_blank">2510.20743v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.HC, cs.AI, cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.80 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20743v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20743v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Empathic Prompting is a novel multimodal framework that enriches Large Language Model (LLM) conversations by unobtrusively integrating implicit non-verbal emotional cues from users. It utilizes a facial expression recognition service to embed affective information as contextual signals during prompting, aiming to improve conversational smoothness and alignment. A preliminary evaluation demonstrated consistent integration of this non-verbal input into coherent LLM outputs, enhancing conversational fluidity.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework is highly relevant for medicine as it enables AI systems to understand and respond to the critical emotional signals of patients, which are often subtle and non-verbal. This capability can significantly enhance patient-AI interactions in domains like mental health support, telemedicine, and patient education, where emotional context is paramount for effective care and communication.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Empathic Prompting can be applied in healthcare to create more empathetic and effective AI-powered communication tools. For instance, it could enable virtual health assistants or chatbots to better understand a patient's emotional state during a consultation (e.g., distress, anxiety, confusion) by analyzing facial expressions. This could lead to more nuanced responses, improved patient support in mental health contexts, enhanced patient engagement in chronic disease management, or more sensitive communication in telemedicine settings where non-verbal cues are often lost.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Novel Framework**: Introduces 'Empathic Prompting' for multimodal human-AI interaction, augmenting LLM conversations with implicit non-verbal context.</li>
                    
                    <li>**Non-Verbal Context Integration**: Captures users' emotional cues via a commercial facial expression recognition service and embeds them as contextual signals within LLM prompts.</li>
                    
                    <li>**Unobtrusive Operation**: The system requires no explicit user control, seamlessly integrating affective information to enhance conversational flow and alignment.</li>
                    
                    <li>**Modular and Scalable Architecture**: Designed with a modular architecture, enabling future integration of additional non-verbal input modalities beyond facial expressions.</li>
                    
                    <li>**Implementation & Preliminary Evaluation**: Implemented with a locally deployed DeepSeek LLM instance and evaluated through a preliminary service and usability study (N=5).</li>
                    
                    <li>**Key Finding**: Demonstrated consistent integration of non-verbal input into coherent LLM outputs, with participants reporting improved conversational fluidity.</li>
                    
                    <li>**High Relevance for Sensitive Domains**: Highlights significant potential for applications in chatbot-mediated communication, particularly in healthcare and education, where emotional signals are critical yet often implicit.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The system design involves a modular architecture integrating a commercial facial expression recognition service with a locally deployed DeepSeek LLM instance. Emotional cues detected from user facial expressions are embedded as contextual signals within the textual prompts sent to the LLM. The system was evaluated through a preliminary service and usability study with a small cohort (N=5) to assess the consistency of non-verbal input integration and conversational fluidity.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The preliminary evaluation successfully demonstrated the consistent integration of non-verbal emotional input into coherent and contextually relevant LLM outputs. Participants reported that the system contributed to enhanced conversational fluidity, suggesting that the implicit affective information significantly improved the AI's understanding and response quality.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology holds potential to transform patient-facing AI applications by adding an empathetic dimension. In clinical settings, it could lead to more nuanced and effective therapeutic chatbots for mental health, improved patient engagement in chronic disease management via emotionally intelligent virtual assistants, and better detection of patient distress or non-compliance in telemedicine consultations. The ability to implicitly gauge emotional states could also aid in triaging or tailoring information delivery in patient education platforms.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The primary limitation noted is the preliminary nature of the evaluation, conducted with a very small sample size (N=5). This limits the generalizability of the findings and positions the work as a 'proof of concept' rather than a robust, large-scale validation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Suggested future directions include scaling the evaluation to a larger and more diverse user population to validate findings more broadly. Additionally, the modular architecture invites integration of further non-verbal modules (e.g., voice intonation, body language) to create an even richer contextual understanding for LLM conversations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Mental health support</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Patient education</span>
                    
                    <span class="tag">Therapeutic chatbots</span>
                    
                    <span class="tag">Remote patient monitoring</span>
                    
                    <span class="tag">Clinical decision support (patient-facing)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Empathic Prompting</span>
                    
                    <span class="tag tag-keyword">Multimodal LLM</span>
                    
                    <span class="tag tag-keyword">Non-verbal communication</span>
                    
                    <span class="tag tag-keyword">Facial expression recognition</span>
                    
                    <span class="tag tag-keyword">Human-AI interaction</span>
                    
                    <span class="tag tag-keyword">Affective computing</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Conversational fluidity</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We present Empathic Prompting, a novel framework for multimodal human-AI
interaction that enriches Large Language Model (LLM) conversations with
implicit non-verbal context. The system integrates a commercial facial
expression recognition service to capture users' emotional cues and embeds them
as contextual signals during prompting. Unlike traditional multimodal
interfaces, empathic prompting requires no explicit user control; instead, it
unobtrusively augments textual input with affective information for
conversational and smoothness alignment. The architecture is modular and
scalable, allowing integration of additional non-verbal modules. We describe
the system design, implemented through a locally deployed DeepSeek instance,
and report a preliminary service and usability evaluation (N=5). Results show
consistent integration of non-verbal input into coherent LLM outputs, with
participants highlighting conversational fluidity. Beyond this proof of
concept, empathic prompting points to applications in chatbot-mediated
communication, particularly in domains like healthcare or education, where
users' emotional signals are critical yet often opaque in verbal exchanges.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>