<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations - Health AI Hub</title>
    <meta name="description" content="Empathic Prompting is a novel multimodal framework designed to enrich Large Language Model (LLM) conversations by integrating implicit non-verbal emotional cont">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">← Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20743v1" target="_blank">2510.20743v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.HC, cs.AI, cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20743v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20743v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Empathic Prompting is a novel multimodal framework designed to enrich Large Language Model (LLM) conversations by integrating implicit non-verbal emotional context, captured via facial expression recognition, directly into prompts. This unobtrusive system has demonstrated consistent integration of emotional cues into coherent LLM outputs, leading to improved conversational fluidity and alignment.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework is highly relevant to healthcare as it enables AI-driven communication systems (chatbots) to perceive and respond to critical, often unstated, emotional signals from patients. This capability can significantly improve the quality, empathy, and effectiveness of patient interactions, particularly in sensitive medical contexts where emotional states are paramount.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research could lead to AI-powered virtual health assistants or mental health chatbots that are more attuned to a user's emotional state. By continuously analyzing facial expressions for cues like anxiety, distress, or confusion, the AI could adapt its responses, tone, and information delivery to be more empathetic, supportive, or clear, improving patient engagement and outcomes in remote care or automated support systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Introduces 'Empathic Prompting,' a novel framework for multimodal human-AI interaction.</li>
                    
                    <li>Enriches LLM conversations by integrating implicit non-verbal context, specifically users' emotional cues.</li>
                    
                    <li>Utilizes a commercial facial expression recognition service to capture emotional cues.</li>
                    
                    <li>Emotional cues are embedded as contextual signals directly into LLM prompts without requiring explicit user control, augmenting textual input unobtrusively.</li>
                    
                    <li>The architecture is modular and scalable, designed to allow integration of additional non-verbal modules.</li>
                    
                    <li>Preliminary evaluation (N=5) showed consistent integration of non-verbal input into coherent LLM outputs.</li>
                    
                    <li>Participants highlighted enhanced conversational fluidity and alignment, suggesting improved interaction quality.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The system design integrates a commercial facial expression recognition service to capture users' emotional cues, which are then embedded as contextual signals during the prompting process for a locally deployed DeepSeek LLM instance. A preliminary service and usability evaluation was conducted with a small sample size of 5 participants (N=5).</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The preliminary evaluation demonstrated successful and consistent integration of non-verbal emotional input into the LLM's generated outputs, ensuring coherence. Participants of the study consistently reported improved conversational fluidity, indicating that the system's integration of emotional context led to more natural and aligned human-AI interactions.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Empathic prompting has the potential to profoundly impact clinical practice by enabling more nuanced and empathetic chatbot-mediated communication in healthcare. It could lead to AI assistants that better understand and respond to patient anxiety, distress, confusion, or satisfaction, thereby improving patient engagement, adherence to treatment, and overall satisfaction in areas like mental health support, patient education, virtual consultations, and long-term care management.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study reports a preliminary service and usability evaluation based on a very small sample size (N=5), which significantly limits the generalizability and statistical robustness of the findings. It serves as a proof of concept rather than a comprehensive validation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research will focus on integrating additional non-verbal modules (beyond facial expressions, e.g., vocal tone, body language) to further enrich contextual understanding. The authors also suggest exploring specific applications in critical domains like healthcare and education, where the effective recognition and response to users' emotional signals are particularly vital.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Mental Health Support</span>
                    
                    <span class="tag">Patient Education</span>
                    
                    <span class="tag">Geriatric Care</span>
                    
                    <span class="tag">Chronic Disease Management</span>
                    
                    <span class="tag">Virtual Patient Assistance</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Empathic Prompting</span>
                    
                    <span class="tag tag-keyword">Multimodal LLM</span>
                    
                    <span class="tag tag-keyword">Non-verbal communication</span>
                    
                    <span class="tag tag-keyword">Facial expression recognition</span>
                    
                    <span class="tag tag-keyword">Human-AI interaction</span>
                    
                    <span class="tag tag-keyword">Emotional intelligence</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Conversational AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We present Empathic Prompting, a novel framework for multimodal human-AI
interaction that enriches Large Language Model (LLM) conversations with
implicit non-verbal context. The system integrates a commercial facial
expression recognition service to capture users' emotional cues and embeds them
as contextual signals during prompting. Unlike traditional multimodal
interfaces, empathic prompting requires no explicit user control; instead, it
unobtrusively augments textual input with affective information for
conversational and smoothness alignment. The architecture is modular and
scalable, allowing integration of additional non-verbal modules. We describe
the system design, implemented through a locally deployed DeepSeek instance,
and report a preliminary service and usability evaluation (N=5). Results show
consistent integration of non-verbal input into coherent LLM outputs, with
participants highlighting conversational fluidity. Beyond this proof of
concept, empathic prompting points to applications in chatbot-mediated
communication, particularly in domains like healthcare or education, where
users' emotional signals are critical yet often opaque in verbal exchanges.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">← Back to all papers</a></p>
    </footer>
</body>
</html>