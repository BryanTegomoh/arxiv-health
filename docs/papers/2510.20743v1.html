<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations - Health AI Hub</title>
    <meta name="description" content="This paper introduces "Empathic Prompting," a novel framework that enhances multimodal human-AI interaction by integrating implicit non-verbal emotional cues in">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20743v1" target="_blank">2510.20743v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.HC, cs.AI, cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20743v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20743v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces "Empathic Prompting," a novel framework that enhances multimodal human-AI interaction by integrating implicit non-verbal emotional cues into Large Language Model (LLM) conversations. The system uses a facial expression recognition service to capture user emotions and embeds this affective information as contextual signals during LLM prompting, aiming for more fluid and contextually aligned dialogues without explicit user control. A preliminary evaluation (N=5) demonstrated consistent integration of non-verbal input into coherent LLM outputs and improved conversational fluidity.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework is highly relevant to medicine and health because it enables AI systems to perceive and integrate patients' implicit emotional states, which are crucial for effective communication, diagnosis, and support, especially in sensitive healthcare contexts where verbal cues alone may be insufficient or misleading.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research enables AI systems, specifically large language models (LLMs), to interpret and respond to a user's emotional state via non-verbal cues (like facial expressions). In healthcare, this can lead to more empathetic and effective AI applications such as mental health support chatbots that better understand user distress, virtual patient navigators that adapt their tone based on user anxiety, or AI companions for the elderly that provide more appropriate emotional support. It could also assist healthcare professionals by providing an AI-driven understanding of a patient's emotional state during virtual interactions, enhancing patient-provider communication and potentially improving health outcomes.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Empathic Prompting Framework:** A new approach for multimodal human-AI interaction that enriches LLM conversations with implicit non-verbal context.</li>
                    
                    <li>**Non-Verbal Context Integration:** Utilizes a commercial facial expression recognition service to capture users' emotional cues.</li>
                    
                    <li>**Implicit Contextual Signals:** Emotional cues are embedded as contextual signals during LLM prompting, rather than requiring explicit user control.</li>
                    
                    <li>**Conversational Alignment:** Aims to achieve conversational and smoothness alignment by augmenting textual input with affective information.</li>
                    
                    <li>**Modular and Scalable Architecture:** The system design is modular, allowing for future integration of additional non-verbal modules beyond facial expressions.</li>
                    
                    <li>**Local LLM Deployment:** Implemented with a locally deployed DeepSeek instance, showcasing a practical, self-contained solution.</li>
                    
                    <li>**Preliminary Evaluation Results:** A small-scale usability evaluation (N=5) showed consistent integration of non-verbal input into coherent LLM outputs, with participants noting improved conversational fluidity.</li>
                    
                    <li>**Healthcare Application Potential:** Highlights significant applications in chatbot-mediated communication, particularly in healthcare, where emotional signals are critical yet often obscured in verbal exchanges.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The system, named Empathic Prompting, integrates a commercial facial expression recognition service to continuously capture users' emotional cues. These non-verbal signals are then embedded as additional contextual information alongside the textual input during the prompting of a Large Language Model (specifically, a locally deployed DeepSeek instance). This process is designed to be unobtrusive, requiring no explicit user control. A modular architecture supports scalability for future non-verbal inputs. A preliminary service and usability evaluation was conducted with N=5 participants to assess the system's performance.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The preliminary evaluation demonstrated consistent and coherent integration of the non-verbal emotional input into the LLM's generated outputs. Participants in the study specifically highlighted the improved conversational fluidity achieved through this empathic prompting mechanism.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Empathic Prompting has the potential to significantly enhance chatbot-mediated communication in healthcare by allowing AI to better understand and respond to patients' implicit emotional states. This could lead to more empathetic patient-AI interactions, improved patient satisfaction, better adherence to treatment plans, and more nuanced support in sensitive areas like mental health, chronic disease management, or end-of-life care, where emotional signals are paramount but often not explicitly verbalized.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The current work represents a proof of concept, with a very small sample size for the preliminary service and usability evaluation (N=5). This limits the generalizability and robustness of the reported findings, requiring further extensive validation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors suggest that the modular and scalable architecture allows for the integration of additional non-verbal modules beyond facial expressions. They also point to broader applications in chatbot-mediated communication, particularly highlighting its relevance in healthcare and education domains where understanding users' emotional signals is critical.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">telemedicine</span>
                    
                    <span class="tag">mental health support</span>
                    
                    <span class="tag">patient education</span>
                    
                    <span class="tag">clinical decision support (patient-facing)</span>
                    
                    <span class="tag">elderly care</span>
                    
                    <span class="tag">rehabilitation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">multimodal LLM</span>
                    
                    <span class="tag tag-keyword">empathic prompting</span>
                    
                    <span class="tag tag-keyword">non-verbal cues</span>
                    
                    <span class="tag tag-keyword">facial expression recognition</span>
                    
                    <span class="tag tag-keyword">human-AI interaction</span>
                    
                    <span class="tag tag-keyword">affective computing</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">conversational AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We present Empathic Prompting, a novel framework for multimodal human-AI
interaction that enriches Large Language Model (LLM) conversations with
implicit non-verbal context. The system integrates a commercial facial
expression recognition service to capture users' emotional cues and embeds them
as contextual signals during prompting. Unlike traditional multimodal
interfaces, empathic prompting requires no explicit user control; instead, it
unobtrusively augments textual input with affective information for
conversational and smoothness alignment. The architecture is modular and
scalable, allowing integration of additional non-verbal modules. We describe
the system design, implemented through a locally deployed DeepSeek instance,
and report a preliminary service and usability evaluation (N=5). Results show
consistent integration of non-verbal input into coherent LLM outputs, with
participants highlighting conversational fluidity. Beyond this proof of
concept, empathic prompting points to applications in chatbot-mediated
communication, particularly in domains like healthcare or education, where
users' emotional signals are critical yet often opaque in verbal exchanges.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>