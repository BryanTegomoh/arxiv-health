<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Monte Carlo Expected Threat (MOCET) Scoring - Health AI Hub</title>
    <meta name="description" content="This paper introduces Monte Carlo Expected Threat (MOCET) Scoring, a novel metric designed to quantify real-world risks associated with advanced AI models, part">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Monte Carlo Expected Threat (MOCET) Scoring</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.16823v1" target="_blank">2511.16823v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Joseph Kim, Saahith Potluri
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI, cs.HC
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.16823v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.16823v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Monte Carlo Expected Threat (MOCET) Scoring, a novel metric designed to quantify real-world risks associated with advanced AI models, particularly ASL-3+ LLMs in biosecurity. MOCET aims to provide an interpretable, automatable, and open-ended framework to inform AI safety cases and keep pace with rapid AI advancements, addressing limitations of existing evaluation benchmarks.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine and public health, primarily through its focus on biosecurity. By creating a metric to quantify AI-driven threats from novice actors in this domain, it directly contributes to preventing potential pandemics or targeted biological attacks that could overwhelm healthcare systems and endanger populations.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This paper describes an AI safety evaluation framework (MOCET) designed to assess the potential for advanced AI models (LLMs) to facilitate harm, specifically highlighting their capacity to 'uplift novice non-state actors' in biosecurity. While not an AI application *for* health treatment or diagnosis, it is an AI *safety application* directly aimed at mitigating AI-enabled biothreats, which have significant implications for global health and security.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The paper highlights the critical need for robust evaluation and measurement of AI Safety Level (ASL) threats, especially from ASL-3+ models capable of empowering novice non-state actors in biosecurity.</li>
                    
                    <li>Existing AI safety metrics such as LAB-Bench, BioLP-bench, and WMDP are acknowledged for their ability to reliably assess model uplift and domain knowledge.</li>
                    
                    <li>A significant gap in current metrics is identified: they often lack the ability to contextualize "real-world risks" and are not sufficiently scalable or open-ended to keep pace with rapid LLM advancements.</li>
                    
                    <li>MOCET (Monte Carlo Expected Threat) Scoring is introduced as a novel metric to address these identified gaps.</li>
                    
                    <li>MOCET is characterized as an interpretable and doubly-scalable metric, meaning it is both automatable and open-ended, facilitating continuous evaluation.</li>
                    
                    <li>Its core function is to quantify real-world risks, providing crucial insights to inform the safety case for large language models (LLMs).</li>
                    
                    <li>The metric's design aims to ensure it remains relevant and effective despite the rapid advancements in AI technology, especially in high-stakes domains like biosecurity.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves proposing and defining MOCET (Monte Carlo Expected Threat) Scoring as a novel, interpretable, and doubly-scalable metric. This metric is designed to overcome the shortcomings of existing AI safety evaluations by providing a framework to quantify "real-world risks" posed by AI models, particularly in sensitive domains like biosecurity, by leveraging a Monte Carlo approach (implied by the name).</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is the conceptualization and introduction of MOCET Scoring, a new metric specifically designed to assess "real-world risks" posed by advanced AI models (ASL-3+), especially concerning biosecurity threats. MOCET is characterized as interpretable, automatable, and open-ended, making it suitable for scalable and continuous AI safety evaluations.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact is substantial, though indirect, as MOCET provides a critical tool for pre-empting public health crises. By enabling more accurate and contextualized assessment of AI's potential to facilitate biological threats, it informs policymakers and researchers on necessary safeguards, thereby protecting clinical systems and patient populations from the devastating effects of novel pathogens or misuse of biotechnologies.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of MOCET itself. However, the paper positions MOCET as a solution addressing critical limitations of *existing* AI safety evaluation metrics, which are noted to lack robust contextualization for real-world risks and sufficient scalability to keep pace with rapid LLM advancements.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly detail future research directions for MOCET, but implies the need for ongoing evaluation frameworks that can adapt to the rapid advancements of LLMs.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Biosecurity</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Infectious Disease Epidemiology</span>
                    
                    <span class="tag">Bioethics</span>
                    
                    <span class="tag">Disaster Preparedness</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">AI Safety</span>
                    
                    <span class="tag tag-keyword">Biosecurity</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Risk Assessment</span>
                    
                    <span class="tag tag-keyword">Threat Evaluation</span>
                    
                    <span class="tag tag-keyword">Monte Carlo</span>
                    
                    <span class="tag tag-keyword">AI Governance</span>
                    
                    <span class="tag tag-keyword">Public Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Evaluating and measuring AI Safety Level (ASL) threats are crucial for guiding stakeholders to implement safeguards that keep risks within acceptable limits. ASL-3+ models present a unique risk in their ability to uplift novice non-state actors, especially in the realm of biosecurity. Existing evaluation metrics, such as LAB-Bench, BioLP-bench, and WMDP, can reliably assess model uplift and domain knowledge. However, metrics that better contextualize "real-world risks" are needed to inform the safety case for LLMs, along with scalable, open-ended metrics to keep pace with their rapid advancements. To address both gaps, we introduce MOCET, an interpretable and doubly-scalable metric (automatable and open-ended) that can quantify real-world risks.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to NeurIPS 2025 BioSafe GenAI</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>