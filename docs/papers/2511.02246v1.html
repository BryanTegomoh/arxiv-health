<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results - Health AI Hub</title>
    <meta name="description" content="This paper introduces an infrastructure to automatically generate realistic medical queries and evaluate Large Language Models (LLMs) for biases, hallucinations">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.02246v1" target="_blank">2511.02246v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Jonathan Liu, Haoling Qiu, Jonathan Lasko, Damianos Karakos, Mahsa Yarmohammadi, Mark Dredze
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI, cs.HC, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.02246v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.02246v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces an infrastructure to automatically generate realistic medical queries and evaluate Large Language Models (LLMs) for biases, hallucinations, and omissions, particularly concerning demographic information. The study reveals that LLM-based evaluators show alarmingly low inter-rater agreement (average Cohen's Kappa $\kappa=0.118$), and statistically significant results on LLM biases are often contingent on specific LLM pairings, potentially leading to non-generalizable conclusions without robust evaluation practices.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critically important for the safe, ethical, and effective deployment of LLMs in healthcare, as it uncovers fundamental reliability and generalizability issues in their evaluation, which could otherwise lead to biased or inconsistent medical advice and patient harm.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper directly addresses the critical need for reliable, consistent, and unbiased performance of AI systems (specifically LLMs) when used as medical chatbots or for providing medical advice. It aims to identify and mitigate issues like hallucinations, omissions, and biases in these AI applications, which is essential for their safe and effective deployment in healthcare settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Developed an infrastructure featuring a query creation pipeline that automatically generates realistic medical questions by sampling patient demographics, histories, disorders, and writing styles.</li>
                    
                    <li>Implemented an evaluation pipeline that assesses LLM responses using multiple LLM-as-a-judge setups, incorporating hallucination and omission detection via agentic workflows, and LLM-as-a-judge for treatment category detection.</li>
                    
                    <li>Found that LLM annotators (when used as evaluators) exhibit extremely low inter-LLM agreement, with an average Cohen's Kappa ($\\kappa$) of 0.118, indicating poor consistency in their judgments.</li>
                    
                    <li>Demonstrated that statistically significant differences in LLM biases (e.g., across writing styles, genders, races) were only observed for specific answering-evaluation LLM pairs, suggesting a lack of generalizability.</li>
                    
                    <li>Recommended that studies employing LLM evaluation, especially in the absence of ground-truth data, should utilize multiple LLMs as evaluators to avoid statistically significant but non-generalizable findings.</li>
                    
                    <li>Suggested that publishing inter-LLM agreement metrics should become standard practice for transparency in LLM evaluation research.</li>
                    
                    <li>Highlighted the critical need for medical chatbots to provide consistent and unbiased advice, particularly in scenarios involving sensitive patient demographic information.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study developed a two-pronged infrastructure: (1) an automated query creation pipeline to generate diverse and realistic medical questions by sampling patient demographics, histories, disorders, and writing styles; and (2) an evaluation pipeline that uses multiple LLM-as-a-judge setups and prompts to detect hallucinations, omissions, and classify treatment categories, leveraging agentic workflows. Baseline case studies included analyzing inter-LLM agreement and the impact of varying both the answering and evaluation LLMs.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary findings indicate exceptionally low inter-LLM annotator agreement, with an average Cohen's Kappa of $\kappa=0.118$, signifying poor consistency when LLMs act as evaluators. Furthermore, statistically significant differences in LLM performance across demographic biases (e.g., writing styles, genders, races) were found to be specific to particular (answering LLM, evaluation LLM) pairs, questioning the generalizability of such results.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings have a significant clinical impact by warning against over-reliance on single-LLM evaluation for medical chatbots. They strongly suggest that without robust multi-LLM evaluation and transparent reporting of agreement metrics, medical AI tools might deliver inconsistent, biased, or non-generalizable advice, compromising patient safety and trust. This necessitates a more rigorous, multi-faceted evaluation approach before LLMs can be confidently integrated into clinical workflows for diverse patient populations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study is presented as a 'baseline study,' indicating it's an initial exploration. A key explicit limitation is the 'absence of ground-truth data' in many LLM evaluation scenarios, which complicates validation and emphasizes the challenges in achieving generalizable results.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors recommend that future studies evaluating LLMs, especially in the absence of ground-truth data, utilize multiple LLMs as evaluators to ensure results are generalizable. They also suggest making the publication of inter-LLM agreement metrics a standard practice for enhanced transparency and reliability of research findings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Telehealth</span>
                    
                    <span class="tag">Patient Education</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                    <span class="tag">AI in Healthcare</span>
                    
                    <span class="tag">Health Equity</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM evaluation</span>
                    
                    <span class="tag tag-keyword">medical chatbots</span>
                    
                    <span class="tag tag-keyword">bias detection</span>
                    
                    <span class="tag tag-keyword">hallucination</span>
                    
                    <span class="tag tag-keyword">inter-rater agreement</span>
                    
                    <span class="tag tag-keyword">generalizability</span>
                    
                    <span class="tag tag-keyword">demographic factors</span>
                    
                    <span class="tag tag-keyword">LLM-as-a-judge</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>