<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Achieving Fairness Without Harm via Selective Demographic Experts - Health AI Hub</title>
    <meta name="description" content="This paper proposes a "fairness-without-harm" machine learning approach to address the critical trade-off between achieving fairness and maintaining high predic">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Achieving Fairness Without Harm via Selective Demographic Experts</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.06293v1" target="_blank">2511.06293v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-09
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xuwei Tan, Yuanlong Wang, Thai-Hoang Pham, Ping Zhang, Xueru Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.06293v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.06293v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper proposes a "fairness-without-harm" machine learning approach to address the critical trade-off between achieving fairness and maintaining high predictive performance, particularly in high-stakes domains like healthcare. The method involves learning distinct representations for different demographic groups and selectively applying "demographic experts" ‚Äì consisting of group-specific representations and personalized classifiers ‚Äì through a no-harm constrained selection mechanism. Evaluations on three real-world medical datasets (eye disease, skin cancer, X-ray diagnosis) and two face datasets demonstrate its effectiveness in ensuring fairness without degrading predictive accuracy for any specific demographic group.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critically important for medicine and health as it provides a solution to deploy AI systems in clinical diagnosis that are both fair and highly accurate, addressing the ethical and practical unacceptability of accuracy degradation for certain patient groups. It ensures that medical AI benefits all demographics equally without causing harm to any.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is to develop fair and robust machine learning models for medical diagnosis (e.g., detecting eye disease, skin cancer, interpreting X-rays) by learning group-specific representations and classifiers to mitigate demographic bias without compromising predictive performance for any specific patient group.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the inherent trade-off between fairness and accuracy in machine learning systems, particularly when applied to human-centered domains like clinical diagnosis.</li>
                    
                    <li>Introduces a novel "fairness-without-harm" paradigm to ensure equitable outcomes without compromising predictive performance for any demographic group.</li>
                    
                    <li>Proposes learning distinct, specialized representations tailored for different demographic populations.</li>
                    
                    <li>Utilizes "demographic experts," which are composed of these group-specific representations coupled with personalized classifiers.</li>
                    
                    <li>Incorporates a "no-harm constrained selection" mechanism to selectively apply these experts, ensuring no performance degradation for any group.</li>
                    
                    <li>Empirical evaluation was conducted on three diverse real-world medical datasets: eye disease diagnosis, skin cancer diagnosis, and X-ray diagnosis, alongside two face datasets.</li>
                    
                    <li>Results confirm the approach's effectiveness in achieving fairness while simultaneously preserving or improving predictive performance across all demographic groups.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed methodology learns distinct data representations for different demographic groups. It then creates "demographic experts," each combining a group-specific representation with a personalized classifier. These experts are selectively applied via a "no-harm constrained selection" process, which ensures that the deployment of group-specific models does not inadvertently degrade the predictive performance for any demographic segment.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The extensive empirical results demonstrate that the proposed "fairness-without-harm" approach successfully achieves fairness across demographic groups without sacrificing predictive performance, particularly in high-stakes medical diagnosis tasks.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This approach has the potential to significantly impact clinical practice by enabling the development and deployment of more equitable and reliable AI diagnostic tools. It can prevent healthcare disparities introduced by biased algorithms, ensuring that all patients, regardless of their demographic background, receive equally accurate diagnoses, thereby improving patient safety and trust in AI-powered medical solutions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Limitations are not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future directions are not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Eye Disease Diagnosis</span>
                    
                    <span class="tag">Skin Cancer Diagnosis</span>
                    
                    <span class="tag">X-ray Diagnosis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Fairness in AI</span>
                    
                    <span class="tag tag-keyword">Machine Learning</span>
                    
                    <span class="tag tag-keyword">Bias Mitigation</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Clinical Diagnosis</span>
                    
                    <span class="tag tag-keyword">Demographic Experts</span>
                    
                    <span class="tag tag-keyword">Group-specific Representations</span>
                    
                    <span class="tag tag-keyword">No-Harm Constraint</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">As machine learning systems become increasingly integrated into
human-centered domains such as healthcare, ensuring fairness while maintaining
high predictive performance is critical. Existing bias mitigation techniques
often impose a trade-off between fairness and accuracy, inadvertently degrading
performance for certain demographic groups. In high-stakes domains like
clinical diagnosis, such trade-offs are ethically and practically unacceptable.
In this study, we propose a fairness-without-harm approach by learning distinct
representations for different demographic groups and selectively applying
demographic experts consisting of group-specific representations and
personalized classifiers through a no-harm constrained selection. We evaluate
our approach on three real-world medical datasets -- covering eye disease, skin
cancer, and X-ray diagnosis -- as well as two face datasets. Extensive
empirical results demonstrate the effectiveness of our approach in achieving
fairness without harm.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>AAAI26; Extended version</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>