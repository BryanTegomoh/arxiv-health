<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel zero-shot pipeline that simultaneously detects, identifies, and localizes harmful content within images, providing pixel-accurate ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04599v1" target="_blank">2512.04599v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Sheng Hang, Chaoxiang He, Hongsheng Hu, Hanqing Hu, Bin Benjamin Zhu, Shi-Feng Sun, Dawu Gu, Shuo Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.75 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04599v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04599v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel zero-shot pipeline that simultaneously detects, identifies, and localizes harmful content within images, providing pixel-accurate masks for critical elements, moving beyond simple image-level NSFW flags. Evaluated on a diverse dataset of drug, sexual, violent, and extremist content, the method achieves high recall (85.8%) and precision (78.1%), demonstrating significant improvement over direct VLM localization and robust performance against adversarial attacks, presenting a practical and explainable solution for content moderation.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This technology is highly relevant for public health and safety by enabling more effective and precise moderation of online content related to illegal drugs, violence, extremism, and harmful sexual content. This can significantly aid in protecting vulnerable populations, preventing the spread of health misinformation, and curbing the online distribution of illicit substances that directly impact health outcomes and mental well-being.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>An AI system for fine-grained content moderation specifically targeting visual drug content. This could be used by public health agencies for surveillance and intervention planning related to drug abuse trends, by online platforms to enforce policies against drug promotion or sales, or within mental health support systems to identify and address drug-related triggers or behaviors.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The research addresses the need for fine-grained content moderation, enabling the identification of specific harmful objects and their exact locations within an image, rather than just a general NSFW flag.</li>
                    
                    <li>A zero-shot pipeline is proposed, integrating a foundation segmentation model (SAM) to generate candidate object masks and a vision-language model (VLM) to score the malicious relevance of these regions using open-vocabulary prompts.</li>
                    
                    <li>The system incorporates a score-weighted fusion step to consolidate malicious object maps and employs an ensemble of multiple segmenters to enhance robustness against adaptive adversarial attacks targeting single segmentation methods.</li>
                    
                    <li>Evaluated on a newly annotated 790-image dataset covering drug, sexual, violent, and extremist content, the method achieved 85.8% element-level recall, 78.1% precision, and a 92.1% segment-success rate.</li>
                    
                    <li>The proposed pipeline significantly outperforms direct zero-shot VLM localization, demonstrating a 27.4% improvement in recall at comparable precision.</li>
                    
                    <li>The method exhibits high robustness against PGD adversarial perturbations designed to attack SAM and VLM, with precision and recall decreasing by no more than 10%.</li>
                    
                    <li>The full pipeline processes an image in seconds, integrates seamlessly into existing VLM workflows, and is presented as the first practical tool for fine-grained, explainable malicious-image moderation.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The pipeline initiates with a foundation segmentation model (SAM) to generate candidate object masks, which are then refined into larger, independent regions. A vision-language model (VLM) is subsequently used to score the malicious relevance of each refined region through open-vocabulary prompts. These scores weight a fusion step that produces a consolidated malicious object map. To enhance robustness against adaptive attacks targeting individual segmentation methods, an ensemble approach across multiple segmenters is implemented.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The system achieved robust performance with 85.8% element-level recall, 78.1% precision, and a 92.1% segment-success rate on a diverse, newly annotated dataset of harmful content. It significantly outperformed direct zero-shot VLM localization by 27.4% in recall at comparable precision. Furthermore, the method demonstrated high resilience against adversarial attacks, with precision and recall decreases of less than 10% when subjected to PGD perturbations.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This tool offers a practical, rapid, and explainable solution for precisely identifying and localizing harmful visual content online. Its deployment can significantly strengthen public health initiatives by enabling social media platforms and digital health providers to more effectively detect and remove content promoting illegal drug sales, violence, or extremist ideologies. This enhanced moderation capability can contribute to safer online environments, protect vulnerable individuals from exposure to detrimental content, and ultimately support improved mental and physical well-being across populations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the proposed method itself. However, the design choice to incorporate an "ensemble across multiple segmenters" implicitly acknowledges a vulnerability of single segmentation methods to "adaptive attacks," which the proposed system aims to mitigate, suggesting a recognized challenge in the field that their solution addresses.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Mental Health</span>
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Social Media Moderation (health context)</span>
                    
                    <span class="tag">Substance Abuse Prevention</span>
                    
                    <span class="tag">Child Protection (health aspect)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Vision-Language Models</span>
                    
                    <span class="tag tag-keyword">Semantic Segmentation</span>
                    
                    <span class="tag tag-keyword">Content Moderation</span>
                    
                    <span class="tag tag-keyword">Malicious Image Detection</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">Zero-shot Learning</span>
                    
                    <span class="tag tag-keyword">Public Health</span>
                    
                    <span class="tag tag-keyword">Digital Forensics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>