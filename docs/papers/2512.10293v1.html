<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings - Health AI Hub</title>
    <meta name="description" content="Disentangled360 is an innovative 3D-aware technology that generates physically aware 360-degree views from a single image by disentangling isotropic and anisotr">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.10293v1" target="_blank">2512.10293v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-11
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Karthikeya KV, Narendra Bandaru
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.10293v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.10293v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Disentangled360 is an innovative 3D-aware technology that generates physically aware 360-degree views from a single image by disentangling isotropic and anisotropic light contributions within a Gaussian Splatting framework. It employs a dual-branch conditioning system for both CT intensity-driven volumetric data and real-world RGB scenes, addressing scale ambiguity and structural realism with a hybrid pose agnostic anchoring method. The framework demonstrates superior image quality (SSIM, LPIPS) and interactive runtime on medical and natural scene datasets, enabling rapid, photorealistic view synthesis for applications like mixed-reality medical supervision.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This technology provides a robust method for generating detailed 360-degree views from single medical images, which is crucial for enhanced diagnostic understanding, preoperative planning, and real-time guidance in interventional procedures. Its ability to simulate radiography and integrate into mixed-reality environments significantly advances medical visualization and training capabilities without extensive computational cost.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI model (Disentangled360) generates physically aware 360¬∞ views from single medical images (e.g., CT scans, radiographs). This capability can be applied to enhance preoperative planning by providing comprehensive spatial understanding, facilitate mixed-reality medical supervision for surgeons, improve robotic perception in surgical or diagnostic settings, and aid in immersive content creation for medical training or education, all without the need for extensive real-world data or expensive simulations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Introduces Disentangled360, a novel 3D-aware technology for single-image 360¬∞ unique view synthesis applicable to medical imaging and natural scene reconstruction.</li>
                    
                    <li>Differentiates isotropic and anisotropic light contributions within a Gaussian Splatting backbone to overcome limitations of current techniques that oversimplify anisotropic light behavior or lack generalizability.</li>
                    
                    <li>Utilizes a dual-branch conditioning framework: one optimized for CT intensity-driven scattering in volumetric data, and the other for real-world RGB scenes via normalized camera embeddings.</li>
                    
                    <li>Incorporates a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, serving as stable pivots to address scale ambiguity and maintain structural realism during scene distillation.</li>
                    
                    <li>Integrates preoperative radiography simulation and consumer-grade 360¬∞ rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality.</li>
                    
                    <li>Achieves superior SSIM and LPIPS performance on diverse datasets including Mip-NeRF 360, RealEstate10K, and DeepDRR, alongside runtime viability for interactive applications.</li>
                    
                    <li>Enables practical applications such as mixed-reality medical supervision, robotic perception, and immersive content creation, without requiring scene-specific finetuning or expensive photon simulations.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>Disentangled360 operates on a Gaussian Splatting backbone, uniquely differentiating between isotropic and anisotropic light contributions for physically aware rendering. It employs a dual-branch conditioning framework, one tailored for CT intensity-driven scattering in volumetric data and the other for RGB scenes using normalized camera embeddings. To ensure structural realism and address scale ambiguity, a hybrid pose agnostic anchoring method is implemented, adaptively sampling depth and material transitions to serve as stable pivots during scene distillation. The entire process is integrated into a singular inference pipeline.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The evaluations demonstrate that Disentangled360 achieves superior SSIM (Structural Similarity Index Measure) and LPIPS (Learned Perceptual Image Patch Similarity) performance compared to existing techniques across a range of datasets, including Mip-NeRF 360, RealEstate10K, and the medical DeepDRR dataset. Runtime assessments confirm its efficiency, highlighting its viability for interactive applications by providing rapid, photorealistic 360¬∞ view synthesis with inherent directionality.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Disentangled360 has significant potential for clinical impact by facilitating advanced mixed-reality medical supervision, where virtual anatomical models derived from single scans can be precisely overlaid for surgical guidance. It enables enhanced preoperative planning by providing comprehensive 360-degree interactive views of patient anatomy. Furthermore, it can improve robotic perception in medical settings and streamline immersive content creation for medical education and training, all while eliminating the need for expensive scene-specific finetuning or resource-intensive photon simulations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of the Disentangled360 framework itself. However, it positions the technology as overcoming common limitations of prior techniques, such as the necessity for scene-specific finetuning or expensive photon simulations.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions for Disentangled360, focusing instead on its immediate capabilities and broad range of applications across medical and natural scene contexts.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Surgical Planning</span>
                    
                    <span class="tag">Interventional Medicine</span>
                    
                    <span class="tag">Medical Robotics</span>
                    
                    <span class="tag">Medical Visualization</span>
                    
                    <span class="tag">Mixed Reality in Healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">360-degree view synthesis</span>
                    
                    <span class="tag tag-keyword">single-image reconstruction</span>
                    
                    <span class="tag tag-keyword">disentangled scene embeddings</span>
                    
                    <span class="tag tag-keyword">Gaussian Splatting</span>
                    
                    <span class="tag tag-keyword">medical imaging</span>
                    
                    <span class="tag tag-keyword">CT simulation</span>
                    
                    <span class="tag tag-keyword">radiography</span>
                    
                    <span class="tag tag-keyword">mixed reality</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360¬∞ unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360¬∞ rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>