<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam - Health AI Hub</title>
    <meta name="description" content="This study evaluated the performance of two large language models, ChatGPT-4o and DeepSeek-R1, on 2,306 multiple-choice questions from the Chinese Pharmacist Li">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20526v1" target="_blank">2511.20526v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-25
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinran Wang, Boran Zhu, Shujuan Zhou, Ziwen Long, Dehua Zhou, Shu Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20526v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20526v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study evaluated the performance of two large language models, ChatGPT-4o and DeepSeek-R1, on 2,306 multiple-choice questions from the Chinese Pharmacist Licensing Examination (2017-2021). DeepSeek-R1 significantly outperformed ChatGPT-4o (90.0% vs. 76.1%, p < 0.001), particularly in foundational and clinical synthesis modules, demonstrating robust alignment with the exam's demands. The findings highlight the potential of domain-specific LLMs for formative evaluation in medical contexts while reinforcing the necessity of human oversight.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to digital health education and professional assessment, providing crucial insights into the current capabilities of LLMs for evaluating critical competencies required for pharmacist licensure. It informs the integration of AI tools in supporting and enhancing pharmacist training and certification processes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the use of Large Language Models (LLMs) for digital health education and assessment workflows, particularly in evaluating the clinical and theoretical competencies of pharmacists for licensure. It explores AI-enabled formative evaluation within a medical context.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study compared ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021).</li>
                    
                    <li>A dataset of 2,306 text-only multiple-choice questions, excluding those with tables or images, was compiled from official exams and training materials.</li>
                    
                    <li>DeepSeek-R1 achieved a significantly higher overall accuracy of 90.0% compared to ChatGPT-4o's 76.1% (p < 0.001).</li>
                    
                    <li>Unit-level analyses revealed consistent performance advantages for DeepSeek-R1, especially in foundational and clinical synthesis modules.</li>
                    
                    <li>While year-by-year performance also favored DeepSeek-R1, these differences did not reach statistical significance in any specific unit-year (all p > 0.05).</li>
                    
                    <li>The results suggest that domain-specific LLMs can demonstrate strong capabilities in high-stakes medical certification assessments.</li>
                    
                    <li>The paper emphasizes the continued necessity of human oversight in ethically and legally sensitive healthcare contexts, despite impressive AI performance.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study utilized a comparative design, assessing ChatGPT-4o and DeepSeek-R1 against a dataset of 2,306 multiple-choice, text-only questions from the 2017-2021 Chinese Pharmacist Licensing Examination. Questions were input in their original Chinese format, and model responses were scored for exact accuracy. Statistical analyses included Pearson's Chi-squared test for overall performance comparison and Fisher's exact test for year-wise multiple-choice accuracy differences.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>DeepSeek-R1 demonstrated a significantly superior overall accuracy of 90.0% compared to ChatGPT-4o's 76.1% (p < 0.001). DeepSeek-R1 consistently outperformed ChatGPT-4o across all unit-level analyses, exhibiting particular strength in foundational and clinical synthesis modules. Although DeepSeek-R1 showed better year-by-year performance, this advantage did not reach statistical significance when analyzed per specific unit-year (all p > 0.05).</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The strong performance of DeepSeek-R1 suggests that advanced, potentially domain-specific, LLMs could serve as valuable tools for formative evaluation, continuous professional development, and self-assessment for pharmacists and pharmacy students. However, the study underscores that such AI tools should augment, not replace, human judgment and oversight in legally and ethically critical tasks like professional licensure and clinical decision-making.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study's primary limitation is the exclusion of questions containing tables or images, meaning the assessment was restricted to text-only multiple-choice items. This may not fully represent the complexity of real-world clinical scenarios or the multimodal nature of some exam questions, potentially overestimating or underestimating LLM performance on comprehensive assessments.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors suggest further investigation into domain-specific models for medical certification contexts. Future research could explore LLM performance on multimodal questions, more complex clinical case studies, and in diverse cultural or linguistic healthcare settings to broaden the understanding of their capabilities and limitations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacy</span>
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Clinical Pharmacy</span>
                    
                    <span class="tag">Health Education</span>
                    
                    <span class="tag">Medical Certification</span>
                    
                    <span class="tag">Digital Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Pharmacist Licensure Exam</span>
                    
                    <span class="tag tag-keyword">ChatGPT-4o</span>
                    
                    <span class="tag tag-keyword">DeepSeek-R1</span>
                    
                    <span class="tag tag-keyword">Medical Education</span>
                    
                    <span class="tag tag-keyword">AI Assessment</span>
                    
                    <span class="tag tag-keyword">Digital Health</span>
                    
                    <span class="tag tag-keyword">Pharmacology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>15 pages, 4 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>