<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam - Health AI Hub</title>
    <meta name="description" content="This study evaluated and compared the performance of two large language models, ChatGPT-4o and DeepSeek-R1, on 2,306 multiple-choice questions from the Chinese ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20526v1" target="_blank">2511.20526v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-25
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinran Wang, Boran Zhu, Shujuan Zhou, Ziwen Long, Dehua Zhou, Shu Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20526v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20526v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study evaluated and compared the performance of two large language models, ChatGPT-4o and DeepSeek-R1, on 2,306 multiple-choice questions from the Chinese Pharmacist Licensing Examination (2017-2021). DeepSeek-R1 significantly outperformed ChatGPT-4o (90.0% vs. 76.1%, p < 0.001), demonstrating robust alignment with the exam's structural and semantic demands. The findings highlight the potential of domain-specific LLMs for AI-enabled formative evaluation in medical contexts, while emphasizing the continued necessity of human oversight.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to digital health education and assessment, demonstrating how accurately LLMs can perform on a high-stakes medical certification exam. Understanding their capabilities and limitations can inform the development of AI-enabled tools for training, evaluating, and supporting pharmacists and other healthcare professionals.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application described is the use of Large Language Models (LLMs) for formative evaluation in digital health education and assessment. Specifically, it involves evaluating LLMs' ability to pass high-stakes certification exams for healthcare professionals (pharmacists), with potential future applications in training, knowledge assessment, and potentially even decision support for pharmacists. It explores the capability of AI models to demonstrate competence in a complex medical domain.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study evaluated ChatGPT-4o and DeepSeek-R1 using 2,306 text-only multiple-choice questions from the Chinese Pharmacist Licensing Examination (2017-2021).</li>
                    
                    <li>Questions containing tables or images were excluded from the dataset.</li>
                    
                    <li>DeepSeek-R1 achieved a significantly higher overall accuracy of 90.0% compared to ChatGPT-4o's 76.1% (p < 0.001).</li>
                    
                    <li>DeepSeek-R1's performance advantage was consistent across unit-level analyses, particularly excelling in foundational and clinical synthesis modules.</li>
                    
                    <li>While DeepSeek-R1 generally performed better year-over-year, this annual performance gap did not reach statistical significance in any specific unit-year (all p > 0.05).</li>
                    
                    <li>The results suggest strong alignment of domain-specific LLMs like DeepSeek-R1 with the demands of high-stakes medical licensure exams.</li>
                    
                    <li>The research reinforces the necessity of human oversight in legally and ethically sensitive medical contexts, despite advanced LLM performance.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study compared ChatGPT-4o and DeepSeek-R1 using a dataset of 2,306 multiple-choice questions (text-only) compiled from the Chinese Pharmacist Licensing Examination (2017-2021), official training materials, and public databases. Questions with tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Statistical analysis included Pearson's Chi-squared test for overall performance comparison and Fisher's exact test for year-wise multiple-choice accuracy differences.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>DeepSeek-R1 significantly outperformed ChatGPT-4o with an overall accuracy of 90.0% versus 76.1% (p < 0.001). DeepSeek-R1 showed consistent advantages in unit-level analyses, especially in foundational and clinical synthesis modules. Although DeepSeek-R1 also demonstrated better year-by-year multiple-choice performance, this specific unit-year performance gap did not reach statistical significance (all p > 0.05).</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The strong performance of DeepSeek-R1 suggests its potential as a valuable tool for AI-enabled formative evaluation in pharmacy education, helping students prepare for licensure exams by providing accurate feedback. However, the findings underscore that while LLMs can augment educational processes, human oversight remains critical in all legally and ethically sensitive aspects of clinical practice and professional certification, ensuring patient safety and professional accountability.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study's scope was limited to multiple-choice questions and specifically excluded questions containing tables or images, which might not fully represent the breadth of real-world clinical scenarios or the models' multimodal understanding capabilities. The evaluation was confined to questions from a single national licensure exam (Chinese Pharmacist Licensing Examination).</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings suggest that domain-specific LLMs warrant further investigation for their application in medical assessment contexts. Future research could explore other specialized models, broader question types (including multimodal), different medical disciplines, and the integration of these models into comprehensive AI-enabled formative evaluation systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacy</span>
                    
                    <span class="tag">Clinical Pharmacology</span>
                    
                    <span class="tag">Medical Education and Training</span>
                    
                    <span class="tag">Health Policy and Certification</span>
                    
                    <span class="tag">Clinical Practice</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Pharmacist Licensing Exam</span>
                    
                    <span class="tag tag-keyword">ChatGPT-4o</span>
                    
                    <span class="tag tag-keyword">DeepSeek-R1</span>
                    
                    <span class="tag tag-keyword">Medical Education</span>
                    
                    <span class="tag tag-keyword">AI Assessment</span>
                    
                    <span class="tag tag-keyword">Clinical Competency</span>
                    
                    <span class="tag tag-keyword">Digital Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>15 pages, 4 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>