<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam - Health AI Hub</title>
    <meta name="description" content="This study evaluated the performance of two large language models, ChatGPT-4o and DeepSeek-R1, on real questions from the Chinese Pharmacist Licensing Examinati">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20526v1" target="_blank">2511.20526v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-25
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinran Wang, Boran Zhu, Shujuan Zhou, Ziwen Long, Dehua Zhou, Shu Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20526v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20526v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study evaluated the performance of two large language models, ChatGPT-4o and DeepSeek-R1, on real questions from the Chinese Pharmacist Licensing Examination. DeepSeek-R1 significantly outperformed ChatGPT-4o, achieving an impressive 90.0% overall accuracy compared to 76.1%, particularly excelling in foundational and clinical synthesis modules.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critical for understanding the current capabilities and limitations of LLMs in high-stakes medical certification and education. It informs the development of AI-enabled tools for training, assessment, and potentially clinical decision support in pharmacy and broader healthcare domains.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application explored is the use of Large Language Models (LLMs) for high-stakes, domain-specific certification tasks within healthcare, specifically evaluating their performance against a national pharmacist licensure exam. This includes potential applications in AI-enabled formative evaluation for healthcare professional education and assessment, as well as developing AI tools to support pharmacists' clinical and theoretical competencies.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLMs are increasingly integrated into digital health education and assessment, making their performance in high-stakes medical certification crucial to evaluate.</li>
                    
                    <li>The study utilized 2,306 text-only multiple-choice questions from the Chinese Pharmacist Licensing Examination (2017-2021) as a benchmark.</li>
                    
                    <li>DeepSeek-R1 demonstrated a statistically significant higher overall accuracy (90.0%) compared to ChatGPT-4o (76.1%, p < 0.001).</li>
                    
                    <li>Unit-level analysis revealed consistent advantages for DeepSeek-R1, especially in foundational and clinical synthesis modules of the exam.</li>
                    
                    <li>While DeepSeek-R1 also favored in year-by-year performance, this specific gap did not reach statistical significance in any individual unit-year.</li>
                    
                    <li>The findings suggest the strong alignment of domain-specific models like DeepSeek-R1 with the structural and semantic requirements of professional medical exams.</li>
                    
                    <li>The research reinforces the necessity of human oversight in legally and ethically sensitive contexts, despite advanced AI performance, particularly in formative evaluation scenarios.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study compiled 2,306 multiple-choice questions, explicitly excluding those with tables or images, from official Chinese Pharmacist Licensing Examinations (2017-2021), training materials, and public databases. Each question was input in its original Chinese format, and model responses were evaluated for exact accuracy. Statistical analysis included Pearson's Chi-squared test for overall performance comparison and Fisher's exact test for year-wise multiple-choice accuracy.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>DeepSeek-R1 significantly outperformed ChatGPT-4o with an overall accuracy of 90.0% compared to 76.1% (p < 0.001). This advantage was consistent across various unit-level analyses, particularly in foundational and clinical synthesis modules. While DeepSeek-R1 also showed better year-by-year performance, this gap was not statistically significant for any specific unit-year (all p > 0.05). DeepSeek-R1 demonstrated robust alignment with the demands of the pharmacist licensure exam.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The high performance of DeepSeek-R1 suggests its potential as a powerful tool for AI-enabled formative evaluation, continuing professional development, and even preparatory training for pharmacy licensure exams. However, the findings also underscore that while LLMs can achieve high accuracy, human oversight remains indispensable in critical, legally, and ethically sensitive clinical and professional contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study was limited to text-only multiple-choice questions, excluding those containing tables or images, which are common in real-world medical examinations. The abstract does not detail potential biases in model training data or generalizability beyond the Chinese context.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors suggest that domain-specific models, like DeepSeek-R1, warrant further investigation for their application in high-stakes professional certification contexts. Future research could explore the performance on questions involving multimodal input (images, tables) and delve deeper into the underlying reasons for performance differences.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacy</span>
                    
                    <span class="tag">Medical Education</span>
                    
                    <span class="tag">Clinical Assessment</span>
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Pharmaceutical Sciences</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Pharmacist Licensure Exam</span>
                    
                    <span class="tag tag-keyword">AI in Medical Education</span>
                    
                    <span class="tag tag-keyword">Digital Health Assessment</span>
                    
                    <span class="tag tag-keyword">ChatGPT-4o</span>
                    
                    <span class="tag tag-keyword">DeepSeek-R1</span>
                    
                    <span class="tag tag-keyword">Clinical Competency</span>
                    
                    <span class="tag tag-keyword">Formative Evaluation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>15 pages, 4 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>