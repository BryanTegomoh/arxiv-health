<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam - Health AI Hub</title>
    <meta name="description" content="This study evaluated the performance of ChatGPT-4o and DeepSeek-R1 on 2,306 multiple-choice questions from the Chinese Pharmacist Licensing Examination (2017-20">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20526v1" target="_blank">2511.20526v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-25
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinran Wang, Boran Zhu, Shujuan Zhou, Ziwen Long, Dehua Zhou, Shu Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20526v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20526v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study evaluated the performance of ChatGPT-4o and DeepSeek-R1 on 2,306 multiple-choice questions from the Chinese Pharmacist Licensing Examination (2017-2021). DeepSeek-R1 significantly outperformed ChatGPT-4o (90.0% vs. 76.1% accuracy), demonstrating robust alignment with the exam's structural and semantic demands, particularly in foundational and clinical synthesis modules. The findings suggest the promise of domain-specific models for medical assessment but underscore the critical need for human oversight in high-stakes contexts.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critical for understanding the current capabilities and limitations of LLMs in high-stakes medical certification and education. It informs the responsible integration of AI tools into digital health assessment workflows, highlighting which models might be more effective for evaluating complex medical knowledge and where human expertise remains indispensable for patient safety and ethical practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The primary AI application is in AI-enabled assessment and educational tools for healthcare professionals, specifically pharmacists. This includes evaluating professional knowledge, potentially aiding in training, and informing the development of AI for high-stakes medical certification.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study compared two LLMs, ChatGPT-4o and DeepSeek-R1, using real questions from the Chinese Pharmacist Licensing Examination (2017-2021).</li>
                    
                    <li>A dataset of 2,306 multiple-choice, text-only questions from official exams and training materials was compiled and used for evaluation.</li>
                    
                    <li>DeepSeek-R1 achieved a significantly higher overall accuracy of 90.0% compared to ChatGPT-4o's 76.1% (p < 0.001).</li>
                    
                    <li>DeepSeek-R1 consistently demonstrated advantages in unit-level analyses, particularly excelling in foundational and clinical synthesis modules.</li>
                    
                    <li>While DeepSeek-R1 also performed better year-over-year, this specific unit-year performance gap was not statistically significant (all p > 0.05).</li>
                    
                    <li>The findings suggest that domain-specific LLMs may be more effective for high-stakes, specialized medical assessment contexts.</li>
                    
                    <li>The research reinforces the necessity of human oversight when LLMs are deployed in legally and ethically sensitive healthcare environments.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study utilized a dataset of 2,306 multiple-choice questions, exclusively text-only, compiled from the Chinese Pharmacist Licensing Examination (2017-2021), official training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format to ChatGPT-4o and DeepSeek-R1, and model responses were evaluated for exact accuracy. Statistical comparisons included Pearson's Chi-squared test for overall performance and Fisher's exact test for year-wise multiple-choice accuracy.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>DeepSeek-R1 significantly outperformed ChatGPT-4o with an overall accuracy of 90.0% versus 76.1% (p < 0.001). Unit-level analyses consistently favored DeepSeek-R1, particularly in foundational and clinical synthesis modules. Although DeepSeek-R1 also showed better performance year-by-year, the performance gap did not reach statistical significance in any specific unit-year (all p > 0.05).</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The robust performance of DeepSeek-R1 suggests its potential as a valuable tool for AI-enabled formative evaluation in pharmacy education and professional development, aiding aspiring pharmacists in preparing for licensure exams. However, the study strongly emphasizes that despite high accuracy, human oversight remains paramount in all legally and ethically sensitive contexts, such as clinical decision-making or licensure, to ensure patient safety and professional accountability.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>A significant limitation noted is the exclusion of questions containing tables or images. This means the assessment was limited to text-only comprehension and reasoning, which may not fully reflect the multimodal nature of real-world clinical scenarios or the entire scope of the actual pharmacist examination.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings suggest further investigation into domain-specific LLMs for medical assessment contexts, possibly exploring their development and optimization. Additionally, future research could focus on incorporating multimodal data (e.g., tables, images) into LLM assessments to better simulate real-world clinical situations. The study also implicitly encourages developing hybrid AI-human workflows to integrate LLM capabilities while ensuring essential human oversight in sensitive healthcare applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacy</span>
                    
                    <span class="tag">Clinical Pharmacy</span>
                    
                    <span class="tag">Health Education</span>
                    
                    <span class="tag">Medical Licensure</span>
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Digital Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">ChatGPT-4o</span>
                    
                    <span class="tag tag-keyword">DeepSeek-R1</span>
                    
                    <span class="tag tag-keyword">Pharmacist Licensing Examination</span>
                    
                    <span class="tag tag-keyword">Medical Education</span>
                    
                    <span class="tag tag-keyword">AI Assessment</span>
                    
                    <span class="tag tag-keyword">Clinical Competency</span>
                    
                    <span class="tag tag-keyword">Domain-Specific Models</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>15 pages, 4 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>