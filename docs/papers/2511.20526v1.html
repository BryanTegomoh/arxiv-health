<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam - Health AI Hub</title>
    <meta name="description" content="This study compared ChatGPT-4o and DeepSeek-R1's performance on real Chinese Pharmacist Licensing Examination questions, finding DeepSeek-R1 significantly outpe">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20526v1" target="_blank">2511.20526v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-25
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xinran Wang, Boran Zhu, Shujuan Zhou, Ziwen Long, Dehua Zhou, Shu Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20526v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20526v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study compared ChatGPT-4o and DeepSeek-R1's performance on real Chinese Pharmacist Licensing Examination questions, finding DeepSeek-R1 significantly outperformed ChatGPT-4o with 90.0% accuracy versus 76.1%. The results suggest that domain-specific LLMs can align robustly with high-stakes medical assessments, highlighting their potential for AI-enabled formative evaluation while emphasizing the continued necessity of human oversight.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research provides crucial insights into the current competency of LLMs in high-stakes medical certification, directly impacting their potential integration into pharmacy education, continuous professional development, and assessment workflows. It helps define where LLMs can assist in knowledge evaluation and where human expertise remains irreplaceable.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Assessing the capability of AI models (LLMs) to perform on high-stakes medical/healthcare professional certification exams (pharmacist licensure), and exploring their potential use in AI-enabled formative evaluation and digital health education for healthcare professionals.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study assessed the capabilities of ChatGPT-4o and DeepSeek-R1 on 2,306 text-only multiple-choice questions from the Chinese Pharmacist Licensing Examination (2017-2021).</li>
                    
                    <li>DeepSeek-R1 demonstrated significantly higher overall accuracy (90.0%) compared to ChatGPT-4o (76.1%), with a p-value < 0.001.</li>
                    
                    <li>Unit-level analyses showed DeepSeek-R1 consistently outperformed, particularly excelling in foundational and clinical synthesis modules.</li>
                    
                    <li>While DeepSeek-R1 generally performed better year-by-year, this performance gap did not reach statistical significance at the specific unit-year level (all p > 0.05).</li>
                    
                    <li>The findings suggest a robust alignment of domain-specific models like DeepSeek-R1 with the structural and semantic demands of a complex medical licensure exam.</li>
                    
                    <li>The research implies potential for LLMs in AI-enabled formative evaluation within digital health education and assessment workflows.</li>
                    
                    <li>The study reinforces the critical necessity of human oversight in legally and ethically sensitive medical contexts, despite advanced AI performance.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study compiled 2,306 multiple-choice questions (text-only) from the Chinese Pharmacist Licensing Examination (2017-2021), training materials, and public databases. Questions containing tables or images were systematically excluded. Each item was input in its original Chinese format to two LLMs: ChatGPT-4o and DeepSeek-R1. Model responses were evaluated for exact accuracy. Statistical comparisons included Pearson's Chi-squared test for overall performance and Fisher's exact test for year-wise multiple-choice accuracy.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>DeepSeek-R1 significantly outperformed ChatGPT-4o with an overall accuracy of 90.0% compared to 76.1% (p < 0.001). DeepSeek-R1 demonstrated consistent advantages across unit-level analyses, showing particular strength in foundational and clinical synthesis modules. Although DeepSeek-R1 also favored year-by-year performance, the differences in specific unit-years did not achieve statistical significance (all p > 0.05).</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The robust performance of DeepSeek-R1 suggests that domain-specific LLMs hold significant potential for enhancing AI-enabled formative evaluation in pharmacy education and professional development, possibly aiding in self-assessment or preparation for licensure exams. However, the study also underscores that for legally and ethically sensitive applications, such as final licensure or direct patient care decisions, human oversight remains absolutely essential to ensure accuracy, accountability, and ethical adherence.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study's primary limitation is the exclusion of questions containing tables or images, meaning the LLMs' ability to process multimodal clinical information was not assessed. It focused solely on multiple-choice questions, which may not fully encompass the diverse range of clinical reasoning and practical application skills required for pharmacist competency. The findings are also specific to the two tested LLMs and the Chinese Pharmacist Exam context.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors suggest further investigation into domain-specific models to fully explore their potential in similar high-stakes contexts. Future research should also explore LLM performance on multimodal questions (incorporating images, tables, or complex data), and on more open-ended or complex clinical reasoning tasks to better simulate real-world pharmacist responsibilities. Investigation into LLMs as personalized learning tools or decision support aids within pharmacy practice could also be beneficial.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacy</span>
                    
                    <span class="tag">Clinical Pharmacology</span>
                    
                    <span class="tag">Health Education</span>
                    
                    <span class="tag">Medical Licensure</span>
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Pharmacist Examination</span>
                    
                    <span class="tag tag-keyword">ChatGPT-4o</span>
                    
                    <span class="tag tag-keyword">DeepSeek-R1</span>
                    
                    <span class="tag tag-keyword">Medical Education</span>
                    
                    <span class="tag tag-keyword">AI Assessment</span>
                    
                    <span class="tag tag-keyword">Clinical Competency</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>15 pages, 4 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>