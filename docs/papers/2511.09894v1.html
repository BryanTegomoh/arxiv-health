<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EgoEMS: A High-Fidelity Multimodal Egocentric Dataset for Cognitive Assistance in Emergency Medical Services - Health AI Hub</title>
    <meta name="description" content="This paper introduces EgoEMS, the first high-fidelity, multimodal egocentric dataset designed to facilitate the development of AI cognitive assistants for Emerg">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>EgoEMS: A High-Fidelity Multimodal Egocentric Dataset for Cognitive Assistance in Emergency Medical Services</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.09894v1" target="_blank">2511.09894v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-13
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Keshara Weerasinghe, Xueren Ge, Tessa Heick, Lahiru Nuwan Wijayasingha, Anthony Cortez, Abhishek Satpathy, John Stankovic, Homa Alemzadeh
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI, cs.CV, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.09894v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.09894v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces EgoEMS, the first high-fidelity, multimodal egocentric dataset designed to facilitate the development of AI cognitive assistants for Emergency Medical Services (EMS). Capturing over 20 hours of realistic simulated emergency scenarios performed by EMS professionals, EgoEMS provides rich annotations and benchmarks crucial for real-time keystep recognition and action quality estimation in high-stakes environments.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research directly addresses the intense cognitive demands faced by EMS first responders in critical situations, aiming to develop AI tools that can act as virtual partners to improve real-time decision-making, data collection, and procedural execution, ultimately enhancing patient survival and care quality.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper describes the development of a dataset (EgoEMS) specifically designed to train AI cognitive assistants for EMS professionals. These AI assistants aim to act as 'virtual partners' by providing real-time support for tasks such as keystep recognition, action quality estimation, data collection, and decision-making during emergency scenarios, ultimately reducing cognitive load on first responders and improving patient outcomes.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>EgoEMS is the first end-to-end, high-fidelity, multimodal, multiperson dataset for EMS, captured from an egocentric (first-person) view.</li>
                    
                    <li>It comprises over 20 hours of data from 233 realistic simulated emergency scenarios, involving 62 participants including 46 EMS professionals.</li>
                    
                    <li>The dataset emphasizes realism, incorporating responder-patient interactions and procedural EMS activities, developed in collaboration with EMS experts and aligned with national standards.</li>
                    
                    <li>Data collection utilized an open-source, low-cost, and replicable system, promoting accessibility and widespread use.</li>
                    
                    <li>Comprehensive annotations include keysteps, timestamped audio transcripts with speaker diarization, action quality metrics, and bounding boxes with segmentation masks.</li>
                    
                    <li>The paper establishes benchmark tasks for real-time multimodal keystep recognition and action quality estimation, critical for evaluating AI assistant performance.</li>
                    
                    <li>The ultimate goal is to enable AI cognitive assistants to alleviate the intense cognitive demands on first responders, supporting real-time data collection and decision-making, thereby improving patient outcomes.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The EgoEMS dataset was constructed by capturing over 20 hours of data from 233 realistic simulated emergency scenarios, performed by 62 participants (including 46 EMS professionals) from an egocentric view. Data collection used an open-source, low-cost system and was developed in collaboration with EMS experts to ensure alignment with national standards. The data is comprehensively annotated with keysteps, timestamped audio transcripts featuring speaker diarization, action quality metrics, and visual annotations including bounding boxes with segmentation masks. The paper also presents benchmarks for real-time multimodal keystep recognition and action quality estimation.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary 'finding' is the successful creation and introduction of EgoEMS, a novel, high-fidelity, multimodal egocentric dataset, specifically tailored for Emergency Medical Services. This dataset provides an unprecedented and standardized resource for developing and evaluating AI cognitive assistants, alongside the establishment of baseline benchmarks for critical tasks like keystep recognition and action quality estimation within this challenging domain.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>EgoEMS has the potential to significantly impact clinical practice by enabling the development of advanced AI systems that can provide real-time cognitive support to EMS professionals. This could lead to more accurate and efficient execution of medical procedures, improved real-time situational awareness, better data capture in high-stress environments, and ultimately, enhanced patient care and survival rates in emergencies.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the EgoEMS dataset. However, the data collection occurs in 'simulated' emergency scenarios, which, despite emphasizing realism and expert collaboration, might not fully capture the complete unpredictability and extreme stressors of actual real-world emergencies.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors express a hope that EgoEMS will inspire the research community to push the boundaries of intelligent EMS systems, leading to further advancements in AI cognitive assistance for first responders and ultimately contributing to improved patient outcomes.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">Paramedicine</span>
                    
                    <span class="tag">Prehospital Care</span>
                    
                    <span class="tag">Critical Care Transport</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">EgoEMS</span>
                    
                    <span class="tag tag-keyword">Emergency Medical Services</span>
                    
                    <span class="tag tag-keyword">AI Cognitive Assistants</span>
                    
                    <span class="tag tag-keyword">Multimodal Dataset</span>
                    
                    <span class="tag tag-keyword">Egocentric Vision</span>
                    
                    <span class="tag tag-keyword">Action Recognition</span>
                    
                    <span class="tag tag-keyword">Action Quality Estimation</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Emergency Medical Services (EMS) are critical to patient survival in emergencies, but first responders often face intense cognitive demands in high-stakes situations. AI cognitive assistants, acting as virtual partners, have the potential to ease this burden by supporting real-time data collection and decision making. In pursuit of this vision, we introduce EgoEMS, the first end-to-end, high-fidelity, multimodal, multiperson dataset capturing over 20 hours of realistic, procedural EMS activities from an egocentric view in 233 simulated emergency scenarios performed by 62 participants, including 46 EMS professionals. Developed in collaboration with EMS experts and aligned with national standards, EgoEMS is captured using an open-source, low-cost, and replicable data collection system and is annotated with keysteps, timestamped audio transcripts with speaker diarization, action quality metrics, and bounding boxes with segmentation masks. Emphasizing realism, the dataset includes responder-patient interactions reflecting real-world emergency dynamics. We also present a suite of benchmarks for real-time multimodal keystep recognition and action quality estimation, essential for developing AI support tools for EMS. We hope EgoEMS inspires the research community to push the boundaries of intelligent EMS systems and ultimately contribute to improved patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to AAAI 2026 (Preprint), 45 pages, 29 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>