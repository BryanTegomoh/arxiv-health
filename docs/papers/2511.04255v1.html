<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection - Health AI Hub</title>
    <meta name="description" content="This paper introduces MedSapiens, a novel approach that adapts Sapiens, a human-centric foundation model for pose estimation, to anatomical landmark detection i">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04255v1" target="_blank">2511.04255v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-06
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Marawan Elbatel, Anbang Wang, Keyuan Liu, Kaouther Mouheb, Enrique Almar-Munoz, Lizhuo Lin, Yanqi Yang, Karim Lekadir, Xiaomeng Li
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04255v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04255v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MedSapiens, a novel approach that adapts Sapiens, a human-centric foundation model for pose estimation, to anatomical landmark detection in medical imaging through multi-dataset pretraining. By revisiting this overlooked baseline, MedSapiens establishes a new state of the art, leveraging the inherent spatial pose localization capabilities of human-centric models to provide strong priors for medical landmark detection.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and automated anatomical landmark detection is a foundational task in medical imaging, critical for precise diagnosis, treatment planning, surgical navigation, and quantitative analysis. MedSapiens offers significant improvements in accuracy and robustness, including in data-scarce scenarios, directly enhancing the reliability and efficiency of these crucial medical processes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper introduces MedSapiens, an AI model based on adapting human-centric foundation models for highly accurate anatomical landmark detection in medical images. This AI application aims to improve the precision and efficiency of automated analysis in healthcare, assisting in tasks such as disease diagnosis, surgical planning, monitoring treatment progression, and quantitative measurements of anatomical structures by providing superior localization of key features.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study revisits the fundamental baseline of adapting human-centric foundation models for anatomical landmark detection in medical imaging, proposing MedSapiens.</li>
                    
                    <li>MedSapiens is an adaptation of Sapiens, a human-centric foundation model originally designed for pose estimation, achieved through multi-dataset pretraining.</li>
                    
                    <li>It demonstrates that human-centric models, optimized for spatial pose localization, provide strong and previously untapped priors for anatomical landmark detection.</li>
                    
                    <li>MedSapiens achieves a new state of the art, outperforming existing models by up to 5.26% over generalist models and 21.81% over specialist models in the average Success Detection Rate (SDR).</li>
                    
                    <li>The model also exhibits strong adaptability to novel downstream tasks with limited annotations, improving upon the few-shot state of the art by 2.69% in SDR.</li>
                    
                    <li>The research highlights the potential of large-scale pre-trained vision models for medical applications, moving beyond traditional domain-specific models.</li>
                    
                    <li>Code and model weights for MedSapiens are made publicly available, promoting reproducibility and further research.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves adapting 'Sapiens', a human-centric foundation model designed for pose estimation, to the task of anatomical landmark detection in medical imaging. This adaptation is performed via multi-dataset pretraining to create 'MedSapiens'. The model's performance is then rigorously benchmarked against existing generalist and specialist state-of-the-art models, and evaluated for its efficacy in limited-data (few-shot) settings, using the average Success Detection Rate (SDR) as the primary metric.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>MedSapiens establishes a new state of the art for anatomical landmark detection across multiple datasets. It achieves substantial performance gains, showing up to a 5.26% improvement in average Success Detection Rate (SDR) over generalist models and an impressive 21.81% improvement over specialist models. Furthermore, MedSapiens demonstrates superior adaptability in limited-data settings, outperforming the few-shot state of the art by 2.69% in SDR.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The enhanced accuracy and robust performance of MedSapiens, particularly in scenarios with limited annotated data, hold significant clinical impact. It can lead to more precise and consistent measurements for disease diagnosis and progression monitoring, improve the accuracy of patient-specific surgical planning and intraoperative guidance, and facilitate more reliable quantitative analysis in medical research. This could translate to better patient outcomes, reduced diagnostic errors, and more efficient healthcare delivery.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the MedSapiens model or the study itself.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest specific future research directions, beyond the general implication that the untapped potential of human-centric foundation models for various medical imaging tasks should be further explored.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Orthopedics</span>
                    
                    <span class="tag">Anatomy</span>
                    
                    <span class="tag">Surgical Planning</span>
                    
                    <span class="tag">Medical Image Analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">medical imaging</span>
                    
                    <span class="tag tag-keyword">anatomical landmark detection</span>
                    
                    <span class="tag tag-keyword">foundation models</span>
                    
                    <span class="tag tag-keyword">human-centric AI</span>
                    
                    <span class="tag tag-keyword">pose estimation</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">pretraining</span>
                    
                    <span class="tag tag-keyword">few-shot learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>