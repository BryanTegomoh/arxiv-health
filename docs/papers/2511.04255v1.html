<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection - Health AI Hub</title>
    <meta name="description" content="This paper introduces MedSapiens, a novel approach that adapts human-centric foundation models, specifically Sapiens (designed for pose estimation), for anatomi">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04255v1" target="_blank">2511.04255v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-06
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Marawan Elbatel, Anbang Wang, Keyuan Liu, Kaouther Mouheb, Enrique Almar-Munoz, Lizhuo Lin, Yanqi Yang, Karim Lekadir, Xiaomeng Li
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04255v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04255v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MedSapiens, a novel approach that adapts human-centric foundation models, specifically Sapiens (designed for pose estimation), for anatomical landmark detection in medical imaging. By leveraging multi-dataset pretraining, MedSapiens achieves a new state-of-the-art performance across various datasets, demonstrating the significant and previously overlooked potential of such models to provide strong spatial priors for medical applications.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and robust anatomical landmark detection is critical for precise diagnosis, surgical planning, treatment delivery (e.g., radiation therapy), and quantitative analysis in various medical imaging modalities, directly impacting patient care and research efficiency.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is to enhance the accuracy and efficiency of anatomical landmark detection in medical images. This involves adapting human-centric foundation models (AI/computer vision models) to identify specific anatomical points (landmarks) on medical scans (e.g., X-rays, CTs, MRIs). This capability can support automated diagnosis, measurement for surgical planning, disease progression monitoring, and AI-assisted image analysis workflows in clinical settings, thereby improving the speed and reliability of medical image interpretation.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Revisits the overlooked baseline of adapting human-centric foundation models for anatomical landmark detection in medical imaging.</li>
                    
                    <li>Utilizes Sapiens, a human-centric foundation model originally developed for pose estimation, as its core architecture.</li>
                    
                    <li>Employs multi-dataset pretraining to adapt Sapiens to the medical imaging domain, creating MedSapiens.</li>
                    
                    <li>Establishes a new state-of-the-art (SOTA) in anatomical landmark detection across multiple medical imaging datasets.</li>
                    
                    <li>Demonstrates significant performance improvements: up to 5.26% over generalist models and 21.81% over specialist models in average Success Detection Rate (SDR).</li>
                    
                    <li>Achieves robust adaptability in limited-data (few-shot) settings, showing a 2.69% improvement over the current few-shot SOTA in SDR.</li>
                    
                    <li>Highlights that human-centric models, inherently optimized for spatial pose localization, provide strong, largely untapped priors for anatomical landmark detection in medical contexts.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study adapts Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through a process of multi-dataset pretraining. MedSapiens' performance is rigorously benchmarked against existing state-of-the-art generalist and specialist models. Additionally, its adaptability to novel downstream tasks with limited annotations is assessed by evaluating its performance in few-shot data settings.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>MedSapiens achieves new state-of-the-art results in anatomical landmark detection across multiple medical imaging datasets, demonstrating up to 5.26% improvement over generalist models and 21.81% improvement over specialist models in average Success Detection Rate (SDR). Furthermore, it shows strong performance in limited-data settings, yielding a 2.69% improvement over the few-shot state of the art in SDR. The core finding is that human-centric foundation models provide powerful, yet largely unexploited, spatial priors for anatomical landmark detection.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research can lead to more accurate, robust, and automated detection of anatomical landmarks in clinical workflows, which is vital for improving diagnostic precision, enhancing the reliability of surgical guidance systems, optimizing radiation therapy planning, and facilitating standardized measurements for disease progression monitoring. Its few-shot learning capability could also significantly reduce the annotation burden for new medical tasks or rare conditions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the MedSapiens model or the study design.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract implicitly suggests that the potential of human-centric foundation models for anatomical landmark detection in medical imaging remains 'largely untapped,' indicating a broad avenue for future exploration and application of these models in various medical contexts and tasks.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Imaging Analysis</span>
                    
                    <span class="tag">Anatomy</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Surgical Planning</span>
                    
                    <span class="tag">Radiation Oncology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Anatomical Landmark Detection</span>
                    
                    <span class="tag tag-keyword">Foundation Models</span>
                    
                    <span class="tag tag-keyword">Human-Centric Models</span>
                    
                    <span class="tag tag-keyword">Pose Estimation</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">State-of-the-Art</span>
                    
                    <span class="tag tag-keyword">Few-Shot Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>