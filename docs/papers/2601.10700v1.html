<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals - Health AI Hub</title>
    <meta name="description" content="This paper introduces LIBERTy, a novel causal framework designed to benchmark concept-based explanations of Large Language Models (LLMs) using structural counte">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.10700v1" target="_blank">2601.10700v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-15
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.10700v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.10700v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces LIBERTy, a novel causal framework designed to benchmark concept-based explanations of Large Language Models (LLMs) using structural counterfactuals, addressing the limitations of costly and imperfect human-written counterfactuals. LIBERTy constructs datasets by defining Structured Causal Models (SCMs) where concept interventions propagate to LLMs for counterfactual generation. The framework, applied across medical and other high-stakes domains, reveals significant room for improving explanation methods and highlights reduced sensitivity of proprietary LLMs to demographic concepts, likely due to post-training mitigation.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>In high-stakes medical domains, understanding how concepts like patient demographics, symptoms, or medical history influence an LLM's diagnostic or predictive output is crucial for trust, accountability, and ethical AI deployment. LIBERTy provides a rigorous method to evaluate and enhance the faithfulness of these concept-based explanations, which is vital for developing reliable clinical decision support systems and mitigating bias in medical AI.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research provides a much-needed benchmark for developing and evaluating faithful explainability methods for Large Language Models (LLMs) in healthcare. This is critical for applications such as aiding in disease diagnosis, ensuring fairness and preventing bias in clinical decision support systems (e.g., when considering demographic factors like gender or experience in patient assessment), and fostering trust in AI tools used by medical professionals and patients.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Current benchmarks for evaluating concept-based explanations of LLMs suffer from reliance on costly, imperfect human-written counterfactuals.</li>
                    
                    <li>LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets) proposes a causal framework to construct datasets with structural counterfactual pairs.</li>
                    
                    <li>The framework is grounded in explicitly defined Structured Causal Models (SCMs) of text generation, where interventions on high-level concepts propagate through the SCM, prompting an LLM to generate the counterfactual.</li>
                    
                    <li>Three new datasets are introduced for benchmarking: disease detection, CV screening, and workplace violence prediction, targeting high-stakes applications.</li>
                    
                    <li>A novel evaluation metric, 'order-faithfulness,' is introduced to assess the faithfulness of concept-based explanations.</li>
                    
                    <li>Extensive evaluation across a range of methods and five LLMs identified substantial headroom for improving existing concept-based explanation techniques.</li>
                    
                    <li>Systematic analysis showed that proprietary LLMs exhibit markedly reduced sensitivity to demographic concepts, a phenomenon attributed to their post-training mitigation strategies.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The LIBERTy framework is built upon explicitly defined Structured Causal Models (SCMs) that model the underlying text generation process. Interventions on specific high-level concepts (e.g., gender, experience) are applied within these SCMs. These interventions propagate through the causal graph, dictating how an LLM subsequently generates the corresponding structural counterfactual text. This process creates datasets of counterfactual pairs. The faithfulness of concept-based explanations is then evaluated against these causally derived reference targets using a newly proposed metric called 'order-faithfulness'.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The paper found substantial headroom for improving the faithfulness of current concept-based explanation methods when evaluated using the LIBERTy framework. A significant observation was that proprietary LLMs exhibited a markedly reduced sensitivity to interventions involving demographic concepts, suggesting the impact of post-training mitigation strategies on their behavior regarding sensitive attributes.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>LIBERTy offers a critical tool for advancing the development of more trustworthy and transparent AI systems in healthcare. By rigorously benchmarking and improving the faithfulness of how LLMs explain their reasoning based on clinical or demographic concepts, it can lead to more reliable diagnostic tools, fairer risk assessments, and more equitable treatment recommendations, ultimately enhancing patient safety and clinician trust in AI-driven medical insights.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail limitations of the LIBERTy framework itself, focusing instead on how it addresses shortcomings of prior benchmarking approaches (e.g., reliance on costly and imperfect human-written counterfactuals).</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper positions LIBERTy as a 'much-needed benchmark for developing faithful explainability methods,' implicitly suggesting that its primary future direction is to serve as a foundation for the community to innovate and improve concept-based explanations for LLMs. The identified 'substantial headroom for improving concept-based explanations' directly prompts further research and development in this domain.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Disease Detection</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">AI Ethics in Healthcare</span>
                    
                    <span class="tag">Personalized Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">Causal Inference</span>
                    
                    <span class="tag tag-keyword">Counterfactuals</span>
                    
                    <span class="tag tag-keyword">Structured Causal Models</span>
                    
                    <span class="tag tag-keyword">Benchmarking</span>
                    
                    <span class="tag tag-keyword">Concept-based Explanations</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>