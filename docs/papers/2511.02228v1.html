<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis - Health AI Hub</title>
    <meta name="description" content="This paper introduces a Collaborative Attention and Consistent-Guided Fusion (CA-CGF) framework for improved Alzheimer's Disease (AD) diagnosis using multimodal">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.02228v1" target="_blank">2511.02228v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Delin Ma, Menghui Zhou, Jun Qi, Yun Yang, Po Yang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.02228v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.02228v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a Collaborative Attention and Consistent-Guided Fusion (CA-CGF) framework for improved Alzheimer's Disease (AD) diagnosis using multimodal MRI and PET neuroimaging. The framework effectively integrates both cross-modal complementary features and crucial modality-specific features, while simultaneously mitigating the inherent distributional differences between modalities. Experimental validation on the ADNI dataset demonstrates that the proposed method achieves superior diagnostic performance compared to existing fusion strategies.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Early and accurate diagnosis of Alzheimer's disease is paramount for initiating timely interventions that can slow disease progression and improve patient outcomes. This research offers a more robust and precise diagnostic tool by optimizing the integration and analysis of commonly used neuroimaging data (MRI and PET), directly supporting better clinical decision-making.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This paper proposes an AI framework utilizing collaborative attention and consistent-guided fusion of MRI and PET scans to improve the early diagnosis of Alzheimer's Disease. It leverages deep learning techniques (specifically, shared and modality-independent encoders, consistency-guided mechanisms) to process and combine multimodal medical images, ultimately aiming to provide more accurate and robust diagnostic predictions for a severe neurodegenerative condition.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Existing multimodal fusion methods for AD diagnosis often prioritize cross-modal complementarity but overlook the diagnostic importance of modality-specific features.</li>
                    
                    <li>The inherent distributional differences between MRI and PET modalities can introduce bias and noise into representations, thereby degrading classification performance.</li>
                    
                    <li>The proposed CA-CGF framework addresses these challenges by employing a novel architecture designed to preserve both shared and specific representations.</li>
                    
                    <li>A Learnable Parameter Representation (LPR) block is introduced within the framework to compensate for potential missing modality information.</li>
                    
                    <li>The architecture incorporates a shared encoder and modality-independent encoders to ensure comprehensive feature extraction, capturing both common and unique diagnostic signals.</li>
                    
                    <li>A consistency-guided mechanism is explicitly employed to align the latent feature distributions across the MRI and PET modalities, reducing inter-modal discrepancies.</li>
                    
                    <li>Evaluations on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset confirm that the CA-CGF method yields superior diagnostic performance for AD compared to other state-of-the-art fusion approaches.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed Collaborative Attention and Consistent-Guided Fusion (CA-CGF) framework is a deep learning-based architecture. It integrates a Learnable Parameter Representation (LPR) block to handle potential missing modality data. Feature extraction is performed using a combination of a shared encoder to capture common representations and modality-independent encoders to preserve specific features for MRI and PET. A critical consistency-guided mechanism is implemented to explicitly align the latent feature distributions between the distinct modalities, reducing biases and noise.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is that the CA-CGF method achieves superior diagnostic performance for Alzheimer's Disease when utilizing combined MRI and PET data, outperforming existing multimodal fusion strategies on the ADNI dataset. This success is attributed to its innovative approach of simultaneously accounting for both cross-modal complementary and modality-specific features, while also explicitly aligning the latent distributions across modalities.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to significantly enhance the accuracy and reliability of early Alzheimer's Disease diagnosis in clinical practice. By providing a more advanced and robust method for interpreting multimodal neuroimaging data, it could enable clinicians to identify AD earlier, facilitate more timely and effective therapeutic interventions, and ultimately lead to improved patient management and quality of life.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail specific limitations or caveats of the proposed method. However, common limitations for such studies (not stated here) often include reliance on a specific dataset (ADNI), generalizability to diverse patient populations or different scanner protocols, and computational requirements for real-world clinical deployment.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions are not explicitly mentioned in the provided abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Neuroradiology</span>
                    
                    <span class="tag">Geriatric Medicine</span>
                    
                    <span class="tag">Cognitive Neuroscience</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Alzheimer's Disease</span>
                    
                    <span class="tag tag-keyword">Multimodal Fusion</span>
                    
                    <span class="tag tag-keyword">MRI</span>
                    
                    <span class="tag tag-keyword">PET</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Early Diagnosis</span>
                    
                    <span class="tag tag-keyword">Collaborative Attention</span>
                    
                    <span class="tag tag-keyword">Consistency-Guided Fusion</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Alzheimer's disease (AD) is the most prevalent form of dementia, and its
early diagnosis is essential for slowing disease progression. Recent studies on
multimodal neuroimaging fusion using MRI and PET have achieved promising
results by integrating multi-scale complementary features. However, most
existing approaches primarily emphasize cross-modal complementarity while
overlooking the diagnostic importance of modality-specific features. In
addition, the inherent distributional differences between modalities often lead
to biased and noisy representations, degrading classification performance. To
address these challenges, we propose a Collaborative Attention and
Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The
proposed model introduces a learnable parameter representation (LPR) block to
compensate for missing modality information, followed by a shared encoder and
modality-independent encoders to preserve both shared and specific
representations. Furthermore, a consistency-guided mechanism is employed to
explicitly align the latent distributions across modalities. Experimental
results on the ADNI dataset demonstrate that our method achieves superior
diagnostic performance compared with existing fusion strategies.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>