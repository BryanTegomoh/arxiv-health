<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A benchmark multimodal oro-dental dataset for large vision-language models - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel, comprehensive multimodal oro-dental dataset derived from 8775 dental checkups, integrating intraoral images, radiographs, and det">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>A benchmark multimodal oro-dental dataset for large vision-language models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04948v1" target="_blank">2511.04948v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-07
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Haoxin Lv, Ijazul Haq, Jin Du, Jiaxin Ma, Binnian Zhu, Xiaobing Dang, Chaoan Liang, Ruxu Du, Yingjie Zhang, Muhammad Saqib
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04948v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04948v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel, comprehensive multimodal oro-dental dataset derived from 8775 dental checkups, integrating intraoral images, radiographs, and detailed textual records. The dataset was utilized to fine-tune state-of-the-art large vision-language models (Qwen-VL 3B/7B), which subsequently achieved substantial performance gains in oro-dental anomaly classification and diagnostic report generation compared to baseline models and GPT-4o.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This dataset provides a critical, large-scale resource to overcome data scarcity in oral healthcare AI development, enabling the training of more robust and accurate multimodal AI models for diagnosis and clinical decision support, which can significantly improve efficiency and quality of patient care in dentistry.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research develops and validates a dataset for training large vision-language models to perform critical tasks in dentistry, including the classification of oro-dental anomalies (diagnosis support) and the generation of complete diagnostic reports from multimodal clinical data. This aims to enhance AI-driven solutions for diagnosis, treatment planning, and clinical documentation in oral healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Development and public release of a large-scale multimodal oro-dental dataset sourced from 8775 dental checkups of 4800 patients over eight years (2018-2025), encompassing a wide age range (10-90 years).</li>
                    
                    <li>The dataset comprises diverse data modalities: 50000 intraoral images, 8056 radiographs, and extensive textual records, including diagnoses, treatment plans, and follow-up notes.</li>
                    
                    <li>Data collection adhered to standard ethical guidelines and was meticulously annotated for benchmarking advanced AI models.</li>
                    
                    <li>State-of-the-art large vision-language models (Qwen-VL 3B and 7B) were fine-tuned using this dataset to demonstrate its utility.</li>
                    
                    <li>Model evaluation was conducted on two crucial oro-dental tasks: classification of six distinct oro-dental anomalies and generation of comprehensive diagnostic reports from combined multimodal inputs.</li>
                    
                    <li>The fine-tuned models exhibited significant performance improvements ("substantial gains") over their base versions and GPT-4o, validating the dataset's effectiveness.</li>
                    
                    <li>The dataset is publicly available, positioning it as a foundational resource for advancing AI-driven research and solutions in oral healthcare.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>A comprehensive multimodal dataset was created from 8775 dental checkups of 4800 patients, comprising 50000 intraoral images, 8056 radiographs, and detailed textual clinical records, collected ethically and annotated for benchmarking. This dataset was then used to fine-tune state-of-the-art large vision-language models (Qwen-VL 3B and 7B). The performance of these fine-tuned models was evaluated on two downstream tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs, with results compared against base Qwen-VL models and GPT-4o.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The fine-tuned large vision-language models, trained on the presented multimodal oro-dental dataset, achieved substantial performance gains in both the classification of six oro-dental anomalies and the generation of complete diagnostic reports, significantly outperforming their base counterparts and GPT-4o. This unequivocally demonstrates the dataset's efficacy in enhancing AI capabilities for complex dental diagnostic and reporting tasks.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to profoundly impact clinical dentistry by facilitating the development of highly accurate and comprehensive AI-powered tools for automated dental diagnosis, anomaly detection, and diagnostic report generation. Such tools could assist dentists in achieving more consistent and earlier diagnoses, streamline clinical workflows, reduce diagnostic errors, and ultimately lead to improved patient outcomes through more precise and personalized treatment planning.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the dataset or the study's methodology.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The publicly available dataset is presented as an essential resource to drive future research in AI dentistry, encouraging the development and advancement of AI-driven oro-dental healthcare solutions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Dentistry</span>
                    
                    <span class="tag">Oral and Maxillofacial Radiology</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                    <span class="tag">Computational Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">multimodal dataset</span>
                    
                    <span class="tag tag-keyword">oro-dental</span>
                    
                    <span class="tag tag-keyword">dentistry</span>
                    
                    <span class="tag tag-keyword">AI in healthcare</span>
                    
                    <span class="tag tag-keyword">vision-language models</span>
                    
                    <span class="tag tag-keyword">dental diagnostics</span>
                    
                    <span class="tag tag-keyword">radiographs</span>
                    
                    <span class="tag tag-keyword">intraoral images</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The advancement of artificial intelligence in oral healthcare relies on the
availability of large-scale multimodal datasets that capture the complexity of
clinical practice. In this paper, we present a comprehensive multimodal
dataset, comprising 8775 dental checkups from 4800 patients collected over
eight years (2018-2025), with patients ranging from 10 to 90 years of age. The
dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual
records, including diagnoses, treatment plans, and follow-up notes. The data
were collected under standard ethical guidelines and annotated for
benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large
vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks:
classification of six oro-dental anomalies and generation of complete
diagnostic reports from multimodal inputs. We compared the fine-tuned models
with their base counterparts and GPT-4o. The fine-tuned models achieved
substantial gains over these baselines, validating the dataset and underscoring
its effectiveness in advancing AI-driven oro-dental healthcare solutions. The
dataset is publicly available, providing an essential resource for future
research in AI dentistry.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>