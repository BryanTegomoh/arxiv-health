<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios - Health AI Hub</title>
    <meta name="description" content="This paper introduces AerialMind, the first large-scale benchmark for Referring Multi-Object Tracking (RMOT) in Unmanned Aerial Vehicle (UAV) scenarios, address">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.21053v1" target="_blank">2511.21053v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Chenglizhao Chen, Shaofeng Liang, Runwei Guan, Xiaolou Sun, Haocheng Zhao, Haiyun Jiang, Tao Huang, Henghui Ding, Qing-Long Han
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.RO, cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.75 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.21053v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.21053v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces AerialMind, the first large-scale benchmark for Referring Multi-Object Tracking (RMOT) in Unmanned Aerial Vehicle (UAV) scenarios, addressing the limitations of ground-level RMOT for comprehensive scene understanding. It also proposes HawkEyeTrack (HETrack), a novel method designed to enhance vision-language representation learning for improved perception in UAV environments, validated through experiments demonstrating the dataset's challenge and the method's effectiveness.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>While primarily a computer science paper, this research is foundational for developing intelligent aerial systems capable of precise object identification and tracking via natural language. This capability could significantly impact medical emergency response, disaster relief, and remote healthcare logistics by enabling UAVs to accurately locate and track specific individuals or critical medical supplies in complex, dynamic environments based on verbal or written instructions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>UAVs equipped with Referring Multi-Object Tracking (RMOT) can autonomously identify and track specific individuals or objects based on natural language commands. In health-related contexts, this could include: 1) Guiding UAVs to locate and monitor injured individuals in disaster zones or remote areas (e.g., 'find the person in a red shirt near the overturned car') for emergency medical response. 2) Tracking the movement of medical personnel or critical supplies in complex environments. 3) Assisting in public health surveillance by identifying and tracking specific disease vectors (e.g., mosquito swarms) or monitoring crowd density for compliance with health regulations. 4) Enhancing biosecurity by precisely tracking suspicious individuals or vehicles in restricted zones based on descriptive inputs.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Referring Multi-Object Tracking (RMOT), a core capability for intelligent robotics using natural language, is currently restricted primarily to ground-level applications, limiting broad-scale scene context capture.</li>
                    
                    <li>UAVs offer significant advantages for RMOT due to their expansive aerial perspectives, superior maneuverability, and capacity for wide-area surveillance.</li>
                    
                    <li>The paper introduces AerialMind, the first large-scale RMOT benchmark specifically designed for UAV scenarios, to bridge a critical research gap in embodied intelligence.</li>
                    
                    <li>An innovative semi-automated framework, COALA (collaborative agent-based labeling assistant), was developed to efficiently construct the AerialMind dataset with high annotation quality while reducing labor costs.</li>
                    
                    <li>A novel method, HawkEyeTrack (HETrack), is proposed to enhance RMOT performance specifically in challenging UAV scenarios.</li>
                    
                    <li>HETrack focuses on collaboratively improving vision-language representation learning, which is crucial for robust aerial perception and natural language interaction.</li>
                    
                    <li>Comprehensive experiments confirm both the challenging nature of the AerialMind dataset and the validated effectiveness of the HETrack method in this novel domain.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involved the construction of AerialMind, the first large-scale RMOT benchmark specifically for UAV scenarios, facilitated by the development of the COALA (semi-automated collaborative agent-based labeling assistant) framework to ensure efficient and high-quality data annotation. Concurrently, the paper proposes HawkEyeTrack (HETrack), a novel method designed to collaboratively enhance vision-language representation learning for improved perception in UAV environments, with its effectiveness comprehensively validated through experiments on the newly created benchmark.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The AerialMind dataset presents a significantly challenging environment for current RMOT methods in UAV scenarios. The proposed HawkEyeTrack (HETrack) method demonstrates effectiveness in collaboratively enhancing vision-language representation learning and improving overall perception, thereby addressing some of these challenges within complex aerial environments.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The development of robust RMOT capabilities in UAVs, as advanced by this research, holds potential for transformative clinical and practical impact. This could include rapidly deploying automated external defibrillators (AEDs) or other critical medical supplies to precisely identified individuals or locations during emergencies, assisting in tracking patients or first responders during large-scale disaster relief operations, or guiding search-and-rescue missions by allowing medical personnel to verbally direct UAVs to locate specific individuals or objects from an aerial perspective.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the proposed method or the constructed dataset. However, its positioning as the "first large-scale RMOT benchmark in UAV scenarios" implies that the field is nascent. This suggests that current methods, including HETrack, are foundational steps, and there is significant scope for further improvements in robustness, accuracy, and real-world adaptability to handle the full complexities and diverse conditions inherent in dynamic UAV operations.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The introduction of the AerialMind benchmark implicitly suggests several future research directions. These include the development of more advanced and robust RMOT models specifically tailored for UAV scenarios, further innovations in vision-language representation learning for aerial contexts, and exploring real-world deployment challenges, including ethical considerations and regulatory frameworks for intelligent aerial systems interacting with natural language commands.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">Disaster Medicine</span>
                    
                    <span class="tag">Search and Rescue</span>
                    
                    <span class="tag">Remote Healthcare Logistics</span>
                    
                    <span class="tag">Public Health Surveillance (with ethical/privacy considerations)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Referring Multi-Object Tracking (RMOT)</span>
                    
                    <span class="tag tag-keyword">Unmanned Aerial Vehicles (UAVs)</span>
                    
                    <span class="tag tag-keyword">Embodied Intelligence</span>
                    
                    <span class="tag tag-keyword">Computer Vision</span>
                    
                    <span class="tag tag-keyword">Robotics</span>
                    
                    <span class="tag tag-keyword">Natural Language Processing</span>
                    
                    <span class="tag tag-keyword">Aerial Surveillance</span>
                    
                    <span class="tag tag-keyword">Benchmark Dataset</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>AAAI 2026</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>