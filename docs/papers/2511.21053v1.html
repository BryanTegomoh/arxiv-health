<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios - Health AI Hub</title>
    <meta name="description" content="This paper introduces AerialMind, the first large-scale benchmark dataset for Referring Multi-Object Tracking (RMOT) in Unmanned Aerial Vehicle (UAV) scenarios,">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.21053v1" target="_blank">2511.21053v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Chenglizhao Chen, Shaofeng Liang, Runwei Guan, Xiaolou Sun, Haocheng Zhao, Haiyun Jiang, Tao Huang, Henghui Ding, Qing-Long Han
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.RO, cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.70 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.21053v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.21053v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces AerialMind, the first large-scale benchmark dataset for Referring Multi-Object Tracking (RMOT) in Unmanned Aerial Vehicle (UAV) scenarios, addressing a critical gap where RMOT research has primarily focused on ground-level contexts. To facilitate this, the authors developed a semi-automated collaborative agent-based labeling assistant (COALA) framework and propose HawkEyeTrack (HETrack), a novel method designed to enhance vision-language representation learning for UAV perception, demonstrating the challenging nature of the dataset and the effectiveness of their approach.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>While primarily a computer science and robotics paper, the advancements in UAV-based Referring Multi-Object Tracking (RMOT) hold significant indirect relevance for medicine and public health by enabling more intelligent aerial systems for wide-area surveillance, search and rescue, and autonomous medical logistics in challenging or remote environments.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Intelligent UAVs equipped with RMOT for natural language-driven search and rescue operations, rapid disaster site assessment to identify medical needs, public health monitoring for large gatherings or specific population movements, and enhanced biosecurity surveillance for detecting and tracking potential threats to human or animal health.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The research addresses a significant gap in Referring Multi-Object Tracking (RMOT) by shifting its focus from traditional ground-level scenarios to complex UAV (Unmanned Aerial Vehicle) environments.</li>
                    
                    <li>AerialMind is introduced as the inaugural large-scale RMOT benchmark dataset specifically designed for UAV scenarios, providing a crucial resource for advancing intelligent aerial systems.</li>
                    
                    <li>A novel semi-automated collaborative agent-based labeling assistant (COALA) framework was developed to efficiently construct the AerialMind dataset, significantly reducing labor costs while maintaining high annotation quality.</li>
                    
                    <li>The paper proposes HawkEyeTrack (HETrack), a new method that collaboratively enhances vision-language representation learning, specifically tailored to improve perception challenges inherent in UAV scenarios.</li>
                    
                    <li>UAVs are highlighted as critical platforms for Embodied Intelligence, necessitating advanced capabilities like natural language interaction for comprehensive tracking and path planning.</li>
                    
                    <li>Comprehensive experiments validate that the AerialMind dataset presents significant challenges, indicating its value for robust model development, and confirm the effectiveness of the proposed HawkEyeTrack method.</li>
                    
                    <li>The work aims to enable intelligent aerial systems with the fundamental capability of precise object detection and tracking guided by natural language instructions, crucial for wide-area surveillance and intelligent robotic applications.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involved constructing AerialMind, the first large-scale RMOT benchmark for UAVs, by developing a semi-automated collaborative agent-based labeling assistant (COALA) framework to optimize annotation. Furthermore, the authors designed HawkEyeTrack (HETrack), a novel method focusing on collaborative vision-language representation learning to improve perception in UAV contexts. Both the dataset and method were validated through comprehensive experiments.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study successfully demonstrated the creation of AerialMind, a challenging and effective large-scale RMOT benchmark for UAV scenarios. It also introduced HawkEyeTrack, a novel method shown to be effective in enhancing vision-language representation learning and perception specific to UAV environments, thus bridging a crucial research gap in aerial intelligence.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology could significantly impact clinical practice and public health by enabling advanced autonomous drones for critical applications. Examples include rapid search and rescue operations for injured individuals in large disaster zones, precise delivery of medical supplies (e.g., vaccines, blood, AEDs) to remote or inaccessible locations, or large-scale monitoring of populations during public health crises with language-guided precision.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract implicitly notes the challenging nature of UAV scenarios for RMOT, suggesting that robust performance across diverse aerial contexts remains an active area of development. While their method is effective, the inherent complexities of aerial perspectives and dynamic environments present ongoing challenges for achieving universally reliable language-guided tracking.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The introduction of AerialMind as a benchmark dataset inherently promotes future research into more robust and efficient RMOT models specifically tailored for UAV applications. Further work will likely focus on improving vision-language representation learning for diverse aerial conditions, enhancing real-time capabilities, and exploring advanced path planning based on complex natural language instructions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Disaster Medicine</span>
                    
                    <span class="tag">Emergency Response</span>
                    
                    <span class="tag">Public Health Surveillance</span>
                    
                    <span class="tag">Remote Healthcare Logistics</span>
                    
                    <span class="tag">Telemedicine (logistics)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Referring Multi-Object Tracking</span>
                    
                    <span class="tag tag-keyword">UAV</span>
                    
                    <span class="tag tag-keyword">AerialMind</span>
                    
                    <span class="tag tag-keyword">Computer Vision</span>
                    
                    <span class="tag tag-keyword">Robotics</span>
                    
                    <span class="tag tag-keyword">Natural Language Processing</span>
                    
                    <span class="tag tag-keyword">Embodied Intelligence</span>
                    
                    <span class="tag tag-keyword">Surveillance</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>AAAI 2026</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>