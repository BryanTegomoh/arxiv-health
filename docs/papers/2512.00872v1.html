<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces TAP-CT, a suite of task-agnostic pretraining methods for 3D Computed Tomography (CT) foundation models, adapting Vision Transformers (ViTs">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.00872v1" target="_blank">2512.00872v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Tim Veenboer, George Yiasemis, Eric Marcus, Vivien Van Veldhuizen, Cees G. M. Snoek, Jonas Teuwen, Kevin B. W. Groot Lipman
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.00872v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.00872v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces TAP-CT, a suite of task-agnostic pretraining methods for 3D Computed Tomography (CT) foundation models, adapting Vision Transformers (ViTs) and DINOv2 for volumetric data. By employing scalable self-supervised pretraining on a large dataset of 105K CT volumes with targeted architectural modifications, TAP-CT achieves stable, robust frozen representations that generalize strongly across diverse downstream tasks. This approach aims to reduce the need for extensive fine-tuning and reliance on task-biased objectives in medical AI.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as it provides a foundational AI model for CT imaging that can generalize across numerous diagnostic tasks. This could significantly accelerate the development of new AI applications in radiology by reducing the need for vast, task-specific labeled datasets and intensive fine-tuning, thereby making advanced AI more accessible and efficient for clinical use.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research focuses on developing robust and versatile AI foundation models for the interpretation of 3D Computed Tomography (CT) scans. These models can be fine-tuned for a wide array of medical applications, including automated disease detection, diagnosis, prognosis, treatment planning, and monitoring across various conditions (e.g., oncology, cardiology, pulmonology). By providing strong, task-agnostic representations, it aims to reduce the need for extensive fine-tuning for specific medical tasks, thereby accelerating the development and deployment of AI in clinical practice for improved patient care and diagnostic efficiency.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical need for strong, task-agnostic medical foundation models that require minimal fine-tuning and overcome biases of existing task-specific pretraining objectives.</li>
                    
                    <li>Introduces TAP-CT, which adapts Vision Transformers (ViTs) and DINOv2 architectures for scalable self-supervised pretraining directly on 3D CT volumes.</li>
                    
                    <li>Incorporates specific architectural modifications including depth-aware patch embeddings, adjusted positional encodings, and volumetric augmentations to handle the 3D nature of CT data while preserving model simplicity.</li>
                    
                    <li>Pretraining was conducted on an extensive in-house dataset comprising 105,000 3D CT volumes, enabling large-scale learning.</li>
                    
                    <li>The approach successfully yields stable and robust frozen representations, indicating high quality and consistency of learned features.</li>
                    
                    <li>Demonstrates strong generalization capabilities of these representations across a variety of downstream tasks without requiring extensive fine-tuning.</li>
                    
                    <li>The authors commit to releasing all pretrained models, experimental configurations, and downstream benchmark code to foster transparency, reproducibility, and establish a powerful low-resource baseline for future medical imaging research.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves adapting Vision Transformers (ViTs) and DINOv2, originally designed for 2D natural images, for 3D volumetric CT data. This adaptation utilizes a self-supervised pretraining paradigm on a large dataset of 105,000 3D CT volumes. Key technical modifications include tailored patch embeddings, positional encodings, and volumetric data augmentations to imbue the architecture with depth-awareness while maintaining the core simplicity of the original models.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is that large-scale 3D self-supervised pretraining of adapted ViTs and DINOv2 on 105,000 CT volumes successfully produces stable, robust frozen representations. These representations demonstrate strong generalization capabilities across various downstream medical imaging tasks, indicating their effectiveness as a task-agnostic foundation for CT analysis.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>TAP-CT has the potential to substantially impact clinical practice by streamlining the integration of AI into radiology workflows. It could enable faster development of accurate diagnostic tools for diverse CT applications, potentially leading to earlier disease detection, improved patient outcomes, and reduced diagnostic workload. By providing a low-resource baseline, it can democratize access to advanced AI for smaller research groups and clinical departments.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed method or its current applicability; it focuses on the strengths and contributions of TAP-CT.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors aim for TAP-CT to serve as a powerful, low-resource baseline for future research in medical imaging, suggesting that further work could involve applying and evaluating these models across an even wider array of medical imaging tasks, datasets, and potentially integrating them into more complex clinical decision-support systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical Artificial Intelligence</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Foundation Models</span>
                    
                    <span class="tag tag-keyword">Computed Tomography</span>
                    
                    <span class="tag tag-keyword">Self-supervised Learning</span>
                    
                    <span class="tag tag-keyword">Vision Transformers</span>
                    
                    <span class="tag tag-keyword">DINOv2</span>
                    
                    <span class="tag tag-keyword">3D Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Task-agnostic Pretraining</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>22 pages, 4 figures, 8 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>