<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Boosting Medical Visual Understanding From Multi-Granular Language Learning - Health AI Hub</title>
    <meta name="description" content="This paper introduces Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to enhance visual understanding in complex domains like">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Boosting Medical Visual Understanding From Multi-Granular Language Learning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.15943v1" target="_blank">2511.15943v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zihan Li, Yiqing Wang, Sina Farsiu, Paul Kinahan
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.15943v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.15943v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to enhance visual understanding in complex domains like medical imaging. It addresses the limitations of existing models like CLIP, which struggle with multi-label and multi-granularity alignment, by integrating structured multi-label supervision, cross-granularity textual descriptions, and soft-label supervision. MGLL significantly outperforms state-of-the-art methods in downstream medical tasks, improving the alignment of visual and textual representations.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as it directly addresses a critical challenge in applying advanced AI to medical imaging: the need to understand complex images associated with multiple diagnoses and described at various levels of detail, thereby improving the accuracy and utility of AI in clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The Multi-Granular Language Learning (MGLL) framework is a medical AI application designed to enhance the accuracy and robustness of AI models in analyzing medical images. By better aligning visual data from medical scans with various granularities of textual information (e.g., radiology reports, clinical notes describing disease categories, diagnostic descriptions, and clinical explanations), MGLL can lead to more precise AI-assisted disease detection, diagnosis, and a deeper understanding of complex medical conditions, thereby supporting clinical decision-making.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Identifies a critical limitation of existing image-text pretraining models (e.g., CLIP) in medical imaging: their focus on single-label, single-granularity alignment is insufficient for the multi-label and multi-granular nature of medical data (e.g., disease categories, diagnostic descriptions).</li>
                    
                    <li>Proposes Multi-Granular Language Learning (MGLL), a novel contrastive learning framework specifically engineered to improve both multi-label and cross-granularity alignment in visual understanding.</li>
                    
                    <li>MGLL incorporates structured multi-label supervision and integrates textual descriptions across different granularities (e.g., high-level clinical explanations, detailed diagnostic findings).</li>
                    
                    <li>The framework introduces soft-label supervision combined with point-wise constraints to further enhance the alignment between visual and textual features.</li>
                    
                    <li>Utilizes smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency within the model, while maintaining computational efficiency.</li>
                    
                    <li>MGLL is designed as a plug-and-play module, allowing for easy integration into existing vision-language models.</li>
                    
                    <li>Demonstrates superior performance over other state-of-the-art methods in various downstream tasks after pretraining on large-scale, custom-constructed multi-granular datasets.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>MGLL is a contrastive learning framework that extends current vision-language models. It integrates structured multi-label supervision and leverages textual descriptions from diverse granularities (e.g., diagnostic descriptions, clinical explanations). The framework employs soft-label supervision with point-wise constraints to refine alignment and uses smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency, all while maintaining computational efficiency. It operates as a plug-and-play module and is pretrained on custom large-scale multi-granular datasets.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>MGLL consistently outperforms other state-of-the-art methods in various downstream tasks, demonstrating its effectiveness in achieving superior multi-label and cross-granularity alignment for medical visual understanding after pretraining on specially constructed large-scale multi-granular datasets.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research can significantly enhance the accuracy and robustness of AI-powered diagnostic tools in medical imaging by enabling models to better interpret the complex, multi-faceted information found in medical reports and images. This leads to more precise disease classification, improved clinical decision support, and more effective integration of textual and visual data, ultimately aiding clinicians in making better-informed decisions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>None explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>None explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">General Medical Imaging</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Disease Diagnosis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Vision-Language Models</span>
                    
                    <span class="tag tag-keyword">Contrastive Learning</span>
                    
                    <span class="tag tag-keyword">Multi-Granularity</span>
                    
                    <span class="tag tag-keyword">Multi-Label</span>
                    
                    <span class="tag tag-keyword">CLIP</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Diagnostic Support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Preprint. 40 pages</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>