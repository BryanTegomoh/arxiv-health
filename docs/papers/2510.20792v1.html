<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation - Health AI Hub</title>
    <meta name="description" content="This paper introduces BadGraph, a novel backdoor attack method targeting latent diffusion models used for text-guided graph generation. BadGraph leverages textu">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20792v1" target="_blank">2510.20792v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Liang Ye, Shengqin Chen, Jiazhu Dai
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.CL, q-bio.BM
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20792v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20792v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces BadGraph, a novel backdoor attack method targeting latent diffusion models used for text-guided graph generation. BadGraph leverages textual triggers to subtly poison training data, embedding hidden vulnerabilities that compel the model to generate attacker-specified subgraphs during inference when these triggers are present, while maintaining normal performance on clean inputs. The study demonstrates the attack's high effectiveness and stealth across multiple datasets, revealing significant security vulnerabilities in these models with critical implications for applications like drug discovery.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Text-guided graph generation is fundamental for de novo drug design, material science, and chemical synthesis, where models generate molecular structures (graphs) based on desired properties. A backdoor attack could lead to the generation of molecules with unintended, potentially harmful, or ineffective substructures when specific textual prompts are used, severely compromising the safety and efficacy of drug discovery pipelines.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research directly impacts AI applications in drug design and molecular generation, specifically text-guided latent diffusion models used to propose new chemical structures for therapeutic or diagnostic purposes. A successful backdoor attack could compromise the integrity and safety of molecules generated for drug development.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the unexamined security vulnerability of backdoor attacks in conditional, specifically text-guided, graph generation latent diffusion models.</li>
                    
                    <li>Proposes BadGraph, an attack method that uses textual triggers to poison training data, implanting covert backdoors.</li>
                    
                    <li>The backdoor induces the generation of attacker-specified subgraphs when triggers appear in inference, without affecting performance on benign inputs.</li>
                    
                    <li>Achieves high attack success rates (e.g., >80% with 24% poisoning rate) with negligible degradation of benign model performance.</li>
                    
                    <li>Ablation studies reveal that the backdoor is implanted during the VAE and diffusion training phases, not during pretraining.</li>
                    
                    <li>Highlights the severe risks posed by such vulnerabilities in critical applications like drug discovery and emphasizes the urgent need for robust defense mechanisms.</li>
                    
                    <li>Experiments were conducted on four benchmark datasets: PubChem, ChEBI-20, PCDes, and MoMu, demonstrating broad applicability.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>BadGraph implements a backdoor attack by poisoning the training data of latent diffusion models for text-guided graph generation. This involves strategically embedding textual triggers within some training samples, associating them with attacker-specified target subgraphs. During the VAE (Variational Autoencoder) and diffusion training phases, the model inadvertently learns to associate these triggers with the generation of the specified subgraphs. Consequently, when an input text containing the trigger is provided during inference, the compromised model is compelled to generate a graph containing the attacker's chosen subgraph, while performing normally on clean inputs.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>BadGraph successfully implants effective and stealthy backdoors in text-guided latent diffusion models for graph generation. Attack success rates reach 50% with less than 10% poisoning and exceed 80% with only 24% poisoning, all while causing negligible performance degradation on benign inputs. Crucially, the backdoor is embedded specifically during the VAE and diffusion training stages of the model, rather than during initial pretraining. These findings underscore a significant security vulnerability in these generative AI models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>In drug discovery and development, backdoored generative AI models could be exploited to design molecules with hidden toxicophores, undesirable pharmacological properties, or even specific 'tags' for tracking or malicious identification. This could lead to the generation of unsafe drug candidates, wasted research resources, or the proliferation of potentially harmful compounds into research pipelines. It necessitates rigorous security audits and robust defense mechanisms for AI models used in sensitive biomedical applications to prevent compromise and ensure the integrity of generated therapeutic agents or materials.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the BadGraph method itself, but it implicitly highlights the significant security vulnerabilities inherent in current latent diffusion models for text-guided graph generation, which BadGraph exploits.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper strongly emphasizes the critical need for developing and implementing robust defense mechanisms to protect latent diffusion models for text-guided graph generation against backdoor attacks, especially given their increasing application in sensitive domains like drug discovery.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Drug Discovery</span>
                    
                    <span class="tag">Medicinal Chemistry</span>
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Cheminformatics</span>
                    
                    <span class="tag">Computational Biology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">latent diffusion models</span>
                    
                    <span class="tag tag-keyword">backdoor attack</span>
                    
                    <span class="tag tag-keyword">graph generation</span>
                    
                    <span class="tag tag-keyword">text-guided</span>
                    
                    <span class="tag tag-keyword">data poisoning</span>
                    
                    <span class="tag tag-keyword">drug discovery</span>
                    
                    <span class="tag tag-keyword">security vulnerability</span>
                    
                    <span class="tag tag-keyword">cheminformatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>