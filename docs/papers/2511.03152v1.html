<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment - Health AI Hub</title>
    <meta name="description" content="This paper introduces a framework for stakeholder-grounded risk assessment of AI systems, leveraging Large Language Models (LLMs) as judges to predict and expla">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03152v1" target="_blank">2511.03152v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Srishti Yadav, Jasmina Gajcin, Erik Miehling, Elizabeth Daly
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03152v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03152v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a framework for stakeholder-grounded risk assessment of AI systems, leveraging Large Language Models (LLMs) as judges to predict and explain risks. It generates stakeholder-specific, interpretable policies highlighting agreements and disagreements on risks, demonstrated across domains including medical AI, and proposes an interactive visualization for conflict reasoning. The findings emphasize that stakeholder perspectives significantly influence risk perception and conflict patterns, advocating for transparent, human-centered AI governance.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework is highly relevant for the responsible and ethical deployment of AI in healthcare, as it provides a structured method to identify, understand, and address the diverse risk perceptions among critical stakeholders like clinicians, patients, administrators, and regulators. By making these varied perspectives explicit, it facilitates the development of safer, more trusted, and effectively governed medical AI systems.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper focuses on an LLM-based framework designed to predict, explain, and manage stakeholder-specific risks associated with AI systems. Its application to 'medical AI' means it addresses how different stakeholders (e.g., patients, clinicians, regulators, developers) perceive and potentially conflict over risks (e.g., diagnostic accuracy, patient safety, data privacy, bias) in AI tools used in healthcare. The framework generates interpretable policies to navigate these conflicts, enhancing transparency and alignment with human-centered AI governance goals specifically within the medical context.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>A novel LLM-based framework is presented for stakeholder-grounded risk assessment of AI systems, employing LLMs as judges to predict and explain risks.</li>
                    
                    <li>The framework utilizes the Risk Atlas Nexus for structured risk modeling and the GloVE explanation method to produce stakeholder-specific, interpretable policies.</li>
                    
                    <li>These policies explicitly show how different stakeholders agree or disagree about identified risks in AI systems.</li>
                    
                    <li>The method is demonstrated using three real-world AI use cases: medical AI, autonomous vehicles, and fraud detection.</li>
                    
                    <li>An interactive visualization is proposed to reveal the 'how' and 'why' conflicts emerge across diverse stakeholder perspectives, enhancing transparency in conflict reasoning.</li>
                    
                    <li>Results indicate a significant influence of stakeholder perspectives on both risk perception and the specific patterns of conflict that arise.</li>
                    
                    <li>The research underscores the necessity of stakeholder-aware explanations for achieving transparent, interpretable, and human-centered AI evaluations and governance.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The framework employs Large Language Models (LLMs) acting as 'judges' to predict and generate explanations for AI system risks. It integrates the Risk Atlas Nexus for systematic risk modeling and a GloVE (Global Vectors for Word Representation, or a similar explanation method) approach to create stakeholder-specific, interpretable policies. These policies articulate areas of agreement and disagreement among stakeholders regarding specific risks. An interactive visualization tool is further proposed to illustrate the emergence and reasoning behind stakeholder conflicts.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary findings reveal that stakeholder perspectives significantly influence both the perception of risks associated with AI systems and the patterns of conflict that emerge from these differing viewpoints. The framework effectively generates transparent, stakeholder-specific policies that elucidate where agreement and disagreement lie, enhancing interpretability of risk assessment.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work has the potential to significantly enhance the clinical integration and governance of medical AI by: (1) fostering greater trust among healthcare providers and patients through transparent risk communication; (2) facilitating more robust ethical reviews and regulatory compliance by explicitly mapping stakeholder concerns; and (3) enabling the design of medical AI systems and deployment protocols that are specifically tailored to mitigate risks identified by diverse clinical and patient communities, ultimately leading to safer and more effective healthcare AI applications.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of the proposed framework. However, potential inherent limitations could include the fidelity of LLMs in accurately simulating complex human stakeholder perspectives and biases, the challenge of grounding LLM-generated explanations in real-world clinical or regulatory contexts, and the scalability of the approach across an exhaustive set of stakeholders and AI use cases.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper emphasizes the critical need for stakeholder-aware explanations to improve the transparency and interpretability of LLM-based evaluations, aligning them with human-centered AI governance goals. This suggests future research directions could involve refining the quality and robustness of these explanations, further developing the interactive visualization for conflict reasoning, and validating the framework's utility and impact in real-world deployment scenarios, particularly within high-stakes environments like healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Patient Safety</span>
                    
                    <span class="tag">Healthcare Policy</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM</span>
                    
                    <span class="tag tag-keyword">Risk Assessment</span>
                    
                    <span class="tag tag-keyword">Stakeholder Conflicts</span>
                    
                    <span class="tag tag-keyword">AI Governance</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Interpretability</span>
                    
                    <span class="tag tag-keyword">Explanatory Policies</span>
                    
                    <span class="tag tag-keyword">Transparency</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>