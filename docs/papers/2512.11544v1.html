<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives - Health AI Hub</title>
    <meta name="description" content="This study systematically evaluated four mainstream Large Language Models (LLMs) for their ability to extract core medical information from noisy clinical narra">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.11544v1" target="_blank">2512.11544v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-12
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yuan Shen, Xiaojun Wu, Linghua Yu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.11544v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.11544v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study systematically evaluated four mainstream Large Language Models (LLMs) for their ability to extract core medical information from noisy clinical narratives, identifying a functional decline termed "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". It revealed that all tested LLMs exhibited significant functional defects, with most experiencing a complete collapse under extreme noise, posing crucial safety warnings for AI application in healthcare.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research provides critical empirical evidence of LLM limitations in handling complex, noisy clinical data, directly impacting patient safety by demonstrating the potential for significant medical misjudgments and functional failures in real-world healthcare applications.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper investigates the application of Large Language Models (LLMs) as auxiliary tools for medical information extraction from unstructured clinical narratives (e.g., patient chief complaints) and their potential role in clinical decision support. It evaluates their performance and limitations in understanding complex medical information, with direct implications for AI tools designed to assist in diagnosis, risk assessment, and patient management within healthcare settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study simulated real-world clinical scenarios using 20 standardized medical probes across five core dimensions to evaluate LLMs.</li>
                    
                    <li>Four mainstream LLMs (GPT-4o, Gemini 2.5, DeepSeek 3.1, Qwen3-Max) were evaluated against gold-standard answers defined by clinical experts and assessed by independent clinicians.</li>
                    
                    <li>All tested models demonstrated functional defects to varying degrees, with Qwen3-Max performing best overall and Gemini 2.5 performing worst.</li>
                    
                    <li>A critical finding was the 'functional collapse' of most models when processing clinical narratives laden with 'extreme noise and redundancy'.</li>
                    
                    <li>GPT-4o, a leading LLM, made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT), highlighting specific safety risks.</li>
                    
                    <li>The research empirically confirms LLMs' 'metabolic dysfunction' in processing clinical information, leading to the novel concept of 'AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)'.</li>
                    
                    <li>These findings serve as a crucial safety warning, emphasizing that current LLMs must be used strictly as auxiliary tools under human expert supervision in healthcare.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study utilized a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs for evaluation. An evaluation system comprising twenty medical probes across five core dimensions simulated a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>All tested LLMs exhibited varying degrees of functional defects in extracting core medical information, with Qwen3-Max showing the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced functional collapse. A severe misjudgment was observed in GPT-4o's risk assessment for PE secondary to DVT. The study innovatively proposed 'AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)' to describe this observed functional decline.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings underscore a significant gap between LLMs' theoretical knowledge and practical clinical application, issuing a crucial safety warning. Clinically, this means LLMs cannot currently be relied upon for autonomous decision-making and must function solely as auxiliary tools under direct human expert supervision to mitigate risks of misjudgment and functional collapse, especially with noisy or complex patient data.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the study design itself. However, it highlights the inherent limitations of current LLMs, which include their susceptibility to 'functional defects' in medical information extraction, 'functional collapse' under conditions of extreme data noise, and the potential for severe medical misjudgments in critical risk assessments (e.g., PE/DVT).</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The research implicitly suggests future directions focused on improving LLM robustness against noisy clinical data, enhancing their reliability for critical medical risk assessments, and developing safer integration strategies for AI in healthcare that strictly prioritize human expert oversight and supervision, thereby bridging the observed gap between theoretical AI capabilities and practical clinical utility.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Patient Safety</span>
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">Medical Diagnostics</span>
                    
                    <span class="tag">AI in Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">AI-MASLD</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Clinical Narratives</span>
                    
                    <span class="tag tag-keyword">Medical Information Extraction</span>
                    
                    <span class="tag tag-keyword">Metabolic Dysfunction</span>
                    
                    <span class="tag tag-keyword">Patient Safety</span>
                    
                    <span class="tag tag-keyword">Artificial Intelligence in Healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>47 pages, 2 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>