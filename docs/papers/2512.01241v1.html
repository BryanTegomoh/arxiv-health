<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>First, do NOHARM: towards clinically safe large language models - Health AI Hub</title>
    <meta name="description" content="This paper introduces NOHARM, a novel benchmark evaluating the clinical safety of 31 large language models (LLMs) using 100 real primary-care-to-specialist cons">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>First, do NOHARM: towards clinically safe large language models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.01241v1" target="_blank">2512.01241v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-01
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> David Wu, Fateme Nateghi Haredasht, Saloni Kumar Maharaj, Priyank Jain, Jessica Tran, Matthew Gwiazdon, Arjun Rustagi, Jenelle Jindal, Jacob M. Koshy, Vinay Kadiyala, Anup Agarwal, Bassman Tappuni, Brianna French, Sirus Jesudasen, Christopher V. Cosgriff, Rebanta Chakraborty, Jillian Caldwell, Susan Ziolkowski, David J. Iberri, Robert Diep, Rahul S. Dalal, Kira L. Newman, Kristin Galetta, J. Carl Pallais, Nancy Wei, Kathleen M. Buchheit, David I. Hong, Ernest Y. Lee, Allen Shih, Vartan Pahalyants, Tamara B. Kaplan, Vishnu Ravi, Sarita Khemani, April S. Liang, Daniel Shirvani, Advait Patil, Nicholas Marshall, Kanav Chopra, Joel Koh, Adi Badhwar, Liam G. McCoy, David J. H. Wu, Yingjie Weng, Sumant Ranji, Kevin Schulman, Nigam H. Shah, Jason Hom, Arnold Milstein, Adam Rodman, Jonathan H. Chen, Ethan Goh
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CY, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.01241v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.01241v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces NOHARM, a novel benchmark evaluating the clinical safety of 31 large language models (LLMs) using 100 real primary-care-to-specialist consultation cases. It reveals that LLMs can generate severely harmful medical recommendations in up to 22.2% of cases, primarily due to errors of omission. The study emphasizes that clinical safety is a distinct performance metric, poorly correlated with existing benchmarks, and highlights the potential of multi-agent approaches for enhanced safety.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>As Large Language Models are increasingly integrated into clinical workflows and direct patient interactions for medical advice, understanding and rigorously validating their clinical safety profiles is paramount to prevent patient harm and foster trust in AI-driven healthcare solutions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper focuses on the application of Large Language Models (LLMs) in generating medical advice and clinical recommendations for both physicians and patients. It specifically evaluates the safety and potential for harm of these AI models when used for clinical decision support or information provision in a healthcare context.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**NOHARM Benchmark:** A new benchmark (Numerous Options Harm Assessment for Risk in Medicine) was developed using 100 real primary-care-to-specialist consultation cases across 10 specialties.</li>
                    
                    <li>**Extensive Annotation:** The benchmark includes 12,747 expert annotations for 4,249 clinical management options to precisely assess harm frequency and severity.</li>
                    
                    <li>**High Harm Rate:** Evaluation of 31 LLMs revealed severe harm in up to 22.2% (95% CI 21.6-22.8%) of cases, with harms of omission constituting 76.6% (95% CI 76.4-76.8%) of all errors.</li>
                    
                    <li>**Distinct Safety Metric:** LLM safety performance showed only a moderate correlation (r = 0.61-0.64) with existing AI and medical knowledge benchmarks, indicating it's a separate, crucial evaluation dimension.</li>
                    
                    <li>**LLM vs. Physician Safety:** The best LLM models demonstrated superior safety performance compared to generalist physicians (mean difference 9.7%, 95% CI 7.0-12.5%).</li>
                    
                    <li>**Multi-agent Improvement:** Employing a diverse multi-agent approach significantly reduced harm compared to solo LLM models (mean difference 8.0%, 95% CI 4.0-12.1%).</li>
                    
                    <li>**Urgency for Explicit Measurement:** The findings underscore the critical need for explicit and dedicated measurement of clinical safety for LLMs used in healthcare, given their potential for severe harm.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study developed NOHARM, a benchmark comprising 100 real primary-care-to-specialist consultation cases spanning 10 medical specialties. These cases involved extensive expert annotation, yielding 12,747 annotations for 4,249 clinical management options, to enable precise measurement of harm frequency and severity. Thirty-one large language models were evaluated on their generated medical recommendations, quantifying severe harm rates, the prevalence of harms of omission, and correlations with existing benchmarks. LLM performance was also compared against generalist physicians and between solo versus diverse multi-agent model approaches.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Up to 22.2% of LLM-generated recommendations resulted in severe harm, with harms of omission accounting for 76.6% of all identified errors. Safety performance showed only a moderate correlation (r = 0.61-0.64) with existing AI and medical knowledge benchmarks. The best LLM models demonstrated superior safety compared to generalist physicians (mean difference 9.7%), and a diverse multi-agent approach further reduced harm by 8.0% compared to solo models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Despite strong performance on general AI benchmarks, current widely used LLMs can produce severely harmful medical advice at non-trivial rates, particularly through errors of omission. This necessitates rigorous, dedicated clinical safety evaluations like NOHARM before LLM deployment in healthcare, and suggests that multi-agent AI systems could be a safer approach for generating medical recommendations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations. However, the specific scope of "100 real primary-care-to-specialist consultation cases" suggests that the findings are most directly applicable to this type of clinical interaction and may require further validation for broader medical contexts such as emergency situations, inpatient care, or direct patient-facing diagnostic support.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper concludes by underscoring that clinical safety is a distinct performance dimension necessitating explicit measurement. This implies future research should focus on developing more comprehensive and robust safety benchmarks, exploring novel mitigation strategies for LLM-generated harm (especially omissions), and integrating continuous safety evaluation into the entire lifecycle of AI development and deployment in medicine.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Primary Care</span>
                    
                    <span class="tag">Specialist Consultations</span>
                    
                    <span class="tag">Various Medical Specialties (10 covered)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Clinical Safety</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Harm Assessment</span>
                    
                    <span class="tag tag-keyword">NOHARM Benchmark</span>
                    
                    <span class="tag tag-keyword">Medical Recommendations</span>
                    
                    <span class="tag tag-keyword">Harms of Omission</span>
                    
                    <span class="tag tag-keyword">AI Evaluation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) are routinely used by physicians and patients for medical advice, yet their clinical safety profiles remain poorly characterized. We present NOHARM (Numerous Options Harm Assessment for Risk in Medicine), a benchmark using 100 real primary-care-to-specialist consultation cases to measure harm frequency and severity from LLM-generated medical recommendations. NOHARM covers 10 specialties, with 12,747 expert annotations for 4,249 clinical management options. Across 31 LLMs, severe harm occurs in up to 22.2% (95% CI 21.6-22.8%) of cases, with harms of omission accounting for 76.6% (95% CI 76.4-76.8%) of errors. Safety performance is only moderately correlated (r = 0.61-0.64) with existing AI and medical knowledge benchmarks. The best models outperform generalist physicians on safety (mean difference 9.7%, 95% CI 7.0-12.5%), and a diverse multi-agent approach reduces harm compared to solo models (mean difference 8.0%, 95% CI 4.0-12.1%). Therefore, despite strong performance on existing evaluations, widely used AI models can produce severely harmful medical advice at nontrivial rates, underscoring clinical safety as a distinct performance dimension necessitating explicit measurement.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>