<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers - Health AI Hub</title>
    <meta name="description" content="SPECTRE introduces a novel, fully transformer-based foundation model for volumetric computed tomography (CT), specifically designed to overcome challenges like ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.17209v1" target="_blank">2511.17209v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-21
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Cris Claessens, Christiaan Viviers, Giacomo D'Amicantonio, Egor Bondarev, Fons van der Sommen
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.17209v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.17209v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">SPECTRE introduces a novel, fully transformer-based foundation model for volumetric computed tomography (CT), specifically designed to overcome challenges like extreme token scaling and geometric anisotropy. It utilizes a hybrid architecture of local and global transformers combined with DINO-style self-distillation and SigLIP-based vision-language pretraining on open CT datasets. This approach yields general-purpose, clinically meaningful CT representations that consistently outperform prior foundation models in zero-shot and fine-tuned settings.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research provides a highly scalable and generalizable AI foundation model for volumetric CT analysis, promising significant improvements in diagnostic accuracy, efficiency, and consistency for radiologists by extracting more robust and clinically relevant features from medical images.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research develops a foundation AI model (SPECTRE) for interpreting and analyzing volumetric CT scans. Such a model can be applied in various healthcare settings to assist radiologists and clinicians in tasks like disease detection, diagnosis, prognosis, treatment planning, and monitoring response to therapy across a wide range of medical conditions (e.g., oncology, cardiology, pulmonology, neurology) by providing advanced computational analysis of medical images.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**SPECTRE Foundation Model:** A fully transformer-based foundation model specifically developed for analyzing volumetric CT scans.</li>
                    
                    <li>**Hybrid Transformer Architecture:** Employs a dual-component design with a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making 3D attention computationally tractable.</li>
                    
                    <li>**Multi-Modal Pretraining:** Combines DINO-style self-distillation, which promotes geometrically consistent features, with SigLIP-based vision-language alignment using paired radiology reports, ensuring clinically meaningful representations.</li>
                    
                    <li>**Open Data Training:** The model is trained exclusively on publicly available CT datasets, demonstrating that high-performing and generalizable representations can be achieved without reliance on private or proprietary data.</li>
                    
                    <li>**Addresses CT Specific Challenges:** Engineered to tackle unique volumetric CT issues, including extreme token scaling, geometric anisotropy, and weak/noisy clinical supervision, which hinder standard deep learning methods.</li>
                    
                    <li>**Superior Performance:** Consistently outperforms previous CT foundation models across multiple benchmarks in both zero-shot evaluation and fine-tuned scenarios.</li>
                    
                    <li>**Generalizable and Meaningful Representations:** Learns robust general-purpose CT representations that are both geometrically consistent (from self-supervision) and clinically meaningful (from vision-language alignment).</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>SPECTRE employs a dual-transformer architecture, comprising a local transformer for fine-grained volumetric feature extraction and a global transformer for capturing whole-scan context, designed to manage the computational demands of large-scale 3D attention. Its pretraining strategy integrates DINO-style self-distillation for learning geometrically consistent features and SigLIP-based vision-language alignment, utilizing paired radiology reports from openly available datasets, to imbue features with clinical relevance. This approach is specifically tailored to address inherent challenges of volumetric CT data, such as anisotropy and extreme token scaling.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>SPECTRE establishes itself as a state-of-the-art foundation model for volumetric CT, demonstrating consistent and superior performance compared to prior models in both zero-shot and fine-tuned evaluations across various CT benchmarks. A key finding is its ability to learn high-performing, generalizable, geometrically consistent, and clinically meaningful CT representations exclusively from openly available data through its innovative transformer architecture and multi-modal pretraining strategy.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The development of SPECTRE could revolutionize clinical radiology by providing a robust, general-purpose AI tool that significantly enhances the automated analysis of CT scans. This could lead to more accurate and earlier disease detection, improved consistency in diagnoses, reduced diagnostic turnaround times, and lessened workload for radiologists, ultimately fostering advancements in patient care and potentially facilitating novel applications in personalized medicine through accessible, high-performing AI models.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly mention any limitations of SPECTRE.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical AI</span>
                    
                    <span class="tag">Computational Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Volumetric CT</span>
                    
                    <span class="tag tag-keyword">Foundation Model</span>
                    
                    <span class="tag tag-keyword">Vision Transformer</span>
                    
                    <span class="tag tag-keyword">Self-Supervised Learning</span>
                    
                    <span class="tag tag-keyword">Vision-Language Pretraining</span>
                    
                    <span class="tag tag-keyword">Radiology</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">3D Attention</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box. The framework jointly optimizes a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making large-scale 3D attention computationally tractable. Notably, SPECTRE is trained exclusively on openly available CT datasets, demonstrating that high-performing, generalizable representations can be achieved without relying on private data. Pretraining combines DINO-style self-distillation with SigLIP-based vision-language alignment using paired radiology reports, yielding features that are both geometrically consistent and clinically meaningful. Across multiple CT benchmarks, SPECTRE consistently outperforms prior CT foundation models in both zero-shot and fine-tuned settings, establishing SPECTRE as a scalable, open, and fully transformer-based foundation model for 3D medical imaging.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>