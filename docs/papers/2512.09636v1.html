<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment - Health AI Hub</title>
    <meta name="description" content="This paper introduces MentraSuite, a unified framework designed to advance reliable mental health reasoning in Large Language Models (LLMs). It proposes MentraB">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.09636v1" target="_blank">2512.09636v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mengxi Xiao, Kailai Yang, Pengde Zhao, Enze Zhang, Ziyan Kuang, Zhiwei Liu, Weiguang Han, Shu Liao, Lianting Huang, Jinpeng Hu, Min Peng, Qianqian Xie, Sophia Ananiadou
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.09636v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.09636v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MentraSuite, a unified framework designed to advance reliable mental health reasoning in Large Language Models (LLMs). It proposes MentraBench, a comprehensive benchmark spanning various reasoning aspects and tasks, and Mindora, a post-trained LLM optimized for faithful and coherent reasoning through a hybrid SFT-RL approach. Mindora achieves superior performance and remarkable reasoning reliability on MentraBench, demonstrating its effectiveness in complex mental health scenarios.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Given that mental health disorders affect hundreds of millions globally and the Web is a primary source of support, this research offers a pathway to scalable and accessible AI assistance. By ensuring LLM reasoning is clinically aligned, consistent, and reliable, MentraSuite reduces risks and enhances the trustworthiness of AI tools for mental health assessment, information, and preliminary support.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves developing and evaluating Large Language Models (LLMs) specifically for assisting in mental health assessment, diagnosis support, intervention planning, and providing reliable information and support for individuals with mental health disorders. This aims to create scalable and accessible AI-powered tools for mental healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical risk of existing LLMs in mental health due to incomplete, inconsistent, or ungrounded reasoning, which limits their utility for appraisal, diagnosis, and intervention planning.</li>
                    
                    <li>Introduces MentraSuite, a unified framework encompassing a benchmark (MentraBench) and a specialized model (Mindora) for enhancing reliable mental-health reasoning.</li>
                    
                    <li>MentraBench is a comprehensive benchmark covering five core reasoning aspects, six tasks, and 13 datasets, designed to evaluate both task performance and five dimensions of reasoning quality: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency.</li>
                    
                    <li>Mindora is a post-trained LLM optimized using a hybrid Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) framework, integrating a novel inconsistency-detection reward mechanism to enforce faithful and coherent reasoning.</li>
                    
                    <li>High-quality reasoning trajectories for training Mindora are generated through a strategic process that filters difficult samples and employs a structured, consistency-oriented rewriting method, producing concise and well-balanced data.</li>
                    
                    <li>Mindora achieved the highest average performance on MentraBench when evaluated against 20 other LLMs, demonstrating its superior capability in mental health reasoning.</li>
                    
                    <li>The model exhibited remarkable performance in reasoning reliability, highlighting its potential for accurate and consistent application in complex mental health contexts.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves the development of MentraSuite, a framework built upon: 1) **MentraBench:** A comprehensive benchmark with five reasoning aspects, six tasks, and 13 datasets, evaluating task performance and five qualitative reasoning dimensions. 2) **Mindora Model:** A post-trained LLM refined via a hybrid Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) framework, using an inconsistency-detection reward to ensure faithful and coherent reasoning. 3) **Data Generation:** A novel strategy for creating high-quality reasoning trajectories, which includes filtering difficult samples and structured, consistency-oriented rewriting for training data.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Mindora consistently achieved the highest average performance on the MentraBench benchmark, outperforming 20 other evaluated LLMs. A crucial finding is its remarkable performance in reasoning reliability, signifying its ability to generate faithful, coherent, and consistent outputs in complex mental health scenarios.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to significantly improve the reliability and accessibility of AI-powered tools in mental health. It could lead to more dependable automated assessment and preliminary diagnostic support, enhance the quality and safety of online mental health information, and potentially serve as a valuable decision-support or screening tool, thereby helping to scale mental health services to address global demand.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the MentraSuite framework or Mindora model.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state any future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Mental Health</span>
                    
                    <span class="tag">Psychiatry</span>
                    
                    <span class="tag">Clinical Psychology</span>
                    
                    <span class="tag">Digital Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Mental Health</span>
                    
                    <span class="tag tag-keyword">Clinical Reasoning</span>
                    
                    <span class="tag tag-keyword">AI Assessment</span>
                    
                    <span class="tag tag-keyword">Benchmarking</span>
                    
                    <span class="tag tag-keyword">Reinforcement Learning</span>
                    
                    <span class="tag tag-keyword">Reliability</span>
                    
                    <span class="tag tag-keyword">Post-Training</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>