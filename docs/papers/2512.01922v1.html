<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding - Health AI Hub</title>
    <meta name="description" content="Med-VCD introduces a novel sparse visual-contrastive decoding method to mitigate hallucinations in medical Large Vision-Language Models (LVLMs) without signific">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.01922v1" target="_blank">2512.01922v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-01
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zahra Mahdavi, Zahra Khodakaramimaghsoud, Hooman Khaloo, Sina Bakhshandeh Taleshani, Erfan Hashemi, Javad Mirzapour Kaleybar, Omid Nejati Manzari
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.01922v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.01922v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">Med-VCD introduces a novel sparse visual-contrastive decoding method to mitigate hallucinations in medical Large Vision-Language Models (LVLMs) without significant inference time overhead. This approach leverages a token-sparsification strategy to select visually informed tokens, thereby enhancing factual accuracy and reducing incorrect outputs in medical applications like visual question answering and report generation. Evaluations across diverse medical datasets show an average 13% increase in factual accuracy and 6% improvement in hallucination accuracy.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Mitigating hallucinations in medical LVLMs is critically important for patient safety and clinical trust. The generation of incorrect or misleading information, even if it appears plausible, could lead to misdiagnosis, inappropriate treatment decisions, or flawed medical reports, directly impacting patient outcomes and professional liability.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is a medical AI application focused on improving the reliability and factual accuracy of Large Vision Language Models (LVLMs) used in healthcare. Specifically, it aims to reduce 'hallucination' (incorrect but plausible outputs) in tasks such as generating medical imaging reports and answering visual questions based on medical images. By making these AI models more trustworthy, Med-VCD enhances their utility and safety for clinical decision support and diagnostic assistance.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Medical LVLMs are highly susceptible to generating hallucinated (plausible but incorrect) outputs, posing significant risks in healthcare applications.</li>
                    
                    <li>Existing hallucination mitigation strategies often involve computationally expensive secondary decoding or rollback procedures, leading to slow inference and potential modality misalignment.</li>
                    
                    <li>Med-VCD proposes a sparse visual-contrastive decoding method specifically designed for medical LVLMs to efficiently mitigate hallucinations.</li>
                    
                    <li>A key innovation is its novel token-sparsification strategy, which dynamically selects visually informed tokens to trim redundancy while preserving critical visual context.</li>
                    
                    <li>This approach effectively reinforces visual evidence during decoding without the time overhead typically associated with other mitigation techniques.</li>
                    
                    <li>The method was rigorously evaluated on eight diverse medical datasets, spanning ophthalmology, radiology, and pathology, for tasks including visual question answering, report generation, and dedicated hallucination benchmarks.</li>
                    
                    <li>Med-VCD demonstrated substantial performance gains, achieving an average increase of 13% in factual accuracy and improving hallucination accuracy by 6% relative to baseline medical LVLMs.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>Med-VCD implements a sparse visual-contrastive decoding method. It incorporates a novel token-sparsification strategy that dynamically identifies and selects visually informed tokens during the decoding process. This on-the-fly selection trims redundant linguistic information while retaining critical visual context, thereby reinforcing visual evidence and reducing the generation of hallucinated content without relying on time-consuming secondary decoding or rollback procedures.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Med-VCD significantly enhances the reliability of medical LVLMs, demonstrating an average increase of 13% in factual accuracy and a 6% improvement in hallucination accuracy. These performance gains were consistently observed across a broad range of medical datasets and tasks, including visual question answering, report generation, and specific hallucination benchmarks within ophthalmology, radiology, and pathology.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By substantially reducing the occurrence of hallucinations, Med-VCD can significantly improve the trustworthiness and utility of AI-powered tools in clinical practice. This will enable medical LVLMs to provide more accurate diagnostic support, generate more reliable imaging reports, and offer safer responses to clinical queries, ultimately leading to better informed clinical decisions, enhanced patient safety, and accelerated adoption of AI in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the limitations of *existing* hallucination mitigation techniques (e.g., slow inference, domain-specificity, modality misalignment). It does not explicitly detail specific limitations or caveats pertaining to the Med-VCD method itself.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state specific future research directions for Med-VCD.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Ophthalmology</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Vision-Language Models</span>
                    
                    <span class="tag tag-keyword">LVLMs</span>
                    
                    <span class="tag tag-keyword">Hallucination Mitigation</span>
                    
                    <span class="tag tag-keyword">Visual Contrastive Decoding</span>
                    
                    <span class="tag tag-keyword">Token Sparsification</span>
                    
                    <span class="tag tag-keyword">Visual Question Answering</span>
                    
                    <span class="tag tag-keyword">Report Generation</span>
                    
                    <span class="tag tag-keyword">Factual Accuracy</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\% and improves hallucination accuracy by 6\% relative to baseline medical LVLMs.</p>
            </section>

            

            
            <section class="paper-section">
                <h2>Journal Reference</h2>
                <p>Computers in Biology and Medicine (2026)</p>
            </section>
            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>