<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-domain Adaptation Networks for Realistic Image Super-resolution - Health AI Hub</title>
    <meta name="description" content="This paper introduces Dual-domain Adaptation Networks (DAN), a novel approach for realistic image super-resolution that efficiently adapts pre-trained models fr">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Dual-domain Adaptation Networks for Realistic Image Super-resolution</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.17217v1" target="_blank">2511.17217v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-21
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Chaowei Fang, Bolin Fu, De Cheng, Lechao Cheng, Guanbin Li
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.85 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.17217v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.17217v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Dual-domain Adaptation Networks (DAN), a novel approach for realistic image super-resolution that efficiently adapts pre-trained models from synthetic to real-world datasets. DAN employs both spatial-domain parameter adaptation, including low-rank adaptation (LoRA), and a unique frequency-domain branch to recover high-frequency details. Experimental evaluations demonstrate the superiority of DAN over existing state-of-the-art models on public realistic SR benchmarks, offering significant implications for applications like medical imaging.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This technology can significantly enhance the quality of medical images obtained from low-resolution acquisitions, potentially improving diagnostic accuracy, enabling visualization of finer anatomical or pathological details, and reducing the need for high-dose or prolonged imaging procedures.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Improving the resolution and quality of various medical images (such as MRI, CT, X-ray, or ultrasound) using advanced deep learning-based super-resolution techniques. This can lead to more precise visualization of anatomical structures or pathological conditions, aiding clinicians in more accurate diagnosis, staging of diseases, treatment efficacy assessment, and potentially reducing the need for invasive procedures or higher radiation doses.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of realistic image super-resolution, which involves complex real-world degradations and limited paired low-resolution/high-resolution (LR-HR) data.</li>
                    
                    <li>Proposes Dual-domain Adaptation Networks (DAN) to efficiently adapt pre-trained super-resolution models, leveraging prior knowledge from large-scale synthetic datasets.</li>
                    
                    <li>Implements a spatial-domain adaptation strategy that selectively updates parameters of pre-trained models and utilizes low-rank adaptation (LoRA) for efficient adjustment of frozen parameters.</li>
                    
                    <li>Integrates a novel frequency-domain adaptation branch that combines spectral data of the input with intermediate features from the spatial backbone to infer high-resolution frequency maps, enhancing detail recovery.</li>
                    
                    <li>Aims to improve generalization, accelerate training, and reduce the dependency on extensive real-world paired data for realistic SR tasks.</li>
                    
                    <li>Achieves superior performance over existing state-of-the-art models on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR.</li>
                    
                    <li>The method is particularly relevant for applications like medical imaging, where high-fidelity reconstruction from low-quality inputs is crucial for diagnosis and analysis.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The Dual-domain Adaptation Networks (DAN) adapt pre-trained SR models by first employing a spatial-domain adaptation that selectively updates certain parameters and uses low-rank adaptation (LoRA) for efficient tuning of frozen parameters. Complementing this, a frequency-domain adaptation branch is incorporated, which takes spectral data of the input and intermediate features from the spatial backbone to infer high-resolution frequency maps, thereby explicitly recovering high-frequency components crucial for image detail and sharpness.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The experimental evaluations on public realistic image SR benchmarks (RealSR, D2CRealSR, DRealSR) consistently demonstrated that the proposed Dual-domain Adaptation Networks (DAN) outperform existing state-of-the-art models. This superiority validates the effectiveness of the dual-domain adaptation strategy in handling complex real-world degradation patterns and efficiently leveraging prior knowledge.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The ability to produce high-resolution medical images from lower-resolution inputs could have transformative clinical impact. It could lead to reduced scanning times (e.g., faster MRI), lower radiation exposure (e.g., lower-dose CT/X-ray), and enhanced detail in real-time imaging (e.g., ultrasound). This could enable earlier and more accurate disease detection, better quantification of pathologies, improved surgical planning, and potentially facilitate novel diagnostic pathways by making high-quality imaging more accessible and safer.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the problem of limited real-world LR-HR data, which the proposed method aims to solve, rather than explicitly detailing limitations of the DAN method itself. However, an implicit limitation could be its continued, albeit reduced, dependency on some real-world data for the adaptation process. The computational overhead of integrating a dual-domain approach and its real-time processing capabilities in a clinical setting are not discussed in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions. However, potential future work could include exploring the application of DAN to specific medical imaging modalities, investigating its robustness to various types of imaging artifacts, optimizing the balance between spatial and frequency domain adaptation, or extending the framework for 3D medical image super-resolution.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Ophthalmology</span>
                    
                    <span class="tag">Microscopy</span>
                    
                    <span class="tag">Ultrasound</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Image Super-resolution</span>
                    
                    <span class="tag tag-keyword">Domain Adaptation</span>
                    
                    <span class="tag tag-keyword">Realistic SR</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Low-rank Adaptation</span>
                    
                    <span class="tag tag-keyword">Frequency Domain</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>