<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare - Health AI Hub</title>
    <meta name="description" content="This paper investigates how contradictory medical evidence in source documents degrades Retrieval-Augmented Generation (RAG) performance in healthcare. It estab">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.06668v1" target="_blank">2511.06668v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Saeedeh Javadi, Sara Mirabi, Manan Gangar, Bahadorreza Ofoghi
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.IR, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.06668v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.06668v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper investigates how contradictory medical evidence in source documents degrades Retrieval-Augmented Generation (RAG) performance in healthcare. It establishes a novel benchmark using Australian TGA consumer medicine information documents and temporally stratified PubMed abstracts to evaluate five LLMs. The study finds that contradictions between highly similar abstracts significantly reduce LLM consistency and factual accuracy, underscoring the necessity for advanced contradiction-aware filtering strategies in high-stakes domains.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>In healthcare, accurate and up-to-date information is critical for patient safety, clinical decision-making, and consumer health literacy. This research directly addresses the risk of large language models providing misinformation when RAG systems encounter conflicting medical evidence, highlighting a crucial barrier to their safe and reliable adoption in clinical and patient-facing applications.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper focuses on improving the safety and reliability of Retrieval-Augmented Generation (RAG) systems powered by Large Language Models (LLMs) for healthcare applications. Specifically, it addresses the critical issue of LLMs generating misinformation or inaccurate responses to medicine-related queries when presented with contradictory or outdated medical information, aiming to develop methods for 'contradiction-aware filtering strategies' to ensure trustworthy AI-driven information in healthcare settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>RAG is proposed as a mitigation strategy for LLM hallucinations in healthcare, grounding outputs in external domain-specific documents.</li>
                    
                    <li>A critical challenge for RAG is introduced when source documents contain outdated or contradictory information, potentially leading to errors.</li>
                    
                    <li>A novel benchmark dataset was created by repurposing headings from Australian Therapeutic Goods Administration (TGA) consumer medicine information documents as natural language questions.</li>
                    
                    <li>PubMed abstracts were retrieved for these questions, stratified across multiple publication years, to enable controlled temporal evaluation of outdated or conflicting evidence.</li>
                    
                    <li>Comparative analysis of five LLMs demonstrated that contradictions between highly similar abstracts degrade RAG performance, leading to inconsistencies and reduced factual accuracy in generated responses.</li>
                    
                    <li>The findings indicate that retrieval similarity alone is insufficient for ensuring reliable medical RAG responses, especially in high-stakes healthcare contexts.</li>
                    
                    <li>The research highlights an urgent need for the development and implementation of contradiction-aware filtering strategies to enhance the trustworthiness of RAG in medicine.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved three main steps: i) creating a benchmark dataset by converting headings from Australian TGA consumer medicine information documents into natural language questions; ii) retrieving PubMed abstracts for these questions, deliberately stratifying them by publication year to introduce temporal inconsistencies and potential contradictions; and iii) conducting a comparative analysis of five different LLMs to assess how they integrate and reconcile this temporally inconsistent information, evaluating the frequency and impact of contradictions on response consistency and factual accuracy.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is that contradictions present in highly similar retrieved source documents significantly degrade the performance of Retrieval-Augmented Generation (RAG) models. This degradation manifests as increased inconsistencies and reduced factual accuracy in the LLM-generated responses to medicine-related queries, indicating that current RAG mechanisms based solely on retrieval similarity are inadequate for reliable medical information synthesis.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Unreliable RAG systems in healthcare could lead to the dissemination of incorrect or outdated medical advice, potentially resulting in suboptimal treatment decisions, medication errors, or patient confusion. The findings underscore that for RAG to be safely deployed in clinical decision support, patient education, or drug information systems, robust mechanisms are essential to detect, reconcile, and present conflicting evidence accurately, thereby safeguarding patient outcomes and maintaining trust in AI-driven healthcare tools.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The study strongly implies a future research direction towards developing and implementing robust 'contradiction-aware filtering strategies' for RAG systems in high-stakes domains like healthcare. This would involve moving beyond simple retrieval similarity to build systems capable of identifying, evaluating, and intelligently handling contradictory evidence to ensure trustworthy and factually accurate responses.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Drug Safety</span>
                    
                    <span class="tag">Evidence-Based Medicine</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Consumer Health Information</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Retrieval-Augmented Generation</span>
                    
                    <span class="tag tag-keyword">medical RAG</span>
                    
                    <span class="tag tag-keyword">contradictory evidence</span>
                    
                    <span class="tag tag-keyword">temporal inconsistency</span>
                    
                    <span class="tag tag-keyword">factual accuracy</span>
                    
                    <span class="tag tag-keyword">consumer medicine information</span>
                    
                    <span class="tag tag-keyword">large language models</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">In high-stakes information domains such as healthcare, where large language
models (LLMs) can produce hallucinations or misinformation, retrieval-augmented
generation (RAG) has been proposed as a mitigation strategy, grounding model
outputs in external, domain-specific documents. Yet, this approach can
introduce errors when source documents contain outdated or contradictory
information. This work investigates the performance of five LLMs in generating
RAG-based responses to medicine-related queries. Our contributions are
three-fold: i) the creation of a benchmark dataset using consumer medicine
information documents from the Australian Therapeutic Goods Administration
(TGA), where headings are repurposed as natural language questions, ii) the
retrieval of PubMed abstracts using TGA headings, stratified across multiple
publication years, to enable controlled temporal evaluation of outdated
evidence, and iii) a comparative analysis of the frequency and impact of
outdated or contradictory content on model-generated responses, assessing how
LLMs integrate and reconcile temporally inconsistent information. Our findings
show that contradictions between highly similar abstracts do, in fact, degrade
performance, leading to inconsistencies and reduced factual accuracy in model
answers. These results highlight that retrieval similarity alone is
insufficient for reliable medical RAG and underscore the need for
contradiction-aware filtering strategies to ensure trustworthy responses in
high-stakes domains.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>