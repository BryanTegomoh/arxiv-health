<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel semi-supervised multi-modal framework designed to enhance brain tumor segmentation by effectively leveraging complementary informa">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.09801v1" target="_blank">2512.09801v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Tien-Dat Chung, Ba-Thinh Lam, Thanh-Huy Nguyen, Thien Nguyen, Nguyen Lan Vi Vu, Hoang-Loc Cao, Phat Kim Huynh, Min Xu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.09801v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.09801v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel semi-supervised multi-modal framework designed to enhance brain tumor segmentation by effectively leveraging complementary information across different MRI sequences. It addresses limitations in existing methods by proposing a Modality-specific Enhancing Module (MEM) and a Complementary Information Fusion (CIF) module. The framework significantly outperforms strong baselines on the BraTS 2019 dataset, especially under very limited labeled data conditions, demonstrating improved Dice and Sensitivity scores.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for advancing automated brain tumor segmentation, which directly impacts diagnostic accuracy, surgical planning, and radiation therapy in oncology. By enabling high performance with limited labeled data, it addresses a major bottleneck in medical AI deployment, making advanced tools more accessible and practical in clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development of a semi-supervised multi-modal deep learning framework for robust and accurate automated segmentation of brain tumors from MRI scans. This technology aims to assist clinicians in precise diagnosis, surgical planning, radiation therapy planning, and monitoring treatment response, especially in scenarios with limited labeled medical data.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of semantic discrepancies and misalignment in existing semi-supervised multi-modal medical image segmentation approaches.</li>
                    
                    <li>Proposes a Modality-specific Enhancing Module (MEM) that strengthens unique semantic cues for each MRI modality using channel-wise attention.</li>
                    
                    <li>Introduces a learnable Complementary Information Fusion (CIF) module to adaptively exchange and fuse complementary knowledge between different modalities.</li>
                    
                    <li>Optimizes the framework using a hybrid objective combining supervised segmentation loss on labeled data and cross-modal consistency regularization on unlabeled data.</li>
                    
                    <li>Evaluated on the BraTS 2019 (HGG subset) dataset, demonstrating consistent and significant outperformance against strong semi-supervised and multi-modal baselines.</li>
                    
                    <li>Achieves notable improvements in both Dice and Sensitivity scores, particularly under scarce supervision settings (1%, 5%, and 10% labeled data).</li>
                    
                    <li>Ablation studies confirm the individual and complementary contributions of MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed framework integrates a Modality-specific Enhancing Module (MEM) that employs channel-wise attention to strengthen unique semantic representations within each modality, and a Complementary Information Fusion (CIF) module designed for adaptive cross-modal knowledge exchange. The entire system is trained using a hybrid objective function, which combines a standard supervised segmentation loss for labeled data with a cross-modal consistency regularization term applied to unlabeled data.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The method consistently surpasses strong semi-supervised and multi-modal baselines across 1%, 5%, and 10% labeled data settings on the BraTS 2019 HGG subset. It achieves significant improvements in both Dice and Sensitivity scores. Ablation studies confirmed that both MEM and CIF modules are critical, working complementarily to address cross-modality discrepancies and enhance segmentation robustness under limited supervision.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This approach has the potential to significantly improve the efficiency and accuracy of brain tumor segmentation in clinical practice. By reducing the reliance on extensive, costly, and time-consuming manual expert annotations, it can accelerate the development and deployment of robust AI-powered tools for diagnosis, treatment planning (e.g., surgical resection margins, radiation dose planning), and monitoring of treatment response, ultimately leading to better patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any specific limitations of the proposed method or its evaluation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neuro-oncology</span>
                    
                    <span class="tag">Diagnostic Radiology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Neurosurgery</span>
                    
                    <span class="tag">Radiation Oncology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">semi-supervised learning</span>
                    
                    <span class="tag tag-keyword">multi-modal imaging</span>
                    
                    <span class="tag tag-keyword">brain tumor segmentation</span>
                    
                    <span class="tag tag-keyword">MRI</span>
                    
                    <span class="tag tag-keyword">channel-wise attention</span>
                    
                    <span class="tag tag-keyword">cross-modal consistency</span>
                    
                    <span class="tag tag-keyword">BraTS</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>9 pages, 3 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>