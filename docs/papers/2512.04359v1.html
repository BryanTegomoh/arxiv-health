<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning - Health AI Hub</title>
    <meta name="description" content="This paper introduces an efficient reinforcement learning framework for large language models (LLMs) to combat entropy collapse, a common issue in RLVR that lim">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04359v1" target="_blank">2512.04359v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Hongye Cao, Zhixin Bai, Ziyue Peng, Boyan Wang, Tianpei Yang, Jing Huo, Yuyao Zhang, Yang Gao
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.70 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04359v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04359v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces an efficient reinforcement learning framework for large language models (LLMs) to combat entropy collapse, a common issue in RLVR that limits reasoning capabilities. By leveraging entropy signals at both semantic and token levels through a novel data organization and algorithmic design, the method enhances policy exploration and significantly improves LLM reasoning performance.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>While a foundational AI paper, improved reasoning capabilities in Large Language Models directly translate to better performance in medical applications, where complex and accurate reasoning is paramount for tasks such as clinical decision support, medical diagnostics, scientific literature analysis, and drug discovery, leading to more reliable AI tools in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Enhancing LLMs' ability to reason through complex medical cases, analyze patient symptoms and history for potential diagnoses, evaluate various treatment protocols, synthesize information from vast medical literature, or contribute to scientific reasoning in drug discovery and basic biomedical research.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the problem of entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR) for LLMs, which hinders policy exploration and limits reasoning.</li>
                    
                    <li>Proposes a novel framework that utilizes entropy signals at both the semantic (meaning) and token (word/subword unit) levels to improve LLM reasoning.</li>
                    
                    <li>Introduces semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to facilitate progressive optimization from easier to harder tasks.</li>
                    
                    <li>Employs a non-uniform token treatment algorithmic design, applying KL regularization to low-entropy tokens critical for policy exploration and stronger constraints on their high-covariance portions.</li>
                    
                    <li>The method achieves joint optimization of data organization and algorithmic design to effectively mitigate entropy collapse.</li>
                    
                    <li>Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate superior performance compared to other entropy-based approaches in improving reasoning.</li>
                    
                    <li>Enhances LLM reasoning capabilities, making them potentially more robust and reliable for complex tasks.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The proposed method integrates two main strategies: 1) **Semantic Entropy-Guided Curriculum Learning:** Training data is structured based on its semantic entropy, allowing for progressive optimization from less complex (low entropy) to more complex (high entropy) reasoning tasks. 2) **Non-Uniform Token Treatment:** At the algorithmic level, KL regularization is specifically applied to low-entropy tokens identified as critical for policy exploration, with stronger constraints imposed on high-covariance portions within these tokens. This dual-level optimization of data and algorithm is designed to effectively manage and leverage entropy signals.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The framework successfully mitigates entropy collapse, leading to a significant enhancement in LLM reasoning capabilities. Empirical validation across 6 distinct benchmarks and using 3 different parameter-scale base models consistently demonstrates that the proposed method outperforms other existing entropy-based approaches in improving reasoning performance.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The enhanced reasoning capabilities of LLMs resulting from this research can indirectly lead to substantial clinical impact. More robust and accurate medical LLMs could improve the precision of diagnostic systems, provide more reliable evidence-based treatment recommendations, better interpret complex patient data and medical literature, and ultimately empower healthcare professionals with more sophisticated and trustworthy AI assistants, contributing to improved patient care and outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the proposed method itself. It highlights the limitation of prior RLVR methods (entropy collapse) which this work aims to address.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions for this specific work.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Medical Diagnostics</span>
                    
                    <span class="tag">Drug Discovery and Development</span>
                    
                    <span class="tag">Biomedical Research</span>
                    
                    <span class="tag">Health Information Processing</span>
                    
                    <span class="tag">Medical Education and Training</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Reinforcement Learning</span>
                    
                    <span class="tag tag-keyword">LLM Reasoning</span>
                    
                    <span class="tag tag-keyword">Entropy Collapse</span>
                    
                    <span class="tag tag-keyword">Semantic Entropy</span>
                    
                    <span class="tag tag-keyword">Token Entropy</span>
                    
                    <span class="tag tag-keyword">Curriculum Learning</span>
                    
                    <span class="tag tag-keyword">KL Regularization</span>
                    
                    <span class="tag tag-keyword">Policy Exploration</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>