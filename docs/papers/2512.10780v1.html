<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting - Health AI Hub</title>
    <meta name="description" content="This paper evaluates the performance of Large Language Models (LLMs) in maternal and newborn healthcare triage when processing Indian languages presented in nat">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.10780v1" target="_blank">2512.10780v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-11
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Manurag Khullar, Utkarsh Desai, Poorva Malviya, Aman Dalmia, Zheyuan Ryan Shi
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.10780v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.10780v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper evaluates the performance of Large Language Models (LLMs) in maternal and newborn healthcare triage when processing Indian languages presented in native versus romanized scripts. It uncovers a consistent and significant degradation (5-12 F1 points) in LLM performance for romanized messages compared to native scripts, using a real-world dataset. Crucially, this performance gap is attributed not to a failure in clinical reasoning or semantic understanding, but to brittle classification outputs despite the LLMs inferring the correct intent.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research highlights a critical safety blind spot in LLM-based health systems, particularly for multilingual populations in India. It directly impacts patient safety and equitable access to care by demonstrating that seemingly intelligent AI models can fail to reliably act on correctly understood medical queries, potentially leading to mis-triage and adverse health outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Evaluating and improving the safety and reliability of LLM-based systems for clinical triage and patient communication in healthcare settings, particularly in contexts involving diverse linguistic and script variations. This directly relates to AI as a clinical decision support tool and for health system automation.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLMs are increasingly deployed in high-stakes clinical applications in India.</li>
                    
                    <li>Indian language speakers frequently use romanized text, but this orthographic variation is rarely evaluated with real-world data.</li>
                    
                    <li>The study benchmarks leading LLMs on a real-world dataset of user queries in maternal and newborn healthcare triage, spanning five Indian languages and Nepali.</li>
                    
                    <li>Results show a consistent performance degradation for romanized messages, with F1 scores trailing native scripts by 5-12 points.</li>
                    
                    <li>This performance gap could lead to approximately 2 million excess triage errors annually at a partner maternal health organization.</li>
                    
                    <li>The issue is not a failure in clinical reasoning or semantic intent inference; LLMs often understand romanized queries correctly.</li>
                    
                    <li>The primary problem lies in the brittleness of LLMs' final classification outputs in the presence of orthographic noise in romanized inputs, even when semantic understanding is evident.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employed a benchmarking approach, evaluating leading Large Language Models (LLMs) on a real-world dataset of user-generated queries. This dataset covered maternal and newborn healthcare triage and included messages in five Indian languages and Nepali, presented in both their native scripts and romanized forms. Performance was assessed primarily using F1 scores. The authors also conducted an analysis to differentiate whether performance degradation was due to a failure in clinical reasoning/semantic understanding or brittle classification despite understanding.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>LLMs exhibit a consistent and substantial degradation in performance (5-12 F1 points) when processing romanized messages compared to native script inputs for healthcare triage. This gap is significant enough to cause millions of excess errors in real-world clinical settings. Crucially, the performance deficit is not due to LLMs misunderstanding the semantic intent or failing in clinical reasoning, but rather their final classification outputs being brittle and unreliable in the presence of orthographic noise from romanized inputs, even when the underlying meaning is inferred correctly.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings underscore a significant risk for the deployment of LLM-based health systems, particularly in regions like India where romanization is common. It implies a potential for increased rates of mis-triage, delayed care, or inappropriate health advice for patients communicating via romanized text. Healthcare providers and developers must be aware that perceived 'understanding' by an LLM does not guarantee reliable clinical action, necessitating rigorous, context-specific evaluation and robust engineering to ensure patient safety and equitable care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract highlights a key limitation of current LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably due to brittle classification outputs in the presence of orthographic noise. The general lack of existing research evaluating this orthographic variation with real-world data is also noted as a broader systemic limitation in LLM development for multilingual contexts.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Implicitly, the research suggests a need for developing more robust LLMs that can reliably translate inferred semantic understanding from orthographically noisy (e.g., romanized) inputs into accurate final classification outputs. Future work should focus on strategies to mitigate this 'safety blind spot' and improve the reliability of LLM-based health systems in handling diverse linguistic and orthographic variations in real-world clinical settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Maternal and Newborn Healthcare</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Triage</span>
                    
                    <span class="tag tag-keyword">Romanization</span>
                    
                    <span class="tag tag-keyword">Indian Languages</span>
                    
                    <span class="tag tag-keyword">Maternal Health</span>
                    
                    <span class="tag tag-keyword">Newborn Health</span>
                    
                    <span class="tag tag-keyword">Orthographic Variation</span>
                    
                    <span class="tag tag-keyword">F1 Score</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>