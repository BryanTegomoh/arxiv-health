<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting - Health AI Hub</title>
    <meta name="description" content="This paper investigates the impact of romanized text input on Large Language Model (LLM) performance in maternal and newborn healthcare triage for Indian langua">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.10780v1" target="_blank">2512.10780v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-11
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Manurag Khullar, Utkarsh Desai, Poorva Malviya, Aman Dalmia, Zheyuan Ryan Shi
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.10780v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.10780v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper investigates the impact of romanized text input on Large Language Model (LLM) performance in maternal and newborn healthcare triage for Indian languages. It reveals a consistent and significant degradation in LLM accuracy (5-12 points F1 score) when processing romanized messages compared to native scripts, leading to a projected 2 million excess errors at a partner organization. The study highlights that this performance gap stems from the brittleness of LLM classification outputs to orthographic noise, even when the models correctly infer the semantic intent of the romanized queries.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research identifies a critical safety blind spot in LLM-based health systems, directly impacting the reliability of clinical triage in high-stakes domains like maternal and newborn healthcare. Failure to address this can lead to millions of preventable errors, posing significant risks to patient safety and care quality, especially in diverse linguistic contexts like India.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the use of Large Language Models (LLMs) for automated or semi-automated triage of patient queries, particularly in maternal and newborn healthcare settings. This involves processing user-generated text (potentially in diverse languages and scripts) to provide critical health guidance and determine urgency, thereby impacting healthcare access, efficiency, and safety for patients.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLMs are increasingly used in high-stakes clinical applications in India, including triage.</li>
                    
                    <li>Speakers of Indian languages frequently use romanized text, but this orthographic variation is rarely evaluated with real-world data.</li>
                    
                    <li>The study benchmarks leading LLMs on a real-world dataset of user-generated queries in five Indian languages and Nepali, focusing on maternal and newborn healthcare triage.</li>
                    
                    <li>Results show a consistent degradation in LLM performance for romanized messages, with F1 scores 5-12 points lower than those for native scripts.</li>
                    
                    <li>This performance gap could translate to approximately 2 million excess errors in triage at a partner maternal health organization in India.</li>
                    
                    <li>Crucially, the performance gap is not due to a failure in clinical reasoning; LLMs often correctly infer the semantic intent of romanized queries.</li>
                    
                    <li>The final classification outputs of LLMs are brittle and susceptible to orthographic noise present in romanized inputs.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved benchmarking leading Large Language Models (LLMs) on a real-world dataset of user-generated queries. This dataset covered five Indian languages and Nepali, presented in both native scripts and romanized forms. The evaluation focused on the task of maternal and newborn healthcare triage, comparing the F1 scores between native script and romanized inputs. Additionally, the methodology involved analyzing whether LLMs correctly inferred semantic intent even when final classification outputs were brittle.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is a consistent and significant degradation in LLM performance for romanized messages, with F1 scores trailing native scripts by 5-12 points. This performance gap is projected to cause nearly 2 million excess triage errors at a partner organization. While LLMs often correctly infer the semantic intent of romanized queries, their final classification outputs demonstrate brittleness due to orthographic noise in the romanized inputs, indicating a failure in reliable action rather than understanding.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings highlight a critical safety concern for LLM deployment in clinical settings, particularly in multilingual regions where romanization is common. It implies that current LLM-based triage systems for maternal and newborn health in India may be significantly less reliable for users communicating in romanized scripts, potentially leading to misclassification of urgency, delayed care, or incorrect medical advice. This necessitates a re-evaluation of LLM safety and efficacy for real-world clinical applications and calls for robust solutions to handle orthographic variations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the study. However, it implicitly points to the limitation of current LLMs in handling orthographic variation reliably in real-world clinical settings.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions. However, the findings strongly imply the need for future work to develop and evaluate LLMs that are more robust to orthographic noise in romanized inputs, ensuring reliable classification even when semantic intent is correctly inferred, especially for high-stakes clinical applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Maternal Healthcare</span>
                    
                    <span class="tag">Newborn Healthcare</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Public Health (Digital Health Systems)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM</span>
                    
                    <span class="tag tag-keyword">Triage</span>
                    
                    <span class="tag tag-keyword">Indian Languages</span>
                    
                    <span class="tag tag-keyword">Romanization</span>
                    
                    <span class="tag tag-keyword">Native Scripts</span>
                    
                    <span class="tag tag-keyword">Maternal Health</span>
                    
                    <span class="tag tag-keyword">Newborn Health</span>
                    
                    <span class="tag tag-keyword">Clinical Applications</span>
                    
                    <span class="tag tag-keyword">Orthography</span>
                    
                    <span class="tag tag-keyword">Performance Degradation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>