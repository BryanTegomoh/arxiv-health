<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting - Health AI Hub</title>
    <meta name="description" content="This paper evaluates the performance of Large Language Models (LLMs) in maternal and newborn healthcare triage when processing Indian languages, specifically co">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.10780v1" target="_blank">2512.10780v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-11
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Manurag Khullar, Utkarsh Desai, Poorva Malviya, Aman Dalmia, Zheyuan Ryan Shi
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.10780v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.10780v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper evaluates the performance of Large Language Models (LLMs) in maternal and newborn healthcare triage when processing Indian languages, specifically comparing native scripts to romanized text. It reveals a consistent and significant degradation in LLM performance (5-12 F1 points lower) for romanized inputs, which could lead to millions of excess triage errors in real-world clinical settings. Crucially, the study finds that LLMs often correctly infer the semantic intent of romanized queries but fail to reliably act on it in final classification due to orthographic noise.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critically important for patient safety in digital health initiatives, especially in multilingual regions like India, as it uncovers a significant reliability gap in LLM-based triage systems when patients communicate using common romanized scripts, potentially leading to misdiagnosis or delayed care in maternal and newborn health.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research evaluates the performance and safety of Large Language Models (LLMs) when used for clinical triage in maternal and newborn healthcare settings. It specifically examines how language script variations (native vs. romanized text) affect the accuracy and reliability of these AI models, highlighting potential risks to patient care and health outcomes in real-world clinical deployments.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLMs are increasingly deployed in high-stakes clinical applications in India, but their performance with orthographic variation (native vs. roman scripts) is rarely evaluated using real-world data.</li>
                    
                    <li>The study benchmarks leading LLMs on a real-world dataset of user-generated queries related to maternal and newborn healthcare triage.</li>
                    
                    <li>The dataset spans five Indian languages and Nepali, comparing LLM performance on native scripts versus romanized text.</li>
                    
                    <li>Results show a consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points.</li>
                    
                    <li>This performance gap is significant, potentially causing nearly 2 million excess errors in triage annually at a partner maternal health organization in India.</li>
                    
                    <li>The performance gap is not due to a failure in clinical reasoning; LLMs often correctly infer the semantic intent of romanized queries.</li>
                    
                    <li>The issue lies in the brittleness of LLMs' final classification outputs in the presence of orthographic noise in romanized inputs, highlighting a critical safety blind spot.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employed a benchmarking approach, evaluating leading LLMs on a real-world dataset of user-generated queries. This dataset comprised inputs in five Indian languages and Nepali, presented in both native and romanized scripts. The specific task was maternal and newborn healthcare triage. Performance was measured using F1 scores to compare the reliability of LLM classification outputs across the two script types.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is a consistent and significant performance degradation of LLMs when processing romanized messages, with F1 scores 5-12 points lower compared to native scripts. This gap translates to a potential for millions of excess triage errors in a real-world clinical setting. A crucial insight is that while LLMs often correctly infer the semantic intent of romanized queries, their final classification outputs remain brittle and unreliable due to orthographic noise.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings highlight a critical safety vulnerability in deploying LLM-based health systems in multilingual, digitally evolving contexts. Inaccurate triage due to romanization could lead to inappropriate or delayed medical advice for pregnant women and newborns, potentially resulting in adverse health outcomes. It necessitates a re-evaluation of current LLM deployment strategies and emphasizes the urgent need for models robust to orthographic variations to ensure equitable and safe healthcare access.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the study itself. However, it implicitly points to a critical limitation of current LLMs: their brittleness in converting correctly inferred semantic intent from noisy romanized inputs into reliable final classification outputs, representing a 'safety blind spot' in their clinical application.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly stated, the findings strongly imply future research should focus on improving the robustness of LLMs to orthographic noise and romanization. This includes developing techniques for more reliable classification outputs even when semantic intent is correctly inferred from noisy inputs, and potentially pre-processing or fine-tuning strategies tailored for diverse orthographic variations in clinical language processing.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Maternal Health</span>
                    
                    <span class="tag">Newborn Health</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Digital Health</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Triage</span>
                    
                    <span class="tag tag-keyword">Indian Languages</span>
                    
                    <span class="tag tag-keyword">Romanization</span>
                    
                    <span class="tag tag-keyword">Native Scripts</span>
                    
                    <span class="tag tag-keyword">Maternal Healthcare</span>
                    
                    <span class="tag tag-keyword">Newborn Healthcare</span>
                    
                    <span class="tag tag-keyword">Orthographic Variation</span>
                    
                    <span class="tag tag-keyword">Clinical Safety</span>
                    
                    <span class="tag tag-keyword">Natural Language Processing</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>