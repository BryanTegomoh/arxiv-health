<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation - Health AI Hub</title>
    <meta name="description" content="FlexICL is a novel visual in-context learning (ICL) framework designed for efficient segmentation of bony regions in pediatric elbow and wrist ultrasound images">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26049v1" target="_blank">2510.26049v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yuyue Zhou, Jessica Knight, Shrimanti Ghosh, Banafshe Felfeliyan, Jacob L. Jaremko, Abhilash R. Hareendranathan
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26049v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26049v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">FlexICL is a novel visual in-context learning (ICL) framework designed for efficient segmentation of bony regions in pediatric elbow and wrist ultrasound images, addressing the challenge of scarce expert annotations. By employing innovative concatenation methods and data augmentation, it achieves robust performance requiring only 5% of training data. The framework significantly outperforms state-of-the-art ICL and conventional segmentation models, demonstrating its potential as a scalable solution for medical imaging.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Automatic segmentation of musculoskeletal structures in ultrasound can significantly improve diagnostic accuracy for pediatric elbow and wrist fractures and aid in treatment planning. This framework helps overcome the barrier of expert annotation scarcity, making deep learning tools more accessible for real-time feedback and confident interpretation by lightly trained medical users.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>FlexICL is a deep learning framework for automated segmentation of bony regions in elbow and wrist ultrasound images. Its application is to assist in the diagnosis of fractures, particularly in pediatric populations, by providing real-time feedback and highlighting key structures. This aims to improve diagnostic accuracy, aid in treatment planning, and enable less-trained users to perform ultrasound exams more confidently, thereby enhancing the efficiency and accessibility of medical imaging diagnostics.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of high cost and time for expert pixel-wise annotations needed to train deep learning models for musculoskeletal ultrasound segmentation, particularly for common pediatric elbow and wrist fractures.</li>
                    
                    <li>Proposes FlexICL, a novel and flexible visual in-context learning (ICL) framework specifically developed for automatic segmentation of bony regions in ultrasound images.</li>
                    
                    <li>Operates in an intra-video segmentation setting, requiring expert annotations for only a small subset of frames (5% of the training images) to accurately segment unseen frames.</li>
                    
                    <li>Introduces novel image concatenation techniques and integrates multiple augmentation strategies, systematically investigated to significantly enhance visual ICL performance with limited labeled data.</li>
                    
                    <li>Achieves robust segmentation performance across four distinct wrist and elbow US datasets, evaluated on 1,252 US sweeps.</li>
                    
                    <li>Demonstrates superior performance, outperforming state-of-the-art visual ICL models (Painter, MAE-VQGAN) and conventional segmentation models (U-Net, TransUNet) by 1-27% in Dice coefficient.</li>
                    
                    <li>Highlights the potential of FlexICL as an efficient and scalable solution for US image segmentation, particularly well-suited for medical imaging applications where labeled data is scarce.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>FlexICL employs a visual in-context learning (ICL) framework. It systematically investigates and introduces novel image concatenation techniques to effectively integrate contextual information from a few expert-annotated example frames with unseen frames for segmentation. The framework also incorporates multiple augmentation strategies during training to enhance robustness, enabling it to learn and perform robustly with a significantly reduced (5%) set of labeled training images in an intra-video segmentation setting.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>FlexICL achieved robust segmentation performance across four wrist and elbow ultrasound datasets, requiring only 5% of the total training images. It quantitatively outperformed state-of-the-art visual ICL models (Painter, MAE-VQGAN) and conventional segmentation models (U-Net, TransUNet) by a Dice coefficient margin of 1-27% across 1,252 US sweeps.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research provides a highly efficient and scalable solution for automating musculoskeletal ultrasound image segmentation, drastically reducing the dependency on extensive manual annotations. Clinically, it could facilitate the real-time identification and highlighting of bony structures during ultrasound examinations, thereby improving diagnostic accuracy for pediatric fractures, especially for clinicians with less specialized training, and streamlining treatment planning.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract, beyond stating that 'These initial results highlight the potential' of the framework.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pediatric Orthopedics</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">Musculoskeletal Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">ultrasound segmentation</span>
                    
                    <span class="tag tag-keyword">in-context learning</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">pediatric fractures</span>
                    
                    <span class="tag tag-keyword">musculoskeletal ultrasound</span>
                    
                    <span class="tag tag-keyword">bony regions</span>
                    
                    <span class="tag tag-keyword">data efficiency</span>
                    
                    <span class="tag tag-keyword">image concatenation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>