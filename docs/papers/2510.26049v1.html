<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation - Health AI Hub</title>
    <meta name="description" content="This paper introduces FlexICL, a novel and flexible in-context learning (ICL) framework designed for efficient segmentation of bony regions in elbow and wrist u">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26049v1" target="_blank">2510.26049v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yuyue Zhou, Jessica Knight, Shrimanti Ghosh, Banafshe Felfeliyan, Jacob L. Jaremko, Abhilash R. Hareendranathan
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26049v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26049v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces FlexICL, a novel and flexible in-context learning (ICL) framework designed for efficient segmentation of bony regions in elbow and wrist ultrasound (US) images, particularly in pediatric populations. FlexICL addresses the challenge of scarce expert annotations by enabling intra-video segmentation with only a small subset of annotated frames, demonstrating significantly enhanced performance and data efficiency compared to existing models.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for pediatric populations as it aims to improve the diagnostic accuracy and treatment planning for common elbow and wrist fractures using ultrasound. By enabling real-time feedback and highlighting key structures, it empowers lightly trained users to confidently perform ultrasound exams, potentially expanding access to effective diagnostics.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>FlexICL is an AI model designed to automatically segment bony regions in elbow and wrist ultrasound images. This application aims to improve diagnostic accuracy for fractures, especially in pediatric patients, by providing real-time feedback and highlighting key structures. This supports healthcare professionals, including those with lighter training, in performing and interpreting ultrasound exams more confidently and efficiently, addressing the challenge of limited expert annotations in medical imaging.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Elbow and wrist fractures are common in pediatric patients, and automatic segmentation of musculoskeletal structures in US can improve diagnostic accuracy and treatment planning by highlighting cortical defects.</li>
                    
                    <li>FlexICL is proposed as a flexible visual in-context learning (ICL) framework for segmenting bony regions in US images, particularly useful when pixel-wise expert annotations are limited.</li>
                    
                    <li>The framework operates in an intra-video segmentation setting, requiring experts to annotate only a small subset of frames, with the model then segmenting unseen frames within the same video.</li>
                    
                    <li>FlexICL systematically investigates various image concatenation techniques and training strategies for visual ICL, introducing novel concatenation methods that significantly boost performance with limited labeled data.</li>
                    
                    <li>By integrating multiple augmentation strategies, FlexICL achieves robust segmentation across four wrist and elbow US datasets, utilizing only 5% of the total training images.</li>
                    
                    <li>The model outperforms state-of-the-art visual ICL models (Painter, MAE-VQGAN) and conventional segmentation models (U-Net, TransUNet) by 1-27% Dice coefficient on 1,252 US sweeps.</li>
                    
                    <li>These results suggest FlexICL is an efficient and scalable solution for US image segmentation, particularly well-suited for medical imaging scenarios where annotated data is scarce.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>FlexICL employs a visual in-context learning (ICL) framework for semantic segmentation. It leverages an intra-video segmentation setting where a small annotated subset of frames guides the model for unannotated frames. The methodology involves systematic investigation and introduction of novel image concatenation techniques, various training strategies, and integration of multiple augmentation strategies to enhance model performance and robustness with limited labeled data.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>FlexICL achieved robust segmentation performance across four US datasets, requiring only 5% of total training images. It significantly outperformed state-of-the-art visual ICL models (Painter, MAE-VQGAN) and conventional segmentation models (U-Net, TransUNet) by 1-27% Dice coefficient on a large test set of 1,252 US sweeps, demonstrating superior data efficiency and accuracy.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>FlexICL has the potential to significantly improve diagnostic accuracy and treatment planning for pediatric elbow and wrist fractures. Its data efficiency can reduce the burden of expert annotation, making advanced diagnostic tools more accessible. By providing real-time feedback and highlighting critical structures, it can empower less experienced users to perform confident and accurate ultrasound exams, potentially decentralizing expertise and improving care in various clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations. It focuses on highlighting the successful initial results and potential of the framework.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly stated as 'future directions', the abstract notes these are 'initial results' highlighting the 'potential' of FlexICL as an efficient and scalable solution. This implicitly suggests future work could involve broader validation across more diverse datasets, integration into clinical workflows, and potentially extending to other anatomical regions or pathology types.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pediatrics</span>
                    
                    <span class="tag">Orthopedics</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">Musculoskeletal Ultrasound</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">In-Context Learning</span>
                    
                    <span class="tag tag-keyword">Ultrasound Segmentation</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Pediatric Fractures</span>
                    
                    <span class="tag tag-keyword">Musculoskeletal Imaging</span>
                    
                    <span class="tag tag-keyword">Data Efficiency</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Semantic Segmentation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>