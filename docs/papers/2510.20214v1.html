<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection - Health AI Hub</title>
    <meta name="description" content="This paper introduces Contrastive Ultrasound Video Representation Learning (CURL), a novel self-supervised framework for objective and accurate fetal movement (">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20214v1" target="_blank">2510.20214v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Talha Ilyas, Duong Nhu, Allison Thomas, Arie Levin, Lim Wei Yap, Shu Gong, David Vera Anaya, Yiwen Jiang, Deval Mehta, Ritesh Warty, Vinayak Smith, Maya Reddy, Euan Wallace, Wenlong Cheng, Zongyuan Ge, Faezeh Marzbanrad
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20214v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20214v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Contrastive Ultrasound Video Representation Learning (CURL), a novel self-supervised framework for objective and accurate fetal movement (FM) detection from extended ultrasound video recordings. By employing a dual-contrastive loss and a task-specific sampling strategy, CURL learns robust motion representations to overcome the subjectivity of traditional methods. Evaluated on an in-house dataset of 92 subjects, CURL achieved a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its potential for reliable prenatal health assessment.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and objective fetal movement detection is vital for assessing prenatal health, as abnormal patterns can signal underlying complications such as placental dysfunction or fetal distress. This method offers a non-invasive, data-driven approach to reduce subjectivity and enhance the reliability of prenatal health assessments, potentially leading to earlier and more effective clinical interventions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is 'Contrastive Ultrasound Video Representation Learning (CURL)', a novel self-supervised learning framework designed to objectively detect fetal movement from extended ultrasound video recordings. This AI aims to overcome the subjectivity and limitations of traditional methods (maternal perception, CTG) to provide reliable and objective analysis for improved prenatal monitoring and clinical decision-making regarding fetal health and potential complications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical need for objective and accurate fetal movement (FM) detection, as traditional methods like maternal perception and cardiotocography (CTG) suffer from subjectivity and limited accuracy.</li>
                    
                    <li>Proposes Contrastive Ultrasound Video Representation Learning (CURL), a novel self-supervised learning framework designed for FM detection from extended fetal ultrasound video recordings.</li>
                    
                    <li>CURL leverages a dual-contrastive loss that incorporates both spatial and temporal contrastive learning to learn robust motion representations from ultrasound data.</li>
                    
                    <li>Introduces a task-specific sampling strategy during self-supervised training to ensure effective separation of movement and non-movement segments.</li>
                    
                    <li>Utilizes a probabilistic fine-tuning approach to enable flexible inference on arbitrarily long ultrasound recordings, enhancing practical applicability.</li>
                    
                    <li>Evaluated on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions, achieving a sensitivity of 78.01% and an AUROC of 81.60%.</li>
                    
                    <li>Highlights the potential of self-supervised contrastive learning for objective fetal movement analysis, paving the way for improved prenatal monitoring and clinical decision-making.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study develops Contrastive Ultrasound Video Representation Learning (CURL), a self-supervised deep learning framework. It employs a dual-contrastive loss, integrating both spatial and temporal contrastive learning, to extract robust motion representations from extended fetal ultrasound video recordings. A task-specific sampling strategy is implemented to effectively differentiate between movement and non-movement segments during the self-supervised training phase. Subsequently, a probabilistic fine-tuning approach allows for flexible inference on arbitrarily long ultrasound recordings. The framework was evaluated on an in-house dataset consisting of 92 subjects, each with 30-minute ultrasound sessions.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The CURL framework demonstrated robust performance in fetal movement detection, achieving a sensitivity of 78.01% and an Area Under the Receiver Operating Characteristic (AUROC) of 81.60% on the evaluation dataset. These results indicate its strong potential for reliable and objective analysis of fetal movements from ultrasound videos.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This objective method for fetal movement detection could significantly improve prenatal monitoring by providing clinicians with more accurate and reliable data than current subjective techniques. It holds the potential for earlier and more precise identification of fetal distress, placental dysfunction, or other complications, enabling timely clinical interventions and ultimately improving maternal and fetal outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any specific limitations of the study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While specific research directions are not detailed, the paper emphasizes that this work 'paves the way for improved prenatal monitoring and clinical decision-making,' suggesting future efforts towards broader clinical implementation, validation on larger and more diverse datasets, and integration into routine prenatal care workflows.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Obstetrics</span>
                    
                    <span class="tag">Maternal-Fetal Medicine</span>
                    
                    <span class="tag">Perinatology</span>
                    
                    <span class="tag">Prenatal Care</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">fetal movement detection</span>
                    
                    <span class="tag tag-keyword">ultrasound</span>
                    
                    <span class="tag tag-keyword">self-supervised learning</span>
                    
                    <span class="tag tag-keyword">contrastive learning</span>
                    
                    <span class="tag tag-keyword">prenatal health</span>
                    
                    <span class="tag tag-keyword">objective assessment</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">obstetrics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Accurate fetal movement (FM) detection is essential for assessing prenatal
health, as abnormal movement patterns can indicate underlying complications
such as placental dysfunction or fetal distress. Traditional methods, including
maternal perception and cardiotocography (CTG), suffer from subjectivity and
limited accuracy. To address these challenges, we propose Contrastive
Ultrasound Video Representation Learning (CURL), a novel self-supervised
learning framework for FM detection from extended fetal ultrasound video
recordings. Our approach leverages a dual-contrastive loss, incorporating both
spatial and temporal contrastive learning, to learn robust motion
representations. Additionally, we introduce a task-specific sampling strategy,
ensuring the effective separation of movement and non-movement segments during
self-supervised training, while enabling flexible inference on arbitrarily long
ultrasound recordings through a probabilistic fine-tuning approach. Evaluated
on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,
CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its
potential for reliable and objective FM analysis. These results highlight the
potential of self-supervised contrastive learning for fetal movement analysis,
paving the way for improved prenatal monitoring and clinical decision-making.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>This is the preprint version of the manuscript submitted to IEEE
  Journal of Biomedical and Health Informatics (JBHI) for review</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>