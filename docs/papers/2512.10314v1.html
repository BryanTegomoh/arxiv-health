<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation - Health AI Hub</title>
    <meta name="description" content="This paper introduces DualProtoSeg, a novel weakly supervised semantic segmentation (WSSS) framework designed for histopathology images, which tackles common is">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.10314v1" target="_blank">2512.10314v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-11
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Anh M. Vu, Khang P. Le, Trang T. K. Vo, Ha Thach, Huy Hung Nguyen, David Yang, Han H. Huynh, Quynh Nguyen, Tuan M. Pham, Tuan-Anh Le, Minh H. N. Le, Thanh-Huy Nguyen, Akash Awasthi, Chandra Mohan, Zhu Han, Hien Van Nguyen
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.10314v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.10314v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces DualProtoSeg, a novel weakly supervised semantic segmentation (WSSS) framework designed for histopathology images, which tackles common issues like region shrinkage and feature heterogeneity. It achieves this by integrating a dual-modal prototype bank that combines text-guided and image-based prototypes with a multi-scale pyramid module for enhanced spatial precision. The method demonstrates state-of-the-art performance on the BCSS-WSSS benchmark, showcasing the effectiveness of leveraging vision-language alignment for improved region discovery in digital pathology.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Weakly supervised semantic segmentation significantly reduces the extensive time and expert effort required for manual pixel-level annotation of histopathology slides, thus accelerating the development of AI-powered diagnostic tools. Improved segmentation accuracy can lead to more precise identification of disease features, potentially enhancing the reliability and efficiency of cancer diagnosis, grading, and prognosis.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development of advanced machine learning models for automated or semi-automated analysis and segmentation of histopathology images. This can assist pathologists in more efficiently and accurately diagnosing diseases (e.g., identifying cancerous regions, grading tumors), reducing manual annotation burden, improving diagnostic workflow, and potentially enhancing consistency across diagnoses in healthcare settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses critical challenges in histopathology WSSS, including inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision.</li>
                    
                    <li>Proposes DualProtoSeg, a prototype-driven framework that leverages vision-language alignment for more robust region discovery under weak supervision.</li>
                    
                    <li>Utilizes a 'dual-modal prototype bank' which integrates CoOp-style learnable prompt tuning for text-based prototypes with learnable image prototypes to capture both semantic and appearance cues.</li>
                    
                    <li>Incorporates a multi-scale pyramid module to mitigate oversmoothing in ViT representations, thereby enhancing spatial precision and localization quality.</li>
                    
                    <li>Achieves state-of-the-art performance on the BCSS-WSSS benchmark for weakly supervised histopathology image segmentation, surpassing existing methods.</li>
                    
                    <li>Detailed analyses reveal the positive impact of text description diversity and context length on model performance.</li>
                    
                    <li>Demonstrates the complementary behavior of text and image prototypes, highlighting the benefits of their joint utilization for improved WSSS in digital pathology.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>DualProtoSeg employs a prototype-driven framework that constructs a dual-modal prototype bank. This bank combines text-based prototypes, generated through CoOp-style learnable prompt tuning (leveraging vision-language models like CLIP), with learnable image prototypes. To counter oversmoothing in Vision Transformer (ViT) representations and improve spatial precision, a multi-scale pyramid module is integrated into the architecture. The model learns from image-level labels to perform pixel-level semantic segmentation.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The DualProtoSeg approach significantly outperforms current state-of-the-art methods on the BCSS-WSSS benchmark. The research confirms that the diversity of text descriptions and their context length are crucial factors for performance. Furthermore, text-based and image-based prototypes exhibit complementary strengths, with their combined application proving highly effective for region discovery and accurate segmentation.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This advancement could substantially decrease the annotation burden for pathologists, enabling faster development and deployment of automated diagnostic tools. By providing more accurate and efficient segmentation of pathological features, DualProtoSeg has the potential to improve the consistency and objectivity of histopathological diagnoses, contributing to better patient management and treatment outcomes, particularly in areas like cancer detection and grading.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed method.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state any future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Histology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Weakly Supervised Semantic Segmentation</span>
                    
                    <span class="tag tag-keyword">Histopathology</span>
                    
                    <span class="tag tag-keyword">Prototype Learning</span>
                    
                    <span class="tag tag-keyword">Vision-Language Alignment</span>
                    
                    <span class="tag tag-keyword">Digital Pathology</span>
                    
                    <span class="tag tag-keyword">Text-Guided Segmentation</span>
                    
                    <span class="tag tag-keyword">Transformers (ViT)</span>
                    
                    <span class="tag tag-keyword">Multi-scale Feature Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>