<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine - Health AI Hub</title>
    <meta name="description" content="This paper introduces TCM-5CEval, an extended and more granular benchmark designed for a deep evaluation of large language models' (LLMs) comprehensive clinical">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.13169v1" target="_blank">2511.13169v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-17
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Tianai Huang, Jiayuan Chen, Lu Lu, Pengcheng Chen, Tianbin Li, Bing Han, Wenchao Tang, Jie Xu, Ming Li
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.13169v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.13169v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces TCM-5CEval, an extended and more granular benchmark designed for a deep evaluation of large language models' (LLMs) comprehensive clinical research competence in Traditional Chinese Medicine (TCM). The evaluation of fifteen prominent LLMs revealed significant performance disparities, identified top models, and critically exposed widespread fragilities in inference stability, particularly a pervasive sensitivity to positional bias from varied question option ordering.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Rigorous evaluation of LLMs in specialized, culturally-rich fields like Traditional Chinese Medicine is paramount to ensure their reliability and safety before any potential clinical application. This research exposes fundamental weaknesses in LLM reasoning stability, which could have serious implications for diagnostic accuracy and treatment recommendations in real-world medical scenarios.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper assesses the foundational capabilities of Large Language Models (LLMs) in understanding, interpreting, and applying complex medical knowledge and clinical reasoning specific to Traditional Chinese Medicine. This research is crucial for developing and evaluating AI systems that could potentially assist clinicians, support medical research, provide diagnostic aid, or contribute to medical education in TCM, thereby directly impacting healthcare delivery and research.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>TCM-5CEval is a comprehensive benchmark, building on TCM-3CEval, designed for rigorous and nuanced evaluation of LLMs in the specialized field of Traditional Chinese Medicine.</li>
                    
                    <li>The benchmark assesses LLMs across five critical dimensions: Core Knowledge (TCM-Exam), Classical Literacy (TCM-LitQA), Clinical Decision-making (TCM-MRCD), Chinese Materia Medica (TCM-CMM), and Clinical Non-pharmacological Therapy (TCM-ClinNPT).</li>
                    
                    <li>Evaluation of fifteen prominent LLMs revealed significant performance disparities, with deepseek_r1 and gemini_2_5_pro identified as top-performing models.</li>
                    
                    <li>Models generally demonstrated proficiency in recalling foundational TCM knowledge but struggled significantly with the interpretative complexities of classical TCM texts.</li>
                    
                    <li>Critically, permutation-based consistency testing uncovered widespread fragilities in model inference across all evaluated LLMs.</li>
                    
                    <li>All models, including the highest-scoring ones, displayed substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust, context-independent understanding.</li>
                    
                    <li>TCM-5CEval has been uploaded to the Medbench platform to promote further research and standardized comparisons in assessing comprehensive TCM abilities of LLMs.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved developing TCM-5CEval, an extended multi-dimensional benchmark for LLM evaluation in TCM. This benchmark comprises five distinct categories: TCM-Exam (Core Knowledge), TCM-LitQA (Classical Literacy), TCM-MRCD (Clinical Decision-making), TCM-CMM (Chinese Materia Medica), and TCM-ClinNPT (Clinical Non-pharmacological Therapy). Fifteen prominent LLMs were then subjected to a thorough evaluation using this benchmark. A critical aspect of the methodology was the implementation of permutation-based consistency testing to assess the robustness and stability of model inference by varying the ordering of question options.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Significant performance disparities were observed among the fifteen evaluated LLMs in TCM-related tasks, with deepseek_r1 and gemini_2_5_pro showing superior performance. While models exhibited competence in recalling foundational TCM knowledge, they consistently struggled with the interpretive complexities inherent in classical TCM texts. A critical discovery was the pervasive inference fragility across all models: permutation-based consistency testing revealed a substantial performance degradation when question option ordering was varied, indicating a widespread sensitivity to positional bias and a lack of robust, stable understanding of the underlying concepts.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings critically highlight that current LLMs, even top-performing ones, are not yet sufficiently robust or reliable for sensitive clinical applications in Traditional Chinese Medicine due to their observed inference fragility and susceptibility to positional bias. This instability could lead to inconsistent or erroneous clinical decision-making, potentially jeopardizing patient safety and treatment efficacy. TCM-5CEval provides a crucial diagnostic tool to identify and address these fundamental weaknesses, guiding the development of more stable, unbiased, and clinically trustworthy AI models essential for future integration into TCM practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study identifies critical limitations in the evaluated LLMs themselves, rather than the benchmark. Specifically, it reveals that all models, even the highest-scoring ones, exhibit widespread fragilities in their inference mechanisms and a pervasive sensitivity to positional bias in question option ordering. This indicates a fundamental lack of robust and stable understanding when processing TCM clinical data.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The release of TCM-5CEval on the Medbench platform is intended to foster further research and standardize comparisons of LLM capabilities in TCM. Future work should focus on developing LLM architectures and training paradigms that specifically address and mitigate issues of positional bias and inference fragility, aiming to achieve more stable, robust, and clinically reliable reasoning in specialized medical domains.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Traditional Chinese Medicine</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Medical Artificial Intelligence</span>
                    
                    <span class="tag">Pharmacology (TCM)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Traditional Chinese Medicine (TCM)</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Clinical Evaluation</span>
                    
                    <span class="tag tag-keyword">Benchmark</span>
                    
                    <span class="tag tag-keyword">Positional Bias</span>
                    
                    <span class="tag tag-keyword">Inference Fragility</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Materia Medica</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>17 pages, 8 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>