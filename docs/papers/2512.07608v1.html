<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metric-Fair Prompting: Treating Similar Samples Similarly - Health AI Hub</title>
    <meta name="description" content="This paper introduces Metric-Fair Prompting, a novel framework guiding Large Language Models (LLMs) to make decisions under individual metric-fairness constrain">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Metric-Fair Prompting: Treating Similar Samples Similarly</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.07608v1" target="_blank">2512.07608v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-08
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Jing Wang, Jie Shen, Xing Niu, Tong Zhang, Jeremy Weiss
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.07608v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.07608v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces Metric-Fair Prompting, a novel framework guiding Large Language Models (LLMs) to make decisions under individual metric-fairness constraints. By processing similar medical questions in joint pairs and imposing a Lipschitz-style constraint on confidence scores, the method treats similar samples similarly, significantly improving LLM accuracy on the MedQA (US) benchmark for multiple-choice medical question answering.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as it aims to improve the accuracy, reliability, and ethical fairness of AI systems used in critical clinical applications like medical education, diagnostic support, and clinical decision-making. By ensuring consistent responses to similar medical scenarios, it helps build trust and mitigate potential biases in AI-assisted healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This AI framework improves the accuracy and fairness of LLMs when answering medical multiple-choice questions. Potential applications include enhanced tools for medical education, assessment of clinical knowledge, and as a component for AI-driven clinical decision support systems where robust and fair reasoning is critical.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Metric-Fair Prompting is a fairness-aware prompting framework designed for LLMs, specifically targeting individual fairness by treating similar instances similarly.</li>
                    
                    <li>The framework is applied to multiple-choice medical question answering, treating each (question, option) pair as a binary instance (correct/incorrect).</li>
                    
                    <li>Question similarity is computed using NLP embeddings, enabling the system to solve items in joint pairs of similar questions rather than in isolation.</li>
                    
                    <li>The prompt enforces a global decision protocol: extract decisive clinical features, map each (question, option) to a confidence score $f(x)$, and apply a Lipschitz-style constraint.</li>
                    
                    <li>The Lipschitz-style constraint ensures that similar inputs receive similar confidence scores, thereby leading to consistent outputs for similar medical questions.</li>
                    
                    <li>Evaluated on the MedQA (US) benchmark, Metric-Fair Prompting demonstrated improved performance over standard single-item prompting.</li>
                    
                    <li>The research suggests that fairness-guided, confidence-oriented reasoning can significantly enhance LLM accuracy in high-stakes clinical multiple-choice question scenarios.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors developed Metric-Fair Prompting, a framework that leverages NLP embeddings to compute question similarity. This allows for the joint processing of similar medical multiple-choice questions. The method guides LLMs to extract decisive clinical features and assign a confidence score $f(x)$ to each (question, option) pair. A Lipschitz-style constraint is then imposed to ensure that similar inputs yield similar scores and consistent outputs, promoting individual fairness. Performance was evaluated on the MedQA (US) benchmark against standard single-item prompting.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Metric-Fair Prompting significantly improved the performance (accuracy) of Large Language Models on the MedQA (US) benchmark for multiple-choice medical question answering compared to traditional single-item prompting. This demonstrates the efficacy of fairness-guided, confidence-oriented reasoning in enhancing LLM accuracy in high-stakes clinical contexts.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This method could lead to more accurate, reliable, and ethically fair AI systems in clinical practice. It has the potential to enhance tools for medical diagnosis, treatment planning, and medical education by ensuring consistency in responses to similar patient cases or clinical queries. This improved trustworthiness could facilitate greater adoption and impact of AI in critical healthcare decision-making.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations or caveats of the research.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Medicine</span>
                    
                    <span class="tag">Medical Education</span>
                    
                    <span class="tag">Diagnostic Support</span>
                    
                    <span class="tag">Medical AI Ethics</span>
                    
                    <span class="tag">Healthcare Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Metric-Fair Prompting</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">individual fairness</span>
                    
                    <span class="tag tag-keyword">medical question answering</span>
                    
                    <span class="tag tag-keyword">NLP embeddings</span>
                    
                    <span class="tag tag-keyword">Lipschitz constraint</span>
                    
                    <span class="tag tag-keyword">MedQA</span>
                    
                    <span class="tag tag-keyword">clinical decision support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.</p>
            </section>

            

            
            <section class="paper-section">
                <h2>Journal Reference</h2>
                <p>NeurIPS 2025 Workshop on Socially Responsible and Trustworthy Foundation Models</p>
            </section>
            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>