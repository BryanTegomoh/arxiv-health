<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis - Health AI Hub</title>
    <meta name="description" content="This paper reveals severe data poisoning vulnerabilities in healthcare AI systems that current defenses and regulations are ill-equipped to handle. Analyzing ei">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.11020v1" target="_blank">2511.11020v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-14
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Farhad Abtahi, Fernando Seoane, Iv√°n Pau, Mario Vega-Barbas
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CR, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.11020v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.11020v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper reveals severe data poisoning vulnerabilities in healthcare AI systems that current defenses and regulations are ill-equipped to handle. Analyzing eight attack scenarios across various AI architectures and infrastructure, the authors demonstrate how attackers can compromise critical medical AI with minimal effort, leading to potentially undetected and widespread harm.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>These vulnerabilities directly threaten patient safety by compromising AI systems used for critical medical decisions like organ transplantation and crisis triage. The integrity of diagnostic tools and treatment recommendations is also at severe risk, potentially leading to incorrect care and undermining trust in AI-driven healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research directly concerns AI systems utilized in healthcare for critical functions such as assisting in resource allocation (e.g., organ transplantation and crisis triage), processing and managing medical documentation, and informing high-stakes clinical decisions. It also touches upon AI integration into general clinical workflows and infrastructure.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Attackers can compromise healthcare AI using only 100-500 poisoned samples, regardless of dataset size, often achieving over 60% success.</li>
                    
                    <li>Detection of these attacks is significantly delayed (6-12 months) or may not occur at all, allowing long-term malicious influence.</li>
                    
                    <li>The distributed nature of healthcare infrastructure creates numerous entry points, enabling insiders with limited technical skill to launch attacks.</li>
                    
                    <li>Privacy laws like HIPAA and GDPR, while crucial, can unintentionally impede the necessary security analyses for detecting data poisoning.</li>
                    
                    <li>Supply chain weaknesses allow a single compromised vendor to poison AI models across 50 to 200 healthcare institutions, magnifying the threat.</li>
                    
                    <li>The 'Medical Scribe Sybil' scenario illustrates how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach.</li>
                    
                    <li>The paper recommends multilayer defenses, including mandatory adversarial robustness testing, ensemble-based detection, privacy-preserving security mechanisms, and a shift towards interpretable AI systems for high-stakes clinical decisions.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study analyzed eight distinct attack scenarios categorized into architectural (CNNs, LLMs, RL agents), infrastructure (federated learning, medical documentation), critical resource allocation (organ transplantation, crisis triage), and supply chain attacks (commercial foundation models). This involved evaluating the effectiveness of attacks based on sample size, success rates, and potential detection timelines.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Attackers can achieve over 60% success in poisoning healthcare AI with just 100-500 samples, often remaining undetected for 6-12 months. Distributed infrastructure facilitates insider attacks, while privacy laws (HIPAA/GDPR) inadvertently hinder detection. A single compromised vendor can spread poisoned models across dozens to hundreds of institutions. The 'Medical Scribe Sybil' attack demonstrates data poisoning through legitimate clinical workflows without a breach. Current regulations lack mandatory adversarial testing, and federated learning complicates attribution.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Compromised healthcare AI could lead to misallocation of critical resources (e.g., organs, triage beds), incorrect diagnoses, and ineffective treatments, resulting in patient harm or mortality. The inherent difficulty in detecting these attacks means that healthcare providers might unknowingly rely on faulty AI, eroding trust in technology and potentially causing widespread adverse clinical outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The paper highlights that current defenses and regulations are inadequate for addressing data poisoning. Specifically, privacy laws like HIPAA and GDPR unintentionally restrict the necessary analyses for attack detection, and existing regulations lack mandatory adversarial robustness testing, leaving systems vulnerable.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors recommend implementing multilayer defenses, including mandatory adversarial robustness testing, ensemble-based detection mechanisms, and privacy-preserving security mechanisms. They also advocate for international coordination on AI security standards and a fundamental shift from opaque 'black-box' AI models towards interpretable systems with verifiable safety guarantees for high-stakes clinical decisions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">organ transplantation</span>
                    
                    <span class="tag">crisis triage</span>
                    
                    <span class="tag">medical documentation</span>
                    
                    <span class="tag">clinical decision support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">data poisoning</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">cybersecurity</span>
                    
                    <span class="tag tag-keyword">adversarial attacks</span>
                    
                    <span class="tag tag-keyword">federated learning</span>
                    
                    <span class="tag tag-keyword">HIPAA</span>
                    
                    <span class="tag tag-keyword">GDPR</span>
                    
                    <span class="tag tag-keyword">supply chain</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size, often achieving over 60 percent success, with detection taking an estimated 6 to 12 months or sometimes not occurring at all. The distributed nature of healthcare infrastructure creates many entry points where insiders with routine access can launch attacks with limited technical skill. Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection. Supply chain weaknesses allow a single compromised vendor to poison models across 50 to 200 institutions. The Medical Scribe Sybil scenario shows how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach. Current regulations lack mandatory adversarial robustness testing, and federated learning can worsen risks by obscuring attribution. We recommend multilayer defenses including required adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards. We also question whether opaque black-box models are suitable for high-stakes clinical decisions, suggesting a shift toward interpretable systems with verifiable safety guarantees.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>