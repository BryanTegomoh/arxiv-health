<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine - Health AI Hub</title>
    <meta name="description" content="This paper introduces TCM-Eval, the first dynamic and extensible benchmark for Traditional Chinese Medicine (TCM), curated from national medical licensing exami">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.07148v1" target="_blank">2511.07148v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zihao Cheng, Yuheng Lu, Huaiqian Ye, Zeming Liu, Minqi Wang, Jingjing Liu, Zihan Li, Wei Fan, Yuanfang Guo, Ruiji Fu, Shifeng She, Gang Wang, Yunhong Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.07148v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.07148v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces TCM-Eval, the first dynamic and extensible benchmark for Traditional Chinese Medicine (TCM), curated from national medical licensing examinations and validated by experts, to address the severe limitations of Large Language Models (LLMs) in TCM due to a lack of standardized benchmarks and high-quality data. Utilizing a novel Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) method for data enrichment, the authors developed ZhiMingTang (ZMT), a state-of-the-art TCM-specific LLM that significantly surpasses the passing threshold for human practitioners.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant as it establishes foundational tools (benchmark and enhanced data) and develops a high-performing AI model specifically for Traditional Chinese Medicine, potentially revolutionizing clinical decision support, education, and research in a field largely underserved by modern AI advancements.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper describes the development of ZhiMingTang (ZMT), a state-of-the-art Large Language Model specifically designed for Traditional Chinese Medicine. This LLM could be applied in various health contexts, such as assisting TCM practitioners with diagnosis and treatment planning, serving as a comprehensive knowledge base for TCM students and professionals, or potentially even for patient education and preliminary consultation in TCM, thereby enhancing the capabilities of AI in a specific medical domain.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Identifies the critical gap in TCM for LLMs: absence of standardized benchmarks and scarcity of high-quality training data.</li>
                    
                    <li>Introduces TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts.</li>
                    
                    <li>Proposes Self-Iterative Chain-of-Thought Enhancement (SI-CoTE), a novel methodology to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling.</li>
                    
                    <li>Establishes a 'virtuous cycle' for data and model co-evolution through SI-CoTE and subsequent model training.</li>
                    
                    <li>Develops ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, trained on the enriched data.</li>
                    
                    <li>Demonstrates that ZMT significantly exceeds the passing threshold required for human TCM practitioners.</li>
                    
                    <li>Releases a public leaderboard to foster community engagement and encourage continuous research and development in TCM LLMs.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves curating TCM-Eval from national medical licensing examinations, validating it with TCM experts, constructing a large-scale training corpus, and proposing Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) which uses rejection sampling to autonomously enrich question-answer pairs with validated reasoning chains. This enhanced data then trains ZhiMingTang (ZMT), a state-of-the-art LLM for TCM, which is subsequently evaluated against human practitioner passing thresholds.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key findings include the successful creation of TCM-Eval as the first dynamic and expert-validated benchmark for TCM LLMs, the development of SI-CoTE as an effective method for self-improving training data quality, and the outstanding performance of ZhiMingTang (ZMT), which significantly surpasses the passing threshold for human TCM practitioners.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact is substantial, potentially enabling advanced AI-powered diagnostic and treatment support tools for TCM practitioners, improving the quality and accessibility of TCM medical education, and providing a standardized framework for evaluating and advancing AI applications in this traditional medical discipline. This could lead to more accurate diagnoses and personalized treatment plans in TCM.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail specific limitations of the developed benchmark or model. It primarily focuses on addressing the existing limitations within the field (absence of benchmarks and data scarcity for LLMs in TCM).</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future directions include fostering community engagement and continuous improvement through the released public leaderboard, encouraging further research and development in TCM LLMs, and implicitly, expanding the capabilities and applications of models like ZhiMingTang in various TCM clinical scenarios.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Traditional Chinese Medicine</span>
                    
                    <span class="tag">Medical Education</span>
                    
                    <span class="tag">Artificial Intelligence in Healthcare</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Traditional Chinese Medicine</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Benchmark</span>
                    
                    <span class="tag tag-keyword">Data Augmentation</span>
                    
                    <span class="tag tag-keyword">Chain-of-Thought</span>
                    
                    <span class="tag tag-keyword">AI in Medicine</span>
                    
                    <span class="tag tag-keyword">Medical Education</span>
                    
                    <span class="tag tag-keyword">ZhiMingTang</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) have demonstrated remarkable capabilities in
modern medicine, yet their application in Traditional Chinese Medicine (TCM)
remains severely limited by the absence of standardized benchmarks and the
scarcity of high-quality training data. To address these challenges, we
introduce TCM-Eval, the first dynamic and extensible benchmark for TCM,
meticulously curated from national medical licensing examinations and validated
by TCM experts. Furthermore, we construct a large-scale training corpus and
propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously
enrich question-answer pairs with validated reasoning chains through rejection
sampling, establishing a virtuous cycle of data and model co-evolution. Using
this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art
LLM specifically designed for TCM, which significantly exceeds the passing
threshold for human practitioners. To encourage future research and
development, we release a public leaderboard, fostering community engagement
and continuous improvement.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Work in Progress</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>