<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision-Language Models for Automated 3D PET/CT Report Generation - Health AI Hub</title>
    <meta name="description" content="This paper introduces PETRG-3D, an end-to-end 3D dual-branch vision-language model designed to automate PET/CT report generation, addressing the challenges of f">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Vision-Language Models for Automated 3D PET/CT Report Generation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20145v1" target="_blank">2511.20145v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-25
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Wenpei Jiao, Kun Shang, Hui Li, Ke Yan, Jiajin Zhang, Guangjie Yang, Lijuan Guo, Yan Wan, Xing Yang, Dakai Jin, Zhaoheng Xie
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20145v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20145v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces PETRG-3D, an end-to-end 3D dual-branch vision-language model designed to automate PET/CT report generation, addressing the challenges of functional imaging and inter-hospital reporting variability. By separately encoding 3D PET and CT volumes and incorporating style-adaptive prompts, the model significantly outperforms existing methods on both natural language and novel clinical efficacy metrics. This work provides a robust foundation for clinically reliable AI in PET/CT reporting, particularly in oncology.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to oncology and nuclear medicine as it offers a solution to reduce the significant clinical workload associated with PET/CT report generation, potentially leading to faster diagnoses, more efficient patient management, and improved standardization of reporting practices across healthcare institutions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves using Vision-Language Models to automatically generate detailed and clinically relevant 3D PET/CT reports. This system aims to assist radiologists and oncologists by automating a time-consuming aspect of medical diagnosis, thereby reducing clinical workload, potentially accelerating patient care in oncology, and improving the availability of diagnostic reporting, particularly in areas with specialist shortages.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical clinical need for automated PET/CT report generation (PETRG) due to a shortage of trained specialists and the rapid expansion of PET/CT scanners in oncology.</li>
                    
                    <li>Proposes PETRG-3D, an end-to-end 3D dual-branch framework that separately encodes whole-body 3D PET and CT volumes to capture comprehensive functional and structural information.</li>
                    
                    <li>Incorporates style-adaptive prompts into PETRG-3D to mitigate inter-hospital variability in clinical reporting practices, enhancing generalizability and clinical utility.</li>
                    
                    <li>Constructed two novel datasets: PETRG-Lym, a multi-center lymphoma dataset (824 reports, 245,509 PET/CT slices), and AutoPET-RG-Lym, a publicly accessible benchmark with 135 expert-written, clinically validated reports.</li>
                    
                    <li>Introduced PETRG-Score, a lymphoma-specific evaluation protocol designed to jointly measure the accuracy of metabolic and structural findings across curated anatomical regions for clinical efficacy assessment.</li>
                    
                    <li>PETRG-3D achieved substantial performance improvements over existing methods, demonstrating a +31.49% increase in ROUGE-L (natural language metric) and an +8.18% improvement in PET-All (clinical efficacy metric).</li>
                    
                    <li>The findings highlight the significant benefits of volumetric dual-modality modeling and style-aware prompting for generating accurate and clinically relevant PET/CT reports.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study proposes PETRG-3D, an end-to-end 3D dual-branch deep learning framework that processes 3D PET and CT volumes independently before multimodal fusion. It integrates style-adaptive prompts to customize report generation based on institutional reporting styles. The methodology also includes the creation of PETRG-Lym (a multi-center, real-world lymphoma dataset) and AutoPET-RG-Lym (a public benchmark), alongside a novel, lymphoma-specific clinical evaluation metric called PETRG-Score, which assesses both metabolic and structural findings.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>PETRG-3D significantly outperforms current methods, demonstrating a substantial improvement of +31.49% in ROUGE-L for natural language generation quality and a notable +8.18% increase in PET-All for clinical efficacy. These results underscore the critical importance and effectiveness of utilizing volumetric dual-modality modeling (PET and CT) and incorporating style-aware prompting for generating accurate, comprehensive, and clinically reliable PET/CT reports.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The proposed PETRG-3D model has the potential to significantly alleviate the workload of radiologists and nuclear medicine physicians by automating the time-consuming process of PET/CT report generation. This could lead to faster report turnaround times, improved efficiency in oncology diagnostics and treatment planning, and potentially enhance the consistency and quality of reporting across different medical centers.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly list limitations of the PETRG-3D model itself. However, the study's focus on lymphoma, while providing a strong foundation, suggests that generalizability to other oncological or non-oncological conditions requiring PET/CT might need further validation. The inherent challenges of PET imaging (metabolic pattern variability, whole-body 3D context) are noted as motivations for the work, rather than limitations of the proposed solution.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors suggest that future research should focus on developing advanced PET/CT-specific models that incorporate more sophisticated disease-aware reasoning capabilities and continue to enhance the robustness and clinical reliability of evaluation protocols.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Nuclear Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">PET/CT</span>
                    
                    <span class="tag tag-keyword">Automated Report Generation</span>
                    
                    <span class="tag tag-keyword">Vision-Language Models</span>
                    
                    <span class="tag tag-keyword">Lymphoma</span>
                    
                    <span class="tag tag-keyword">Oncology</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">3D Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Positron emission tomography/computed tomography (PET/CT) is essential in oncology, yet the rapid expansion of scanners has outpaced the availability of trained specialists, making automated PET/CT report generation (PETRG) increasingly important for reducing clinical workload. Compared with structural imaging (e.g., X-ray, CT, and MRI), functional PET poses distinct challenges: metabolic patterns vary with tracer physiology, and whole-body 3D contextual information is required rather than local-region interpretation. To advance PETRG, we propose PETRG-3D, an end-to-end 3D dual-branch framework that separately encodes PET and CT volumes and incorporates style-adaptive prompts to mitigate inter-hospital variability in reporting practices. We construct PETRG-Lym, a multi-center lymphoma dataset collected from four hospitals (824 reports w/ 245,509 paired PET/CT slices), and construct AutoPET-RG-Lym, a publicly accessible PETRG benchmark derived from open imaging data but equipped with new expert-written, clinically validated reports (135 cases). To assess clinical utility, we introduce PETRG-Score, a lymphoma-specific evaluation protocol that jointly measures metabolic and structural findings across curated anatomical regions. Experiments show that PETRG-3D substantially outperforms existing methods on both natural language metrics (e.g., +31.49\% ROUGE-L) and clinical efficacy metrics (e.g., +8.18\% PET-All), highlighting the benefits of volumetric dual-modality modeling and style-aware prompting. Overall, this work establishes a foundation for future PET/CT-specific models emphasizing disease-aware reasoning and clinically reliable evaluation. Codes, models, and AutoPET-RG-Lym will be released.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>