<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss - Health AI Hub</title>
    <meta name="description" content="This paper proposes a novel framework to enhance automated landmark detection in pelvic fluoroscopy, particularly addressing the limitation of current methods t">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.21575v1" target="_blank">2511.21575v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Chou Mo, Yehyun Suh, J. Ryan Martin, Daniel Moyer
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.21575v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.21575v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper proposes a novel framework to enhance automated landmark detection in pelvic fluoroscopy, particularly addressing the limitation of current methods that assume a fixed Antero-Posterior view. It integrates 2D/3D landmark registration loss into the training process of a U-Net model to improve accuracy under realistic intra-operative conditions where patient pose is variable. The authors detail a comparative analysis of their proposed method against baseline U-Net and a U-Net trained/fine-tuned with Pose Estimation Loss.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Automated landmark detection is crucial for efficiently understanding patient anatomy and positioning during intra-operative imaging. By improving accuracy and robustness under variable patient poses, this research makes such tools more clinically viable and reliable for surgical planning and guidance.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an enhanced landmark detection model (U-Net based) designed to accurately identify anatomical landmarks in pelvic fluoroscopy images. This AI system assists medical professionals in understanding patient anatomic structure and positioning during intra-operative procedures, potentially improving surgical efficiency, precision, and outcomes, especially when patient pose is variable.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Current automated landmark detection methods for pelvic fluoroscopy often inaccurately assume a fixed Antero-Posterior view of the pelvis.</li>
                    
                    <li>The proposed framework introduces a novel approach by incorporating 2D/3D landmark registration loss directly into the training of a U-Net landmark prediction model.</li>
                    
                    <li>This 2D/3D registration loss is designed to explicitly account for and adapt to variations in patient pose and imaging unit orientation.</li>
                    
                    <li>Performance analysis involves comparing three models: a baseline U-Net, a U-Net trained with Pose Estimation Loss (PEL), and a U-Net fine-tuned with PEL.</li>
                    
                    <li>Evaluation is conducted under realistic intra-operative conditions where patient pose is variable, mimicking real clinical scenarios.</li>
                    
                    <li>The primary goal is to demonstrate enhanced landmark detection accuracy and robustness of the proposed method under non-standard imaging orientations.</li>
                    
                    <li>The research aims to provide more reliable anatomical understanding and positioning guidance for medical professionals during intra-operative procedures.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employs a U-Net architecture for landmark prediction. The core innovation lies in integrating a 2D/3D landmark registration loss (referred to as Pose Estimation Loss) directly into the U-Net's training objective. Performance is evaluated by comparing a standard U-Net, a U-Net trained entirely with the new loss, and a U-Net initially trained conventionally and then fine-tuned with the registration loss, specifically under conditions of variable patient pose.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The paper introduces a novel framework for robust landmark detection in pelvic fluoroscopy by integrating 2D/3D registration loss during U-Net training. It outlines a comparative analysis methodology to demonstrate that this approach, particularly when incorporating Pose Estimation Loss, significantly improves landmark detection accuracy under variable patient pose conditions compared to traditional U-Net models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to significantly enhance the utility of automated landmark detection in real-world surgical and interventional settings. By providing more accurate and reliable anatomical understanding despite variations in patient position or C-arm orientation, it can lead to improved surgical precision, reduced operative time, and potentially better patient outcomes by facilitating more precise intra-operative guidance.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The primary limitation addressed by this work is the common assumption of a fixed Antero-Posterior view in most current pelvic fluoroscopy landmark detection methods, which hinders their practical application in dynamic intra-operative environments.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract, but implicitly, future work would involve extensive validation on diverse clinical datasets, exploration of real-time application in surgical workflows, and generalization to other anatomical regions or imaging modalities.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Orthopedic Surgery</span>
                    
                    <span class="tag">Interventional Radiology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Computer-Assisted Surgery</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">landmark detection</span>
                    
                    <span class="tag tag-keyword">pelvic fluoroscopy</span>
                    
                    <span class="tag tag-keyword">2D/3D registration</span>
                    
                    <span class="tag tag-keyword">U-Net</span>
                    
                    <span class="tag tag-keyword">pose estimation</span>
                    
                    <span class="tag tag-keyword">intra-operative imaging</span>
                    
                    <span class="tag tag-keyword">medical imaging</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>9 pages, 3 figures, 1 table</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>