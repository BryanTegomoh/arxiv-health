<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel framework for enhancing automated landmark detection in pelvic fluoroscopy, addressing the limitation of existing methods that ass">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.21575v1" target="_blank">2511.21575v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Chou Mo, Yehyun Suh, J. Ryan Martin, Daniel Moyer
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.21575v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.21575v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel framework for enhancing automated landmark detection in pelvic fluoroscopy, addressing the limitation of existing methods that assume a fixed Antero-Posterior view. It proposes integrating a 2D/3D landmark registration loss into the training of a U-Net model to improve accuracy under realistic intra-operative conditions with variable patient pose. The study analyzes the performance differences between a baseline U-Net, a U-Net trained with this Pose Estimation Loss, and a fine-tuned version.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Improving landmark detection accuracy despite variable patient positioning in pelvic fluoroscopy is crucial for enhancing surgical navigation, precise anatomical assessment, and overall patient safety during intra-operative procedures, especially in areas like orthopedic or interventional radiology.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an enhanced automated landmark detection model for pelvic fluoroscopy images. This model assists medical professionals in accurately understanding patient anatomy and positioning during intra-operative procedures, by accounting for variations in patient orientation. This improves precision and efficiency in image-guided surgical interventions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Automated landmark detection is vital for understanding patient anatomy and positioning during intra-operative imaging in medical procedures.</li>
                    
                    <li>Current landmark detection methods for pelvic fluoroscopy are limited by their assumption of a fixed Antero-Posterior (AP) view, which is often inaccurate in real clinical scenarios due to variable patient or imaging unit orientation.</li>
                    
                    <li>The proposed solution is a novel framework that incorporates a 2D/3D landmark registration loss function, termed "Pose Estimation Loss," directly into the training regimen of a U-Net deep learning model.</li>
                    
                    <li>This integration aims to make the landmark detection model significantly more robust to variations in patient or imaging unit orientation during fluoroscopy.</li>
                    
                    <li>The study conducts a comparative analysis of landmark detection accuracy across three models: a standard (baseline) U-Net, a U-Net trained from scratch with the new Pose Estimation Loss, and a U-Net initially trained then fine-tuned with this loss.</li>
                    
                    <li>Performance evaluation is specifically designed and conducted under realistic intra-operative scenarios to assess adaptability and accuracy under conditions of variable patient pose.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The research involves developing a novel framework that modifies the training objective of a U-Net convolutional neural network by integrating a 2D/3D landmark registration loss function, referred to as 'Pose Estimation Loss.' This loss guides the model to predict landmarks more accurately under varying orientations by leveraging 3D anatomical information. The methodology includes a comparative study analyzing the landmark detection performance of a standard U-Net, a U-Net trained entirely with the new loss, and a U-Net fine-tuned with the loss, all evaluated under conditions of variable patient pose.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>While the abstract focuses on the *analysis* of performance differences, it strongly implies that incorporating the 2D/3D registration loss (Pose Estimation Loss) into U-Net training, particularly through fine-tuning, leads to enhanced landmark detection accuracy compared to a baseline U-Net under realistic intra-operative conditions where patient pose is variable.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This enhanced landmark detection capability directly contributes to more reliable and precise intra-operative guidance, potentially reducing surgical time, minimizing radiation exposure from repeated imaging, and improving outcomes for procedures requiring accurate pelvic anatomical understanding despite patient movement or imaging angle variations. It supports safer and more efficient minimally invasive procedures.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations or caveats of the proposed framework or study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Orthopedic Surgery</span>
                    
                    <span class="tag">Surgical Navigation</span>
                    
                    <span class="tag">Interventional Radiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">pelvic fluoroscopy</span>
                    
                    <span class="tag tag-keyword">landmark detection</span>
                    
                    <span class="tag tag-keyword">2D/3D registration</span>
                    
                    <span class="tag tag-keyword">U-Net</span>
                    
                    <span class="tag tag-keyword">pose estimation loss</span>
                    
                    <span class="tag tag-keyword">intra-operative imaging</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">medical imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>9 pages, 3 figures, 1 table</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>