<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss - Health AI Hub</title>
    <meta name="description" content="This paper proposes a novel framework to enhance automated landmark detection in pelvic fluoroscopy, addressing the common limitation of models assuming a fixed">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.21575v1" target="_blank">2511.21575v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Chou Mo, Yehyun Suh, J. Ryan Martin, Daniel Moyer
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.21575v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.21575v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper proposes a novel framework to enhance automated landmark detection in pelvic fluoroscopy, addressing the common limitation of models assuming a fixed Antero-Posterior view. It integrates a 2D/3D landmark registration loss into the training of a U-Net model to improve accuracy under variable patient or imaging unit orientations. The authors analyze the performance of this enhanced U-Net against baseline methods, particularly under realistic intra-operative conditions with varying patient pose.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and robust automated landmark detection in pelvic fluoroscopy, even with varying patient poses, is crucial for improving real-time surgical guidance, diagnosis, and post-operative assessment. This reduces the need for manual adjustments and enhances the efficiency and safety of image-guided procedures.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is an enhanced landmark detection model using deep learning (U-Net) and 2D/3D registration loss to accurately identify anatomical landmarks in pelvic fluoroscopy, particularly under variable patient positioning. This improves real-time anatomical understanding and guidance for medical professionals during intra-operative procedures, leading to more efficient and potentially safer interventions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Current automated landmark detection methods in pelvic fluoroscopy typically assume a fixed Antero-Posterior (AP) view, which is often violated in real intra-operative settings.</li>
                    
                    <li>The proposed solution is a novel framework that incorporates a 2D/3D landmark registration loss directly into the training process of a U-Net landmark prediction model.</li>
                    
                    <li>This approach aims to make landmark detection more robust and accurate when the pelvis or imaging unit deviates from the standard AP orientation.</li>
                    
                    <li>Performance analysis involves comparing three models: a baseline U-Net, a U-Net trained with Pose Estimation Loss (PEL), and a U-Net fine-tuned with PEL.</li>
                    
                    <li>The evaluation is conducted under realistic intra-operative conditions designed to simulate variable patient pose.</li>
                    
                    <li>The primary goal is to provide medical professionals with a more efficient and reliable tool for understanding patient anatomy and positioning using intra-operative imaging, irrespective of viewing angle.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study proposes a novel framework where a U-Net landmark prediction model is trained using an additional 2D/3D landmark registration loss. This loss function presumably guides the U-Net to produce outputs that are consistent with 3D anatomical structures even when viewed from different 2D projections. The performance is evaluated by comparing a standard U-Net, a U-Net trained with Pose Estimation Loss (PEL), and a U-Net fine-tuned with PEL, specifically under conditions where patient pose is intentionally varied.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The abstract describes the proposed methodology and the experimental setup for analysis, but does not present specific results or key findings. It states the intent to "analyze the performance difference" between the proposed enhanced U-Net models and baseline, implying the expectation of improved accuracy and robustness under variable pose conditions.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to significantly improve the accuracy and reliability of automated anatomical structure and positioning understanding during image-guided pelvic procedures. By enabling robust landmark detection irrespective of patient or imaging unit orientation, it could lead to more precise surgical planning, reduced operative time, decreased radiation exposure (by minimizing repeated imaging for optimal views), and improved patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily discusses the limitations of existing methods (fixed AP view assumption) but does not detail any specific limitations or caveats of the proposed methodology or study itself. The absence of reported findings in the abstract means its practical effectiveness under all real-world conditions is yet to be demonstrated.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions. However, potential future work could involve validating the model across diverse patient populations, integrating it into real-time surgical navigation systems, extending the approach to other anatomical regions, or exploring different 2D/3D registration techniques.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Orthopedics</span>
                    
                    <span class="tag">Image-guided surgery</span>
                    
                    <span class="tag">Interventional Radiology</span>
                    
                    <span class="tag">Neurosurgery (potentially for spine or related imaging)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Pelvic fluoroscopy</span>
                    
                    <span class="tag tag-keyword">Landmark detection</span>
                    
                    <span class="tag tag-keyword">2D/3D registration</span>
                    
                    <span class="tag tag-keyword">U-Net</span>
                    
                    <span class="tag tag-keyword">Pose estimation</span>
                    
                    <span class="tag tag-keyword">Intra-operative imaging</span>
                    
                    <span class="tag tag-keyword">Medical imaging</span>
                    
                    <span class="tag tag-keyword">Deep learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>9 pages, 3 figures, 1 table</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>