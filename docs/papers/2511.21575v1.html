<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel deep learning framework to enhance automated landmark detection in pelvic fluoroscopy, specifically addressing the challenge of va">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.21575v1" target="_blank">2511.21575v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-26
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Chou Mo, Yehyun Suh, J. Ryan Martin, Daniel Moyer
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.21575v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.21575v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel deep learning framework to enhance automated landmark detection in pelvic fluoroscopy, specifically addressing the challenge of variable patient or imaging unit orientation. It integrates a 2D/3D landmark registration loss directly into the training of a U-Net model, aiming to improve detection accuracy and robustness under realistic intra-operative conditions. The study analyzes the performance of this approach against baseline U-Net models.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and automated landmark detection in pelvic fluoroscopy is crucial for precise surgical planning, navigation, and assessment during intra-operative procedures, enabling medical professionals to understand patient anatomy and positioning efficiently, especially in real-world scenarios where patient orientation varies.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>An AI model (U-Net) is developed and evaluated to automate the detection of anatomical landmarks in medical images (pelvic fluoroscopy). This application assists medical professionals in accurately understanding patient anatomic structure and positioning during surgery, thereby enhancing efficiency and precision in medical procedures.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses a critical limitation in current pelvic fluoroscopy landmark detection models, which often assume a fixed Antero-Posterior (AP) view and perform suboptimally under variable patient or imaging unit orientations.</li>
                    
                    <li>Proposes a novel framework that incorporates a 2D/3D landmark registration loss, referred to as "Pose Estimation Loss," into the training process of a U-Net based landmark prediction model.</li>
                    
                    <li>The U-Net architecture serves as the foundation for predicting anatomical landmarks in the pelvic fluoroscopy images.</li>
                    
                    <li>The primary goal of the 2D/3D registration loss is to make the landmark detection model robust and accurate across varying patient poses encountered in realistic intra-operative settings.</li>
                    
                    <li>Performance evaluation involves comparing three strategies: a baseline U-Net, a U-Net trained from scratch with the Pose Estimation Loss, and a U-Net initially trained and then fine-tuned with the Pose Estimation Loss.</li>
                    
                    <li>The research is conducted under conditions designed to simulate realistic intra-operative scenarios where patient pose is variable, emphasizing practical applicability.</li>
                    
                    <li>The framework aims to provide medical professionals with a more efficient and reliable understanding of patient anatomic structure and positioning during procedures.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study proposes a novel framework that integrates a 2D/3D landmark registration loss (Pose Estimation Loss) into the training of a U-Net model for automated landmark detection in pelvic fluoroscopy. Performance is analyzed by comparing landmark detection accuracy among a baseline U-Net, a U-Net trained with Pose Estimation Loss, and a U-Net fine-tuned with Pose Estimation Loss, all under realistic intra-operative conditions with variable patient pose.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The abstract indicates an analysis of performance differences between the proposed U-Net model incorporating 2D/3D registration loss and baseline U-Net models under variable pose conditions. While specific quantitative results are not detailed in the abstract, the framework is designed to enhance landmark detection accuracy and robustness in challenging, real-world scenarios where patient orientation is not fixed.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research can significantly improve the efficiency and reliability of intra-operative procedures involving the pelvis by providing more robust and accurate automated landmark detection, irrespective of patient or imaging unit orientation. This advancement could lead to more precise surgical guidance, reduced procedure times, enhanced consistency in anatomical assessment, and ultimately, improved patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed framework or the study itself. It primarily highlights a limitation of existing landmark detection methods that this work aims to overcome.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions are not explicitly mentioned in the provided abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Orthopedic Surgery</span>
                    
                    <span class="tag">Interventional Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical Robotics/Navigation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Pelvic fluoroscopy</span>
                    
                    <span class="tag tag-keyword">landmark detection</span>
                    
                    <span class="tag tag-keyword">2D/3D registration</span>
                    
                    <span class="tag tag-keyword">U-Net</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">pose estimation</span>
                    
                    <span class="tag tag-keyword">intra-operative imaging</span>
                    
                    <span class="tag tag-keyword">medical image analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>9 pages, 3 figures, 1 table</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>