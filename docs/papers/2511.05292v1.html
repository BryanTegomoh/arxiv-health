<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs - Health AI Hub</title>
    <meta name="description" content="CuisineSense is a novel wearable-based system designed to accurately infer Chinese cuisine intake by integrating hand motions from a smartwatch and head dynamic">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.05292v1" target="_blank">2511.05292v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-07
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Jiaxi Yin, Pengcheng Wang, Han Ding, Fei Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.05292v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.05292v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">CuisineSense is a novel wearable-based system designed to accurately infer Chinese cuisine intake by integrating hand motions from a smartwatch and head dynamics from smart glasses. It employs a two-stage detection pipeline for eating state identification and fine-grained food classification, demonstrating high accuracy across 11 diverse Chinese food categories. This system offers an unobtrusive and practical solution for dietary monitoring, addressing limitations of existing methods concerning privacy and food diversity.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and objective food intake detection is critical for effective dietary monitoring, which is a cornerstone for preventing and managing chronic diseases such as diabetes, obesity, and cardiovascular conditions. This system provides a non-invasive and bias-reduced tool to gather such data, overcoming significant challenges faced by traditional dietary assessment methods.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI system (CuisineSense) uses machine learning algorithms (implied by 'classification' and 'detection pipeline' based on IMU data) to infer food intake and classify food types from wearable sensors. This serves as an automated, unobtrusive tool for dietary monitoring, which can be applied in personalized health management, clinical dietary counseling, and large-scale public health interventions for chronic disease prevention.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The paper addresses the significant gap in accurate food intake detection for diverse Chinese cuisine, overcoming limitations of traditional self-report (recall bias), camera-based (privacy concerns), and existing wearable methods (limited food types).</li>
                    
                    <li>It proposes CuisineSense, a multi-sensor system that leverages Inertial Measurement Unit (IMU) data from a smartwatch (hand motion cues) and smart glasses (head dynamics) for unobtrusive data collection.</li>
                    
                    <li>A two-stage detection pipeline is implemented: the first stage identifies 'eating states' by distinguishing characteristic temporal patterns, while the second stage performs fine-grained classification of food types during the identified intake periods.</li>
                    
                    <li>A dedicated dataset was constructed for evaluation, comprising 27.5 hours of IMU recordings from 10 participants consuming 11 distinct categories of Chinese food.</li>
                    
                    <li>CuisineSense achieved high accuracy in both its eating state detection and subsequent fine-grained food classification tasks, validating its effectiveness.</li>
                    
                    <li>The system provides a practical, privacy-preserving, and objective approach to dietary assessment, offering a valuable tool for chronic disease prevention and personalized nutritional management.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The CuisineSense system integrates Inertial Measurement Unit (IMU) data from a smartwatch (capturing hand motions) and smart glasses (capturing head dynamics). It employs a two-stage machine learning pipeline: Stage 1 identifies 'eating states' by differentiating characteristic temporal patterns from non-eating activities. Stage 2, triggered upon eating state detection, performs fine-grained classification of 11 specific Chinese food types based on the combined sensor data captured during the identified food intake.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>CuisineSense demonstrated high accuracy in both its two-stage detection pipeline: effectively identifying eating states and subsequently classifying specific Chinese food types. The system's performance on a custom dataset (27.5 hours of IMU recordings across 11 food categories from 10 participants) validated its ability to offer a practical and unobtrusive solution for wearable-based dietary monitoring.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This system could significantly enhance the objectivity and reliability of dietary assessment in clinical settings and for public health initiatives. It provides a means for continuous, long-term monitoring of food intake without requiring active user input or raising privacy concerns, thus aiding in personalized nutritional counseling, adherence tracking for specific diets (e.g., diabetic diets), and large-scale epidemiological studies on dietary patterns and chronic disease risk among Chinese populations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations. However, potential limitations inferred could include the current scope being limited to 11 specific Chinese food categories and evaluation on a relatively small cohort of 10 participants. Generalizability to other cuisines or larger, more diverse populations would likely require further validation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Nutrition</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Chronic Disease Management</span>
                    
                    <span class="tag">Preventive Medicine</span>
                    
                    <span class="tag">Dietetics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">wearable sensors</span>
                    
                    <span class="tag tag-keyword">IMU</span>
                    
                    <span class="tag tag-keyword">Chinese cuisine</span>
                    
                    <span class="tag tag-keyword">food intake detection</span>
                    
                    <span class="tag tag-keyword">dietary monitoring</span>
                    
                    <span class="tag tag-keyword">smartwatch</span>
                    
                    <span class="tag tag-keyword">smart glasses</span>
                    
                    <span class="tag tag-keyword">chronic disease prevention</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Accurate food intake detection is vital for dietary monitoring and chronic
disease prevention. Traditional self-report methods are prone to recall bias,
while camera-based approaches raise concerns about privacy. Furthermore,
existing wearable-based methods primarily focus on a limited number of food
types, such as hamburgers and pizza, failing to address the vast diversity of
Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that
classifies Chinese food types by integrating hand motion cues from a smartwatch
with head dynamics from smart glasses. To filter out irrelevant daily
activities, we design a two-stage detection pipeline. The first stage
identifies eating states by distinguishing characteristic temporal patterns
from non-eating behaviors. The second stage then conducts fine-grained food
type recognition based on the motions captured during food intake. To evaluate
CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings
across 11 food categories and 10 participants. Experiments demonstrate that
CuisineSense achieves high accuracy in both eating state detection and food
classification, offering a practical solution for unobtrusive, wearable-based
dietary monitoring.The system code is publicly available at
https://github.com/joeeeeyin/CuisineSense.git.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>5 pages</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>