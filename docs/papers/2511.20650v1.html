<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities - Health AI Hub</title>
    <meta name="description" content="MedROV introduces the first real-time open-vocabulary object detection model for diverse medical imaging, addressing the critical limitation of traditional clos">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.20650v1" target="_blank">2511.20650v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-25
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Tooba Tehreem Sheikh, Jean Lahoud, Rao Muhammad Anwer, Fahad Shahbaz Khan, Salman Khan, Hisham Cholakkal
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.20650v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.20650v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">MedROV introduces the first real-time open-vocabulary object detection model for diverse medical imaging, addressing the critical limitation of traditional closed-set detectors in identifying novel pathologies. It achieves this by curating a large multi-modal dataset (Omnis), employing pseudo-labeling for data heterogeneity, and leveraging pre-trained foundation models with contrastive learning. The model effectively detects both known and novel medical structures, setting a new benchmark for speed and accuracy.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This innovation is crucial for medical AI as it overcomes the rigidity of closed-set detectors, allowing diagnostic systems to identify a wider range of abnormalities and novel pathologies without requiring extensive retraining for every new condition. This significantly enhances diagnostic accuracy, efficiency, and adaptability in clinical settings, potentially leading to earlier intervention.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>MedROV is an AI model designed for real-time open-vocabulary object detection in diverse medical imaging modalities (e.g., X-ray, CT, MRI, ultrasound, histology). This application of AI aims to significantly enhance medical diagnosis by enabling the detection of various anatomical structures, pathologies, or anomalies ‚Äì including those that the model hasn't been explicitly trained on (novel labels). This capability supports faster and more accurate disease screening, aids in clinical workflows by automatically identifying critical features, and provides a powerful tool for medical research by uncovering new patterns.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical limitation of closed-set medical object detection by enabling open-vocabulary detection of novel labels, a previously underexplored area due to data scarcity.</li>
                    
                    <li>Introduces MedROV, the first real-time (70 FPS) open-vocabulary detection model specifically designed for diverse medical imaging modalities.</li>
                    
                    <li>Curates Omnis, a large-scale dataset comprising 600K detection samples across nine medical imaging modalities, foundational for open-vocabulary learning.</li>
                    
                    <li>Employs a pseudo-labeling strategy to effectively handle missing annotations within multi-source datasets, improving data utility and model robustness.</li>
                    
                    <li>Enhances generalization by incorporating knowledge from large pre-trained foundation models and leveraging contrastive learning with cross-modal representations.</li>
                    
                    <li>Demonstrates significant performance improvements, achieving an average absolute improvement of 40 mAP50 over previous state-of-the-art medical foundation models.</li>
                    
                    <li>Outperforms traditional closed-set detectors by more than 3 mAP50, while maintaining real-time processing capabilities for practical clinical applications.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>MedROV's methodology involves curating Omnis, a large-scale (600K samples, 9 modalities) dataset to enable open-vocabulary learning. It addresses missing annotations in multi-source data via a pseudo-labeling strategy. The model integrates knowledge from a large pre-trained foundation model and employs contrastive learning coupled with cross-modal representations to learn robust features capable of detecting both known and novel medical structures.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>MedROV achieves real-time open-vocabulary detection at 70 FPS, capable of detecting both known and novel medical structures. It significantly outperforms the previous state-of-the-art medical foundation model by an average absolute improvement of 40 mAP50 and surpasses closed-set detectors by more than 3 mAP50, establishing a new benchmark in medical detection.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The ability of MedROV to detect novel medical structures in real-time offers a transformative clinical impact, enabling more adaptable and robust AI diagnostic tools. This can lead to earlier and more accurate identification of emergent or rare conditions, reduce the burden on clinicians by improving automated screening, and ultimately enhance patient outcomes by accelerating diagnosis and treatment pathways across various medical imaging modalities.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of MedROV. Common considerations for such models could include generalization to extremely rare conditions not represented in the training distribution, or the interpretability of 'novel' detections.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions. Potential avenues could involve extending to 3D medical imaging, integrating more diverse data types beyond visual (e.g., electronic health records), or developing robust methods for human-in-the-loop validation of novel detections.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Diagnostics</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Imaging AI</span>
                    
                    <span class="tag">Anatomy detection</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Open-vocabulary detection</span>
                    
                    <span class="tag tag-keyword">Medical imaging</span>
                    
                    <span class="tag tag-keyword">Real-time AI</span>
                    
                    <span class="tag tag-keyword">Foundation models</span>
                    
                    <span class="tag tag-keyword">Contrastive learning</span>
                    
                    <span class="tag tag-keyword">Pseudo-labeling</span>
                    
                    <span class="tag tag-keyword">Multi-modal data</span>
                    
                    <span class="tag tag-keyword">Object detection</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>