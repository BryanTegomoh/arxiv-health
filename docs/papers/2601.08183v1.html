<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards - Health AI Hub</title>
    <meta name="description" content="This paper introduces GI-Bench, a comprehensive benchmark to systematically evaluate Multimodal Large Language Models (MLLMs) in gastrointestinal endoscopy acro">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.08183v1" target="_blank">2601.08183v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-13
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yan Zhu, Te Luo, Pei-Yao Fu, Zhen Zhang, Zi-Long Wang, Yi-Fan Qu, Zi-Han Geng, Jia-Qi Xu, Lu Yao, Li-Yun Ma, Wei Su, Wei-Feng Chen, Quan-Lin Li, Shuo Wang, Ping-Hong Zhou
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.08183v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.08183v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces GI-Bench, a comprehensive benchmark to systematically evaluate Multimodal Large Language Models (MLLMs) in gastrointestinal endoscopy across a five-stage clinical workflow, comparing their performance against human endoscopists and trainees. It reveals that while top MLLMs can rival junior endoscopists in diagnostic reasoning, they suffer from critical 'spatial grounding bottlenecks' and a 'fluency-accuracy paradox' due to hallucination in report generation. The study highlights significant strengths in reasoning but profound weaknesses in visual localization and factual reporting, indicating a knowledge-experience dissociation.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research provides a crucial, systematic assessment of MLLMs' capabilities and limitations within a complex medical domain like gastrointestinal endoscopy. By pinpointing specific weaknesses such as spatial grounding and factual hallucination, it directly informs the safe and responsible development and deployment of AI tools in medicine, ensuring that clinicians understand where human oversight is indispensable.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research evaluates Multimodal Large Language Models (MLLMs) as potential AI assistants for gastrointestinal endoscopy. This includes their application in automating or supporting tasks such as identifying and localizing gastrointestinal lesions from endoscopic images, assisting in diagnosis, generating findings descriptions, and potentially aiding in management decisions within a clinical workflow. The paper specifically benchmarks MLLM performance against human medical professionals to assess their clinical utility.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>GI-Bench, a panoramic benchmark encompassing 20 fine-grained GI lesion categories, was constructed for MLLM evaluation in endoscopy.</li>
                    
                    <li>Twelve state-of-the-art MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management.</li>
                    
                    <li>Model performance was rigorously benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and a multi-dimensional Likert scale.</li>
                    
                    <li>Top-tier MLLMs, exemplified by Gemini-3-Pro, achieved state-of-the-art performance, outperforming trainees (Macro-F1 0.641 vs. 0.492) and rivaling junior endoscopists (Macro-F1 0.727; p>0.05) in diagnostic reasoning.</li>
                    
                    <li>A critical 'spatial grounding bottleneck' was identified: human lesion localization (mIoU >0.506) significantly outperformed the best MLLM (mIoU 0.345; p<0.05).</li>
                    
                    <li>A 'fluency-accuracy paradox' was observed: MLLM-generated reports had superior linguistic readability (p<0.05) but significantly lower factual correctness (p<0.05) due to 'over-interpretation' and visual hallucination.</li>
                    
                    <li>GI-Bench maintains a dynamic leaderboard (https://roterdl.github.io/GIBench/) to track the evolving performance of MLLMs in clinical endoscopy.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study constructed GI-Bench, a comprehensive benchmark covering 20 fine-grained gastrointestinal lesion categories. Twelve state-of-the-art MLLMs were evaluated across a five-stage clinical workflow (anatomical localization, lesion identification, diagnosis, findings description, management). Model performance was quantitatively assessed using Macro-F1 for diagnostic accuracy, mean Intersection-over-Union (mIoU) for spatial localization, and qualitatively via a multi-dimensional Likert scale for aspects like report quality. Performance was directly benchmarked against 3 junior endoscopists and 3 residency trainees, with statistical significance determined using p-values.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Gemini-3-Pro achieved state-of-the-art performance among the MLLMs. Top models demonstrated strong diagnostic reasoning, outperforming trainees (Macro-F1 0.641 vs. 0.492) and nearing junior endoscopists (0.727; p>0.05). However, a significant 'spatial grounding bottleneck' was identified, with MLLMs performing substantially worse in lesion localization (best mIoU 0.345) compared to humans (mIoU >0.506; p<0.05). Furthermore, a 'fluency-accuracy paradox' revealed that while MLLM-generated reports were linguistically superior (p<0.05), they suffered from significantly lower factual correctness (p<0.05) due to visual 'over-interpretation' and hallucination.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings suggest that MLLMs hold promise for assisting in specific stages of gastrointestinal endoscopy, particularly in initial diagnostic reasoning, potentially enhancing efficiency or aiding less experienced clinicians. However, their current limitations in precise spatial localization and propensity for factual errors and hallucinations in generated reports pose significant barriers to autonomous clinical deployment. For safe clinical integration, MLLMs would require robust human oversight, especially for visually critical tasks and official documentation, until these fundamental accuracy issues are resolved.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Current MLLMs exhibit a critical 'spatial grounding bottleneck,' making them unreliable for precise lesion localization, which is crucial in endoscopy. They also demonstrate a 'fluency-accuracy paradox,' where linguistically fluent reports are often factually incorrect due to over-interpretation and hallucination of visual features, limiting their utility for reliable clinical documentation. The study primarily benchmarks against junior endoscopists and trainees, which may not fully reflect the capabilities of highly experienced senior endoscopists.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research and development should prioritize addressing the 'spatial grounding bottleneck' to improve MLLMs' precise anatomical localization capabilities. Efforts must also focus on mitigating visual 'over-interpretation' and hallucination to enhance the factual correctness and reliability of MLLM-generated clinical reports. The dynamic GI-Bench leaderboard will serve as an ongoing platform to track and incentivize these critical advancements in MLLM performance for clinical endoscopy.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Gastroenterology</span>
                    
                    <span class="tag">Endoscopy</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Multimodal Large Language Models (MLLMs)</span>
                    
                    <span class="tag tag-keyword">Gastrointestinal Endoscopy</span>
                    
                    <span class="tag tag-keyword">Clinical Benchmark</span>
                    
                    <span class="tag tag-keyword">Diagnostic Reasoning</span>
                    
                    <span class="tag tag-keyword">Spatial Grounding</span>
                    
                    <span class="tag tag-keyword">Medical AI</span>
                    
                    <span class="tag tag-keyword">Hallucination</span>
                    
                    <span class="tag tag-keyword">GI-Bench</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p>0.05). However, a critical "spatial grounding bottleneck" persisted; human lesion localization (mIoU >0.506) significantly outperformed the best model (0.345; p<0.05). Furthermore, qualitative analysis revealed a "fluency-accuracy paradox": models generated reports with superior linguistic readability compared with humans (p<0.05) but exhibited significantly lower factual correctness (p<0.05) due to "over-interpretation" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>45 pages, 17 figures, 6 tables. Leaderboard available at: https://roterdl.github.io/GIBench/ . Includes supplementary material</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>