<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making - Health AI Hub</title>
    <meta name="description" content="This paper investigates how audio large language models (LLMs) perform in clinical decision-making, revealing that paralinguistic cues in patient voices introdu">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.06592v1" target="_blank">2511.06592v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zhi Rui Tam, Yun-Nung Chen
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, eess.AS
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.06592v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.06592v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper investigates how audio large language models (LLMs) perform in clinical decision-making, revealing that paralinguistic cues in patient voices introduce significant biases. It demonstrates a severe modality bias and age disparities, indicating that these models can base clinical decisions on voice characteristics rather than medical evidence, thereby risking the perpetuation of healthcare disparities.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critically important for ensuring equitable, safe, and evidence-based healthcare. The demonstrated biases in AI decisions, particularly those influenced by non-medical vocal characteristics, could lead to misdiagnoses, inappropriate treatments, and exacerbate existing healthcare disparities, directly threatening patient trust and outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper focuses on the application of audio Large Language Models (LLMs) in clinical settings for decision-making. Specifically, it examines how these AI models process audio inputs from patients and generate medical recommendations (e.g., surgical recommendations), highlighting potential biases introduced by paralinguistic cues (age, gender, emotion in voice) that could lead to erroneous or discriminatory clinical decisions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study evaluated audio LLMs on 170 clinical cases, using synthesized speech from 36 voice profiles varying in age, gender, and emotion.</li>
                    
                    <li>A severe modality bias was identified: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations.</li>
                    
                    <li>Age disparities of up to 12% were observed in clinical decisions between young and elderly voices, which largely persisted despite the application of chain-of-thought prompting.</li>
                    
                    <li>Explicit reasoning was successful in eliminating detected gender bias in the models' clinical recommendations.</li>
                    
                    <li>The impact of emotional cues on clinical decision-making could not be accurately assessed due to the poor emotion recognition performance of the audio LLMs.</li>
                    
                    <li>The findings highlight that current audio LLMs are susceptible to making clinical decisions based on non-medical patient voice characteristics.</li>
                    
                    <li>The authors conclude that bias-aware architectures are urgently required before these models are deployed in clinical environments.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved evaluating audio LLMs using 170 clinical cases. Each case was synthesized into speech across 36 distinct voice profiles, designed to represent variations in age (young/elderly), gender, and emotion. The LLMs' clinical recommendations, particularly for surgical interventions, were compared between audio inputs and identical text-based inputs. Further analyses examined age and gender disparities, with methods like chain-of-thought prompting and explicit reasoning employed to investigate and mitigate biases. Emotion impact was also assessed based on recognition performance.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study revealed a severe modality bias, with surgical recommendations differing by up to 35% between audio and text inputs, and one model exhibiting an 80% reduction in recommendations for audio. Age disparities of up to 12% between young and elderly voices were observed and largely resistant to chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion could not be determined due to the models' poor emotion recognition capabilities.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The potential clinical impact is significant: unchecked biases in audio LLMs could lead to differential and potentially substandard care based on non-medical factors like a patient's voice. This risk of perpetuating or creating new healthcare disparities underscores the urgent need for robust validation and bias mitigation strategies to ensure AI tools contribute positively and equitably to patient care, preventing misdiagnosis or delayed treatment for vulnerable populations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>A primary limitation noted is the poor recognition performance of the evaluated audio LLMs concerning emotion, which precluded a comprehensive assessment of how emotional paralinguistic cues might influence clinical decision-making.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper explicitly states the necessity for developing and integrating "bias-aware architectures" as an urgent priority for future research and development before the safe and ethical clinical deployment of audio LLMs.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">General Medicine</span>
                    
                    <span class="tag">Surgery</span>
                    
                    <span class="tag">Patient Assessment</span>
                    
                    <span class="tag">Healthcare Equity</span>
                    
                    <span class="tag">Medical AI Ethics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">audio LLMs</span>
                    
                    <span class="tag tag-keyword">clinical decision-making</span>
                    
                    <span class="tag tag-keyword">modality bias</span>
                    
                    <span class="tag tag-keyword">healthcare disparities</span>
                    
                    <span class="tag tag-keyword">paralinguistic cues</span>
                    
                    <span class="tag tag-keyword">age bias</span>
                    
                    <span class="tag tag-keyword">medical AI</span>
                    
                    <span class="tag tag-keyword">AI ethics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">As large language models transition from text-based interfaces to audio
interactions in clinical settings, they might introduce new vulnerabilities
through paralinguistic cues in audio. We evaluated these models on 170 clinical
cases, each synthesized into speech from 36 distinct voice profiles spanning
variations in age, gender, and emotion. Our findings reveal a severe modality
bias: surgical recommendations for audio inputs varied by as much as 35%
compared to identical text-based inputs, with one model providing 80% fewer
recommendations. Further analysis uncovered age disparities of up to 12%
between young and elderly voices, which persisted in most models despite
chain-of-thought prompting. While explicit reasoning successfully eliminated
gender bias, the impact of emotion was not detected due to poor recognition
performance. These results demonstrate that audio LLMs are susceptible to
making clinical decisions based on a patient's voice characteristics rather
than medical evidence, a flaw that risks perpetuating healthcare disparities.
We conclude that bias-aware architectures are essential and urgently needed
before the clinical deployment of these models.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>