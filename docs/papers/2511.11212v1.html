<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MAFM^3: Modular Adaptation of Foundation Models for Multi-Modal Medical AI - Health AI Hub</title>
    <meta name="description" content="This paper introduces MAFM^3, a novel framework designed to adapt a single medical imaging foundation model to diverse tasks, domains, and modalities through li">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MAFM^3: Modular Adaptation of Foundation Models for Multi-Modal Medical AI</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.11212v1" target="_blank">2511.11212v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-14
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mohammad Areeb Qazi, Munachiso S Nwadike, Ibrahim Almakky, Mohammad Yaqub, Numan Saeed
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.11212v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.11212v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MAFM^3, a novel framework designed to adapt a single medical imaging foundation model to diverse tasks, domains, and modalities through lightweight modular components. This approach enables dynamic activation of specialized capabilities at inference, addressing data scarcity by avoiding separate pre-training for every scenario. Empirically, MAFM^3 improved performance on prognosis and segmentation tasks when adapting a chest CT model, and achieved a 5% Dice score improvement in multimodality segmentation by incorporating PET scans.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>MAFM^3 significantly enhances the practical utility of advanced AI in medical imaging by enabling a single, robust foundation model to tackle a broader spectrum of clinical tasks and integrate various imaging modalities. This could streamline the development of AI tools for more comprehensive and accurate diagnostics and prognostics.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research focuses on developing a modular and adaptable AI framework for analyzing multi-modal medical images (e.g., CT and PET scans) to perform diverse clinical tasks, such as disease prognosis and image segmentation. It aims to create efficient, unified AI systems that can learn and adapt across various medical domains, modalities, and tasks, thereby addressing data scarcity challenges in specialized medical AI development.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of data scarcity in medical imaging by proposing a unified adaptation framework for foundation models, rather than building separate models for each domain, modality, or task.</li>
                    
                    <li>Introduces MAFM^3 (Modular Adaptation of Foundation Models for Multi-Modal Medical AI), which uses lightweight modular components to expand a single foundation model's capabilities.</li>
                    
                    <li>These modular components serve as specialized 'skill sets' that are flexibly activated at inference time, depending on the specific input type (modality) or clinical objective (task).</li>
                    
                    <li>The framework provides a unified and expandable solution for efficient multitask and multimodality adaptation, contrasting with conventional isolated adaptation methods.</li>
                    
                    <li>Empirical validation involved adapting a chest CT foundation model, initially trained for classification, to perform prognosis and segmentation tasks, demonstrating improved performance on both.</li>
                    
                    <li>Further validation showed that incorporating PET scans into MAFM^3 for multimodality analysis resulted in a 5% improvement in Dice score for segmentation compared to respective baselines.</li>
                    
                    <li>The findings establish that foundation models, when equipped with these modular components, can effectively evolve beyond their initial training scope to become versatile multitask, multimodality systems for medical imaging.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The MAFM^3 framework integrates lightweight modular components with a pre-trained foundation model. These modules are designed as task- and modality-specific 'skill sets' that are dynamically activated during inference based on the input imaging data and the desired clinical output. The empirical validation involved adapting a chest CT foundation model (initially trained for classification) to new tasks: prognosis and segmentation. Additionally, the framework's multimodality capabilities were tested by integrating PET scans alongside CT data for segmentation tasks, measuring performance improvements using metrics like Dice score.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The MAFM^3 framework demonstrated improved performance on both prognosis and segmentation tasks when adapting a chest CT foundation model. A significant finding was the 5% improvement in Dice score for segmentation when MAFM^3 incorporated PET scans, showcasing its effectiveness in multimodality adaptation. These results collectively indicate that foundation models, augmented with modular components, can overcome their initial training limitations to become versatile multitask and multimodality systems.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to simplify and accelerate the deployment of AI in clinical practice by reducing the need for numerous specialized models for different tasks or modalities. A single, adaptable foundation model capable of handling diverse diagnostic and prognostic tasks across various imaging types (e.g., CT, PET) could lead to more efficient workflows, potentially offering clinicians more accurate and comprehensive insights from patient imaging data, thereby improving patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of the MAFM^3 framework or the experimental setup, focusing primarily on its contributions and positive results. It does not mention computational overhead, generalization across all possible medical domains/modalities, or comparison against a wider range of baseline adaptation methods.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly outlined as future research directions, the concluding statement that foundation models 'can evolve into multitask, multimodality systems for medical imaging' implies a future where MAFM^3 could be expanded to a broader array of medical imaging modalities, anatomical regions, and clinical tasks, potentially including different medical domains beyond chest imaging or more complex predictive modeling.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Chest Imaging</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Foundation Models</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Multi-Modal</span>
                    
                    <span class="tag tag-keyword">Multi-Task</span>
                    
                    <span class="tag tag-keyword">Modular Adaptation</span>
                    
                    <span class="tag tag-keyword">Chest CT</span>
                    
                    <span class="tag tag-keyword">PET Scan</span>
                    
                    <span class="tag tag-keyword">Segmentation</span>
                    
                    <span class="tag tag-keyword">Prognosis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Foundational models are trained on extensive datasets to capture the general trends of a domain. However, in medical imaging, the scarcity of data makes pre-training for every domain, modality, or task challenging. Instead of building separate models, we propose MAFM^3 (Modular Adaptation of Foundation Models for Multi-Modal Medical AI), a framework that enables a single foundation model to expand into diverse domains, tasks, and modalities through lightweight modular components. These components serve as specialized skill sets that allow the system to flexibly activate the appropriate capability at the inference time, depending on the input type or clinical objective. Unlike conventional adaptation methods that treat each new task or modality in isolation, MAFM^3 provides a unified and expandable framework for efficient multitask and multimodality adaptation. Empirically, we validate our approach by adapting a chest CT foundation model initially trained for classification into prognosis and segmentation modules. Our results show improved performance on both tasks. Furthermore, by incorporating PET scans, MAFM^3 achieved an improvement in the Dice score 5% compared to the respective baselines. These findings establish that foundation models, when equipped with modular components, are not inherently constrained to their initial training scope but can evolve into multitask, multimodality systems for medical imaging. The code implementation of this work can be found at https://github.com/Areeb2735/CTscan_prognosis_VLM</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>2 figures, 3 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>