<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MedM2T: A MultiModal Framework for Time-Aware Modeling with Electronic Health Record and Electrocardiogram Data - Health AI Hub</title>
    <meta name="description" content="MedM2T is a novel time-aware multimodal framework designed to integrate heterogeneous Electronic Health Record (EHR) and Electrocardiogram (ECG) data, addressin">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MedM2T: A MultiModal Framework for Time-Aware Modeling with Electronic Health Record and Electrocardiogram Data</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27321v1" target="_blank">2510.27321v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yu-Chen Kuo, Yi-Ju Tseng
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27321v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27321v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">MedM2T is a novel time-aware multimodal framework designed to integrate heterogeneous Electronic Health Record (EHR) and Electrocardiogram (ECG) data, addressing complexities arising from their distinct temporal structures and multimodality. It demonstrates superior predictive performance over state-of-the-art models across critical clinical tasks, highlighting its robustness for both chronic and acute disease dynamics.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework offers a powerful new approach to integrate and analyze complex, heterogeneous medical data, which is crucial for improving the accuracy of predicting significant health outcomes and disease progression, thereby facilitating earlier, more targeted interventions and optimizing patient management.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The MedM2T framework is a medical AI application that leverages multimodal deep learning to integrate and analyze heterogeneous temporal medical data (EHR and ECG). Its primary application is to provide advanced predictive capabilities for various clinical outcomes, including chronic disease risk (CVD), acute event prognosis (in-hospital mortality), and resource management (ICU length-of-stay). This can aid clinicians in risk stratification, early intervention, and personalized patient care.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Multimodal and Time-Aware Framework:** MedM2T integrates disparate medical data modalities like sparse EHR time series and dense ECG time series, with a design that is extendable to an arbitrary number of modalities.</li>
                    
                    <li>**Advanced Temporal Handling:** It employs a Sparse Time Series Encoder for flexible processing of irregular EHR data and a Hierarchical Time-Aware Fusion module for capturing both micro- and macro-temporal patterns from dense ECG data.</li>
                    
                    <li>**Cross-Modal Interaction:** The framework incorporates Bi-Modal Attention to effectively extract and leverage interactions between different data modalities, enhancing predictive power.</li>
                    
                    <li>**Granularity Gap Mitigation:** Modality-specific pre-trained encoders are utilized, and their resultant features are aligned within a shared encoder to address inherent granularity differences between data types.</li>
                    
                    <li>**Superior Performance:** MedM2T significantly outperformed state-of-the-art multimodal learning frameworks and existing time series models on MIMIC-IV and MIMIC-IV-ECG datasets across three diverse clinical tasks.</li>
                    
                    <li>**Diverse Clinical Task Validation:** The framework was validated for 90-day cardiovascular disease (CVD) prediction, in-hospital mortality prediction, and ICU Length-of-Stay (LOS) regression, showcasing its applicability to both chronic and acute conditions.</li>
                    
                    <li>**Promising Clinical Tool:** With high AUROC (e.g., 0.947 for CVD prediction) and low MAE (e.g., 2.31 for LOS regression), MedM2T is positioned as a robust and broadly applicable tool for clinical prediction.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>MedM2T is a deep learning framework comprising three core components: (i) a Sparse Time Series Encoder designed to handle irregular and sparse EHR data, (ii) a Hierarchical Time-Aware Fusion module that processes dense time series like ECGs to capture both micro- and macro-temporal patterns, and (iii) a Bi-Modal Attention mechanism to extract and integrate cross-modal interactions. The framework also mitigates granularity gaps between modalities by employing modality-specific pre-trained encoders and aligning their features within a shared encoder.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>MedM2T demonstrated superior performance across all evaluated clinical prediction tasks. For 90-day cardiovascular disease prediction, it achieved an AUROC of 0.947 and an AUPRC of 0.706. For in-hospital mortality prediction, it yielded an AUROC of 0.901 and an AUPRC of 0.558. In ICU Length-of-Stay regression, MedM2T achieved a Mean Absolute Error (MAE) of 2.31. These results consistently surpassed state-of-the-art multimodal learning frameworks and existing time series models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The enhanced accuracy in predicting chronic diseases like CVD, acute outcomes such as in-hospital mortality, and critical resource metrics like ICU length-of-stay, could significantly improve risk stratification, enable proactive clinical interventions, and optimize hospital resource allocation. This has the potential to lead to more personalized patient care, reduce adverse events, and ultimately improve patient outcomes in both chronic disease management and acute care settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract. However, typical limitations for such deep learning models might include the need for extensive, high-quality multimodal datasets for training, challenges in model interpretability for clinical insights, and generalizability to external, diverse patient populations or varying healthcare systems.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract. Potential future research could involve integrating additional data modalities (e.g., medical imaging, genomics), exploring methods for improving model interpretability to provide actionable clinical insights, and conducting prospective validation studies in real-world clinical settings to assess generalizability and real-time utility.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Critical Care Medicine</span>
                    
                    <span class="tag">Hospital Medicine</span>
                    
                    <span class="tag">Predictive Analytics</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Electronic Health Records</span>
                    
                    <span class="tag tag-keyword">ECG</span>
                    
                    <span class="tag tag-keyword">Multimodal Learning</span>
                    
                    <span class="tag tag-keyword">Time Series Analysis</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Clinical Prediction</span>
                    
                    <span class="tag tag-keyword">Cardiovascular Disease</span>
                    
                    <span class="tag tag-keyword">Mortality Prediction</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The inherent multimodality and heterogeneous temporal structures of medical
data pose significant challenges for modeling. We propose MedM2T, a time-aware
multimodal framework designed to address these complexities. MedM2T integrates:
(i) Sparse Time Series Encoder to flexibly handle irregular and sparse time
series, (ii) Hierarchical Time-Aware Fusion to capture both micro- and
macro-temporal patterns from multiple dense time series, such as ECGs, and
(iii) Bi-Modal Attention to extract cross-modal interactions, which can be
extended to any number of modalities. To mitigate granularity gaps between
modalities, MedM2T uses modality-specific pre-trained encoders and aligns
resulting features within a shared encoder. We evaluated MedM2T on MIMIC-IV and
MIMIC-IV-ECG datasets for three tasks that encompass chronic and acute disease
dynamics: 90-day cardiovascular disease (CVD) prediction, in-hospital mortality
prediction, and ICU length-of-stay (LOS) regression. MedM2T outperformed
state-of-the-art multimodal learning frameworks and existing time series
models, achieving an AUROC of 0.947 and an AUPRC of 0.706 for CVD prediction;
an AUROC of 0.901 and an AUPRC of 0.558 for mortality prediction; and Mean
Absolute Error (MAE) of 2.31 for LOS regression. These results highlight the
robustness and broad applicability of MedM2T, positioning it as a promising
tool in clinical prediction. We provide the implementation of MedM2T at
https://github.com/DHLab-TSENG/MedM2T.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>This preprint version of the manuscript has been submitted to the
  IEEE Journal of Biomedical and Health Informatics (JBHI) for review. The
  implementation of MedM2T is available at
  https://github.com/DHLab-TSENG/MedM2T</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>