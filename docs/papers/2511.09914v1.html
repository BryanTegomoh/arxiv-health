<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OIDA-QA: A Multimodal Benchmark for Analyzing the Opioid Industry Documents Archive - Health AI Hub</title>
    <meta name="description" content="This paper introduces OIDA-QA, a novel multimodal benchmark and an AI assistant designed to systematically analyze the complex, vast UCSF-JHU Opioid Industry Do">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>OIDA-QA: A Multimodal Benchmark for Analyzing the Opioid Industry Documents Archive</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.09914v1" target="_blank">2511.09914v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-13
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xuan Shen, Brian Wingenroth, Zichao Wang, Jason Kuen, Wanrong Zhu, Ruiyi Zhang, Yiwei Wang, Lichun Ma, Anqi Liu, Hongfu Liu, Tong Sun, Kevin S. Hawkins, Kate Tasker, G. Caleb Alexander, Jiuxiang Gu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.09914v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.09914v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces OIDA-QA, a novel multimodal benchmark and an AI assistant designed to systematically analyze the complex, vast UCSF-JHU Opioid Industry Documents Archive (OIDA). By extracting rich multimodal information and leveraging domain-specific multimodal Large Language Models, the work significantly improves information extraction and question-answering from these critical documents. Preliminary results demonstrate enhanced performance in uncovering the systemic failures that contributed to the opioid crisis.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is profoundly relevant to public health by enabling a deeper, more systematic understanding of the intricate corporate, regulatory, and healthcare system failures that fueled the opioid crisis. Improved analytical tools for these documents can directly inform evidence-based policy changes, support legal accountability, and develop more effective preventative strategies to mitigate future public health emergencies.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Development of multimodal Large Language Models (LLMs) and AI assistants for advanced information extraction and question-answering from complex, domain-specific healthcare-related legal and corporate documents. This application enables comprehensive analysis of public health crises, identifying systemic failures in healthcare practices, regulatory systems, and corporate governance to inform future policy and prevention strategies.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the significant challenge of analyzing the vast, complex, multimodal, and specialized UCSF-JHU Opioid Industry Documents Archive (OIDA) to understand systemic failures.</li>
                    
                    <li>Constructed a large-scale multimodal benchmark (OIDA-QA) comprising 400k training and 10k testing documents, capturing textual content, visual elements, and layout structures.</li>
                    
                    <li>Generated a substantial dataset of 360k training and 10k testing QA pairs using multiple AI models to facilitate domain-specific question answering.</li>
                    
                    <li>Developed domain-specific multimodal Large Language Models (LLMs) explicitly tailored to leverage multimodal inputs for improved task performance in analyzing the archive.</li>
                    
                    <li>Incorporated historical QA pairs as contextual grounding for current queries and introduced an importance-based page classifier to include precise page references within answers, enhancing accuracy and relevance.</li>
                    
                    <li>Preliminary results indicate tangible improvements in document information extraction and question-answering tasks achieved by their AI assistant.</li>
                    
                    <li>The created dataset and models are made publicly available to foster further research and analysis in this critical public health domain.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves organizing the original UCSF-JHU Opioid Industry Documents Archive (OIDA) by document attributes to construct a large-scale benchmark (400k training, 10k testing). Multimodal information, including textual content, visual elements, and layout structures, is extracted from each document. Using multiple AI models, a dataset of 360k training and 10k testing QA pairs is generated. Domain-specific multimodal Large Language Models (LLMs) are then developed, incorporating historical QA pairs for contextual grounding and an importance-based page classifier to provide precise page references within answers, thereby enhancing response accuracy and relevance.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Preliminary results demonstrate notable improvements in document information extraction and question-answering tasks when utilizing their developed AI assistant. This indicates the effectiveness of their multimodal benchmark and domain-specific LLMs in processing and analyzing complex, specialized documents from the opioid industry archive.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>While not directly focused on individual patient treatment, this work has significant public health and policy clinical impact. By facilitating a granular and comprehensive analysis of the opioid industry documents, it provides crucial insights into the mechanisms of the crisis. This can inform public health officials, policymakers, and legal entities to devise more effective regulatory frameworks, establish accountability, and implement targeted interventions, ultimately aiming to prevent similar crises and improve population health outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract explicitly states that the results are 'preliminary,' which suggests that the findings may require further extensive validation, testing, and broader application to fully ascertain the system's robustness, generalizability, and long-term efficacy. No other specific limitations or caveats are detailed within the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions. However, the 'preliminary results' implicitly suggest ongoing work to further refine, validate, and potentially expand the capabilities of the developed models and benchmark, possibly exploring more complex query types, integrating additional data sources, or deploying the AI assistant for wider public health applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Healthcare Policy</span>
                    
                    <span class="tag">Addiction Medicine</span>
                    
                    <span class="tag">Health Law</span>
                    
                    <span class="tag">Medical Ethics</span>
                    
                    <span class="tag">Epidemiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">opioid crisis</span>
                    
                    <span class="tag tag-keyword">public health</span>
                    
                    <span class="tag tag-keyword">AI</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">multimodal analysis</span>
                    
                    <span class="tag tag-keyword">document analysis</span>
                    
                    <span class="tag tag-keyword">UCSF-JHU Opioid Industry Documents Archive</span>
                    
                    <span class="tag tag-keyword">benchmarking</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The opioid crisis represents a significant moment in public health that reveals systemic shortcomings across regulatory systems, healthcare practices, corporate governance, and public policy. Analyzing how these interconnected systems simultaneously failed to protect public health requires innovative analytic approaches for exploring the vast amounts of data and documents disclosed in the UCSF-JHU Opioid Industry Documents Archive (OIDA). The complexity, multimodal nature, and specialized characteristics of these healthcare-related legal and corporate documents necessitate more advanced methods and models tailored to specific data types and detailed annotations, ensuring the precision and professionalism in the analysis. In this paper, we tackle this challenge by organizing the original dataset according to document attributes and constructing a benchmark with 400k training documents and 10k for testing. From each document, we extract rich multimodal information-including textual content, visual elements, and layout structures-to capture a comprehensive range of features. Using multiple AI models, we then generate a large-scale dataset comprising 360k training QA pairs and 10k testing QA pairs. Building on this foundation, we develop domain-specific multimodal Large Language Models (LLMs) and explore the impact of multimodal inputs on task performance. To further enhance response accuracy, we incorporate historical QA pairs as contextual grounding for answering current queries. Additionally, we incorporate page references within the answers and introduce an importance-based page classifier, further improving the precision and relevance of the information provided. Preliminary results indicate the improvements with our AI assistant in document information extraction and question-answering tasks. The dataset and models are publicly available at: https://huggingface.co/opioidarchive</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted by AAAI 2026 Artificial Intelligence for Social Impact Track</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>