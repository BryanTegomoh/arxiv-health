<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA Dataset - Health AI Hub</title>
    <meta name="description" content="This paper audits the fairness of automated segmentation labels in the MAMA-MIA breast cancer dataset, revealing significant age and ethnic biases. It identifie">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA Dataset</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27421v1" target="_blank">2510.27421v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Aditya Parikh, Sneha Das, Aasa Feragen
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27421v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27421v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper audits the fairness of automated segmentation labels in the MAMA-MIA breast cancer dataset, revealing significant age and ethnic biases. It identifies an intrinsic age-related bias against younger patients, hypothesized to be linked to physiological factors, and demonstrates how data aggregation can obscure site-specific ethnic biases. The study underscores the critical need for granular fairness evaluations in deep learning models for diagnostic imaging.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Unaddressed biases in automated medical image segmentation can lead to inequitable quality of care for certain patient populations, particularly in critical areas like breast cancer diagnosis. This research highlights how AI models, if not carefully evaluated for fairness, can exacerbate existing health disparities.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper investigates the fairness and bias of deep learning models used for automated breast cancer tumor segmentation, a key application of AI in medical diagnosis and image analysis. It aims to improve the reliability and equity of AI tools in clinical settings by identifying and understanding biases.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The research addresses the underexplored area of fairness evaluation in deep learning image segmentation, specifically for breast cancer tumor segmentation.</li>
                    
                    <li>It audits the quality of automated segmentation labels within the MAMA-MIA dataset, analyzing performance across age, ethnicity, and data source.</li>
                    
                    <li>A significant intrinsic age-related bias was discovered, indicating poorer segmentation quality for younger patients, which persisted even after controlling for data source.</li>
                    
                    <li>The authors hypothesize that this age bias is likely linked to physiological factors, known challenges for both human radiologists and automated systems.</li>
                    
                    <li>The study demonstrates that aggregating data from multiple sources can influence and potentially obscure site-specific ethnic biases.</li>
                    
                    <li>It emphasizes the necessity of investigating model performance and data characteristics at a granular level to identify and address hidden biases.</li>
                    
                    <li>Unaddressed segmentation bias can lead to disparities in diagnostic quality and potentially worsen patient outcomes across clinical decision points.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study performed an audit of automated segmentation labels within the MAMA-MIA breast cancer tumor segmentation dataset. It evaluated automated segmentation quality by stratifying performance across patient age, ethnicity, and the original data source, controlling for confounding factors like data source during bias analysis.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>['An intrinsic age-related bias exists in automated segmentation, resulting in poorer quality for younger breast cancer patients, which holds true even after accounting for the data source.', 'Physiological factors are hypothesized to be a key driver of this age-related bias, posing challenges for both AI and human experts.', 'Aggregating data from multiple sites can influence and potentially mask site-specific ethnic biases, stressing the need for detailed, granular data analysis.']</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Identifying and understanding these biases is crucial for developing more equitable AI tools in breast cancer diagnosis. This research can guide the development of fairer deep learning models that deliver consistent quality of care across diverse patient demographics, reducing disparities in diagnostic accuracy and treatment planning. It also alerts clinicians and developers to potential pitfalls of deploying biased algorithms in real-world settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract implicitly notes that physiological factors present an inherent challenge for current automated systems (and radiologists), indicating a limitation in the ability of existing models to accurately segment tumors across all patient demographics, particularly younger patients. The observed biases highlight a current deficiency in equitable performance.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should focus on investigating the specific physiological factors contributing to the age-related bias and developing robust methods to mitigate these biases in segmentation models. There's also a clear call for more granular data analysis during model development, especially when combining multi-source datasets, to prevent the obscuring of site-specific ethnic biases.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical AI</span>
                    
                    <span class="tag">Public Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Deep learning</span>
                    
                    <span class="tag tag-keyword">Fairness</span>
                    
                    <span class="tag tag-keyword">Bias</span>
                    
                    <span class="tag tag-keyword">Image segmentation</span>
                    
                    <span class="tag tag-keyword">Breast cancer</span>
                    
                    <span class="tag tag-keyword">Age bias</span>
                    
                    <span class="tag tag-keyword">Ethnic bias</span>
                    
                    <span class="tag tag-keyword">MAMA-MIA</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Deep learning models aim to improve diagnostic workflows, but fairness
evaluation remains underexplored beyond classification, e.g., in image
segmentation. Unaddressed segmentation bias can lead to disparities in the
quality of care for certain populations, potentially compounded across clinical
decision points and amplified through iterative model development. Here, we
audit the fairness of the automated segmentation labels provided in the breast
cancer tumor segmentation dataset MAMA-MIA. We evaluate automated segmentation
quality across age, ethnicity, and data source. Our analysis reveals an
intrinsic age-related bias against younger patients that continues to persist
even after controlling for confounding factors, such as data source. We
hypothesize that this bias may be linked to physiological factors, a known
challenge for both radiologists and automated systems. Finally, we show how
aggregating data from multiple data sources influences site-specific ethnic
biases, underscoring the necessity of investigating data at a granular level.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Medical Imaging Meets EurIPS (NeurIPS-endorsed workshop) - MedEurIPS</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>