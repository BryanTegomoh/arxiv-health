<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback - Health AI Hub</title>
    <meta name="description" content="This research investigates the critical challenge of aligning AI with human moral preferences in high-stakes domains, noting that such preferences are often dyn">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.10032v1" target="_blank">2511.10032v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-13
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Vijay Keswani, Cyrus Cousins, Breanna Nguyen, Vincent Conitzer, Hoda Heidari, Jana Schaich Borg, Walter Sinnott-Armstrong
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.HC, cs.AI, cs.CY
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.10032v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.10032v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This research investigates the critical challenge of aligning AI with human moral preferences in high-stakes domains, noting that such preferences are often dynamic rather than static. Grounded in kidney allocation, the study reveals significant temporal instability in participants' responses and decision-making models, leading to decreased predictive performance of AI systems over time. The findings highlight the urgent need for AI alignment methods to account for evolving human preferences to ensure trustworthiness and prevent harm.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>In high-stakes medical domains like organ allocation, AI misalignment due to dynamic human moral preferences can severely jeopardize system trustworthiness, lead to ethically questionable decisions, and result in serious individual and societal harms. This research provides empirical evidence for the instability of preferences, necessitating more robust and adaptive AI alignment strategies in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research is directly relevant to the development and deployment of AI systems in healthcare, specifically those designed to assist with complex ethical decisions like organ allocation. It addresses the fundamental challenge of ensuring AI alignment with human values, which are shown to be dynamic, impacting the reliability and trustworthiness of AI in medical decision-making.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Current AI alignment methods often assume static human moral preferences, which this paper challenges as unrealistic.</li>
                    
                    <li>The study investigates the extent of temporal changes in human moral preferences and their impact on AI alignment, focusing on high-stakes healthcare applications.</li>
                    
                    <li>Methodology involved over 400 participants responding to pairwise comparisons of hypothetical kidney transplant patients across 3-5 sessions.</li>
                    
                    <li>Participants exhibited 'response instability,' changing their answers to identical scenarios 6-20% of the time across different sessions.</li>
                    
                    <li>'Model instability' was also observed, showing significant shifts in individual participants' underlying decision-making models over time.</li>
                    
                    <li>The predictive performance of simple AI models decreased as a direct function of both response and model instability, and also diminished over time.</li>
                    
                    <li>The research raises fundamental normative and technical challenges for AI alignment, emphasizing the need to define 'what to align to' when preferences are dynamic.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>A longitudinal study design was employed, involving over 400 participants who completed 3-5 separate sessions. During each session, participants provided responses to pairwise comparisons of hypothetical kidney transplant patient scenarios to elicit their moral preferences regarding organ allocation.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Participants demonstrated an average 'response instability' of 6-20%, changing their response to the same scenario when presented at different times. Additionally, significant 'model instability' was observed, reflecting shifts in participants' retrofitted decision-making models over time. The predictive performance of simple AI models was found to decrease with increasing response and model instability, and notably diminished as a function of time, underscoring the challenge of static alignment.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This study underscores that AI systems designed for critical medical decision-making, such as organ allocation, risk inherent bias and suboptimal outcomes if they assume static human moral preferences. Future clinical AI applications must develop mechanisms to continuously monitor and adapt to the temporal evolution of human ethical reasoning, ensuring that AI-driven recommendations remain aligned with current societal and individual values to maintain trust and ethical integrity.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations, but implicitly suggests challenges in distinguishing 'legitimate' moral changes from 'noise' (e.g., attention deficits, cognitive biases). The study also uses 'simple AI models,' which might not fully capture the complexities of advanced AI alignment techniques. Furthermore, the findings highlight a problem without proposing specific technical solutions for dynamic alignment.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research must focus on better understanding the 'object of alignment' when human preferences are dynamic, distinguishing between legitimate moral change and arbitrary fluctuations. This necessitates developing novel AI alignment methodologies capable of accounting for and adapting to evolving moral reasoning over time, potentially through continuous learning or context-aware preference elicitation.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Nephrology</span>
                    
                    <span class="tag">Transplant Medicine</span>
                    
                    <span class="tag">Medical Ethics</span>
                    
                    <span class="tag">Health Informatics</span>
                    
                    <span class="tag">Bioethics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">AI alignment</span>
                    
                    <span class="tag tag-keyword">moral preferences</span>
                    
                    <span class="tag tag-keyword">temporal instability</span>
                    
                    <span class="tag tag-keyword">kidney allocation</span>
                    
                    <span class="tag tag-keyword">human-AI interaction</span>
                    
                    <span class="tag tag-keyword">preference elicitation</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">decision support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Alignment methods in moral domains seek to elicit moral preferences of human stakeholders and incorporate them into AI. This presupposes moral preferences as static targets, but such preferences often evolve over time. Proper alignment of AI to dynamic human preferences should ideally account for "legitimate" changes to moral reasoning, while ignoring changes related to attention deficits, cognitive biases, or other arbitrary factors. However, common AI alignment approaches largely neglect temporal changes in preferences, posing serious challenges to proper alignment, especially in high-stakes applications of AI, e.g., in healthcare domains, where misalignment can jeopardize the trustworthiness of the system and yield serious individual and societal harms. This work investigates the extent to which people's moral preferences change over time, and the impact of such changes on AI alignment. Our study is grounded in the kidney allocation domain, where we elicit responses to pairwise comparisons of hypothetical kidney transplant patients from over 400 participants across 3-5 sessions. We find that, on average, participants change their response to the same scenario presented at different times around 6-20% of the time (exhibiting "response instability"). Additionally, we observe significant shifts in several participants' retrofitted decision-making models over time (capturing "model instability"). The predictive performance of simple AI models decreases as a function of both response and model instability. Moreover, predictive performance diminishes over time, highlighting the importance of accounting for temporal changes in preferences during training. These findings raise fundamental normative and technical challenges relevant to AI alignment, highlighting the need to better understand the object of alignment (what to align to) when user preferences change significantly over time.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>To appear in the AAAI 2026 Alignment Track</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>