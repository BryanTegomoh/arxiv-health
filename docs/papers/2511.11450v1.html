<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation - Health AI Hub</title>
    <meta name="description" content="VoxTell introduces a novel vision-language model capable of performing free-text prompted universal 3D medical image segmentation, translating natural language ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.11450v1" target="_blank">2511.11450v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-14
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Maximilian Rokuss, Moritz Langenberg, Yannick Kirchhoff, Fabian Isensee, Benjamin Hamm, Constantin Ulrich, Sebastian Regnery, Lukas Bauer, Efthimios Katsigiannopulos, Tobias Norajitra, Klaus Maier-Hein
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.11450v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.11450v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">VoxTell introduces a novel vision-language model capable of performing free-text prompted universal 3D medical image segmentation, translating natural language descriptions directly into 3D masks. Trained on a diverse dataset of over 62K CT, MRI, and PET volumes spanning 1K+ anatomical and pathological classes, it achieves state-of-the-art zero-shot performance across modalities. The model excels on familiar concepts, generalizes effectively to related unseen classes, and demonstrates strong robustness to linguistic and clinical variations.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This innovation significantly automates and democratizes access to advanced 3D medical image analysis, enabling clinicians and researchers to generate precise segmentations from natural language queries. This capability can accelerate diagnosis, optimize treatment planning, enhance surgical precision, and streamline research workflows without requiring specialized technical expertise in image processing.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>VoxTell is an AI model that automates and enhances the precise segmentation of anatomical structures and pathologies within 3D medical scans (CT, MRI, PET). By allowing medical professionals to use free-text clinical descriptions as prompts, it streamlines the process of identifying regions of interest for diagnosis, surgical planning, radiation dosage calculation, disease progression tracking, and medical research, thereby improving efficiency and accuracy in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>VoxTell is a pioneering vision-language model designed for text-prompted 3D medical image segmentation, mapping free-form textual descriptions to 3D masks.</li>
                    
                    <li>The model's training leveraged a massive dataset comprising over 62,000 CT, MRI, and PET volumes, encompassing more than 1,000 distinct anatomical and pathological classes.</li>
                    
                    <li>It employs a sophisticated multi-stage vision-language fusion mechanism integrated across decoder layers, ensuring robust alignment of textual and visual features at multiple scales.</li>
                    
                    <li>Achieves state-of-the-art zero-shot performance across various medical imaging modalities on previously unseen datasets.</li>
                    
                    <li>Demonstrates strong generalization capabilities, accurately segmenting familiar concepts while effectively extending to related, previously unseen classes.</li>
                    
                    <li>Exhibits high robustness to diverse linguistic variations, including complex clinical language, and can perform accurate instance-specific segmentation based on real-world text prompts.</li>
                    
                    <li>The code for VoxTell is publicly available, promoting reproducibility and further research.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>VoxTell is a vision-language model that utilizes a multi-stage fusion process across its decoder layers to align textual and visual features at multiple scales. It was trained on a large dataset of over 62K 3D medical volumes (CT, MRI, PET) annotated with more than 1K anatomical and pathological classes, learning to generate 3D segmentation masks in response to free-form text prompts.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key findings include VoxTell achieving state-of-the-art zero-shot segmentation performance across various unseen 3D medical imaging datasets and modalities. It demonstrates robust cross-modality transfer, high accuracy on familiar concepts, effective generalization to related unseen classes, and precise instance-specific segmentation even with complex clinical language, indicating its practical utility and broad applicability.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>VoxTell has the potential to profoundly impact clinical practice by providing an intuitive, rapid, and highly specific method for 3D medical image segmentation. This could significantly reduce the manual effort required for image annotation in tasks like tumor volume quantification, organ contouring for radiation therapy, or lesion detection, leading to faster diagnostic pathways, more accurate treatment planning, and enhanced quantitative assessment of disease progression and response to therapy across numerous specialties.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Gastroenterology</span>
                    
                    <span class="tag">Anatomy</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Medical Imaging Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">3D medical image segmentation</span>
                    
                    <span class="tag tag-keyword">vision-language model</span>
                    
                    <span class="tag tag-keyword">text-prompted</span>
                    
                    <span class="tag tag-keyword">zero-shot learning</span>
                    
                    <span class="tag tag-keyword">multi-modality imaging</span>
                    
                    <span class="tag tag-keyword">CT</span>
                    
                    <span class="tag tag-keyword">MRI</span>
                    
                    <span class="tag tag-keyword">PET</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>