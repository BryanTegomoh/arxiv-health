<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis - Health AI Hub</title>
    <meta name="description" content="This paper introduces fairness-aware Low-Rank Adaptation (LoRA) for fine-tuning Vision-Language Models (VLMs) to mitigate diagnostic accuracy disparities across">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.03477v1" target="_blank">2512.03477v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zijian Gu, Yuxi Liu, Zhenhao Zhang, Song Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.03477v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.03477v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces fairness-aware Low-Rank Adaptation (LoRA) for fine-tuning Vision-Language Models (VLMs) to mitigate diagnostic accuracy disparities across demographic groups in medical tasks, specifically glaucoma diagnosis. Their key contribution is a differentiable MaxAccGap loss enabling end-to-end fairness optimization. The GR-LoRA method achieved a 69% reduction in diagnostic accuracy disparities on 10,000 glaucoma fundus images while maintaining 53.15% overall accuracy, utilizing only 0.24% trainable parameters for practical deployment.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for addressing algorithmic bias in medical AI, ensuring equitable diagnostic performance across diverse patient populations. By mitigating demographic disparities in glaucoma diagnosis, it helps prevent misdiagnosis or delayed treatment for underserved groups, fostering more trustworthy and accessible healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development and fine-tuning of vision-language models (VLMs) to improve the fairness and accuracy of medical glaucoma diagnosis from fundus images, specifically reducing diagnostic accuracy disparities across different demographic groups. This aims to enable the practical deployment of equitable AI tools in healthcare, even in resource-constrained environments.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Vision-Language Models (VLMs) for medical imaging exhibit significant diagnostic accuracy disparities across different demographic groups, necessitating fairness-aware optimization.</li>
                    
                    <li>The authors propose fairness-aware Low-Rank Adaptation (LoRA) for medical VLMs, combining parameter efficiency (only 0.24% trainable parameters) with explicit fairness optimization.</li>
                    
                    <li>A novel differentiable MaxAccGap loss function is introduced, enabling direct, end-to-end optimization of accuracy parity across demographic groups during model training.</li>
                    
                    <li>Three methods are presented: FR-LoRA (MaxAccGap regularization), GR-LoRA (inverse frequency weighting for gradients), and Hybrid-LoRA (combining both mechanisms).</li>
                    
                    <li>Evaluated on a dataset of 10,000 glaucoma fundus images, the GR-LoRA method successfully reduced diagnostic accuracy disparities by 69%, while maintaining an overall accuracy of 53.15%.</li>
                    
                    <li>Ablation studies revealed that strong regularization strength is optimal for fairness with minimal accuracy trade-off, and race-specific optimization alone yielded a 60% disparity reduction.</li>
                    
                    <li>The high parameter efficiency of the approach (0.24% trainable parameters) makes it suitable for practical deployment of fair medical AI, particularly in resource-constrained healthcare settings.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors developed fairness-aware fine-tuning for Vision-Language Models (VLMs) using Low-Rank Adaptation (LoRA). Their core algorithmic innovation is a differentiable MaxAccGap loss function designed for end-to-end optimization of accuracy parity across demographic groups. Three specific methods were proposed: FR-LoRA (integrating MaxAccGap regularization), GR-LoRA (applying inverse frequency weighting to gradient contributions), and Hybrid-LoRA (combining both). The approach was evaluated on a dataset comprising 10,000 glaucoma fundus images.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The GR-LoRA method achieved a substantial 69% reduction in diagnostic accuracy disparities across demographic groups. This was accomplished while preserving a respectable overall diagnostic accuracy of 53.15%. Ablation studies highlighted that strong regularization strength optimizes fairness with a minimal accuracy trade-off, and that race-specific optimization alone could reduce disparity by 60%. The proposed fairness-aware LoRA approach requires only 0.24% trainable parameters, indicating high efficiency.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This method has significant clinical impact by enabling the practical deployment of more equitable AI diagnostic tools for conditions like glaucoma. By minimizing diagnostic accuracy disparities across demographic groups, it can lead to more consistent and reliable diagnoses for all patients, irrespective of their background. The approach's high parameter efficiency also means it can be readily integrated into existing clinical systems, including those in resource-constrained environments, making fair medical AI more accessible.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed methods or the study itself.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state any future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Ophthalmology</span>
                    
                    <span class="tag">Glaucoma</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Fairness</span>
                    
                    <span class="tag tag-keyword">Vision-Language Models (VLMs)</span>
                    
                    <span class="tag tag-keyword">Glaucoma Diagnosis</span>
                    
                    <span class="tag tag-keyword">Algorithmic Bias</span>
                    
                    <span class="tag tag-keyword">Low-Rank Adaptation (LoRA)</span>
                    
                    <span class="tag tag-keyword">MaxAccGap Loss</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Diagnostic Disparity</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>10 pages, 3 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>