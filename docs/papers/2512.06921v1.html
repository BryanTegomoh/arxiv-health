<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification - Health AI Hub</title>
    <meta name="description" content="This paper introduces NeuroABench, the first multimodal benchmark specifically designed to evaluate Multimodal Large Language Models' (MLLMs) ability to identif">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.06921v1" target="_blank">2512.06921v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-07
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ziyang Song, Zelin Zang, Xiaofan Ye, Boqiang Xu, Long Bai, Jinlin Wu, Hongliang Ren, Hongbin Liu, Jiebo Luo, Zhen Lei
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.06921v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.06921v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces NeuroABench, the first multimodal benchmark specifically designed to evaluate Multimodal Large Language Models' (MLLMs) ability to identify neurosurgical anatomy from video. It reveals that current state-of-the-art MLLMs have significant limitations in this critical area, achieving only 40.87% accuracy, which is comparable to a novice trainee but substantially below the average neurosurgical trainee's performance.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Precise anatomical understanding is fundamental for safe and effective surgical practice. This research aims to advance AI capabilities in interpreting surgical videos to enhance surgical education, assist in pre-operative planning, and provide real-time intraoperative guidance, ultimately improving patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper evaluates and aims to improve MLLMs for precise anatomical understanding in neurosurgical videos. This AI application can be used to develop tools for enhancing surgical education and training (e.g., interactive learning platforms, automated assessment), providing real-time surgical assistance (e.g., anatomical landmark identification during surgery), and aiding in pre-operative planning and post-operative review for surgeons.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Existing MLLMs for surgical video understanding primarily focus on procedures and workflows, neglecting critical anatomical comprehension required by surgeons.</li>
                    
                    <li>NeuroABench is introduced as the first multimodal benchmark for evaluating anatomical understanding in neurosurgical videos.</li>
                    
                    <li>The benchmark comprises 9 hours of annotated neurosurgical videos covering 89 distinct procedures and identifying 68 clinical anatomical structures.</li>
                    
                    <li>A novel multimodal annotation pipeline with multiple review cycles was employed to ensure the quality and accuracy of the NeuroABench dataset.</li>
                    
                    <li>Experiments on over 10 state-of-the-art MLLMs showed significant limitations, with the best model achieving only 40.87% accuracy in anatomical identification tasks.</li>
                    
                    <li>An informative test with four neurosurgical trainees revealed an average accuracy of 46.5%, with the best student at 56% and the lowest at 28%.</li>
                    
                    <li>The best MLLM performs comparably to the lowest-scoring student but lags significantly behind the group's average, highlighting a substantial gap in human-level anatomical understanding.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved creating NeuroABench through a novel multimodal annotation pipeline with multiple review cycles, annotating 9 hours of neurosurgical video for 68 anatomical structures across 89 procedures. Model performance was evaluated by testing over 10 state-of-the-art MLLMs on anatomical identification tasks using this benchmark. Human performance was assessed by conducting an informative test on a subset of the data with four neurosurgical trainees for comparison.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>State-of-the-art MLLMs exhibit significant limitations in neurosurgical anatomical identification, with the best model achieving only 40.87% accuracy. This performance is notably below that of experienced neurosurgical trainees, who averaged 46.5% accuracy (with a range of 28%-56%), indicating a substantial gap between current AI capabilities and human-level comprehension.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>NeuroABench provides a crucial, standardized benchmark to drive the development of more accurate and robust MLLMs for neurosurgical applications. Improved anatomical identification by MLLMs can significantly enhance surgical education, offer advanced tools for surgical planning, and potentially provide real-time intraoperative assistance by highlighting critical structures, thereby contributing to increased surgical precision and patient safety.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The primary limitation is the significant performance gap between current MLLMs and human neurosurgical trainees in anatomical identification. The human comparison group was small (four trainees), serving as an informative test rather than a large-scale validation. The benchmark is specific to neurosurgery, and its direct applicability to other surgical domains is not addressed.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should focus on developing more sophisticated MLLM architectures and training methodologies to improve anatomical understanding, bridging the identified performance gap with human surgeons. Expansion of such benchmarks to other surgical specialties could also be a valuable next step, leading to broader applications in surgical AI.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurosurgery</span>
                    
                    <span class="tag">Surgical Education</span>
                    
                    <span class="tag">Medical Robotics</span>
                    
                    <span class="tag">Medical Imaging Analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">NeuroABench</span>
                    
                    <span class="tag tag-keyword">Multimodal Large Language Models</span>
                    
                    <span class="tag tag-keyword">Neurosurgical Anatomy</span>
                    
                    <span class="tag tag-keyword">Surgical Video Understanding</span>
                    
                    <span class="tag tag-keyword">Anatomical Identification</span>
                    
                    <span class="tag tag-keyword">Benchmarking</span>
                    
                    <span class="tag tag-keyword">Surgical Education</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted by IEEE ICIA 2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>