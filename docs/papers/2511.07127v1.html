<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks - Health AI Hub</title>
    <meta name="description" content="REACT-LLM introduces a novel benchmark to systematically evaluate the integration of Large Language Models (LLMs) with causal features for enhancing clinical pr">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.07127v1" target="_blank">2511.07127v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Linna Wang, Zhixuan You, Qihui Zhang, Jiunan Wen, Ji Shi, Yimin Chen, Yusen Wang, Fanqi Ding, Ziliang Feng, Li Lu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.07127v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.07127v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">REACT-LLM introduces a novel benchmark to systematically evaluate the integration of Large Language Models (LLMs) with causal features for enhancing clinical prognostic performance. The study reveals that while LLMs perform reasonably well in clinical prognostics, they currently do not outperform traditional machine learning models, and direct integration of causal features derived from discovery algorithms yields limited performance gains, primarily due to violated causal assumptions in complex clinical data. The benchmark, however, points towards a more promising, yet unspecified, synergy.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for advancing the development of actionable, trustworthy, and robust AI systems in healthcare by systematically investigating how LLMs can leverage causal insights for improved clinical decision making and patient prognostics.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research focuses on evaluating the integration of Large Language Models (LLMs) with causal features to improve performance in clinical prognostic tasks and risk prediction. This directly applies AI (LLMs) to enhance clinical decision making and patient outcome prediction in healthcare settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Addressed**: Lack of comprehensive benchmarks to assess LLM integration with causal features for clinical risk prediction and prognostic tasks.</li>
                    
                    <li>**Proposed Solution**: Introduction of REACT-LLM, a new benchmark designed to evaluate if combining LLMs with causal features can enhance clinical prognostic performance.</li>
                    
                    <li>**Extensive Scope**: Evaluates 7 distinct clinical outcomes across 2 real-world datasets, providing a broad assessment context.</li>
                    
                    <li>**Comprehensive Model Comparison**: Compares 15 prominent LLMs, 6 traditional Machine Learning (ML) models, and integrates features derived from 3 Causal Discovery (CD) algorithms.</li>
                    
                    <li>**LLM Performance vs. Traditional ML**: LLMs perform reasonably in clinical prognostics but have not yet demonstrated superior performance compared to traditional ML models.</li>
                    
                    <li>**Limited Causal Feature Integration Gains**: Integrating causal features derived from CD algorithms into LLMs offers limited performance improvements.</li>
                    
                    <li>**Reason for Limitation**: This limitation is attributed to the strict assumptions of many CD methods, which are often violated in the complex, real-world nature of clinical data.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The REACT-LLM benchmark was developed to evaluate the performance of LLMs and traditional ML models in clinical prognostic tasks, both independently and with the integration of causal features. It involved testing 15 LLMs and 6 traditional ML models across 7 clinical outcomes from 2 real-world datasets. Causal features were derived using 3 causal discovery (CD) algorithms and then integrated into the models to assess their impact on performance.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>LLMs demonstrate reasonable performance in clinical prognostic tasks but currently do not outperform traditional machine learning models. Integrating causal features derived from causal discovery (CD) algorithms into LLMs offers limited performance gains, primarily because the strict assumptions of many CD methods are frequently violated in complex clinical data environments.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Understanding the current capabilities and limitations of LLMs when combined with causal features is vital for guiding the development of future AI tools in healthcare. This research can lead to the creation of more transparent, reliable, and actionable prognostic models, ultimately improving clinical practice and informed decision-making.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>A primary limitation identified is that the strict assumptions inherent in many causal discovery (CD) methods are often violated by the complexity and noise present in real-world clinical data, which significantly restricts the performance gains when integrating CD-derived causal features directly into LLMs.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While direct integration of causal features yielded limited improvement, the benchmark "reveals a more promising synergy." This suggests future research should focus on exploring alternative, more sophisticated, or indirect methods for integrating causal insights with LLMs beyond direct feature input to unlock their full potential.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Making</span>
                    
                    <span class="tag">Prognostics</span>
                    
                    <span class="tag">Risk Assessment</span>
                    
                    <span class="tag">Predictive Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM</span>
                    
                    <span class="tag tag-keyword">Causal Learning</span>
                    
                    <span class="tag tag-keyword">Clinical Prognosis</span>
                    
                    <span class="tag tag-keyword">Risk Prediction</span>
                    
                    <span class="tag tag-keyword">Benchmarking</span>
                    
                    <span class="tag tag-keyword">Causal Discovery</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Machine Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) and causal learning each hold strong potential
for clinical decision making (CDM). However, their synergy remains poorly
understood, largely due to the lack of systematic benchmarks evaluating their
integration in clinical risk prediction. In real-world healthcare, identifying
features with causal influence on outcomes is crucial for actionable and
trustworthy predictions. While recent work highlights LLMs' emerging causal
reasoning abilities, there lacks comprehensive benchmarks to assess their
causal learning and performance informed by causal features in clinical risk
prediction. To address this, we introduce REACT-LLM, a benchmark designed to
evaluate whether combining LLMs with causal features can enhance clinical
prognostic performance and potentially outperform traditional machine learning
(ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a
limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2
real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and
3 causal discovery (CD) algorithms. Our findings indicate that while LLMs
perform reasonably in clinical prognostics, they have not yet outperformed
traditional ML models. Integrating causal features derived from CD algorithms
into LLMs offers limited performance gains, primarily due to the strict
assumptions of many CD methods, which are often violated in complex clinical
data. While the direct integration yields limited improvement, our benchmark
reveals a more promising synergy.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>