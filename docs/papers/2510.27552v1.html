<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality - Health AI Hub</title>
    <meta name="description" content="This study investigates the effectiveness of domain-specific pre-training on multilingual BERT models for medical NLP tasks across Dutch, Romanian, and Spanish.">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27552v1" target="_blank">2510.27552v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yinghao Luo, Lang Zhou, Amrish Jhingoer, Klaske Vliegenthart Jongbloed, Carlijn Jordans, Ben Werkhoven, Tom Seinen, Erik van Mulligen, Casper Rokx, Yunlei Li
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27552v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27552v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study investigates the effectiveness of domain-specific pre-training on multilingual BERT models for medical NLP tasks across Dutch, Romanian, and Spanish. It demonstrates that adapting models to specific medical domains significantly enhances performance, with clinical domain models outperforming general biomedical ones, and also reveals promising cross-lingual transferability, offering a strategy to address data scarcity in low-resource healthcare languages.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for improving healthcare accessibility and quality in multilingual settings by enabling more accurate and efficient processing of medical information across diverse languages. By developing robust NLP tools for low-resource languages, it directly supports enhanced patient care, streamlined clinical workflows, and broader dissemination of medical knowledge globally.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The development and evaluation of multilingual AI-powered Natural Language Processing (NLP) systems (specifically BERT-based models) designed to process clinical notes. These systems are applied to critical healthcare tasks such as automated patient screening and named entity recognition, aiming to enhance medical information extraction and improve healthcare applications, particularly in low-resource language settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of limited domain-specific NLP tools for multilingual healthcare, particularly in low-resource languages.</li>
                    
                    <li>Utilizes multilingual BERT (mBERT) and evaluates the impact of further pre-training on domain-specific corpora (clinical vs. general biomedical).</li>
                    
                    <li>Conducted four distinct pre-training experiments to create medical domain-adapted models for Dutch, Romanian, and Spanish.</li>
                    
                    <li>Evaluated model performance on three downstream tasks: automated patient screening (Dutch clinical notes) and Named Entity Recognition (NER) for Romanian and Spanish clinical notes.</li>
                    
                    <li>Found that domain adaptation significantly improved task performance, with clinical domain-adapted models consistently outperforming more general biomedical ones.</li>
                    
                    <li>Observed clear evidence of cross-lingual transferability, suggesting that domain knowledge acquired in one language can benefit other languages.</li>
                    
                    <li>Provides actionable guidance for developing multilingual medical NLP systems to mitigate data scarcity and enhance performance in low-resource language settings.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved further pre-training multilingual BERT models on domain-specific corpora, differentiating between clinical and general biomedical texts, across four experimental setups. These domain-adapted models were then fine-tuned on three downstream medical tasks: automated patient screening using Dutch clinical notes, and Named Entity Recognition (NER) on Romanian and Spanish clinical notes. Additionally, investigations explored the underlying reasons for observed performance differences.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Domain adaptation through further pre-training significantly enhanced model performance on medical tasks. Specifically, models adapted to the clinical domain consistently outperformed those adapted to a more general biomedical domain. Furthermore, the study provided strong evidence of cross-lingual transferability, indicating that models could leverage learned domain knowledge across different languages. Underlying factors contributing to these performance differences were also investigated.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has significant clinical impact by providing a viable pathway to develop high-performing NLP tools for clinical applications in multilingual and low-resource language environments. It can lead to more efficient automated patient screening, improved extraction of critical information from clinical notes, and better support for clinical decision-making, thereby enhancing the quality and efficiency of healthcare delivery, especially where data for training traditional models is scarce.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the presented research. However, the study addresses the inherent challenges of limited domain-specific NLP tools and training data for low-resource languages, implying these are ongoing obstacles in the field.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings are highlighted as providing "meaningful guidance for developing multilingual medical NLP systems to mitigate the lack of training data and thereby improve the model performance." This suggests future work should focus on implementing and scaling these domain-adapted and cross-lingual approaches to a broader array of low-resource languages and diverse medical tasks, aiming for real-world deployment in healthcare applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Multilingual healthcare</span>
                    
                    <span class="tag">Clinical natural language processing</span>
                    
                    <span class="tag">Medical informatics</span>
                    
                    <span class="tag">Biomedical informatics</span>
                    
                    <span class="tag">Patient screening</span>
                    
                    <span class="tag">Electronic Health Records (EHR) analysis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">multilingual BERT</span>
                    
                    <span class="tag tag-keyword">medical NLP</span>
                    
                    <span class="tag tag-keyword">domain adaptation</span>
                    
                    <span class="tag tag-keyword">cross-lingual transfer</span>
                    
                    <span class="tag tag-keyword">low-resource languages</span>
                    
                    <span class="tag tag-keyword">clinical notes</span>
                    
                    <span class="tag tag-keyword">named entity recognition</span>
                    
                    <span class="tag tag-keyword">healthcare applications</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">In multilingual healthcare applications, the availability of domain-specific
natural language processing(NLP) tools is limited, especially for low-resource
languages. Although multilingual bidirectional encoder representations from
transformers (BERT) offers a promising motivation to mitigate the language gap,
the medical NLP tasks in low-resource languages are still underexplored.
Therefore, this study investigates how further pre-training on domain-specific
corpora affects model performance on medical tasks, focusing on three
languages: Dutch, Romanian and Spanish. In terms of further pre-training, we
conducted four experiments to create medical domain models. Then, these models
were fine-tuned on three downstream tasks: Automated patient screening in Dutch
clinical notes, named entity recognition in Romanian and Spanish clinical
notes. Results show that domain adaptation significantly enhanced task
performance. Furthermore, further differentiation of domains, e.g. clinical and
general biomedical domains, resulted in diverse performances. The clinical
domain-adapted model outperformed the more general biomedical domain-adapted
model. Moreover, we observed evidence of cross-lingual transferability.
Moreover, we also conducted further investigations to explore potential reasons
contributing to these performance differences. These findings highlight the
feasibility of domain adaptation and cross-lingual ability in medical NLP.
Within the low-resource language settings, these findings can provide
meaningful guidance for developing multilingual medical NLP systems to mitigate
the lack of training data and thereby improve the model performance.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>