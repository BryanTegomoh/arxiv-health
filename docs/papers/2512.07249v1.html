<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IFFair: Influence Function-driven Sample Reweighting for Fair Classification - Health AI Hub</title>
    <meta name="description" content="This paper introduces IFFair, a novel pre-processing method designed to mitigate bias in machine learning models by dynamically reweighting training samples. Le">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>IFFair: Influence Function-driven Sample Reweighting for Fair Classification</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.07249v1" target="_blank">2512.07249v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-08
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Jingran Yang, Min Zhang, Lingfeng Zhang, Zhaohui Wang, Yonggang Zhang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.07249v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.07249v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces IFFair, a novel pre-processing method designed to mitigate bias in machine learning models by dynamically reweighting training samples. Leveraging influence functions, IFFair adjusts sample weights based on their disparate impact on different unprivileged groups, without altering the model's architecture or decision boundaries. Experiments show that IFFair successfully reduces bias across multiple established fairness metrics while achieving a better trade-off between utility and fairness compared to existing pre-processing techniques.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Machine learning is increasingly deployed in healthcare for diagnosis, risk assessment, and treatment recommendations; however, biased algorithms can lead to unequal access to care, misdiagnosis, or inappropriate treatment for certain patient populations, exacerbating existing health disparities. IFFair offers a mechanism to develop more equitable AI systems in medicine, ensuring fair and non-discriminatory outcomes for all patients.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research provides a foundational AI method for ensuring fairness in machine learning models. When applied in health, it would make AI-powered diagnostic tools, treatment recommendation systems, patient risk assessments, and healthcare resource allocation algorithms more equitable and less discriminatory. For example, it could prevent an AI from disproportionately misdiagnosing or undertreating certain demographic groups, thereby improving health outcomes and reducing disparities.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical issue of machine learning algorithms learning and exacerbating biases from data, leading to discriminatory decisions against unprivileged groups.</li>
                    
                    <li>Proposes IFFair, a pre-processing methodology for fair classification, which operates by dynamically adjusting training sample weights.</li>
                    
                    <li>Utilizes influence functions to quantify the impact disparity of individual training samples on different demographic or protected groups.</li>
                    
                    <li>The method guides sample reweighting during training to reduce this influence disparity, thereby promoting fairness without modifying network structure, data features, or learned decision boundaries.</li>
                    
                    <li>Demonstrated effectiveness in mitigating bias across multiple fairness metrics, including demographic parity, equalized odds, equality of opportunity, and error rate parity, often without introducing new conflicts.</li>
                    
                    <li>Achieves an improved balance between model utility and fairness metrics compared to other established pre-processing methods for bias mitigation.</li>
                    
                    <li>The approach's non-invasive nature (no network/feature modification) makes it potentially adaptable to various existing ML classification pipelines.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>IFFair is a pre-processing method that employs influence functions to quantify the impact of individual training samples on the model's predictions for different protected groups. This influence disparity serves as a guiding signal to dynamically adjust the weights of training samples during the model's training process. By reweighting samples, the method aims to reduce the disproportionate influence of certain data points on group-specific outcomes, thereby mitigating bias without requiring changes to the underlying neural network architecture, data feature representation, or the model's final decision boundaries.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study found that IFFair effectively mitigates bias across a spectrum of commonly accepted fairness metrics, including demographic parity, equalized odds, equality of opportunity, and error rate parity. Importantly, it achieves this bias reduction without introducing conflicts between different fairness objectives. Furthermore, IFFair demonstrated a superior trade-off between overall model utility (e.g., accuracy) and the achieved fairness levels when compared against other existing pre-processing methods for bias optimization.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Implementing IFFair in medical AI applications could lead to more trustworthy and ethically sound clinical decision support systems, diagnostic tools, and predictive models. By ensuring fairness across patient demographics, it can help prevent biased care, reduce health disparities, and promote equitable access to medical resources and treatments. This could be crucial in areas like disease risk prediction (e.g., for certain ethnic groups), medical image interpretation, or treatment recommendations, where biased models could otherwise lead to adverse health outcomes for vulnerable populations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed method or the experimental evaluation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support Systems</span>
                    
                    <span class="tag">Predictive Health Analytics</span>
                    
                    <span class="tag">Medical Imaging Diagnostics</span>
                    
                    <span class="tag">Precision Medicine</span>
                    
                    <span class="tag">Public Health Interventions</span>
                    
                    <span class="tag">Patient Risk Stratification</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Fair Machine Learning</span>
                    
                    <span class="tag tag-keyword">Algorithmic Bias</span>
                    
                    <span class="tag tag-keyword">Influence Function</span>
                    
                    <span class="tag tag-keyword">Sample Reweighting</span>
                    
                    <span class="tag tag-keyword">Pre-processing</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Demographic Parity</span>
                    
                    <span class="tag tag-keyword">Equalized Odds</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>