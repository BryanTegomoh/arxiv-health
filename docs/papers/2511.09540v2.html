<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs - Health AI Hub</title>
    <meta name="description" content="This paper introduces vMFCoOp, a novel framework designed to enhance prompt learning for biomedical Vision-Language Models (VLMs) by addressing semantic misalig">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.09540v2" target="_blank">2511.09540v2</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-12
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Minye Shao, Sihan Guo, Xinrun Li, Xingyu Miao, Haoran Duan, Yang Long
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.09540v2" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.09540v2" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces vMFCoOp, a novel framework designed to enhance prompt learning for biomedical Vision-Language Models (VLMs) by addressing semantic misalignment and scalability issues between LLMs and CLIP variants. It achieves robust biomedical prompting and superior few-shot classification by inversely estimating von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, leveraging Unified Semantic Anchors for alignment. vMFCoOp consistently outperforms state-of-the-art methods across diverse medical datasets, modalities, and anatomical regions, demonstrating significant improvements in accuracy, generalization, and clinical applicability.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research significantly advances the development of robust and generalizable AI models for medical image analysis, especially in data-scarce environments. It enables more accurate and adaptable diagnostic support systems by improving the semantic understanding and alignment between different AI foundation models in complex biomedical contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research focuses on improving the performance and robustness of AI models (Vision-Language Models, VLMs) for medical image analysis and diagnosis. Specifically, it aims to enhance prompt learning and few-shot classification in complex biomedical imaging scenarios, thereby providing more accurate and generalizable AI tools for clinicians across various medical imaging modalities and anatomical regions. This could lead to better diagnostic support, disease detection, and more scalable AI solutions for healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses critical challenges in LLM-guided context optimization (CoOp) for biomedical VLMs, specifically semantic misalignment between LLMs and CLIP, and lack of scalability.</li>
                    
                    <li>Identifies the limitation of conventional Euclidean-space optimization, which amplifies modality gaps and destabilizes few-shot adaptation in complex biomedical imaging.</li>
                    
                    <li>Proposes vMFCoOp, a framework that models unified representations by inversely estimating von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold.</li>
                    
                    <li>Achieves robust alignment of semantic biases between arbitrary LLMs and CLIP backbones through the use of Unified Semantic Anchors.</li>
                    
                    <li>The framework is grounded in three complementary constraints, contributing to its robustness and improved performance.</li>
                    
                    <li>Demonstrates consistent and significant improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions.</li>
                    
                    <li>Outperforms state-of-the-art methods in terms of accuracy, generalization, and clinical applicability, particularly in few-shot classification scenarios for biomedical tasks.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The vMFCoOp framework inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold. This approach facilitates the alignment of semantic biases between arbitrary LLMs and CLIP backbones by utilizing Unified Semantic Anchors. The methodology is further strengthened by incorporating three complementary constraints to ensure robust and stable few-shot adaptation.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>vMFCoOp consistently achieved superior performance across a broad spectrum of biomedical imaging tasks, including 14 medical datasets, 12 imaging modalities, and 13 anatomical regions. It significantly outperformed state-of-the-art methods in few-shot classification accuracy, generalization capabilities, and overall clinical applicability, demonstrating its effectiveness in bridging modality gaps and improving semantic alignment.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The development of vMFCoOp has the potential to lead to more reliable and adaptable AI tools for medical diagnosis and analysis, especially for rare conditions or emerging imaging techniques where large labeled datasets are unavailable. This can improve diagnostic accuracy, reduce the burden on clinicians for prompt engineering, and accelerate the deployment of cutting-edge AI in diverse clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly stated in the abstract. However, potential limitations might include the computational complexity associated with hyperspherical manifold optimization or the need for careful tuning of the 'three complementary constraints' for optimal performance across all medical tasks.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors aim to continuously expand the vMFCoOp framework to encompass more downstream applications in the biomedical domain. This suggests an ongoing effort to broaden its utility and applicability beyond few-shot classification.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical AI</span>
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Biomedical Informatics</span>
                    
                    <span class="tag">Computer-Aided Diagnosis</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">vMFCoOp</span>
                    
                    <span class="tag tag-keyword">Prompt Learning</span>
                    
                    <span class="tag tag-keyword">Biomedical VLMs</span>
                    
                    <span class="tag tag-keyword">Hyperspherical Manifold</span>
                    
                    <span class="tag tag-keyword">von Mises-Fisher Distribution</span>
                    
                    <span class="tag tag-keyword">Few-Shot Learning</span>
                    
                    <span class="tag tag-keyword">Semantic Alignment</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted as an Oral Presentation at AAAI 2026 Main Technical Track (this version is not peer-reviewed; it is the extended version)</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>