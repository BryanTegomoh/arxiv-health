<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring - Health AI Hub</title>
    <meta name="description" content="CompEvent introduces a novel complex neural network framework for holistic, full-process fusion of event camera data and RGB frames to tackle challenging low-li">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.14469v1" target="_blank">2511.14469v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-18
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mingchen Zhong, Xin Lu, Dong Li, Senyan Xu, Ruixuan Jiang, Xueyang Fu, Baocai Yin
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.80 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.14469v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.14469v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">CompEvent introduces a novel complex neural network framework for holistic, full-process fusion of event camera data and RGB frames to tackle challenging low-light video deblurring. By leveraging complex-valued convolutions and processing in both spatial and frequency domains, it achieves superior spatiotemporal fusion, maximizing complementary learning between modalities. This approach significantly strengthens low-light video deblurring capabilities, outperforming state-of-the-art methods.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This technology offers the potential to provide critically clear and deblurred visual data in challenging low-light medical environments, improving the reliability and efficacy of patient monitoring, diagnostic procedures, and autonomous medical device operation where robust visual perception is essential.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research provides a foundational improvement for video data quality, directly benefiting medical AI applications that process video streams. For example, AI systems for automated disease detection in endoscopic videos, AI-guided surgical robots requiring precise visual input, AI for fall detection or behavioral analysis in patient monitoring, or AI for analyzing fast-moving biological processes under a microscope, would all perform more robustly and accurately with high-quality, deblurred, and enhanced low-light video provided by methods like CompEvent.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical problem of low-light video deblurring, which is significantly challenged by dim lighting and motion blur from long exposures in applications like surveillance and autonomous driving.</li>
                    
                    <li>Proposes CompEvent, a novel complex neural network framework designed for holistic, full-process fusion of high-temporal-resolution event data and traditional RGB frames.</li>
                    
                    <li>Overcomes limitations of existing staged fusion strategies by enabling deep, continuous spatiotemporal integration of complementary information from both modalities.</li>
                    
                    <li>Features a 'Complex Temporal Alignment GRU' component, which utilizes complex-valued convolutions and an iterative GRU process to achieve precise temporal alignment and continuous fusion between video and event streams.</li>
                    
                    <li>Includes a 'Complex Space-Frequency Learning module' that performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion of spatial structures and system-level characteristics.</li>
                    
                    <li>Leverages the unique holistic representation capabilities of complex-valued neural networks to maximize complementary learning and synergistic integration between the event and RGB input streams.</li>
                    
                    <li>Extensive experiments demonstrate that CompEvent achieves significant improvements in low-light video deblurring performance, substantially outperforming existing state-of-the-art methods.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>CompEvent employs a complex neural network framework for holistic full-process spatiotemporal fusion. It integrates two core components: a Complex Temporal Alignment GRU, which uses complex-valued convolutions and an iterative GRU to achieve temporal alignment and continuous fusion of video and event streams, and a Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains for deep feature fusion. The network leverages the inherent properties of complex-valued computations for enhanced representation and complementary learning.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study found that CompEvent successfully achieves superior full-process spatiotemporal fusion of event and RGB data. This holistic approach effectively maximizes the complementary learning between the two modalities, resulting in a significant strengthening of low-light video deblurring capabilities. Extensive experiments validated that CompEvent consistently outperforms existing state-of-the-art methods in addressing this challenging task.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>CompEvent's ability to generate sharp, deblurred video from low-light conditions could have transformative clinical impacts by providing clinicians and autonomous medical systems with highly reliable visual information. This could translate to improved precision for robotic-assisted surgeries in dimly lit cavities, enhanced accuracy for nighttime patient monitoring to detect subtle changes, more robust navigation and operation for autonomous medical delivery or diagnostic robots in varied hospital lighting, and clearer imagery for emergency diagnostics.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Emergency Medicine</span>
                    
                    <span class="tag">Critical Care</span>
                    
                    <span class="tag">Surgical Robotics</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                    <span class="tag">Patient Monitoring</span>
                    
                    <span class="tag">Autonomous Medical Devices</span>
                    
                    <span class="tag">Medical Surveillance</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Low-light video enhancement</span>
                    
                    <span class="tag tag-keyword">Video deblurring</span>
                    
                    <span class="tag tag-keyword">Event cameras</span>
                    
                    <span class="tag tag-keyword">RGB fusion</span>
                    
                    <span class="tag tag-keyword">Complex-valued neural networks</span>
                    
                    <span class="tag tag-keyword">Spatiotemporal fusion</span>
                    
                    <span class="tag tag-keyword">Deep learning</span>
                    
                    <span class="tag tag-keyword">Computer Vision</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at https://github.com/YuXie1/CompEvent.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>