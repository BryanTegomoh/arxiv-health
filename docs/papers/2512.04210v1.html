<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment - Health AI Hub</title>
    <meta name="description" content="This paper introduces an iterative post-deployment alignment framework using Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to enh">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04210v1" target="_blank">2512.04210v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Huy Nghiem, Swetasudha Panda, Devashish Khatwani, Huy V. Nguyen, Krishnaram Kenthapadi, Hal Daum√©
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI, cs.CL, cs.CY
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04210v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04210v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces an iterative post-deployment alignment framework using Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to enhance the safety and trustworthiness of healthcare AI assistants. Evaluating four LLMs on the CARES-18K benchmark, the framework achieved up to 42% improvement in harmful query detection, while highlighting critical trade-offs with erroneous refusals and architecture-dependent calibration biases.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research directly addresses a major barrier to healthcare AI deployment: ensuring patient safety and building trust. By developing robust methods to prevent AI assistants from generating unsafe responses or inappropriately refusing legitimate medical queries, it aims to make these tools reliable and ethically sound for clinical use.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The primary AI application is the development and refinement of 'conversational medical assistants' or 'healthcare AI assistants'. This involves ensuring their safety, trustworthiness, and appropriate responses in clinical or patient-facing scenarios, balancing helpfulness with the avoidance of harmful or inappropriate advice.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Developed an iterative post-deployment alignment framework specifically for healthcare LLMs, focusing on balancing safety and helpfulness.</li>
                    
                    <li>The framework applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models using domain-specific safety signals.</li>
                    
                    <li>Evaluated four different LLM architectures (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles on the CARES-18K adversarial robustness benchmark.</li>
                    
                    <li>Achieved significant safety improvements, demonstrating up to a 42% increase in safety-related metrics for harmful query detection.</li>
                    
                    <li>Identified critical trade-offs between enhanced safety (detecting harmful queries) and an increase in erroneous refusals for benign queries.</li>
                    
                    <li>Revealed architecture-dependent calibration biases, suggesting that alignment effectiveness varies across different LLM models.</li>
                    
                    <li>Ablation studies determined conditions under which self-evaluation is reliable versus when external or finetuned judges are necessary for maximizing performance gains.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study utilized an iterative post-deployment alignment framework. This framework refines Large Language Models (LLMs) by applying Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) against domain-specific safety signals. Four distinct LLM architectures (Llama-3B, Llama-8B, Meditron-8B, Mistral-7B) were evaluated over multiple alignment cycles using the CARES-18K benchmark, designed for adversarial robustness in healthcare. Ablation studies were conducted to compare the effectiveness and reliability of self-evaluation mechanisms against external or finetuned human judges for maximizing performance gains.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The alignment framework led to a substantial improvement in safety, with up to a 42% increase in harmful query detection. However, this safety gain presented trade-offs, often leading to an increase in erroneous refusals for benign queries. The research also revealed architecture-dependent calibration biases among the evaluated LLMs, indicating varying responses to alignment strategies. Furthermore, the study identified specific conditions for when self-evaluation mechanisms are reliable versus when external or finetuned judges are essential for achieving optimal alignment performance.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work has significant clinical impact by providing a practical methodology to develop safer and more trustworthy conversational AI assistants for healthcare settings. By mitigating risks of unsafe medical advice and inappropriate query refusals, it can foster greater adoption of AI tools by clinicians and patients. This enhanced reliability can improve patient education, support symptom assessment, streamline administrative tasks, and ultimately contribute to safer and more efficient patient care within the medical ecosystem.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract implicitly notes several limitations, including the observed 'interesting trade-offs against erroneous refusals,' indicating that achieving perfect safety without impacting helpfulness remains a challenge. The finding of 'architecture-dependent calibration biases' suggests that the alignment framework's effectiveness may vary significantly across different LLM models, necessitating tailored approaches. Additionally, the identification of scenarios where 'external or finetuned judges are necessary' implies that fully automated and universally reliable alignment without human oversight is not yet achieved.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper underscores the importance of 'adopting best practices that balance patient safety, user trust, and clinical utility.' This suggests future research should focus on further refining these best practices, optimizing the delicate balance between safety and helpfulness, and potentially developing more robust, architecture-agnostic alignment strategies. Continued efforts to enhance self-evaluation mechanisms to reduce reliance on resource-intensive external or finetuned judges would also be a valuable future direction.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Conversational AI in Healthcare</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Patient Education</span>
                    
                    <span class="tag">Digital Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Safety Alignment</span>
                    
                    <span class="tag tag-keyword">KTO</span>
                    
                    <span class="tag tag-keyword">DPO</span>
                    
                    <span class="tag tag-keyword">Adversarial Robustness</span>
                    
                    <span class="tag tag-keyword">Medical Assistants</span>
                    
                    <span class="tag tag-keyword">Patient Safety</span>
                    
                    <span class="tag tag-keyword">Preference Alignment</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>ML4H 2025 Proceedings, Best Paper Award</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>