<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment - Health AI Hub</title>
    <meta name="description" content="This paper presents an iterative post-deployment alignment framework that leverages Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO)">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04210v1" target="_blank">2512.04210v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Huy Nghiem, Swetasudha Panda, Devashish Khatwani, Huy V. Nguyen, Krishnaram Kenthapadi, Hal Daum√©
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI, cs.CL, cs.CY
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04210v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04210v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper presents an iterative post-deployment alignment framework that leverages Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to enhance the safety and trustworthiness of conversational medical AI assistants. By refining Large Language Models (LLMs) against domain-specific safety signals, the framework achieved up to a 42% improvement in detecting harmful queries, while also highlighting critical trade-offs with erroneous refusals and architecture-dependent calibration biases. This work contributes to balancing patient safety, user trust, and clinical utility in healthcare AI.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is paramount for the safe and ethical integration of AI into clinical practice, directly addressing how to build trustworthy AI medical assistants that can provide helpful information while reliably safeguarding against harmful advice or inappropriate refusals, thereby directly impacting patient safety and clinician adoption.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper focuses on developing and refining safe, reliable, and trustworthy conversational AI assistants (Large Language Models) specifically for use in healthcare. This involves improving their ability to detect and appropriately respond to harmful queries, avoid unsafe compliance, and balance helpfulness with patient safety, thereby enabling their responsible deployment in clinical and patient-facing applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of ensuring LLM safety and trustworthiness in healthcare by preventing unsafe compliance without over-refusing benign medical queries.</li>
                    
                    <li>Introduces an iterative post-deployment alignment framework utilizing Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) for model refinement.</li>
                    
                    <li>Evaluates four distinct LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) on the CARES-18K benchmark for adversarial robustness across multiple refinement cycles.</li>
                    
                    <li>Demonstrates significant safety improvements, achieving up to a 42% increase in harmful query detection metrics.</li>
                    
                    <li>Identifies inherent trade-offs between enhanced safety (harmful query detection) and the potential for increased erroneous refusals of benign queries.</li>
                    
                    <li>Exposes architecture-dependent calibration biases among the evaluated LLMs, suggesting varying responses to the alignment process.</li>
                    
                    <li>Performs ablation studies to determine the reliability of self-evaluation versus external or finetuned judges for maximizing performance gains.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employed an iterative post-deployment alignment framework for LLMs, applying Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Four LLMs (Llama-3B, Llama-8B, Meditron-8B, Mistral-7B) were evaluated using the CARES-18K benchmark, a dataset for adversarial robustness, over multiple refinement cycles. Ablation studies were conducted to compare the efficacy of self-evaluation against external or finetuned judges in optimizing performance gains.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The alignment framework resulted in a substantial improvement of up to 42% in safety-related metrics for detecting harmful queries. However, these gains were accompanied by notable trade-offs against erroneous refusals of benign queries. The study also revealed architecture-dependent calibration biases across the different LLM models. Ablation studies provided insights into when self-evaluation is a reliable mechanism for performance improvement and when the intervention of external or finetuned judges becomes essential.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research provides a concrete framework to develop safer and more trustworthy AI-powered conversational assistants for healthcare. By significantly improving the detection of harmful queries, it directly enhances patient safety and fosters greater trust among users and clinicians, facilitating the responsible deployment of AI in clinical settings and supporting more reliable medical information access.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The observed 'interesting trade-offs against erroneous refusals' indicate that achieving perfect safety without impacting helpfulness remains a challenge. Additionally, the discovery of 'architecture-dependent calibration biases' suggests that the alignment process may require model-specific tuning or could yield inconsistent results across different LLM architectures, posing a barrier to universal applicability.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Implicitly, future research should focus on refining alignment techniques to minimize the trade-offs between safety and helpfulness, potentially through more sophisticated refusal policies. Further work is also needed to understand and mitigate architecture-dependent calibration biases to ensure consistent and robust safety performance across a broader range of LLMs. Continuous development and evaluation of best practices for balancing patient safety, user trust, and clinical utility are also crucial for the future design and deployment of healthcare AI.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">General healthcare</span>
                    
                    <span class="tag">Clinical decision support</span>
                    
                    <span class="tag">Digital health</span>
                    
                    <span class="tag">Medical informatics</span>
                    
                    <span class="tag">AI in medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">LLM safety</span>
                    
                    <span class="tag tag-keyword">preference alignment</span>
                    
                    <span class="tag tag-keyword">Kahneman-Tversky Optimization</span>
                    
                    <span class="tag tag-keyword">Direct Preference Optimization</span>
                    
                    <span class="tag tag-keyword">conversational medical assistants</span>
                    
                    <span class="tag tag-keyword">patient safety</span>
                    
                    <span class="tag tag-keyword">adversarial robustness</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>ML4H 2025 Proceedings, Best Paper Award</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>