<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography - Health AI Hub</title>
    <meta name="description" content="ProtoEFNet introduces a novel video-based prototype learning model for continuous ejection fraction (EF) estimation from echocardiography, designed to be inhere">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.03339v1" target="_blank">2512.03339v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-03
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yeganeh Ghamary, Victoria Wu, Hooman Vaseli, Christina Luong, Teresa Tsang, Siavash Bigdeli, Purang Abolmaesumi
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.03339v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.03339v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">ProtoEFNet introduces a novel video-based prototype learning model for continuous ejection fraction (EF) estimation from echocardiography, designed to be inherently interpretable. It addresses the limitations of traditional manual methods and opaque deep learning models by learning dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. The model achieves accuracy comparable to non-interpretable counterparts while providing valuable clinical insights, enhanced by a novel Prototype Angular Separation (PAS) loss.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Ejection fraction is a cornerstone metric for diagnosing and managing critical cardiac conditions, particularly heart failure. This research enhances the clinical utility of automated EF estimation by making it inherently interpretable, fostering greater trust among clinicians and facilitating more accurate, consistent, and explainable diagnoses.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>AI application for automated, accurate, and inherently interpretable estimation of cardiac ejection fraction from echocardiography videos to assist in the diagnosis, monitoring, and management of heart failure and other cardiac conditions, reducing manual effort and interobserver variability.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenges of manual, time-consuming, and variable EF estimation, as well as the 'black-box' nature of current deep learning EF prediction models that limit clinical trust.</li>
                    
                    <li>Introduces ProtoEFNet, a novel video-based deep learning model for continuous EF regression, designed for inherent interpretability rather than post-hoc explanations.</li>
                    
                    <li>Learns dynamic spatiotemporal prototypes directly from echocardiogram videos, which correspond to clinically meaningful cardiac motion patterns.</li>
                    
                    <li>Proposes the Prototype Angular Separation (PAS) loss to enforce discriminative representations across the continuous EF spectrum, improving model performance.</li>
                    
                    <li>Achieves EF estimation accuracy on par with non-interpretable deep learning models on the EchonetDynamic dataset.</li>
                    
                    <li>Provides clinically relevant insights by making its decision-making process transparent through understandable prototypes.</li>
                    
                    <li>The ablation study demonstrates that the PAS loss significantly boosts performance, increasing the F1 score by 2% (from 77.67% to 79.64%).</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>ProtoEFNet is a video-based deep learning model utilizing prototype learning for continuous EF regression. It processes echocardiogram videos to learn dynamic spatiotemporal prototypes, which represent distinct, clinically meaningful cardiac motion patterns. A crucial aspect of its methodology is the inclusion of the novel Prototype Angular Separation (PAS) loss, which encourages the model to learn more discriminative representations across the continuous range of EF values. The model was evaluated on the EchonetDynamic dataset.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>['ProtoEFNet successfully achieves EF estimation accuracy comparable to its non-interpretable deep learning counterparts.', 'The model offers inherent interpretability by learning and leveraging dynamic spatiotemporal prototypes that align with clinically meaningful cardiac motion patterns, providing transparent insights into its predictions.', "The proposed Prototype Angular Separation (PAS) loss significantly improves the model's performance, evidenced by a 2% increase in F1 score (from 77.67% to 79.64%) in an ablation study."]</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This model has the potential to significantly streamline and standardize EF estimation in clinical practice, reducing reliance on manual tracing and mitigating interobserver variability. Its inherent interpretability is critical for building clinician trust and accelerating the adoption of AI in echocardiography, leading to more transparent and reliable diagnoses of conditions like heart failure. This enables clinicians to understand the 'why' behind a prediction, aiding in better treatment planning and patient communication.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations. It indicates that the accuracy is 'on par' with non-interpretable models, implying its primary advantage is interpretability rather than superior raw accuracy.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions are not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Cardiovascular Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Ejection Fraction</span>
                    
                    <span class="tag tag-keyword">Echocardiography</span>
                    
                    <span class="tag tag-keyword">Interpretable AI</span>
                    
                    <span class="tag tag-keyword">Prototype Learning</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Cardiac Function</span>
                    
                    <span class="tag tag-keyword">Heart Failure</span>
                    
                    <span class="tag tag-keyword">Cardiovascular Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>11 pages, Accepted in IMIMIC Workshop at MICCAI 2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>