<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces MedCalc-Eval, the largest benchmark for evaluating Large Language Models' (LLMs) medical quantitative reasoning, addressing a critical gap">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27267v1" target="_blank">2510.27267v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Kangkun Mao, Jinru Ding, Jiayuan Chen, Mouxiao Bian, Ruiyao Chen, Xinwei Peng, Sijie Ren, Linyang Li, Jie Xu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27267v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27267v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MedCalc-Eval, the largest benchmark for evaluating Large Language Models' (LLMs) medical quantitative reasoning, addressing a critical gap in existing assessments. Furthermore, it develops MedCalc-Env, a reinforcement learning environment that significantly enhances an LLM's capabilities in multi-step clinical calculation and reasoning, achieving state-of-the-art performance.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate quantitative reasoning is fundamental to clinical decision-making, impacting drug dosages, risk stratification, and diagnostic scoring. Improving LLMs' capabilities in this area can lead to more reliable AI-powered clinical decision support systems and reduce calculation errors in practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research develops and evaluates AI (Large Language Models) for performing complex medical calculations and quantitative reasoning. This directly supports clinical decision-making, patient assessment, and potentially clinical training by providing more accurate and robust AI tools for medical professionals.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Identifies a critical gap in LLM evaluation: lack of robust assessment for quantitative reasoning crucial to clinical decision-making.</li>
                    
                    <li>Introduces MedCalc-Eval, a novel benchmark with over 700 diverse medical calculation tasks, significantly larger than previous datasets.</li>
                    
                    <li>MedCalc-Eval encompasses two main task types: equation-based calculations (e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar, GCS).</li>
                    
                    <li>Tasks span multiple medical specialties including internal medicine, surgery, pediatrics, and cardiology, ensuring broad applicability.</li>
                    
                    <li>Presents MedCalc-Env, a reinforcement learning environment built on InternBootcamp, designed to foster multi-step clinical reasoning and planning in LLMs.</li>
                    
                    <li>Fine-tuning a Qwen2.5-32B model within MedCalc-Env achieved state-of-the-art results on MedCalc-Eval, demonstrating substantial improvements.</li>
                    
                    <li>Key performance gains include enhanced numerical sensitivity, more accurate formula selection, and increased reasoning robustness in medical calculations.</li>
                    
                    <li>Highlights remaining challenges: unit conversion complexities, multi-condition logical reasoning, and nuanced contextual understanding.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved creating MedCalc-Eval, a benchmark with 700+ medical calculation tasks categorized into equation-based and rule-based systems. It also developed MedCalc-Env, a reinforcement learning environment based on the InternBootcamp framework, to enable multi-step clinical reasoning. A Qwen2.5-32B model was then fine-tuned within this MedCalc-Env to optimize its performance on the MedCalc-Eval benchmark.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>MedCalc-Eval is established as the largest benchmark for medical calculation. The MedCalc-Env reinforcement learning approach significantly boosts LLM performance (specifically Qwen2.5-32B) on these tasks, achieving state-of-the-art results. This improvement is characterized by notable gains in numerical sensitivity, formula selection accuracy, and robustness of reasoning.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research directly contributes to developing more reliable and accurate AI tools for clinical practice. Improved medical calculation capabilities in LLMs can enhance clinical decision support, aid in accurate diagnostic scoring and risk assessment, optimize drug dosing, and potentially reduce medication errors and misdiagnoses stemming from computational inaccuracies.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Despite significant advancements, the study identifies several remaining challenges for LLMs in medical calculations: accurately handling unit conversions, performing complex multi-condition logical reasoning, and achieving deep contextual understanding necessary for nuanced clinical scenarios.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should focus on addressing the identified limitations, specifically improving LLMs' abilities in unit conversion, mastering multi-conditional logic within clinical contexts, and enhancing their overall contextual understanding to tackle more complex, real-world medical calculation scenarios.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Internal Medicine</span>
                    
                    <span class="tag">Surgery</span>
                    
                    <span class="tag">Pediatrics</span>
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">General Clinical Practice</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">medical calculation</span>
                    
                    <span class="tag tag-keyword">quantitative reasoning</span>
                    
                    <span class="tag tag-keyword">benchmark</span>
                    
                    <span class="tag tag-keyword">reinforcement learning</span>
                    
                    <span class="tag tag-keyword">clinical decision-making</span>
                    
                    <span class="tag tag-keyword">MedCalc-Eval</span>
                    
                    <span class="tag tag-keyword">MedCalc-Env</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">As large language models (LLMs) enter the medical domain, most benchmarks
evaluate them on question answering or descriptive reasoning, overlooking
quantitative reasoning critical to clinical decision-making. Existing datasets
like MedCalc-Bench cover few calculation tasks and fail to reflect real-world
computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical
calculation abilities, comprising 700+ tasks across two types: equation-based
(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,
Glasgow Coma Scale). These tasks span diverse specialties including internal
medicine, surgery, pediatrics, and cardiology, offering a broader and more
challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement
learning environment built on the InternBootcamp framework, enabling multi-step
clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this
environment achieves state-of-the-art results on MedCalc-Eval, with notable
gains in numerical sensitivity, formula selection, and reasoning robustness.
Remaining challenges include unit conversion, multi-condition logic, and
contextual understanding.
  Code and datasets are available at
https://github.com/maokangkun/MedCalc-Eval.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>