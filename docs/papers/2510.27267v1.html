<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces MedCalc-Eval, the largest benchmark for evaluating Large Language Models' (LLMs) medical calculation abilities, comprising 700+ tasks rang">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.27267v1" target="_blank">2510.27267v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-31
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Kangkun Mao, Jinru Ding, Jiayuan Chen, Mouxiao Bian, Ruiyao Chen, Xinwei Peng, Sijie Ren, Linyang Li, Jie Xu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.27267v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.27267v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MedCalc-Eval, the largest benchmark for evaluating Large Language Models' (LLMs) medical calculation abilities, comprising 700+ tasks ranging from equation-based formulas to rule-based scoring systems. To improve performance, the authors developed MedCalc-Env, a reinforcement learning environment, which, when used to fine-tune a Qwen2.5-32B model, achieved state-of-the-art results on MedCalc-Eval with enhanced numerical sensitivity and reasoning robustness.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate quantitative reasoning is paramount in clinical practice for tasks like drug dosing, patient risk assessment, and diagnostic scoring. This work directly addresses a critical gap in LLM capabilities, aiming to make these models more reliable and useful as decision support tools in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research focuses on advancing the medical calculation capabilities of Large Language Models (LLMs) to support quantitative reasoning and clinical decision-making. This involves developing benchmarks for evaluating medical AI performance and creating reinforcement learning environments to fine-tune LLMs for improved accuracy in complex medical calculations and multi-step clinical reasoning.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Existing medical LLM benchmarks primarily focus on qualitative reasoning, overlooking quantitative reasoning crucial for clinical decision-making.</li>
                    
                    <li>MedCalc-Eval is introduced as the largest benchmark for medical calculation, featuring over 700 tasks, including equation-based (e.g., Cockcroft-Gault, BMI) and rule-based scoring systems (e.g., Apgar, Glasgow Coma Scale).</li>
                    
                    <li>The benchmark tasks span diverse medical specialties such as internal medicine, surgery, pediatrics, and cardiology, offering a comprehensive evaluation setting.</li>
                    
                    <li>MedCalc-Env, a reinforcement learning environment built on the InternBootcamp framework, was developed to facilitate multi-step clinical reasoning and planning for medical calculations.</li>
                    
                    <li>Fine-tuning a Qwen2.5-32B model within the MedCalc-Env environment resulted in state-of-the-art performance on the MedCalc-Eval benchmark.</li>
                    
                    <li>The performance gains observed include notable improvements in numerical sensitivity, accurate formula selection, and reasoning robustness of the LLM.</li>
                    
                    <li>Identified remaining challenges include accurate unit conversion, handling complex multi-condition logic, and deeper contextual understanding in medical scenarios.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involved two main components: first, the creation of MedCalc-Eval, a comprehensive benchmark comprising 700+ medical calculation tasks categorized into equation-based and rule-based systems spanning multiple specialties. Second, the development of MedCalc-Env, a reinforcement learning environment built on the InternBootcamp framework, designed to train LLMs for multi-step clinical reasoning and planning. A Qwen2.5-32B model was then fine-tuned within MedCalc-Env and evaluated against the MedCalc-Eval benchmark.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The fine-tuned Qwen2.5-32B model, leveraging the MedCalc-Env reinforcement learning environment, achieved state-of-the-art results on the MedCalc-Eval benchmark. This included significant improvements in the LLM's numerical sensitivity, its ability to correctly select and apply medical formulas, and overall robustness in its quantitative reasoning processes.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to significantly enhance the utility of LLMs in clinical settings by enabling them to accurately perform complex medical calculations. This could lead to more precise drug prescriptions, improved patient risk stratification, more consistent application of diagnostic and prognostic scores, and ultimately, better-informed clinical decision-making, thereby reducing medical errors and improving patient safety.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study notes several remaining challenges for LLMs in medical calculations, specifically in areas such as accurate unit conversion, handling intricate multi-condition logical rules (e.g., in complex clinical algorithms), and achieving a deeper, more nuanced contextual understanding of clinical scenarios required for robust quantitative reasoning.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should focus on overcoming the identified limitations, specifically by developing methods to improve LLMs' capabilities in precise unit conversion, enhancing their ability to interpret and apply complex multi-condition logic, and advancing their contextual understanding within medical domains to further solidify their quantitative reasoning prowess.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Internal Medicine</span>
                    
                    <span class="tag">Surgery</span>
                    
                    <span class="tag">Pediatrics</span>
                    
                    <span class="tag">Cardiology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medical LLMs</span>
                    
                    <span class="tag tag-keyword">Quantitative Reasoning</span>
                    
                    <span class="tag tag-keyword">Clinical Calculation</span>
                    
                    <span class="tag tag-keyword">Benchmark</span>
                    
                    <span class="tag tag-keyword">Reinforcement Learning</span>
                    
                    <span class="tag tag-keyword">Medical Decision Support</span>
                    
                    <span class="tag tag-keyword">Evaluation</span>
                    
                    <span class="tag tag-keyword">State-of-the-Art</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">As large language models (LLMs) enter the medical domain, most benchmarks
evaluate them on question answering or descriptive reasoning, overlooking
quantitative reasoning critical to clinical decision-making. Existing datasets
like MedCalc-Bench cover few calculation tasks and fail to reflect real-world
computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical
calculation abilities, comprising 700+ tasks across two types: equation-based
(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,
Glasgow Coma Scale). These tasks span diverse specialties including internal
medicine, surgery, pediatrics, and cardiology, offering a broader and more
challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement
learning environment built on the InternBootcamp framework, enabling multi-step
clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this
environment achieves state-of-the-art results on MedCalc-Eval, with notable
gains in numerical sensitivity, formula selection, and reasoning robustness.
Remaining challenges include unit conversion, multi-condition logic, and
contextual understanding.
  Code and datasets are available at
https://github.com/maokangkun/MedCalc-Eval.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>