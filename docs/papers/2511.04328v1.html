<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation - Health AI Hub</title>
    <meta name="description" content="This paper introduces RxSafeBench, a novel framework and comprehensive benchmark for systematically evaluating the medication safety capabilities of Large Langu">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04328v1" target="_blank">2511.04328v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-06
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Jiahao Zhao, Luxin Xu, Minghuan Tan, Lichao Zhang, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04328v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04328v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces RxSafeBench, a novel framework and comprehensive benchmark for systematically evaluating the medication safety capabilities of Large Language Models (LLMs) in simulated clinical consultations. It addresses critical gaps in real-world data and realistic evaluation by generating inquiry-diagnosis dialogues with embedded medication risks, supported by the RxRisk DB knowledge base. The findings reveal that current LLMs significantly struggle to integrate contraindication and drug interaction knowledge, particularly when risks are implied rather than explicit, underscoring major challenges for AI-driven clinical decision support.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Ensuring medication safety is paramount in healthcare, and this research directly addresses the critical need to validate the safety performance of LLMs integrated into clinical decision support systems. By identifying shortcomings in handling drug interactions and contraindications, it aims to prevent potential iatrogenic harm and enhance patient safety in AI-assisted medical practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper focuses on the evaluation and improvement of Large Language Models (LLMs) designed to function as AI-driven clinical decision support systems, specifically for identifying and mitigating medication safety issues in patient consultations. This directly applies to enhancing the reliability and safety of AI tools used by healthcare professionals for medication management.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Identifies a critical research gap in LLM medication safety due to the lack of real-world datasets and underexplored evaluation in realistic clinical consultation settings.</li>
                    
                    <li>Proposes a framework that simulates clinical consultations to systematically assess LLM medication safety capabilities.</li>
                    
                    <li>Constructs RxRisk DB, a dedicated medication safety database comprising 6,725 contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.</li>
                    
                    <li>Develops RxSafeBench, a benchmark with 2,443 high-quality consultation scenarios generated through a two-stage filtering strategy to ensure clinical realism and professional quality.</li>
                    
                    <li>Evaluates leading open-source and proprietary LLMs using structured multiple-choice questions to test their ability to recommend safe medications in simulated patient contexts.</li>
                    
                    <li>Key finding: Current LLMs struggle significantly to integrate contraindication and drug interaction knowledge, with performance notably worse when risks are implied rather than explicitly stated.</li>
                    
                    <li>Highlights the need for improvements in LLM reliability for medication safety through better prompting strategies and task-specific tuning.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved developing a framework to simulate clinical consultations, generating inquiry-diagnosis dialogues with embedded medication risks. A comprehensive medication safety database, RxRisk DB, was constructed. A two-stage filtering strategy was employed to create RxSafeBench, a benchmark of 2,443 high-quality consultation scenarios. Leading open-source and proprietary LLMs were then evaluated using structured multiple-choice questions designed to assess their ability to recommend safe medications under simulated patient contexts.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Current Large Language Models exhibit significant difficulty in integrating complex contraindication and drug interaction knowledge. This struggle is particularly pronounced when medication risks are implicitly suggested within the patient's context rather than explicitly stated, indicating a deficiency in inferential reasoning for medication safety.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>RxSafeBench provides a crucial, standardized tool for rigorously evaluating the medication safety of AI models, thereby facilitating the development and responsible deployment of safer AI-driven clinical decision support systems. The identified limitations of LLMs in recognizing implied risks necessitate increased caution for clinicians adopting these technologies and guide developers to prioritize robustness in handling nuanced medication safety scenarios, ultimately aiming to reduce medication errors and improve patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The evaluation relies on simulated clinical consultations and structured multiple-choice questions, which may not fully replicate the dynamic, ambiguous, and free-form nature of real-world clinical interactions or the exhaustive scope of medication safety issues. The abstract implies the current limitations are specific to LLM's integration capabilities for contraindications and interactions.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should focus on improving LLM reliability through advanced prompting strategies and task-specific fine-tuning. This will aim to enhance their ability to integrate complex medication safety knowledge, particularly for implicitly conveyed risks, thereby advancing safer and more trustworthy AI-driven clinical decision support systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Clinical Medicine</span>
                    
                    <span class="tag">Patient Safety</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                    <span class="tag">Clinical Pharmacy</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medication Safety</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Clinical Decision Support</span>
                    
                    <span class="tag tag-keyword">Drug Interactions</span>
                    
                    <span class="tag tag-keyword">Contraindications</span>
                    
                    <span class="tag tag-keyword">Benchmark</span>
                    
                    <span class="tag tag-keyword">AI in Healthcare</span>
                    
                    <span class="tag tag-keyword">Patient Safety</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>To appear in BIBM2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>