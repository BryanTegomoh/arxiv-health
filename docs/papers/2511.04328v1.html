<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation - Health AI Hub</title>
    <meta name="description" content="This paper introduces RxSafeBench, a novel benchmark and framework for systematically evaluating the medication safety capabilities of Large Language Models (LL">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04328v1" target="_blank">2511.04328v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-06
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Jiahao Zhao, Luxin Xu, Minghuan Tan, Lichao Zhang, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04328v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04328v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces RxSafeBench, a novel benchmark and framework for systematically evaluating the medication safety capabilities of Large Language Models (LLMs) in simulated clinical consultations. It addresses the lack of real-world data and evaluation in realistic settings by creating a dedicated medication safety database (RxRisk DB) and generating high-quality consultation scenarios. The study reveals that current LLMs struggle significantly with integrating contraindication and drug interaction knowledge, particularly when risks are implicitly stated, underscoring critical safety concerns for AI-driven clinical decision support.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Ensuring medication safety is paramount in healthcare, and as LLMs are increasingly integrated into clinical decision support, their potential to cause harm through unsafe medication recommendations is a critical concern. This research directly addresses this by providing a robust method to identify and mitigate such risks, crucial for patient well-being and regulatory compliance in an AI-driven medical landscape.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research evaluates the medication safety capabilities of Large Language Models (LLMs) within simulated clinical consultations. It aims to identify and address challenges in LLM-based systems to ensure safer and more reliable AI-driven clinical decision support, particularly concerning medication recommendations and risk avoidance for patients.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical gap in LLM medication safety research due to the scarcity of real-world datasets and limited evaluation in realistic clinical consultation settings.</li>
                    
                    <li>Proposes a comprehensive framework for simulating and evaluating clinical consultations to systematically assess LLM medication safety capabilities.</li>
                    
                    <li>Constructs RxRisk DB, a dedicated medication safety database comprising 6,725 contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.</li>
                    
                    <li>Develops RxSafeBench, a benchmark of 2,443 high-quality consultation scenarios generated through inquiry diagnosis dialogues with embedded medication risks and validated by a two-stage filtering strategy for clinical realism and professional quality.</li>
                    
                    <li>Evaluates leading open-source and proprietary LLMs using structured multiple-choice questions to test their ability to recommend safe medications under simulated patient contexts.</li>
                    
                    <li>Identifies a significant limitation in current LLMs: their struggle to integrate contraindication and drug interaction knowledge, especially when medication risks are implied rather than explicitly stated in patient dialogues.</li>
                    
                    <li>Highlights the necessity for improved prompting strategies and task-specific tuning to enhance the reliability and safety of LLM-based clinical decision support systems.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves a novel framework for simulating clinical consultations. It initiates by generating inquiry diagnosis dialogues with embedded medication risks. A dedicated medication safety database, RxRisk DB, was constructed from a vast collection of contraindications, drug interactions, and indication-drug pairs. These dialogues and database entries underwent a two-stage filtering strategy to ensure clinical realism and professional quality, resulting in RxSafeBench, comprising 2,443 high-quality scenarios. Leading open-source and proprietary LLMs were subsequently evaluated using structured multiple-choice questions within these simulated patient contexts to assess their capacity for recommending safe medications.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Current Large Language Models consistently demonstrate significant difficulty in integrating and applying medication safety knowledge, specifically regarding contraindications and drug interactions. This deficiency is markedly more pronounced when medication risks are implicitly present within the patient context rather than explicitly stated, indicating a fundamental lack of robust contextual understanding and inferential reasoning in existing models concerning medication safety.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This benchmark and its findings provide clinicians and developers with a crucial, validated tool to identify and address medication safety vulnerabilities in LLM-powered medical systems before deployment. By precisely pinpointing where LLMs fail, it enables targeted improvements in model training, prompting strategies, and fine-tuning, ultimately leading to safer, more reliable AI tools for clinical decision support and significantly reducing the risk of adverse drug events in patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study identifies a significant limitation inherent in current LLMs: their consistent struggle to accurately identify and integrate medication contraindication and drug interaction knowledge, especially when these risks are subtly implied rather than explicitly stated within a consultation context. This finding highlights a critical barrier to the safe and reliable application of LLMs in clinical decision-making.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings strongly suggest that future research should concentrate on enhancing LLM reliability through advanced prompting techniques and highly specific task-based tuning. This is particularly crucial for improving their ability to effectively integrate complex medication safety knowledge and accurately interpret implied risks within nuanced clinical dialogues. RxSafeBench is positioned as a foundational tool for developing and rigorously evaluating such advanced and safer AI models.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Patient Safety</span>
                    
                    <span class="tag">Internal Medicine</span>
                    
                    <span class="tag">General Practice</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medication Safety</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Clinical Decision Support</span>
                    
                    <span class="tag tag-keyword">Drug Interactions</span>
                    
                    <span class="tag tag-keyword">Contraindications</span>
                    
                    <span class="tag tag-keyword">Benchmark</span>
                    
                    <span class="tag tag-keyword">Simulated Consultation</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>To appear in BIBM2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>