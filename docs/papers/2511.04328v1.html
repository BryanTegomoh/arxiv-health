<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation - Health AI Hub</title>
    <meta name="description" content="This paper introduces RxSafeBench, a novel benchmark framework designed to evaluate the medication safety capabilities of Large Language Models (LLMs) in simula">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04328v1" target="_blank">2511.04328v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-06
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Jiahao Zhao, Luxin Xu, Minghuan Tan, Lichao Zhang, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04328v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04328v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces RxSafeBench, a novel benchmark framework designed to evaluate the medication safety capabilities of Large Language Models (LLMs) in simulated clinical consultations. It addresses the scarcity of real-world data by generating realistic patient dialogues with embedded medication risks, backed by a comprehensive RxRisk DB. The study reveals that current LLMs struggle significantly with integrating contraindication and drug interaction knowledge, especially when risks are implicitly presented, highlighting critical areas for improving AI-driven clinical decision support.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Medication safety is foundational to patient care, and as LLMs become integral to clinical decision support, their ability to accurately identify drug contraindications and interactions is crucial for preventing adverse drug events, minimizing patient harm, and ensuring optimal therapeutic outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper focuses on identifying and mitigating medication safety issues in Large Language Models (LLMs) when used as AI-driven clinical decision support tools for medical consultations. Its application is to ensure these AI systems can recommend safe medications, integrate complex drug knowledge (e.g., contraindications, drug interactions), and thereby enhance patient safety in healthcare settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical gap in medication safety evaluation for LLMs, necessitated by the lack of real-world data and realistic clinical simulation scenarios.</li>
                    
                    <li>Proposes a novel framework that simulates clinical consultations to systematically assess LLM performance in identifying medication safety issues.</li>
                    
                    <li>Constructs RxRisk DB, a dedicated medication safety database comprising 6,725 contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.</li>
                    
                    <li>Develops RxSafeBench, a high-quality benchmark with 2,443 clinically realistic consultation scenarios generated through a two-stage filtering process to ensure professional quality.</li>
                    
                    <li>Evaluates leading open-source and proprietary LLMs using structured multiple-choice questions that test their ability to recommend safe medications under simulated patient contexts.</li>
                    
                    <li>Reveals that current LLMs significantly struggle to integrate complex contraindication and drug interaction knowledge, particularly when these risks are implied rather than explicitly stated within the dialogue.</li>
                    
                    <li>RxSafeBench is presented as the first comprehensive benchmark for evaluating medication safety in LLMs, aiming to advance safer and more trustworthy AI in clinical decision support.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study's methodology involves proposing a framework for simulating clinical consultations by generating inquiry diagnosis dialogues embedded with medication risks. It establishes the RxRisk DB, a comprehensive database of contraindications, drug interactions, and indication-drug pairs. A rigorous two-stage filtering strategy is applied to ensure clinical realism and professional quality, leading to the creation of the RxSafeBench benchmark, consisting of 2,443 high-quality consultation scenarios. Leading LLMs (both open-source and proprietary) are then evaluated using structured multiple-choice questions designed to test their capacity for recommending safe medications within these simulated patient contexts.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is that current Large Language Models demonstrate substantial deficiencies in accurately integrating knowledge pertaining to drug contraindications and drug interactions. This inability is particularly evident and problematic in scenarios where medication risks are implicitly suggested by the patient's context rather than explicitly stated, indicating a significant gap in their contextual reasoning and nuanced understanding of medication safety.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has direct clinical impact by highlighting critical vulnerabilities in the medication safety capabilities of current LLM-based clinical decision support systems. By providing the RxSafeBench benchmark, it offers a crucial tool for developers and clinicians to rigorously evaluate and improve the reliability of AI in preventing adverse drug events. This work guides the development of safer AI through focused improvements in prompting and fine-tuning, ultimately enhancing patient safety and the trustworthiness of AI in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study identifies significant limitations in current LLMs' ability to integrate complex medication safety knowledge, specifically concerning contraindications and drug interactions. These models particularly struggle when safety risks are implied through patient context rather than explicitly stated, highlighting a lack of robust contextual understanding and inferential capabilities critical for real-world clinical application.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The findings suggest that future research should concentrate on enhancing LLM reliability through advanced prompting strategies and task-specific fine-tuning. This is aimed at improving their ability to integrate complex medication safety knowledge, especially when risks are presented implicitly. Further expansion of benchmarks like RxSafeBench to cover a broader spectrum of medication management challenges could also be beneficial.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Medical AI</span>
                    
                    <span class="tag">Patient Safety</span>
                    
                    <span class="tag">Clinical Pharmacy</span>
                    
                    <span class="tag">Drug Discovery</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medication Safety</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Clinical Decision Support</span>
                    
                    <span class="tag tag-keyword">Drug Interactions</span>
                    
                    <span class="tag tag-keyword">Contraindications</span>
                    
                    <span class="tag tag-keyword">Benchmark</span>
                    
                    <span class="tag tag-keyword">Simulated Consultation</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>To appear in BIBM2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>