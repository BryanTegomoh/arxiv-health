<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation - Health AI Hub</title>
    <meta name="description" content="This paper introduces RxSafeBench, a novel framework and benchmark designed to rigorously evaluate Large Language Models' medication safety capabilities within ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.04328v1" target="_blank">2511.04328v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-06
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Jiahao Zhao, Luxin Xu, Minghuan Tan, Lichao Zhang, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.04328v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.04328v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces RxSafeBench, a novel framework and benchmark designed to rigorously evaluate Large Language Models' medication safety capabilities within simulated clinical consultations. It reveals that current LLMs significantly struggle to integrate medication contraindication and interaction knowledge, particularly when risks are implied rather than explicitly stated. The work highlights critical challenges for ensuring safer, more trustworthy AI-driven clinical decision support systems.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is paramount for enhancing patient safety by identifying and mitigating potential medication errors in AI-powered clinical decision support systems. By rigorously evaluating LLM medication safety, it directly contributes to reducing adverse drug events and fostering more reliable healthcare AI technologies.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Evaluating and improving the medication safety capabilities of Large Language Models (LLMs) intended for use in clinical consultation, medication recommendation, and AI-driven clinical decision support systems to ensure safer patient outcomes.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses a critical gap in LLM research regarding medication safety, stemming from a lack of real-world datasets and limited evaluation in realistic clinical consultation settings.</li>
                    
                    <li>Proposes a comprehensive framework for simulating clinical consultations, which generates inquiry diagnosis dialogues embedding medication risks.</li>
                    
                    <li>Developed 'RxRisk DB', a dedicated medication safety database comprising 6,725 contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.</li>
                    
                    <li>Constructed 'RxSafeBench', a high-quality benchmark containing 2,443 clinically realistic consultation scenarios, ensured through a rigorous two-stage filtering strategy.</li>
                    
                    <li>Evaluated leading open-source and proprietary LLMs using structured multiple-choice questions to test their ability to recommend safe medications under simulated patient contexts.</li>
                    
                    <li>Identified a significant weakness in current LLMs: their inability to effectively integrate contraindication and drug interaction knowledge, especially when these risks are implied rather than explicitly stated.</li>
                    
                    <li>Provides foundational insights for improving LLM reliability in medication safety tasks through better prompting strategies and task-specific fine-tuning.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves creating a simulation framework for clinical consultations, generating inquiry diagnosis dialogues with embedded medication risks. A dedicated medication safety database, RxRisk DB, was constructed from a vast collection of contraindications, drug interactions, and indication-drug pairs. This data then underwent a two-stage filtering process to create RxSafeBench, a benchmark of 2,443 high-quality consultation scenarios. Leading LLMs were subsequently evaluated using structured multiple-choice questions that assessed their ability to recommend safe medications within these simulated patient contexts.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The primary finding is that current Large Language Models struggle significantly with integrating medication safety knowledge, particularly concerning contraindications and drug interactions. This difficulty is exacerbated when medication risks are implicitly suggested within a patient's context rather than explicitly stated, highlighting a critical gap in their understanding and application of complex medical information.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has a direct clinical impact by providing a much-needed benchmark for evaluating AI systems intended for clinical use, particularly in medication prescribing and review. By exposing current LLM limitations in medication safety, it guides developers towards creating more reliable and trustworthy AI tools that can genuinely assist healthcare professionals in preventing adverse drug events, thereby improving patient outcomes and healthcare quality.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>While not explicitly stated as limitations of *this* study in the abstract, the findings inherently point to the current limitations of LLMs themselves in handling complex, nuanced medication safety decisions. The evaluation relies on simulated consultations, which, while comprehensive, may not fully replicate the myriad complexities and unpredictable elements of real-world clinical practice.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper suggests that future work should focus on improving the reliability of LLMs in medication safety tasks through better prompting strategies and specific fine-tuning for such clinical contexts. The RxSafeBench benchmark is presented as a valuable tool to advance the development of safer and more trustworthy AI-driven clinical decision support systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Clinical Informatics</span>
                    
                    <span class="tag">Patient Safety</span>
                    
                    <span class="tag">Drug Management</span>
                    
                    <span class="tag">Medical AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Medication Safety</span>
                    
                    <span class="tag tag-keyword">Large Language Models</span>
                    
                    <span class="tag tag-keyword">Clinical Decision Support</span>
                    
                    <span class="tag tag-keyword">Drug Interactions</span>
                    
                    <span class="tag tag-keyword">Contraindications</span>
                    
                    <span class="tag tag-keyword">AI Evaluation</span>
                    
                    <span class="tag tag-keyword">Benchmark</span>
                    
                    <span class="tag tag-keyword">Patient Safety</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>To appear in BIBM2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>