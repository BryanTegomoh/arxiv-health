<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models - Health AI Hub</title>
    <meta name="description" content="DSFedMed tackles the challenge of deploying computationally intensive Foundation Models (FMs) in federated medical image segmentation by proposing a dual-scale ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.16073v1" target="_blank">2601.16073v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-22
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Hanwen Zhang, Qiaojin Shen, Yuxi Liu, Yuesheng Zhu, Guibo Luo
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.DC
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.16073v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.16073v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">DSFedMed tackles the challenge of deploying computationally intensive Foundation Models (FMs) in federated medical image segmentation by proposing a dual-scale framework. It leverages mutual knowledge distillation between a centralized FM and lightweight client models, supported by synthetic medical images and a learnability-guided sample selection strategy. This approach significantly boosts efficiency and performance, achieving a 2% Dice score improvement and a 90% reduction in communication costs and inference time.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework is crucial for healthcare as it enables the deployment of powerful AI models (Foundation Models) in privacy-sensitive, resource-limited federated medical environments. By improving segmentation accuracy while drastically cutting down communication and computational costs, it facilitates wider, more efficient, and secure adoption of AI in medical diagnosis and analysis.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research applies advanced AI (federated learning, foundation models, knowledge distillation) to automate and improve medical image segmentation. This application allows for the precise delineation of anatomical structures, tumors, lesions, or other regions of interest from medical scans (e.g., MRI, CT, X-ray). This capability is crucial for accurate disease diagnosis, personalized treatment planning (e.g., radiation therapy, surgical navigation), monitoring disease progression, and quantifying disease burden, ultimately leading to more efficient and effective patient care, especially in scenarios with distributed data and privacy concerns.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the high computational demands, communication overhead, and inference costs of Foundation Models in federated medical imaging settings.</li>
                    
                    <li>Proposes DSFedMed, a dual-scale federated framework enabling mutual knowledge distillation between a centralized Foundation Model and lightweight client models.</li>
                    
                    <li>Utilizes a set of high-quality synthetic medical images to replace real public datasets for knowledge distillation.</li>
                    
                    <li>Incorporates a learnability-guided sample selection strategy to enhance the efficiency and effectiveness of the dual-scale distillation process.</li>
                    
                    <li>The mutual distillation allows the Foundation Model to transfer general knowledge to clients while also incorporating client-specific insights to refine itself.</li>
                    
                    <li>Achieves an average 2% improvement in Dice score across five medical imaging segmentation datasets.</li>
                    
                    <li>Demonstrates significant efficiency gains, reducing communication costs and inference time by nearly 90% compared to existing federated foundation model baselines.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>DSFedMed employs a dual-scale federated learning architecture. It facilitates mutual knowledge distillation where a large, centralized Foundation Model (FM) transfers its general learned representations to smaller, lightweight models deployed at client sites. Concurrently, the lightweight client models provide distilled, client-specific insights back to refine the central FM. To support this distillation, the framework uses a set of high-quality synthetic medical images, replacing the need for real public datasets. A learnability-guided sample selection strategy is implemented to optimize the efficiency and effectiveness of this dual-scale distillation process.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The DSFedMed framework demonstrated significant improvements, achieving an average increase of 2% in the Dice score for medical image segmentation tasks. Crucially, it delivered substantial efficiency gains, reducing both communication costs and inference time by nearly 90% when benchmarked against existing federated foundation model baselines.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>DSFedMed offers a pathway to integrate highly performant Foundation Models into clinical practice for medical image analysis, particularly in settings constrained by data privacy regulations (e.g., GDPR, HIPAA), limited computational resources, or slow network infrastructure. By enabling efficient segmentation with reduced communication and inference times, it can accelerate diagnostic workflows, support more widespread deployment of AI tools in hospitals and clinics, and potentially lead to more accurate and timely diagnoses across diverse patient populations.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations or caveats of the proposed DSFedMed framework. Potential limitations, not explicitly stated but inherent to the approach, might include the quality and representativeness of the synthetically generated medical images for distillation, or the generalizability of the "learnability-guided sample selection" across highly diverse medical datasets.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions. However, based on the work, potential future work could include exploring different synthetic data generation techniques, evaluating the framework on a wider range of medical imaging modalities and segmentation tasks, or investigating adaptive strategies for the "learnability-guided sample selection."</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Federated Learning</span>
                    
                    <span class="tag tag-keyword">Foundation Models</span>
                    
                    <span class="tag tag-keyword">Medical Image Segmentation</span>
                    
                    <span class="tag tag-keyword">Knowledge Distillation</span>
                    
                    <span class="tag tag-keyword">Dual-Scale</span>
                    
                    <span class="tag tag-keyword">Synthetic Data</span>
                    
                    <span class="tag tag-keyword">Efficiency</span>
                    
                    <span class="tag tag-keyword">Privacy</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>