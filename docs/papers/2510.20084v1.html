<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models - Health AI Hub</title>
    <meta name="description" content="This paper introduces ShapeX, a novel framework designed to provide explainable AI for time series classification models by focusing on shapelets. ShapeX segmen">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20084v1" target="_blank">2510.20084v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Bosong Huang, Ming Jin, Yuxuan Liang, Johan Barthelemy, Debo Cheng, Qingsong Wen, Chenghao Liu, Shirui Pan
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20084v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20084v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces ShapeX, a novel framework designed to provide explainable AI for time series classification models by focusing on shapelets. ShapeX segments time series into meaningful shapelet-driven segments and utilizes Shapley values to assess their saliency, thereby addressing the crucial need for transparency in high-stakes applications like healthcare. The framework demonstrates superior precision and causal fidelity in identifying relevant subsequences compared to existing explanation methods, grounding explanations in causal relationships rather than mere correlations.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Explainable AI in healthcare is paramount for building trust and enabling clinical adoption of machine learning models. ShapeX offers a robust method to understand *why* a time series classification model made a particular decision on medical data (e.g., an ECG or EEG), which is critical for diagnosis, prognosis, and treatment planning in high-stakes clinical scenarios.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research directly contributes to medical AI by enhancing the interpretability and trustworthiness of time series classification models used in healthcare. It allows clinicians and researchers to understand *why* an AI model made a particular prediction based on physiological or clinical time-series data, by identifying key underlying 'shapelets' (meaningful subsequences). This improved transparency and causal insight can lead to more reliable diagnoses, better informed treatment decisions, and increased confidence in AI-driven health recommendations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the limitation of current Post-Hoc Time Series Explanation (PHTSE) methods, which overlook shapelets as primary drivers of classification outcomes, focusing instead on timestep-level attribution.</li>
                    
                    <li>Proposes ShapeX, an innovative framework that segments time series based on specific, meaningful shapelet-driven subsequences.</li>
                    
                    <li>Employs Shapley values to quantitatively assess the saliency and contribution of these shapelet-driven segments to the classification output.</li>
                    
                    <li>Integrates the Shapelet Describe-and-Detect (SDD) framework as its core to effectively learn a diverse and essential set of shapelets for classification.</li>
                    
                    <li>A key innovation is its ability to produce explanations that reveal causal relationships, leveraging the 'atomicity properties' of shapelets, in contrast to merely identifying correlations.</li>
                    
                    <li>Experimental validation on both synthetic and real-world datasets demonstrates ShapeX's superior performance over existing methods.</li>
                    
                    <li>ShapeX enhances the precision in identifying the most relevant subsequences and significantly improves the causal fidelity of time series explanations.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>ShapeX is an innovative framework that first segments time series into 'shapelet-driven segments'. At its core, the Shapelet Describe-and-Detect (SDD) framework is utilized to learn a diverse and relevant set of shapelets essential for the underlying classification task. Subsequently, Shapley values are employed to quantify the saliency and contribution of these identified shapelet-driven segments to the final classification outcome. The approach emphasizes leveraging the 'atomicity properties' of shapelets to ensure that explanations reflect causal relationships rather than mere correlations.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>ShapeX demonstrably outperforms existing post-hoc time series explanation methods. It achieves superior precision in identifying the most relevant subsequences within time series data. Crucially, it provides explanations with enhanced 'causal fidelity,' meaning the identified shapelets are true drivers of the classification outcome, offering a deeper understanding beyond simple correlations. This significantly bridges the gap in current XAI for time series by making fundamental features (shapelets) central to the explanation.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research could significantly enhance the trustworthiness and clinical utility of AI models in healthcare. By providing clear, causal explanations based on specific physiological patterns (shapelets), clinicians can gain a deeper understanding of model predictions (e.g., disease diagnosis, risk stratification). This interpretability facilitates more confident clinical decision-making, supports regulatory approval for medical AI devices, and could lead to earlier and more precise interventions by highlighting the exact contributing factors from complex patient time-series data.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of the ShapeX framework itself. However, implicit limitations could include the computational cost associated with Shapley value calculations, particularly for very long time series or numerous shapelets, and the generalizability of shapelets learned by SDD across diverse patient populations or disease variations.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly outline future research directions. However, potential future work could involve exploring the integration of ShapeX with multivariate time series data, optimizing the computational efficiency of the framework for real-time clinical applications, or conducting extensive human-in-the-loop studies with clinicians to validate the practical interpretability and utility of ShapeX's explanations in specific medical contexts.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Cardiology (ECG analysis, arrhythmia detection)</span>
                    
                    <span class="tag">Neurology (EEG analysis, seizure prediction)</span>
                    
                    <span class="tag">Critical Care Medicine (vital signs monitoring, sepsis prediction)</span>
                    
                    <span class="tag">Endocrinology (continuous glucose monitoring for diabetes management)</span>
                    
                    <span class="tag">Wearable Health Technology (activity recognition, anomaly detection)</span>
                    
                    <span class="tag">Physiological Monitoring</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Time series classification</span>
                    
                    <span class="tag tag-keyword">Explainable AI (XAI)</span>
                    
                    <span class="tag tag-keyword">Shapelets</span>
                    
                    <span class="tag tag-keyword">Post-hoc explanations</span>
                    
                    <span class="tag tag-keyword">Shapley values</span>
                    
                    <span class="tag tag-keyword">Causal fidelity</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Feature attribution</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>