<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models - Health AI Hub</title>
    <meta name="description" content="ShapeX is an innovative framework addressing the limitations of existing post-hoc time series explanation methods by focusing on shapelet-driven segments rather">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">← Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20084v1" target="_blank">2510.20084v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Bosong Huang, Ming Jin, Yuxuan Liang, Johan Barthelemy, Debo Cheng, Qingsong Wen, Chenghao Liu, Shirui Pan
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20084v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20084v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">ShapeX is an innovative framework addressing the limitations of existing post-hoc time series explanation methods by focusing on shapelet-driven segments rather than individual timesteps. It leverages the Shapelet Describe-and-Detect (SDD) framework and Shapley values to identify salient subsequences, providing explanations that reveal causal relationships instead of mere correlations. Experimental results confirm ShapeX's superior performance in precision and causal fidelity compared to current approaches.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>In high-stakes medical applications, transparent and trustworthy explanations for time series classification models are critical for diagnosis, prognosis, and treatment decisions. ShapeX provides a more causally-aware and precise explanation, which can build clinician trust and facilitate the adoption of AI in clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>ShapeX can be applied to enhance the explainability of AI models that classify time-series medical data (e.g., ECGs for arrhythmia detection, EEGs for seizure prediction, continuous glucose monitoring for diabetes management, vital signs for predicting adverse events). By identifying specific 'shapelets' (key subsequences) in a patient's data and attributing causality, it can help clinicians understand the rationale behind an AI's diagnosis, prognosis, or intervention recommendation, thereby increasing trust and facilitating more informed clinical decisions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Existing post-hoc time series explanation (PHTSE) methods predominantly use timestep-level feature attribution, overlooking the fundamental role of key shapelets in classification outcomes.</li>
                    
                    <li>ShapeX introduces a novel framework that segments time series into meaningful shapelet-driven segments for explanation.</li>
                    
                    <li>It employs Shapley values to quantitatively assess the saliency and contribution of these shapelet-driven segments to the classification decision.</li>
                    
                    <li>At its core, ShapeX utilizes the Shapelet Describe-and-Detect (SDD) framework, which effectively learns a diverse set of shapelets essential for classification.</li>
                    
                    <li>ShapeX is designed to reveal causal relationships in explanations, attributed to the atomicity properties of shapelets, rather than just correlations.</li>
                    
                    <li>Experimental evaluations on both synthetic and real-world datasets demonstrate ShapeX's superiority over existing methods in identifying the most relevant subsequences.</li>
                    
                    <li>The framework significantly enhances both the precision and, crucially, the causal fidelity of time series explanations.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>ShapeX operates by first segmenting time series into meaningful 'shapelet-driven segments'. This segmentation is facilitated by the 'Shapelet Describe-and-Detect (SDD) framework', which learns a diverse set of shapelets crucial for the underlying classification task. Subsequently, ShapeX employs 'Shapley values' to assess and quantify the saliency and contribution of these identified shapelet-driven segments to the model's classification outcome. This approach provides explanations at a semantically richer subsequence level, aiming to reveal causal relationships.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key findings indicate that ShapeX significantly outperforms existing post-hoc time series explanation methods. It demonstrates superior capabilities in accurately identifying the most relevant subsequences within time series data. Importantly, ShapeX substantially enhances both the precision and, critically, the causal fidelity of the generated time series explanations, as validated through extensive experiments on diverse synthetic and real-world datasets.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>ShapeX has the potential to profoundly impact clinical practice by providing more interpretable and trustworthy AI models for medical time series data. Clinicians could gain deeper insights into *why* a model made a specific prediction (e.g., detecting early signs of sepsis from vital signs or diagnosing an arrhythmia from an ECG), enabling more confident clinical decision-making, improved patient safety, and potentially uncovering novel disease biomarkers. This enhanced transparency is crucial for the regulatory approval and widespread adoption of AI in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the ShapeX framework or the experimental study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly mention specific future research directions for ShapeX.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Critical Care Monitoring</span>
                    
                    <span class="tag">Remote Patient Monitoring</span>
                    
                    <span class="tag">Diabetology (glucose monitoring)</span>
                    
                    <span class="tag">Oncology (biomarker analysis)</span>
                    
                    <span class="tag">Digital Health Diagnostics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Time Series Classification</span>
                    
                    <span class="tag tag-keyword">Explainable AI (XAI)</span>
                    
                    <span class="tag tag-keyword">Shapelets</span>
                    
                    <span class="tag tag-keyword">Post Hoc Explanations</span>
                    
                    <span class="tag tag-keyword">Shapley Values</span>
                    
                    <span class="tag tag-keyword">Causal Fidelity</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                    <span class="tag tag-keyword">Feature Attribution</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">← Back to all papers</a></p>
    </footer>
</body>
</html>