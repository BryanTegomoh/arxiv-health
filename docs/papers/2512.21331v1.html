<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning - Health AI Hub</title>
    <meta name="description" content="TICON introduces a transformer-based tile representation contextualizer that unifies and enriches embeddings from various tile-level pathology foundation models">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.21331v1" target="_blank">2512.21331v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-24
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofi√®ne Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis Maria Vakalopoulou, Dimitris Samaras
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.21331v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.21331v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">TICON introduces a transformer-based tile representation contextualizer that unifies and enriches embeddings from various tile-level pathology foundation models by integrating crucial slide-level context. This novel approach significantly improves performance across diverse computational pathology tasks, establishing new state-of-the-art results on both tile-level and slide-level benchmarks, even enabling the creation of powerful slide-level foundation models with substantially less training data.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate interpretation of histopathology images is fundamental for disease diagnosis, prognosis, and treatment planning. TICON's ability to provide context-rich, unified representations of tissue tiles significantly enhances the precision and reliability of computational pathology tools, potentially leading to more accurate and efficient diagnostic workflows.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>TICON is an AI model (transformer-based contextualizer) designed to improve the interpretation and analysis of whole slide images (WSIs) in histopathology. Its application lies in enhancing the performance of AI systems used for disease diagnosis, grading, and prognosis by providing richer, more contextualized representations of tissue samples. This can lead to more accurate and efficient AI-powered diagnostic tools for pathologists, ultimately benefiting patient care.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>TICON addresses the critical need for slide-level context in interpreting small tiles from Whole Slide Images (WSIs), a limitation of standard tile encoder-based pipelines.</li>
                    
                    <li>It employs a transformer-based architecture as a universal contextualizer, designed to process and enrich embeddings derived from *any* existing tile-level pathology foundation model.</li>
                    
                    <li>The model is pretrained using a masked modeling objective, allowing it to simultaneously unify and contextualize representations from diverse upstream tile encoders.</li>
                    
                    <li>TICON-contextualized embeddings demonstrate significant performance improvements, achieving new state-of-the-art results on multiple tile-level benchmarks (HEST-Bench, THUNDER, CATCH).</li>
                    
                    <li>It also leads to superior performance on slide-level benchmarks, specifically Patho-Bench, highlighting its ability to capture rich global slide information.</li>
                    
                    <li>A key finding is that an aggregator pretrained on TICON representations can form a high-performing slide-level foundation model using only 11K WSIs, remarkably outperforming existing state-of-the-art models trained with up to 350K WSIs.</li>
                    
                    <li>The unified model approach means that computational pathology pipelines can leverage the strengths of different tile encoders for various tasks while benefiting from TICON's contextualization.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>TICON is a transformer-based model pretrained with a masked modeling objective. It takes as input embeddings generated by existing, diverse tile-level pathology foundation models. It then processes these tile embeddings to integrate slide-level contextual information, producing enriched, contextualized representations. An aggregator can subsequently be pretrained on these TICON-generated embeddings to form a slide-level foundation model.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>TICON-contextualized embeddings achieved new state-of-the-art results on tile-level benchmarks (HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (Patho-Bench). Furthermore, a TICON-based slide-level foundation model, trained on only 11K WSIs, surpassed the performance of existing state-of-the-art slide-level models requiring up to 350K WSIs, demonstrating superior data efficiency and generalization.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By providing more accurate and robust representations of histopathology data, TICON can improve the performance of AI-powered diagnostic tools, aiding pathologists in detecting subtle disease features, quantifying biomarkers, and predicting patient outcomes. This could lead to earlier diagnosis, more personalized treatment strategies, and reduced inter-observer variability in pathology reports, ultimately benefiting patient care and accelerating medical research.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract primarily highlights the limitations of prior *tile encoder-based pipelines* (failing to model slide-level context, varying performance across tasks) which TICON aims to solve. Explicit limitations of the TICON model itself are not detailed in the abstract, but the inherent complexity of transformer models and the need for significant computational resources for training can be inferred.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper explicitly mentions pretraining an aggregator on TICON to form a slide-level foundation model, which suggests a direction for building more comprehensive and data-efficient WSI analysis systems. Further work could explore fine-tuning TICON for specific diagnostic tasks or integrating it with multi-modal data for enhanced predictive power.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Histopathology</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Digital Pathology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Histopathology</span>
                    
                    <span class="tag tag-keyword">Whole Slide Imaging (WSI)</span>
                    
                    <span class="tag tag-keyword">Tile Contextualizer</span>
                    
                    <span class="tag tag-keyword">Transformer</span>
                    
                    <span class="tag tag-keyword">Foundation Models</span>
                    
                    <span class="tag tag-keyword">Computational Pathology</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Medical Image Analysis</span>
                    
                    <span class="tag tag-keyword">Representation Learning</span>
                    
                    <span class="tag tag-keyword">Masked Modeling</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>