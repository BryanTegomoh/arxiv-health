<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning - Health AI Hub</title>
    <meta name="description" content="TICON is a novel transformer-based tile contextualizer designed to generate rich, context-aware embeddings from diverse tile-level foundation models in histopat">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.21331v1" target="_blank">2512.21331v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-24
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofi√®ne Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis Maria Vakalopoulou, Dimitris Samaras
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.21331v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.21331v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">TICON is a novel transformer-based tile contextualizer designed to generate rich, context-aware embeddings from diverse tile-level foundation models in histopathology Whole Slide Images (WSI). It addresses the crucial need for slide-level contextual information by unifying and enhancing existing tile representations, leading to significant performance improvements across a wide array of computational pathology tasks. Notably, TICON enables the development of highly efficient slide-level foundation models that achieve state-of-the-art results with substantially less training data.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Accurate and context-aware interpretation of histopathology Whole Slide Images is fundamental for precise disease diagnosis, prognosis, and treatment planning in oncology and other medical fields. TICON's ability to provide superior, contextualized embeddings can enhance the reliability and performance of AI-powered diagnostic tools, directly impacting patient care and accelerating medical research.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>TICON is an AI foundation model designed to improve the accuracy and efficiency of computational pathology. By creating rich, contextualized embeddings from Whole Slide Images (WSIs), it enhances the capabilities of AI systems used for various medical applications, including automated disease detection, cancer grading and subtyping, tissue classification, and prognostic analysis. It serves as a critical component for developing more advanced and reliable AI tools to assist pathologists in diagnosis and research.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Problem Addressed**: Standard tile encoders for WSI often lack essential slide-level context and struggle to unify representations from diverse, task-specific tile-level foundation models.</li>
                    
                    <li>**TICON Architecture**: It is a transformer-based tile representation contextualizer featuring a single, shared encoder, designed to process and enrich existing tile-level embeddings.</li>
                    
                    <li>**Training Objective**: TICON is pretrained using a masked modeling objective, which allows it to learn rich contextual relationships between tiles and unify representations from various underlying tile-level pathology foundation models.</li>
                    
                    <li>**Versatility & Unification**: The model is engineered to contextualize embeddings derived from *any* tile-level foundation model, offering a unified approach to representation learning in computational pathology.</li>
                    
                    <li>**State-of-the-Art Performance**: TICON-contextualized embeddings consistently achieve new state-of-the-art results across multiple tile-level benchmarks (HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (Patho-Bench).</li>
                    
                    <li>**Data Efficiency for Slide-Level Models**: An aggregator pretrained on TICON-contextualized embeddings forms a slide-level foundation model that outperforms existing state-of-the-art models, remarkably using only 11K WSIs compared to up to 350K WSIs.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>TICON utilizes a transformer-based architecture as a shared encoder, specifically designed to act as a contextualizer for tile embeddings. It is pretrained using a masked modeling objective, where the model learns to infer or reconstruct masked tile features based on their surrounding context within a Whole Slide Image. This pretraining enables TICON to take embeddings from diverse, pre-existing tile-level pathology foundation models and output richer, contextually aware representations. Subsequently, an aggregator is pretrained on these TICON-contextualized embeddings to form a comprehensive slide-level foundation model.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study demonstrates that TICON significantly improves the quality and contextual relevance of tile embeddings, leading to new state-of-the-art performance on several critical computational pathology benchmarks, including HEST-Bench, THUNDER, CATCH (tile-level), and Patho-Bench (slide-level). A key discovery is TICON's efficiency, as it enables a slide-level foundation model to achieve superior performance using only 11,000 WSIs, vastly outperforming models requiring up to 350,000 WSIs.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>TICON has the potential to profoundly impact clinical pathology by enabling the development of more accurate, robust, and data-efficient AI systems for WSI analysis. This could lead to faster and more consistent diagnoses for complex diseases like cancer, improved precision in disease grading and prognostication, and more personalized treatment strategies. Its ability to leverage existing tile encoders also facilitates integration into current digital pathology workflows, potentially accelerating the adoption of advanced AI in clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations. However, an implicit aspect is that TICON operates on embeddings generated by *other* tile-level foundation models. While this offers versatility, its ultimate performance is, to some extent, dependent on the quality and pretraining of these initial tile encoders.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The work highlights the successful pretraining of an aggregator on TICON to form a powerful slide-level foundation model, suggesting further research into developing more advanced and comprehensive slide-level AI tools using these enriched representations. Future directions could also involve exploring TICON's application to an even broader spectrum of histopathology tasks, diverse tissue types, and its integration into end-to-end clinical diagnostic pipelines.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Digital Pathology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Histopathology</span>
                    
                    <span class="tag tag-keyword">Whole Slide Images</span>
                    
                    <span class="tag tag-keyword">Transformer</span>
                    
                    <span class="tag tag-keyword">Representation Learning</span>
                    
                    <span class="tag tag-keyword">Contextualization</span>
                    
                    <span class="tag tag-keyword">Foundation Models</span>
                    
                    <span class="tag tag-keyword">Computational Pathology</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>