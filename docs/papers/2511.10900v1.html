<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering - Health AI Hub</title>
    <meta name="description" content="This paper addresses the limitation of large language models (LLMs) in medical question answering, which often overlook domain-specific expertise like clinical ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.10900v1" target="_blank">2511.10900v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-14
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Xueren Ge, Sahil Murtaza, Anthony Cortez, Homa Alemzadeh
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.10900v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.10900v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper addresses the limitation of large language models (LLMs) in medical question answering, which often overlook domain-specific expertise like clinical subject areas and certification levels. The authors introduce EMSQA, a new medical dataset, and propose Expert-CoT prompting and ExpertRAG, a retrieval-augmented generation pipeline, both designed to leverage this structured context. Their methods significantly improve accuracy and enable a 32B expertise-augmented LLM to pass simulated EMS certification exams.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine and health as it aims to make AI assistants more precise and reliable for emergency medical services (EMS) professionals. By incorporating specific clinical and professional context, it can enhance training, decision support, and ultimately improve patient care in high-stakes pre-hospital settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research focuses on developing expert-guided AI systems (LLMs with Expert-CoT and ExpertRAG) for accurate medical question answering in emergency medicine. It aims to create AI tools capable of understanding and responding to medical queries specific to different clinical areas and professional certification levels, potentially assisting in training, certification, and decision support for EMS professionals by grounding responses in subject area-aligned documents and real-world patient data.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Identified a critical gap where existing LLMs for medical QA overlook domain-specific expertise, such as clinical subject areas (e.g., trauma, airway) and certification levels (e.g., EMT, Paramedic).</li>
                    
                    <li>Developed EMSQA, a novel 24.3K multiple-choice question dataset covering 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents, 2M tokens).</li>
                    
                    <li>Introduced Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject areas and certification levels.</li>
                    
                    <li>Developed ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data.</li>
                    
                    <li>Experiments across 4 LLMs showed Expert-CoT improved accuracy by up to 2.05% over vanilla CoT prompting.</li>
                    
                    <li>Combining Expert-CoT with ExpertRAG yielded an accuracy gain of up to 4.59% compared to standard RAG baselines.</li>
                    
                    <li>A 32B expertise-augmented LLM notably passed all computer-adaptive EMS certification simulation exams, demonstrating high practical competency.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved creating EMSQA, a 24.3K multiple-choice dataset covering 10 clinical subject areas and 4 EMS certification levels, along with curated, subject area-aligned knowledge bases. Two methods were developed: (i) Expert-CoT, a prompting strategy for chain-of-thought reasoning conditioned on specific clinical subject areas and certification levels, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses using subject area-aligned documents and real-world patient data. These methods were evaluated on 4 different large language models.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Expert-CoT improved accuracy by up to 2.05% over vanilla CoT prompting. The combination of Expert-CoT and ExpertRAG achieved an accuracy gain of up to 4.59% compared to standard RAG baselines. Critically, a 32B expertise-augmented LLM successfully passed all computer-adaptive EMS certification simulation exams.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings suggest a strong potential for AI to serve as a highly competent tool in Emergency Medical Services. This could lead to more effective and context-aware training programs for EMTs and Paramedics, provide reliable and tailored decision support during critical incidents, and ultimately enhance the quality and safety of patient care delivered in pre-hospital environments.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state any future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Emergency Medical Services (EMS)</span>
                    
                    <span class="tag">Trauma</span>
                    
                    <span class="tag">Airway Management</span>
                    
                    <span class="tag">Pre-hospital Care</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Retrieval-Augmented Generation (RAG)</span>
                    
                    <span class="tag tag-keyword">Chain-of-Thought (CoT)</span>
                    
                    <span class="tag tag-keyword">Emergency Medical Services (EMS)</span>
                    
                    <span class="tag tag-keyword">Medical Question Answering</span>
                    
                    <span class="tag tag-keyword">Domain-Specific AI</span>
                    
                    <span class="tag tag-keyword">Certification Simulation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted by AAAI 2026</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>