<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation - Health AI Hub</title>
    <meta name="description" content="UniX introduces a novel unified medical foundation model for chest X-ray understanding and generation that overcomes the conflicting goals of these tasks by dec">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.11522v1" target="_blank">2601.11522v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-16
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ruiheng Zhang, Jingfeng Yao, Huangxuan Zhao, Hao Yan, Xiao He, Lei Chen, Zhou Wei, Yong Luo, Zengmao Wang, Lefei Zhang, Dacheng Tao, Bo Du
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.11522v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.11522v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">UniX introduces a novel unified medical foundation model for chest X-ray understanding and generation that overcomes the conflicting goals of these tasks by decoupling them into an autoregressive and a diffusion branch, respectively. It employs a crucial cross-modal self-attention mechanism to dynamically guide generation with understanding features. This architecture achieves significant improvements: 46.1% in understanding (Micro-F1) and 24.2% in generation (FD-RadDino) with high parameter efficiency, establishing a scalable paradigm for synergistic medical image AI.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Unifying chest X-ray understanding (e.g., disease diagnosis, report generation) and high-fidelity generation (e.g., synthetic data augmentation, image reconstruction) within a single, efficient model represents a significant leap for AI in radiology, promising more versatile and robust tools for clinical decision support and medical research.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The UniX model can be applied in healthcare to enhance diagnostic capabilities by assisting radiologists in interpreting chest X-rays, potentially leading to earlier and more accurate detection of various thoracic pathologies (e.g., pneumonia, pneumothorax, lung nodules). Its generation capabilities can be used to create realistic synthetic chest X-ray images for training new AI models, augmenting datasets, or educating medical students and residents without compromising patient privacy.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the inherent conflict in unifying medical image understanding (semantic abstraction) and generation (pixel-level reconstruction) within a single model, which compromises performance in existing parameter-shared autoregressive approaches.</li>
                    
                    <li>Proposes UniX, a decoupled architecture featuring an autoregressive branch dedicated to visual understanding and a diffusion branch for high-fidelity image generation.</li>
                    
                    <li>Introduces a critical cross-modal self-attention mechanism to dynamically guide the diffusion-based generation process by incorporating understanding features from the autoregressive branch.</li>
                    
                    <li>Incorporates a rigorous data cleaning pipeline and a multi-stage training strategy to optimize synergistic collaboration between the understanding and generation tasks.</li>
                    
                    <li>Achieves substantial performance gains on two representative benchmarks: 46.1% improvement in understanding (Micro-F1) and 24.2% gain in generation quality (FD-RadDino).</li>
                    
                    <li>Demonstrates high parameter efficiency, utilizing only a quarter of the parameters of prior models like LLM-CXR while achieving superior or on-par performance with task-specific models.</li>
                    
                    <li>Establishes a scalable paradigm for synergistic medical image understanding and generation, providing a more effective and efficient approach for medical foundation models.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>UniX utilizes a novel decoupled neural architecture comprising an autoregressive branch for visual understanding and a separate diffusion branch for high-fidelity image generation. A core innovation is the cross-modal self-attention mechanism, which enables dynamic guidance of the generative diffusion process by features extracted from the understanding branch. The model's training is further bolstered by a rigorous data cleaning pipeline and a multi-stage training strategy, optimizing the synergistic collaboration between the distinct understanding and generation tasks.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>UniX achieved a significant 46.1% improvement in understanding performance (measured by Micro-F1) and a notable 24.2% gain in generation quality (measured by FD-RadDino) across two representative benchmarks. Remarkably, these performance gains were accomplished using only one-quarter of the parameters compared to previous large models like LLM-CXR, demonstrating superior efficiency while matching or exceeding the performance of dedicated task-specific models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By offering a unified and highly performant model for both interpreting and generating chest X-rays, UniX has the potential to substantially enhance clinical workflows. It could improve diagnostic accuracy through advanced understanding capabilities, automate aspects of radiology report generation, and facilitate the creation of high-fidelity synthetic images for medical education, research, and privacy-preserving data augmentation, ultimately supporting better patient care and more efficient medical imaging practices.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations of the UniX model itself. It primarily highlights the shortcomings of existing approaches that struggle to unify understanding and generation due to conflicting goals.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract establishes UniX as a "scalable paradigm for synergistic medical image understanding and generation," implying broad potential for future applications and extensions. However, it does not explicitly outline specific future research directions for the model or its underlying methodology.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">medical foundation model</span>
                    
                    <span class="tag tag-keyword">chest X-ray</span>
                    
                    <span class="tag tag-keyword">autoregression</span>
                    
                    <span class="tag tag-keyword">diffusion model</span>
                    
                    <span class="tag tag-keyword">visual understanding</span>
                    
                    <span class="tag tag-keyword">image generation</span>
                    
                    <span class="tag tag-keyword">cross-modal self-attention</span>
                    
                    <span class="tag tag-keyword">radiology AI</span>
                    
                    <span class="tag tag-keyword">multi-task learning</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Codes and models are available at https://github.com/ZrH42/UniX</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>