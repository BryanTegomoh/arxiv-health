<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation - Health AI Hub</title>
    <meta name="description" content="UniX is a novel medical foundation model designed for chest X-ray understanding and generation, addressing the inherent conflict between semantic abstraction an">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.11522v1" target="_blank">2601.11522v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-16
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ruiheng Zhang, Jingfeng Yao, Huangxuan Zhao, Hao Yan, Xiao He, Lei Chen, Zhou Wei, Yong Luo, Zengmao Wang, Lefei Zhang, Dacheng Tao, Bo Du
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.11522v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.11522v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">UniX is a novel medical foundation model designed for chest X-ray understanding and generation, addressing the inherent conflict between semantic abstraction and pixel-level reconstruction tasks. It achieves this by decoupling the tasks into distinct autoregressive (understanding) and diffusion (generation) branches, synergistically linked by a cross-modal self-attention mechanism. This architecture, combined with rigorous data processing and multi-stage training, significantly improves both understanding and generation performance compared to prior unified models, establishing a scalable paradigm for medical imaging AI.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Unifying high-fidelity generation and accurate understanding of chest X-rays within a single model can accelerate medical research, improve diagnostic accuracy, facilitate clinician training, and enable the creation of high-quality synthetic data for rare conditions, addressing data scarcity in medical AI.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This AI model aims to enhance the understanding (e.g., diagnosis of conditions from X-rays) and generation (e.g., synthetic data for training, research, or privacy-preserving applications) of chest X-rays. This could lead to more accurate and efficient medical diagnoses, improved training tools for healthcare professionals, and advancement in medical image analysis research.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the conflict between visual understanding (semantic abstraction) and generation (pixel-level reconstruction) in medical foundation models.</li>
                    
                    <li>Introduces a decoupled architecture: an autoregressive branch for chest X-ray understanding and a diffusion branch for high-fidelity chest X-ray generation.</li>
                    
                    <li>Implements a crucial cross-modal self-attention mechanism to dynamically guide the generation process using features from the understanding branch.</li>
                    
                    <li>Employs a rigorous data cleaning pipeline and a multi-stage training strategy to enhance model performance and task synergy.</li>
                    
                    <li>Achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino) on representative benchmarks.</li>
                    
                    <li>Demonstrates significant parameter efficiency, using only a quarter of the parameters compared to LLM-CXR while outperforming it.</li>
                    
                    <li>Establishes a scalable paradigm for synergistic medical image understanding and generation, achieving performance comparable to task-specific models.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>UniX decouples chest X-ray understanding and generation into two distinct, specialized branches: an autoregressive branch for understanding and a diffusion branch for generation. A key innovation is a cross-modal self-attention mechanism that allows understanding features to dynamically guide the diffusion-based generation process. The model's development included a rigorous data cleaning pipeline and a multi-stage training strategy to optimize performance and ensure synergistic task collaboration.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>UniX achieved a substantial 46.1% improvement in chest X-ray understanding performance (measured by Micro-F1) and a 24.2% gain in generation quality (measured by FD-RadDino) on benchmark datasets. Notably, it accomplished these gains with only a quarter of the parameters used by prior unified models like LLM-CXR, demonstrating superior efficiency while performing on par with dedicated task-specific models.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology has the potential to significantly impact clinical practice by providing a more powerful and efficient AI tool for chest X-ray analysis. It could lead to more accurate and faster diagnosis of various pulmonary conditions, improve training for medical residents through high-fidelity synthetic image generation, and facilitate the development of new diagnostic algorithms by mitigating medical data scarcity through realistic image synthesis.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the UniX model or its methodology. However, as with all AI models, generalizability to diverse patient populations and rare pathologies, and the need for continuous validation in real-world clinical settings, are typical considerations.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The work establishes a scalable paradigm, suggesting future research can explore applying this decoupled, synergistic approach to other medical imaging modalities or expanding its capabilities to integrate with multimodal clinical data (e.g., electronic health records) for even richer understanding and generation tasks.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Imaging AI</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Chest X-ray</span>
                    
                    <span class="tag tag-keyword">Medical Foundation Model</span>
                    
                    <span class="tag tag-keyword">Autoregression</span>
                    
                    <span class="tag tag-keyword">Diffusion Models</span>
                    
                    <span class="tag tag-keyword">Image Understanding</span>
                    
                    <span class="tag tag-keyword">Image Generation</span>
                    
                    <span class="tag tag-keyword">Cross-modal Attention</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Codes and models are available at https://github.com/ZrH42/UniX</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>