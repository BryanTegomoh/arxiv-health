<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect - Health AI Hub</title>
    <meta name="description" content="This paper investigates the complex interaction between AI bias (class imbalance) and human bias (base rate neglect) in decision-making contexts involving AI-ba">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.14591v1" target="_blank">2511.14591v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-18
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Nick von Felten, Johannes Sch√∂ning, Klaus Opwis, Nicolas Scharowksi
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.HC, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.14591v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.14591v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper investigates the complex interaction between AI bias (class imbalance) and human bias (base rate neglect) in decision-making contexts involving AI-based decision-support systems. It found that class imbalance negatively impacted participants' ability to appropriately calibrate their reliance on AI, and crucially, identified a mutually reinforcing effect between these two biases, leading to a compound human-AI bias.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>In medical diagnosis and treatment planning, AI decision-support systems are increasingly common. This research highlights how inherent biases in AI training data (e.g., class imbalance for rare diseases) combined with human cognitive biases (e.g., neglecting disease prevalence) can lead to inappropriate reliance on AI, potentially resulting in misdiagnoses or suboptimal patient care, especially for conditions with skewed prevalence.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>An AI-based decision-support system used for classifying diseases.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study focuses on the interaction between AI bias (class imbalance) and human bias (base rate neglect), rather than studying them in isolation.</li>
                    
                    <li>It specifically examined how class imbalance in an AI's training data affects human reliance calibration on an AI decision-support system.</li>
                    
                    <li>A within-subject online study (N=46) required participants to classify three diseases using AI decision support systems trained on either balanced or unbalanced datasets.</li>
                    
                    <li>A key finding was that class imbalance in the AI's training data disrupted participants' ability to appropriately calibrate their reliance on the AI system.</li>
                    
                    <li>The research identified mutually reinforcing effects between AI's class imbalance and human base rate neglect, demonstrating the existence of a 'compound human-AI bias'.</li>
                    
                    <li>The authors advocate for an interactionist perspective in human-AI research to understand these complex bias interactions.</li>
                    
                    <li>The findings suggest the combined impact of human and AI biases can be more detrimental than either bias in isolation.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>A within-subject online study was conducted with 46 participants. Participants classified three different diseases using an AI-based decision-support system. The AI system was presented in two conditions: trained on a balanced dataset or trained on an unbalanced dataset, allowing for the assessment of how class imbalance affects human reliance and its interaction with base rate neglect.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study revealed two primary findings: (1) Class imbalance in the AI's training data significantly disrupted participants' appropriate calibration of their reliance on the AI decision-support system. (2) There were mutually reinforcing effects observed between AI-induced class imbalance and human base rate neglect, providing evidence for a compound human-AI bias.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>These findings are critical for the development and deployment of AI in healthcare. They suggest that clinicians using AI diagnostic tools may miscalibrate their reliance due to AI biases interacting with their own cognitive biases. This could lead to an increased risk of misdiagnoses, especially for rare diseases or conditions with low prevalence that are often underrepresented in training data. To mitigate this, AI systems need robust bias mitigation strategies, and training for clinicians must address not just AI capabilities but also the potential for synergistic human-AI biases to ensure safe and effective patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations. However, typical limitations for such a study might include the relatively small sample size (N=46) and the potential ecological validity concerns of an online study compared to real-world clinical environments.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors advocate for an interactionist perspective in studying human-AI systems. They specifically call for further research into the mutually reinforcing effects of biases that arise from the complex interplay between humans and AI, to better understand and mitigate these compound biases.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Diagnostic imaging</span>
                    
                    <span class="tag">Pathology</span>
                    
                    <span class="tag">Clinical decision support</span>
                    
                    <span class="tag">Epidemiology</span>
                    
                    <span class="tag">Rare disease diagnosis</span>
                    
                    <span class="tag">Precision medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">AI bias</span>
                    
                    <span class="tag tag-keyword">class imbalance</span>
                    
                    <span class="tag tag-keyword">base rate neglect</span>
                    
                    <span class="tag tag-keyword">human-AI interaction</span>
                    
                    <span class="tag tag-keyword">decision support</span>
                    
                    <span class="tag tag-keyword">appropriate reliance</span>
                    
                    <span class="tag tag-keyword">compound bias</span>
                    
                    <span class="tag tag-keyword">medical diagnostics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Humans increasingly interact with artificial intelligence (AI) in decision-making. However, both AI and humans are prone to biases. While AI and human biases have been studied extensively in isolation, this paper examines their complex interaction. Specifically, we examined how class imbalance as an AI bias affects people's ability to appropriately rely on an AI-based decision-support system, and how it interacts with base rate neglect as a human bias. In a within-subject online study (N= 46), participants classified three diseases using an AI-based decision-support system trained on either a balanced or unbalanced dataset. We found that class imbalance disrupted participants' calibration of AI reliance. Moreover, we observed mutually reinforcing effects between class imbalance and base rate neglect, offering evidence of a compound human-AI bias. Based on these findings, we advocate for an interactionist perspective and further research into the mutually reinforcing effects of biases in human-AI interaction.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>