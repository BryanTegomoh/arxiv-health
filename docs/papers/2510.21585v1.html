<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects - Health AI Hub</title>
    <meta name="description" content="REVE is a novel foundation model for Electroencephalography (EEG) designed to overcome the challenges of data heterogeneity across diverse recording setups. By ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.21585v1" target="_blank">2510.21585v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-24
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yassine El Ouahidi, Jonathan Lys, Philipp Th√∂lke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, q-bio.NC
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.21585v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.21585v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">REVE is a novel foundation model for Electroencephalography (EEG) designed to overcome the challenges of data heterogeneity across diverse recording setups. By employing a unique 4D positional encoding and large-scale masked autoencoding pretraining on an unprecedented 25,000 subjects, REVE achieves state-of-the-art performance and strong generalization across 10 downstream EEG tasks, significantly advancing AI applications in clinical neuroscience.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This breakthrough model dramatically improves the ability to build robust and universally applicable AI tools for EEG analysis, which is crucial for developing reliable diagnostics, prognostics, and monitoring systems in neurology and other clinical settings, overcoming the current hurdle of data heterogeneity.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>REVE is a medical AI application designed to enhance the analysis and interpretation of EEG data. By leveraging large-scale pretraining, it provides a robust foundation model for various clinical tasks, including the automated detection of seizures, accurate staging of sleep, and potentially aiding in brain-computer interfaces for rehabilitation or monitoring cognitive states in medical settings. It reduces the reliance on task-specific data, making advanced EEG analysis more accessible and standardized across diverse clinical environments.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of generalizing EEG foundation models across diverse datasets with varying protocols, devices, and electrode configurations.</li>
                    
                    <li>Introduces REVE (Representation for EEG with Versatile Embeddings), featuring a novel 4D positional encoding scheme that allows it to process EEG signals of arbitrary length and electrode arrangements.</li>
                    
                    <li>Pretrained using a masked autoencoding objective on the largest EEG dataset to date: over 60,000 hours of data from 92 datasets spanning 25,000 subjects.</li>
                    
                    <li>Achieves state-of-the-art results on 10 diverse downstream EEG tasks, including significant clinical applications like seizure detection, sleep staging, and motor imagery classification.</li>
                    
                    <li>Demonstrates strong generalization capabilities with minimal to no fine-tuning and effectively models nuanced spatio-temporal patterns in EEG signals.</li>
                    
                    <li>Aims to accelerate progress in clinical neuroscience and standardize EEG research by providing open-source code, pretrained weights, and tutorials.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>REVE is a transformer-based foundation model pretrained using a masked autoencoding objective. Its core innovation is a novel 4D positional encoding scheme that allows it to robustly handle EEG signals regardless of their length or electrode configuration. This model was extensively pretrained on over 60,000 hours of EEG data from 92 public datasets, encompassing 25,000 subjects, representing the largest-scale EEG pretraining effort reported.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>REVE achieved state-of-the-art performance across 10 diverse downstream EEG tasks, including vital clinical applications such as motor imagery classification, seizure detection, sleep staging, cognitive load estimation, and emotion recognition. Crucially, it demonstrated strong generalization capabilities, often requiring little to no fine-tuning, and effectively captured nuanced spatio-temporal dynamics within EEG signals.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>REVE's ability to generalize across varied EEG setups and excel in tasks like seizure detection and sleep staging has profound clinical implications. It can accelerate the development of more reliable, widely applicable, and standardized AI-powered diagnostic and monitoring tools in neurology and sleep medicine. By providing a robust pretrained model, it reduces the need for extensive, task-specific data collection and model training in clinical research, thereby democratizing advanced EEG analysis and potentially leading to faster deployment of new clinical applications.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the REVE model or the study conducted.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly detailing future research directions for the model's architecture, the paper indicates that by releasing code, pretrained weights, and tutorials, REVE will serve as a foundational tool to support standardized EEG research and significantly accelerate progress across various applications in clinical neuroscience, enabling a broader range of future studies and advancements.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Clinical Neurophysiology</span>
                    
                    <span class="tag">Sleep Medicine</span>
                    
                    <span class="tag">Psychiatry</span>
                    
                    <span class="tag">Cognitive Neuroscience</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">EEG</span>
                    
                    <span class="tag tag-keyword">Foundation Model</span>
                    
                    <span class="tag tag-keyword">Pretraining</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Generalization</span>
                    
                    <span class="tag tag-keyword">Seizure Detection</span>
                    
                    <span class="tag tag-keyword">Sleep Staging</span>
                    
                    <span class="tag tag-keyword">Clinical Neuroscience</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Foundation models have transformed AI by reducing reliance on task-specific
data through large-scale pretraining. While successful in language and vision,
their adoption in EEG has lagged due to the heterogeneity of public datasets,
which are collected under varying protocols, devices, and electrode
configurations. Existing EEG foundation models struggle to generalize across
these variations, often restricting pretraining to a single setup, resulting in
suboptimal performance, in particular under linear probing. We present REVE
(Representation for EEG with Versatile Embeddings), a pretrained model
explicitly designed to generalize across diverse EEG signals. REVE introduces a
novel 4D positional encoding scheme that enables it to process signals of
arbitrary length and electrode arrangement. Using a masked autoencoding
objective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasets
spanning 25,000 subjects, representing the largest EEG pretraining effort to
date. REVE achieves state-of-the-art results on 10 downstream EEG tasks,
including motor imagery classification, seizure detection, sleep staging,
cognitive load estimation, and emotion recognition. With little to no
fine-tuning, it demonstrates strong generalization, and nuanced spatio-temporal
modeling. We release code, pretrained weights, and tutorials to support
standardized EEG research and accelerate progress in clinical neuroscience.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Code available at: https://brain-bzh.github.io/reve/</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>