<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning - Health AI Hub</title>
    <meta name="description" content="This paper presents an innovative offline speech decoding pipeline designed to directly regress acoustic speech from electrocorticographic (ECoG) signals. It le">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04618v1" target="_blank">2512.04618v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mohamed Baha Ben Ticha, Xingchen Ran, Guillaume Saldanha, Ga√´l Le Godais, Phil√©mon Roussel, Marc Aubert, Amina Fontanell, Thomas Costecalde, Lucas Struber, Serpil Karakas, Shaomin Zhang, Philippe Kahane, Guillaume Charvet, St√©phan Chabard√®s, Blaise Yvert
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04618v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04618v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper presents an innovative offline speech decoding pipeline designed to directly regress acoustic speech from electrocorticographic (ECoG) signals. It leverages an encoder-decoder deep neural architecture integrating Vision Transformers and contrastive learning to enhance decoding performance, particularly for surface ECoG recordings. A key contribution is the first reported attempt to decode speech from a fully implantable and wireless epidural recording system, opening avenues for long-term clinical applications in patients with severe paralysis.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant for developing advanced speech Brain-Computer Interfaces (BCIs) that can restore communication for individuals with severe paralysis. By demonstrating speech decoding from a fully implantable and wireless epidural system, it significantly advances the feasibility of less invasive, long-term neuroprosthetic solutions, improving patient autonomy and quality of life.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research applies advanced AI methodologies, specifically Vision Transformers and contrastive learning within a deep neural architecture, to enhance the direct regression of speech from electrocorticographic (ECoG) signals. The AI's application is to enable a medical device (Speech BCI) to reconstruct intelligible speech, thereby restoring communication for individuals with severe paralysis, a critical healthcare need.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of reconstructing speech from surface ECoG signals in a streaming mode, aiming to enhance direct regression of cortical signals into acoustic speech.</li>
                    
                    <li>Proposes an offline speech decoding pipeline built on an encoder-decoder deep neural architecture.</li>
                    
                    <li>Integrates Vision Transformers (ViT) and contrastive representation learning to optimize neural decoding from ECoG.</li>
                    
                    <li>Evaluated on two distinct ECoG datasets: one from clinical subdural electrodes in an epileptic patient, and another from a fully implantable WIMAGINE epidural system.</li>
                    
                    <li>Achieves the first known successful decoding of speech from a fully implantable and wireless epidural recording system.</li>
                    
                    <li>The use of a fully implantable wireless system is significant for potential long-term, chronic application outside of hospital settings.</li>
                    
                    <li>While currently an 'offline' system, it lays groundwork for developing more robust 'streaming' speech BCIs for individuals with severe paralysis.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employs an encoder-decoder deep neural architecture for speech decoding. The core innovation lies in the integration of Vision Transformers (ViT) for processing neural signals and the application of contrastive representation learning to optimize the mapping between ECoG signals and acoustic speech features. This architecture enables direct regression from cortical signals to speech, evaluated offline on two distinct human ECoG datasets (subdural and epidural recordings).</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The developed pipeline successfully decodes speech from ECoG signals, demonstrating the effectiveness of combining Vision Transformers and contrastive learning for this task. Crucially, the research reports the first successful attempt to decode speech from a fully implantable and wireless epidural recording system (WIMAGINE), indicating significant progress toward less invasive and long-duration BCI applications.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has the potential to profoundly impact the clinical management of communication disorders in severely paralyzed patients. The successful demonstration of speech decoding from a fully implantable and wireless epidural system suggests a future where BCIs are less invasive, require fewer surgical procedures, and can be used continuously for extended periods, providing more natural and accessible communication solutions outside of acute care settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The current speech decoding pipeline operates in an 'offline' mode, meaning it processes recorded data rather than providing real-time output. The abstract also implies that while promising, further work is still needed to achieve results with surface ECoG that are fully 'comparable' to those obtained with intracortical data, which often provide higher signal quality.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions include transitioning from offline to streaming speech reconstruction to enable real-time communication. The success with the fully implantable wireless epidural system strongly suggests further development towards long-term, chronic clinical use. Additionally, ongoing optimization of neural decoders for surface ECoG will likely focus on achieving performance parity with more invasive intracortical methods.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Neuroprosthetics</span>
                    
                    <span class="tag">Rehabilitation Medicine</span>
                    
                    <span class="tag">Neurosurgery</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Neural Decoding</span>
                    
                    <span class="tag tag-keyword">Speech BCI</span>
                    
                    <span class="tag tag-keyword">ECoG</span>
                    
                    <span class="tag tag-keyword">Vision Transformers</span>
                    
                    <span class="tag tag-keyword">Contrastive Learning</span>
                    
                    <span class="tag tag-keyword">Epidural</span>
                    
                    <span class="tag tag-keyword">Paralysis</span>
                    
                    <span class="tag tag-keyword">Communication</span>
                    
                    <span class="tag tag-keyword">Wireless Implantable</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>