<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning - Health AI Hub</title>
    <meta name="description" content="This paper introduces an offline speech decoding pipeline leveraging an encoder-decoder deep neural architecture, Vision Transformers, and contrastive learning ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04618v1" target="_blank">2512.04618v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mohamed Baha Ben Ticha, Xingchen Ran, Guillaume Saldanha, Ga√´l Le Godais, Phil√©mon Roussel, Marc Aubert, Amina Fontanell, Thomas Costecalde, Lucas Struber, Serpil Karakas, Shaomin Zhang, Philippe Kahane, Guillaume Charvet, St√©phan Chabard√®s, Blaise Yvert
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04618v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04618v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces an offline speech decoding pipeline leveraging an encoder-decoder deep neural architecture, Vision Transformers, and contrastive learning to directly regress acoustic speech from ECoG signals. It evaluates this approach on clinical subdural and, notably, a fully implantable and wireless epidural ECoG system (WIMAGINE), representing a significant step towards long-term, practical speech BCIs for paralyzed individuals.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is critical for developing advanced Brain-Computer Interfaces that can restore communication for individuals with severe paralysis, enabling them to directly synthesize speech from their brain signals and significantly improving their quality of life and independence.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development of advanced neural decoders for Speech Brain-Computer Interfaces (BCIs). Specifically, it uses Vision Transformers and contrastive representation learning to directly regress acoustic speech from ECoG signals. This aims to restore communication for individuals with severe paralysis, representing a significant medical AI application in assistive technology and neurological rehabilitation.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the current challenge of directly reconstructing streaming acoustic speech from ECoG signals, moving beyond phoneme/word prediction.</li>
                    
                    <li>Proposes an offline speech decoding pipeline built upon an encoder-decoder deep neural architecture.</li>
                    
                    <li>Integrates advanced machine learning techniques: Vision Transformers and contrastive representation learning to enhance direct regression.</li>
                    
                    <li>Evaluated on two distinct ECoG datasets: one from clinical subdural electrodes in an epileptic patient, and another from the fully implantable WIMAGINE epidural system in a motor BCI participant.</li>
                    
                    <li>Presents the first known attempt to decode speech from a fully implantable and wireless epidural recording system (WIMAGINE).</li>
                    
                    <li>This novel approach utilizing epidural and wireless technology offers significant perspectives for less invasive, long-term Brain-Computer Interface applications.</li>
                    
                    <li>Optimizing neural decoders is highlighted as critical for achieving robust speech decoding from surface ECoG.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employs an offline speech decoding pipeline utilizing an encoder-decoder deep neural architecture. It integrates Vision Transformers and contrastive representation learning to achieve direct regression of ECoG signals into acoustic speech. The approach was evaluated using two distinct datasets: one obtained from clinical subdural ECoG in an epileptic patient, and another from the fully implantable, wireless WIMAGINE epidural system in a motor BCI trial participant.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The paper successfully developed and demonstrated a novel deep neural network architecture for directly decoding acoustic speech from ECoG signals. Critically, it represents the first reported attempt to achieve speech decoding from a fully implantable and wireless epidural recording system (WIMAGINE), showcasing its feasibility and opening avenues for less invasive, long-term BCI solutions.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work significantly advances the feasibility of practical speech BCIs by demonstrating decoding from a fully implantable, wireless, epidural system. This approach promises less invasive surgeries, reduced infection risk, and long-term stability, making speech restoration more accessible and sustainable for individuals with severe communication impairments due to paralysis.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The current implementation is an "offline" decoding pipeline, indicating that real-time streaming speech reconstruction is not yet achieved. While progress is made with ECoG, further work is explicitly stated as needed to obtain results comparable to those achieved with intracortical data. The evaluation is based on a limited number of datasets (one epileptic patient, one motor BCI participant).</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should focus on optimizing neural decoders to achieve results with surface ECoG comparable to those with intracortical recordings, transitioning from offline processing to real-time streaming speech synthesis, and further developing the long-term utility and robustness of fully implantable, wireless epidural systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Neurorehabilitation</span>
                    
                    <span class="tag">Brain-Computer Interfaces</span>
                    
                    <span class="tag">Epileptology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Speech BCI</span>
                    
                    <span class="tag tag-keyword">ECoG</span>
                    
                    <span class="tag tag-keyword">Neural Decoding</span>
                    
                    <span class="tag tag-keyword">Vision Transformers</span>
                    
                    <span class="tag tag-keyword">Contrastive Learning</span>
                    
                    <span class="tag tag-keyword">Wireless Implant</span>
                    
                    <span class="tag tag-keyword">Epidural Electrodes</span>
                    
                    <span class="tag tag-keyword">Paralysis</span>
                    
                    <span class="tag tag-keyword">Communication</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>