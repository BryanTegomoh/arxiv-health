<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning - Health AI Hub</title>
    <meta name="description" content="This paper presents an offline speech decoding pipeline that utilizes an encoder-decoder deep neural architecture, integrating Vision Transformers and contrasti">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.04618v1" target="_blank">2512.04618v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mohamed Baha Ben Ticha, Xingchen Ran, Guillaume Saldanha, Ga√´l Le Godais, Phil√©mon Roussel, Marc Aubert, Amina Fontanell, Thomas Costecalde, Lucas Struber, Serpil Karakas, Shaomin Zhang, Philippe Kahane, Guillaume Charvet, St√©phan Chabard√®s, Blaise Yvert
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.04618v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.04618v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper presents an offline speech decoding pipeline that utilizes an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning, to directly regress ECoG signals into acoustic speech. The study addresses the challenge of reconstructing speech from surface ECoG and notably demonstrates the first attempt to decode speech from a fully implantable, wireless epidural recording system, offering significant potential for long-term Brain-Computer Interface (BCI) applications in communication.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is profoundly relevant to medicine by advancing Speech Brain-Computer Interfaces (BCIs), which offer a critical communication pathway for individuals with severe paralysis. By demonstrating speech decoding from a fully implantable, wireless epidural system, it paves the way for less invasive and more sustainable long-term neuroprosthetic solutions, significantly improving patient autonomy and quality of life.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI (Vision Transformers, contrastive learning) is applied to decode overt speech directly from brain signals (ECoG) with the goal of developing an assistive communication device for individuals with severe paralysis. This is a direct medical AI application aimed at improving patient quality of life and communication capabilities.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The research tackles the challenge of directly reconstructing acoustic speech in a streaming mode from cortical signals, specifically focusing on surface electrocorticography (ECoG).</li>
                    
                    <li>It proposes an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, which is critical for optimizing neural decoders for ECoG data.</li>
                    
                    <li>The methodology integrates Vision Transformers (ViT) and contrastive representation learning to enhance the direct regression of ECoG signals into acoustic speech.</li>
                    
                    <li>The approach was evaluated on two distinct datasets: ECoG from clinical subdural electrodes in an epileptic patient and data from the fully implantable WIMAGINE epidural system in a motor BCI participant.</li>
                    
                    <li>A significant contribution is the reported first attempt to decode speech using data acquired from a fully implantable and wireless epidural recording system.</li>
                    
                    <li>The use of an epidural system is less invasive than subdural or intracortical implants, offering a pathway for more accessible and durable long-term BCI solutions.</li>
                    
                    <li>The work aims to advance speech BCIs for individuals with severe paralysis, enabling them to communicate by translating neural activity directly into speech.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study implements an offline speech decoding pipeline utilizing an encoder-decoder deep neural architecture. This architecture incorporates Vision Transformers (ViT) for processing the complex patterns within ECoG signals and employs contrastive representation learning to optimize the mapping and direct regression of these neural signals into acoustic speech. The pipeline was validated using ECoG data from two clinical sources: subdural electrodes in an epileptic patient and the WIMAGINE fully implantable epidural system.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The central finding is the successful application of an advanced neural decoding pipeline, leveraging Vision Transformers and contrastive learning, to directly regress ECoG signals into acoustic speech. Crucially, this study marks the first reported instance of decoding speech from a fully implantable and wireless epidural recording system, demonstrating the feasibility and potential of this less invasive BCI platform for speech reconstruction.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact of this research is substantial, accelerating the development of Speech BCIs for patients suffering from severe paralysis. The successful demonstration with a fully implantable, wireless epidural system represents a significant step towards enabling durable, less invasive, and user-friendly communication solutions, potentially transforming the lives of individuals who are currently unable to speak by providing them with a means of naturalistic communication.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The current pipeline operates in an 'offline' mode, indicating that real-time, streaming speech reconstruction ‚Äì a key challenge for practical BCIs ‚Äì has not yet been achieved. The abstract also notes that further work is necessary to obtain comparable decoding results with surface ECoG recordings relative to those previously achieved with more invasive intracortical data.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The successful demonstration with a fully implantable and wireless epidural system clearly points towards future research focusing on optimizing these less invasive systems for chronic, long-term speech BCI applications. The next steps will likely involve transitioning from offline processing to real-time, streaming speech reconstruction to achieve practical, functional communication for users.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Neurology</span>
                    
                    <span class="tag">Neurorehabilitation</span>
                    
                    <span class="tag">Brain-Computer Interfaces (BCI)</span>
                    
                    <span class="tag">Neuroscience</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Neural Decoding</span>
                    
                    <span class="tag tag-keyword">Speech BCI</span>
                    
                    <span class="tag tag-keyword">ECoG</span>
                    
                    <span class="tag tag-keyword">Vision Transformers</span>
                    
                    <span class="tag tag-keyword">Contrastive Learning</span>
                    
                    <span class="tag tag-keyword">Epidural Electrodes</span>
                    
                    <span class="tag tag-keyword">Paralysis</span>
                    
                    <span class="tag tag-keyword">Communication</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>