<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications - Health AI Hub</title>
    <meta name="description" content="This paper introduces SciTrust 2.0, a comprehensive framework designed to evaluate the trustworthiness of Large Language Models (LLMs) in scientific application">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.25908v1" target="_blank">2510.25908v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-29
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Emily Herron, Junqi Yin, Feiyi Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.25908v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.25908v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces SciTrust 2.0, a comprehensive framework designed to evaluate the trustworthiness of Large Language Models (LLMs) in scientific applications across four key dimensions: truthfulness, adversarial robustness, scientific safety, and scientific ethics. The evaluation of seven prominent LLMs revealed that general-purpose industry models significantly outperformed science-specialized models, which exhibited critical deficiencies in logical reasoning, ethical understanding, and safety, particularly in high-risk scientific domains like biosecurity.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>The findings are highly relevant to medicine as LLMs are increasingly deployed in high-stakes clinical and biomedical research settings. The identified trustworthiness gaps, especially in science-specialized models regarding truthfulness, safety (e.g., biosecurity concerns), and ethics, could lead to erroneous diagnoses, unsafe treatment recommendations, or unethical research proposals, directly impacting patient safety and public health.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper provides a framework for evaluating the trustworthiness of Large Language Models (LLMs) in scientific applications. This is critical for the safe and ethical deployment of medical AI applications across various domains, including drug discovery, genomics, diagnostic support, and medical literature synthesis. The identified vulnerabilities of LLMs in high-risk areas like biosecurity directly impact the reliability and safety considerations for any AI system used in medical or public health contexts, especially where biological agents, chemicals, or sensitive research are involved. The framework's focus on safety and ethics directly informs the development of trustworthy AI for health and medicine.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Introduces SciTrust 2.0, a comprehensive framework for evaluating LLM trustworthiness in scientific applications.</li>
                    
                    <li>The framework assesses LLMs across four critical dimensions: truthfulness, adversarial robustness, scientific safety, and scientific ethics.</li>
                    
                    <li>Developed novel, open-ended truthfulness benchmarks using a verified reflection-tuning pipeline and expert validation.</li>
                    
                    <li>Created a novel ethics benchmark tailored for scientific research contexts, covering eight subcategories including dual-use research and bias.</li>
                    
                    <li>Evaluated seven LLMs (four science-specialized, three general-purpose industry models) using accuracy, semantic similarity, and LLM-based scoring.</li>
                    
                    <li>General-purpose industry models, notably GPT-o4-mini, consistently outperformed science-specialized models across all trustworthiness dimensions.</li>
                    
                    <li>Science-specialized models demonstrated significant deficiencies in logical/ethical reasoning and concerning vulnerabilities in high-risk safety domains (e.g., biosecurity, chemical weapons).</li>
                    
                    <li>The framework is open-sourced to foster the development of more trustworthy AI systems and advance research on model safety and ethics in scientific contexts.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study introduced SciTrust 2.0, an evaluation framework spanning truthfulness, adversarial robustness, scientific safety, and scientific ethics. It developed novel open-ended truthfulness benchmarks via a reflection-tuning pipeline and expert validation, and a scientific ethics benchmark with eight subcategories (e.g., dual-use research, bias). Seven LLMs (four science-specialized, three general-purpose industry models) were evaluated using multiple metrics including accuracy, semantic similarity measures, and LLM-based scoring.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>General-purpose industry LLMs consistently outperformed science-specialized models across all dimensions of trustworthiness. GPT-o4-mini showed superior performance in truthfulness and adversarial robustness. Science-specialized models exhibited significant deficiencies in logical and ethical reasoning, and critical vulnerabilities in safety evaluations, particularly concerning high-risk areas such as biosecurity and chemical weapons research.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings underscore the critical need for rigorous trustworthiness evaluation of LLMs, particularly those marketed as 'science-specialized,' before their integration into medical applications. Unaddressed deficiencies in safety, ethics, and truthfulness could lead to severe consequences in clinical decision-making, drug development, or public health interventions, necessitating careful validation and potentially favoring robust general-purpose models for sensitive tasks, or significant improvements in specialized models.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the study's methodology or scope. However, implications arise from the specific set of LLMs evaluated and the benchmarks developed, suggesting that performance may vary with different models or evaluation tasks.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors plan to open-source the SciTrust 2.0 framework, aiming to provide a foundational tool for developing more trustworthy AI systems. This will also facilitate further research into model safety and ethics specifically within scientific and high-stakes medical contexts.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Drug Discovery</span>
                    
                    <span class="tag">Biomedical Research</span>
                    
                    <span class="tag">Public Health Informatics</span>
                    
                    <span class="tag">Medical Ethics</span>
                    
                    <span class="tag">Pathogen Genomics</span>
                    
                    <span class="tag">Toxicology</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLMs</span>
                    
                    <span class="tag tag-keyword">Trustworthiness</span>
                    
                    <span class="tag tag-keyword">Scientific Applications</span>
                    
                    <span class="tag tag-keyword">Truthfulness</span>
                    
                    <span class="tag tag-keyword">Adversarial Robustness</span>
                    
                    <span class="tag tag-keyword">Scientific Safety</span>
                    
                    <span class="tag tag-keyword">Scientific Ethics</span>
                    
                    <span class="tag tag-keyword">Biosecurity</span>
                    
                    <span class="tag tag-keyword">Benchmarking</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Preprint Submitted to ACM Transactions on AI for Science (TAIS)</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>