<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications - Health AI Hub</title>
    <meta name="description" content="SciTrust 2.0 introduces a comprehensive framework to evaluate the trustworthiness of Large Language Models (LLMs) in scientific applications, spanning truthfuln">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.25908v1" target="_blank">2510.25908v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-29
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Emily Herron, Junqi Yin, Feiyi Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.25908v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.25908v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">SciTrust 2.0 introduces a comprehensive framework to evaluate the trustworthiness of Large Language Models (LLMs) in scientific applications, spanning truthfulness, adversarial robustness, scientific safety, and ethics. The study surprisingly found that general-purpose industry LLMs, particularly GPT-o4-mini, consistently outperformed science-specialized models across all dimensions, while the latter exhibited significant deficiencies in logical and ethical reasoning and alarming vulnerabilities in high-risk domains like biosecurity.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>The trustworthiness of LLMs is paramount in medical and health research, where high-stakes decisions impact patient care, public health, and biosecurity. Deficiencies in truthfulness, ethical reasoning, and safety, especially concerning biosecurity and dual-use research, directly pose risks to medical advancements and clinical applications.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research evaluates a framework (SciTrust 2.0) for assessing the trustworthiness of LLMs. This framework would be directly applied to any LLM-based AI system intended for use in medical and health-related scientific applications. Examples include AI for drug discovery, diagnostic support, personalized medicine, public health surveillance, clinical decision support systems, and tools assisting in biodefense or pathogen research. The framework ensures these AI applications are truthful, robust against attacks, safe in their scientific outputs (especially in high-risk biological or chemical contexts), and ethically sound (addressing bias, dual-use concerns).</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>SciTrust 2.0 is a novel, comprehensive framework designed to evaluate LLM trustworthiness in scientific contexts across four critical dimensions: truthfulness, adversarial robustness, scientific safety, and scientific ethics.</li>
                    
                    <li>The framework incorporates novel, open-ended truthfulness benchmarks developed through a verified reflection-tuning pipeline and expert validation, along with a new ethics benchmark covering eight subcategories, including dual-use research and bias.</li>
                    
                    <li>Evaluation was conducted on seven prominent LLMs, comprising four science-specialized models and three general-purpose industry models, using metrics such as accuracy, semantic similarity, and LLM-based scoring.</li>
                    
                    <li>General-purpose industry models, like GPT-o4-mini, demonstrated superior overall performance across all trustworthiness dimensions compared to science-specialized models.</li>
                    
                    <li>Science-specialized models exhibited notable deficiencies in logical and ethical reasoning capabilities, alongside concerning safety vulnerabilities, particularly in high-risk areas such as biosecurity and chemical weapons research.</li>
                    
                    <li>The findings highlight a critical need for enhanced safety and ethical considerations in science-specialized LLMs, challenging the assumption that domain-specific models inherently offer greater trustworthiness.</li>
                    
                    <li>The framework is open-sourced to provide a foundation for developing more trustworthy AI systems and to foster further research on model safety and ethics within scientific applications.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The SciTrust 2.0 framework evaluates LLMs across four dimensions (truthfulness, adversarial robustness, scientific safety, scientific ethics). Novel, open-ended truthfulness benchmarks were created via a reflection-tuning pipeline and expert validation. A new scientific ethics benchmark covering eight subcategories (e.g., dual-use research, bias) was also developed. Seven LLMs (four science-specialized, three general-purpose) were evaluated using multiple metrics including accuracy, semantic similarity measures, and LLM-based scoring.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>General-purpose industry LLMs consistently outperformed science-specialized models across all trustworthiness dimensions, with GPT-o4-mini excelling in truthfulness and adversarial robustness. Science-specialized models showed significant weaknesses in logical and ethical reasoning and concerning safety vulnerabilities, particularly in high-risk domains like biosecurity and chemical weapons research.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>These findings are critical for the responsible deployment of LLMs in medical contexts. If science-specialized models designed for tasks like drug discovery, diagnostics, or clinical guideline generation exhibit poor ethical reasoning or safety vulnerabilities (e.g., generating harmful biological information), they could directly endanger patient safety, propagate medical misinformation, or facilitate misuse in biosecurity. The superior performance of general-purpose models suggests they might offer more reliable foundations for medical AI, but rigorous evaluation using frameworks like SciTrust 2.0 remains essential before any clinical integration.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly detail specific limitations of the SciTrust 2.0 study itself, such as the scope of scientific domains covered, the specific types of 'science-specialized' models, or potential biases in the expert validation or LLM-based scoring methods. It primarily focuses on the limitations of LLMs regarding their trustworthiness in scientific applications.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors intend to open-source the SciTrust 2.0 framework to facilitate the development of more trustworthy AI systems. This will also advance research on model safety and ethics, particularly within sensitive scientific and medical contexts, encouraging community collaboration on robust evaluation and improvement of LLMs.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Biomedical Research</span>
                    
                    <span class="tag">Pharmacology</span>
                    
                    <span class="tag">Toxicology</span>
                    
                    <span class="tag">Public Health Preparedness</span>
                    
                    <span class="tag">Medical Ethics</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Drug Discovery</span>
                    
                    <span class="tag">Biosecurity</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">LLM trustworthiness</span>
                    
                    <span class="tag tag-keyword">Scientific AI</span>
                    
                    <span class="tag tag-keyword">Truthfulness</span>
                    
                    <span class="tag tag-keyword">Adversarial robustness</span>
                    
                    <span class="tag tag-keyword">Scientific safety</span>
                    
                    <span class="tag tag-keyword">Biosecurity</span>
                    
                    <span class="tag tag-keyword">Medical ethics</span>
                    
                    <span class="tag tag-keyword">AI evaluation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Preprint Submitted to ACM Transactions on AI for Science (TAIS)</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>