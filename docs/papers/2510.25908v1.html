<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications - Health AI Hub</title>
    <meta name="description" content="This paper introduces SciTrust 2.0, a comprehensive framework for evaluating the trustworthiness of Large Language Models (LLMs) in scientific applications acro">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.25908v1" target="_blank">2510.25908v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-29
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Emily Herron, Junqi Yin, Feiyi Wang
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.25908v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.25908v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces SciTrust 2.0, a comprehensive framework for evaluating the trustworthiness of Large Language Models (LLMs) in scientific applications across four dimensions: truthfulness, adversarial robustness, scientific safety, and scientific ethics. The evaluation of seven prominent LLMs revealed that general-purpose industry models generally outperformed science-specialized models, with the latter demonstrating significant deficiencies, particularly concerning safety in high-risk areas like biosecurity and chemical weapons.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework's focus on high-stakes scientific applications is critically relevant to medicine and health, as LLMs are increasingly deployed in drug discovery, clinical research, diagnostics, and public health, where trustworthiness is paramount. Identifying safety vulnerabilities, particularly in biosecurity, directly impacts medical defense and public health preparedness against biological and chemical threats.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This framework provides a critical tool for evaluating and improving the trustworthiness, safety, and ethical performance of LLMs intended for various medical and health-related applications. These applications could include drug discovery, biomedical research analysis, clinical decision support systems, pandemic preparedness, biosecurity threat assessment, and generating health information, ensuring the AI systems are truthful, robust, safe from generating harmful content (e.g., related to bioweapons or toxins), and free from harmful biases.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>SciTrust 2.0 is a novel, comprehensive framework designed to evaluate LLM trustworthiness specifically for scientific applications.</li>
                    
                    <li>The framework assesses LLMs across four critical dimensions: truthfulness, adversarial robustness, scientific safety, and scientific ethics.</li>
                    
                    <li>It incorporates novel, open-ended truthfulness benchmarks developed through a verified reflection-tuning pipeline and expert validation.</li>
                    
                    <li>A novel ethics benchmark tailored for scientific research contexts, covering eight subcategories including dual-use research and bias, was also developed.</li>
                    
                    <li>Seven prominent LLMs (four science-specialized, three general-purpose industry models) were evaluated using diverse metrics like accuracy, semantic similarity, and LLM-based scoring.</li>
                    
                    <li>General-purpose industry models consistently outperformed science-specialized models across all trustworthiness dimensions, with GPT-o4-mini showing superior performance in truthfulness and adversarial robustness.</li>
                    
                    <li>Science-specialized models exhibited significant deficiencies in logical and ethical reasoning capabilities and concerning vulnerabilities in safety evaluations, particularly in high-risk domains such as biosecurity and chemical weapons.</li>
                    
                    <li>The open-sourcing of SciTrust 2.0 aims to provide a foundation for developing more trustworthy AI systems and advancing research in model safety and ethics for scientific contexts.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study introduced SciTrust 2.0, a framework evaluating LLM trustworthiness across four dimensions: truthfulness, adversarial robustness, scientific safety, and scientific ethics. It developed novel, open-ended truthfulness benchmarks via a reflection-tuning pipeline and expert validation, alongside a novel scientific ethics benchmark with eight subcategories (e.g., dual-use research, bias). Seven LLMs (four science-specialized, three general-purpose) were evaluated using multiple metrics including accuracy, semantic similarity measures, and LLM-based scoring.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>General-purpose industry LLMs consistently outperformed science-specialized models across all four trustworthiness dimensions. GPT-o4-mini demonstrated superior performance in truthfulness assessments and adversarial robustness. Science-specialized models showed significant deficiencies in logical and ethical reasoning capabilities, alongside concerning vulnerabilities in safety evaluations, particularly when handling high-risk domains such as biosecurity and chemical weapons.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>Ensuring the trustworthiness of LLMs through frameworks like SciTrust 2.0 is essential for their safe and ethical integration into medical applications, preventing erroneous diagnostics, biased treatment recommendations, or unsafe drug development. Mitigating identified safety vulnerabilities, especially concerning biosecurity and chemical threats, directly impacts medical preparedness, public health security, and the responsible use of AI in sensitive research.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The study identified significant deficiencies in current science-specialized LLMs regarding logical and ethical reasoning, indicating they are not yet sufficiently reliable for complex scientific decision-making. Furthermore, concerning vulnerabilities in safety evaluations, particularly in high-risk domains like biosecurity and chemical weapons, highlight the current risks of deploying these models in critical medical and public health contexts without rigorous oversight.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The open-sourcing of the SciTrust 2.0 framework is intended to provide a foundation for developing more trustworthy AI systems. Future research should focus on advancing model safety and ethics specifically within scientific contexts, addressing the identified vulnerabilities to ensure responsible LLM deployment in high-stakes applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Drug Discovery</span>
                    
                    <span class="tag">Clinical Research</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Biodefense</span>
                    
                    <span class="tag">Biosecurity</span>
                    
                    <span class="tag">Toxicology</span>
                    
                    <span class="tag">Medical Ethics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Trustworthiness</span>
                    
                    <span class="tag tag-keyword">Scientific Applications</span>
                    
                    <span class="tag tag-keyword">AI Safety</span>
                    
                    <span class="tag tag-keyword">AI Ethics</span>
                    
                    <span class="tag tag-keyword">Biosecurity</span>
                    
                    <span class="tag tag-keyword">Adversarial Robustness</span>
                    
                    <span class="tag tag-keyword">Truthfulness</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Preprint Submitted to ACM Transactions on AI for Science (TAIS)</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>