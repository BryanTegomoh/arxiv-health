<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies - Health AI Hub</title>
    <meta name="description" content="This paper introduces a novel proof-of-concept for artificial palpation using a self-supervised learning framework, specifically an encoder-decoder model, to le">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.16596v1" target="_blank">2511.16596v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zohar Rimon, Elisei Shafer, Tal Tepper, Efrat Shimron, Aviv Tamar
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.16596v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.16596v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a novel proof-of-concept for artificial palpation using a self-supervised learning framework, specifically an encoder-decoder model, to learn a rich representation of tactile measurements on soft bodies. The learned representation, designed to capture intricate patterns beyond simple force maps, is demonstrated to be effective for downstream tasks such as tactile imaging and change detection, moving toward more objective and automated medical examinations.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medicine as it aims to automate and standardize palpation, a fundamental diagnostic technique, potentially leading to earlier and more consistent detection of abnormalities and improved surgical precision.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research focuses on developing an AI-powered robotic system that can perform artificial palpation. The AI, specifically using representation learning, processes tactile sensor data collected by a robot from soft bodies. This system aims to provide enhanced diagnostic capabilities for medical professionals by automating tactile imaging and change detection, potentially aiding in early detection of abnormalities or monitoring conditions that traditionally rely on human touch.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Proposes an artificial palpation method leveraging self-supervised representation learning via an encoder-decoder framework.</li>
                    
                    <li>The core idea is to learn a comprehensive representation from a sequence of tactile measurements, capturing intricate patterns beyond simple force maps.</li>
                    
                    <li>Aims to use this learned representation for critical downstream medical tasks like tactile imaging and detecting changes within palpated objects.</li>
                    
                    <li>Validation conducted using both a developed simulation environment and a real-world dataset, including MRI ground truth images of soft objects.</li>
                    
                    <li>Utilized a robot equipped with a tactile sensor to collect palpation sequences on physical objects.</li>
                    
                    <li>The model predicts sensory readings at various object positions, and the learned representation's utility is demonstrated for imaging and change detection.</li>
                    
                    <li>Challenges the current state-of-the-art in artificial palpation, which primarily relies on simple force mapping, by advocating for a more sophisticated data-driven approach.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves an encoder-decoder self-supervised learning architecture trained on sequences of tactile measurements. Data collection was performed using a robotic arm equipped with a tactile sensor on soft objects. Ground truth data for real-world validation was obtained via Magnetic Resonance Imaging (MRI). A simulation environment was also developed for initial validation. The model was trained to predict sensory readings, and its learned representation was then evaluated for tactile imaging and change detection capabilities.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study demonstrated that an encoder-decoder framework can successfully learn a meaningful representation from tactile measurements. This learned representation was shown to be effective and useful for generating tactile images and detecting subtle changes within soft objects, surpassing the capabilities of simple force mapping methods.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The clinical impact could be transformative, enabling objective, standardized, and potentially automated palpation for various medical examinations. This could lead to earlier disease detection (e.g., small tumors or tissue abnormalities), assist surgeons in distinguishing tissue types, improve diagnostic accuracy, and provide quantitative data where currently only qualitative human assessment exists.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>As a 'proof of concept,' the study indicates that significant further development is needed. The abstract implies that robust performance relies on 'enough training data,' suggesting current datasets may not be exhaustive. It also points to the 'current state of the art' being simple force maps, implying the complexity of 'intricate patterns' is a challenge that needs extensive data and model training.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future work would involve scaling up data collection to provide 'enough training data' to capture more intricate tactile patterns. Further investigation into the generalization capabilities of the learned representation across diverse pathologies and tissue types is also implied. Expanding the scope to include additional downstream tasks beyond imaging and change detection, and transitioning from proof-of-concept to robust clinical application, are also critical.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Diagnostic Radiology</span>
                    
                    <span class="tag">Oncology (tumor detection)</span>
                    
                    <span class="tag">Surgery (tissue characterization)</span>
                    
                    <span class="tag">Physical Examination</span>
                    
                    <span class="tag">Rehabilitation Medicine</span>
                    
                    <span class="tag">Pathology (tissue analysis)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Artificial Palpation</span>
                    
                    <span class="tag tag-keyword">Representation Learning</span>
                    
                    <span class="tag tag-keyword">Self-Supervised Learning</span>
                    
                    <span class="tag tag-keyword">Tactile Sensing</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Change Detection</span>
                    
                    <span class="tag tag-keyword">Soft Robotics</span>
                    
                    <span class="tag tag-keyword">Medical Diagnostics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Palpation, the use of touch in medical examination, is almost exclusively performed by humans. We investigate a proof of concept for an artificial palpation method based on self-supervised learning. Our key idea is that an encoder-decoder framework can learn a $\textit{representation}$ from a sequence of tactile measurements that contains all the relevant information about the palpated object. We conjecture that such a representation can be used for downstream tasks such as tactile imaging and change detection. With enough training data, it should capture intricate patterns in the tactile measurements that go beyond a simple map of forces -- the current state of the art. To validate our approach, we both develop a simulation environment and collect a real-world dataset of soft objects and corresponding ground truth images obtained by magnetic resonance imaging (MRI). We collect palpation sequences using a robot equipped with a tactile sensor, and train a model that predicts sensory readings at different positions on the object. We investigate the representation learned in this process, and demonstrate its use in imaging and change detection.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>