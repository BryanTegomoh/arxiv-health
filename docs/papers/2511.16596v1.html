<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies - Health AI Hub</title>
    <meta name="description" content="This paper introduces a proof-of-concept for artificial palpation using a self-supervised learning approach, specifically an encoder-decoder framework, to learn">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.16596v1" target="_blank">2511.16596v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zohar Rimon, Elisei Shafer, Tal Tepper, Efrat Shimron, Aviv Tamar
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.16596v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.16596v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a proof-of-concept for artificial palpation using a self-supervised learning approach, specifically an encoder-decoder framework, to learn a rich representation from tactile measurements on soft bodies. This learned representation is demonstrated to be effective for downstream tasks such as tactile imaging and detecting internal changes, validated through both simulation and real-world robotic palpation with MRI ground truth.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research provides a foundational step toward automating and standardizing medical diagnostic palpation, potentially leading to more objective, repeatable, and precise examinations for early disease detection, tissue characterization, and surgical guidance.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research applies AI (representation learning, self-supervised learning) to enable robotic systems to perform artificial palpation. The goal is to develop capabilities for tactile imaging and change detection in soft bodies, directly translating to medical applications like automated or assisted diagnosis of abnormalities (e.g., tumors, lesions) in biological tissues, enhancing diagnostic accuracy, and potentially standardizing or enabling remote medical examinations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Proposes an artificial palpation method using self-supervised representation learning to overcome the current human-centric nature of medical palpation.</li>
                    
                    <li>Utilizes an encoder-decoder framework to learn a comprehensive 'representation' from sequences of tactile measurements, aiming to capture intricate patterns beyond simple force maps.</li>
                    
                    <li>The learned representation is conjectured and demonstrated to be effective for downstream tasks, specifically tactile imaging and detecting changes within soft objects.</li>
                    
                    <li>Validation involves both the development of a custom simulation environment and the collection of a real-world dataset using a robot equipped with a tactile sensor on soft objects, with corresponding ground truth images obtained via MRI.</li>
                    
                    <li>A model was trained in a self-supervised manner to predict sensory readings at different positions on the object, facilitating the learning of the underlying representation.</li>
                    
                    <li>The study successfully investigated the learned representation and showcased its practical application in performing tactile imaging (reconstructing internal features) and identifying changes in palpated bodies.</li>
                    
                    <li>The approach aims to move beyond the current state-of-the-art in tactile sensing, which often relies on simpler force mapping, by capturing more intricate patterns indicative of internal object properties.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology centers on an encoder-decoder framework employing self-supervised learning. A robot equipped with a tactile sensor collects sequences of tactile measurements from soft bodies. This process is performed in both a custom simulation environment and a real-world setup, where real soft objects are correlated with MRI ground truth images. The model is trained to predict sensory readings at various object positions, from which a latent representation is learned and subsequently evaluated for tactile imaging and change detection tasks.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The core finding is the successful demonstration that a self-supervised encoder-decoder framework can learn a meaningful, high-information representation from tactile sequences. This representation is then effectively utilized for practical downstream applications such as reconstructing tactile images of internal structures and accurately detecting subtle changes within soft, palpated objects, surpassing simple force mapping capabilities.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technology has the potential to revolutionize diagnostic processes by enabling automated, quantitative, and highly sensitive palpation, which could facilitate earlier and more accurate tumor detection, provide precise intraoperative guidance (e.g., identifying tissue abnormalities or boundaries during surgery), and offer objective assessment of tissue properties, thereby reducing subjectivity inherent in human medical examination.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract frames the work as a 'proof of concept,' implying that the current scope is foundational and likely limited in terms of real-world complexity, scale of data, diversity of pathological conditions, and generalizability across varied patient anatomies. Specific performance metrics or comprehensive clinical validation are not detailed.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>While not explicitly stated, the 'proof of concept' nature implies future work will involve scaling up data collection to larger and more diverse datasets, testing on a wider range of complex soft tissues and pathologies, enhancing the robustness and generalizability of the learned representations, integrating with existing medical workflows, and ultimately moving towards rigorous clinical trials for validation and practical deployment.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Diagnostic imaging</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Surgery</span>
                    
                    <span class="tag">Internal medicine</span>
                    
                    <span class="tag">Rehabilitation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Artificial palpation</span>
                    
                    <span class="tag tag-keyword">Tactile sensing</span>
                    
                    <span class="tag tag-keyword">Representation learning</span>
                    
                    <span class="tag tag-keyword">Self-supervised learning</span>
                    
                    <span class="tag tag-keyword">Medical imaging</span>
                    
                    <span class="tag tag-keyword">Change detection</span>
                    
                    <span class="tag tag-keyword">Soft robotics</span>
                    
                    <span class="tag tag-keyword">Diagnostic tools</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Palpation, the use of touch in medical examination, is almost exclusively performed by humans. We investigate a proof of concept for an artificial palpation method based on self-supervised learning. Our key idea is that an encoder-decoder framework can learn a $\textit{representation}$ from a sequence of tactile measurements that contains all the relevant information about the palpated object. We conjecture that such a representation can be used for downstream tasks such as tactile imaging and change detection. With enough training data, it should capture intricate patterns in the tactile measurements that go beyond a simple map of forces -- the current state of the art. To validate our approach, we both develop a simulation environment and collect a real-world dataset of soft objects and corresponding ground truth images obtained by magnetic resonance imaging (MRI). We collect palpation sequences using a robot equipped with a tactile sensor, and train a model that predicts sensory readings at different positions on the object. We investigate the representation learned in this process, and demonstrate its use in imaging and change detection.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>