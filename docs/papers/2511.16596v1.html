<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies - Health AI Hub</title>
    <meta name="description" content="This paper introduces a proof-of-concept for artificial palpation using a self-supervised learning framework to learn a rich representation of touch on soft bod">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.16596v1" target="_blank">2511.16596v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zohar Rimon, Elisei Shafer, Tal Tepper, Efrat Shimron, Aviv Tamar
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.16596v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.16596v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a proof-of-concept for artificial palpation using a self-supervised learning framework to learn a rich representation of touch on soft bodies. An encoder-decoder model processes sequences of tactile measurements, aiming to capture intricate patterns beyond simple force maps for improved diagnostic tasks. The approach is validated through a simulation environment and a real-world dataset combining robotic palpation with MRI ground truth, demonstrating its potential for tactile imaging and change detection.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for advancing medical diagnostics by enabling robots to perform highly sensitive palpation, potentially leading to earlier and more consistent detection of anomalies, tumors, or inflammation that are currently reliant on subjective human touch.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research contributes to the development of AI-powered robotic systems for medical diagnosis. Specifically, it focuses on 'artificial palpation' to enable robots to detect subtle changes and create internal images of soft tissues using touch, mimicking and potentially augmenting human diagnostic capabilities. This could lead to AI-assisted diagnosis in clinics, robotic surgery with enhanced tactile feedback, and early disease detection.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Proposes an artificial palpation method based on self-supervised representation learning, moving beyond traditional force mapping.</li>
                    
                    <li>Utilizes an encoder-decoder framework to learn a comprehensive representation from sequences of tactile measurements on soft objects.</li>
                    
                    <li>Conjectures that this learned representation is suitable for downstream tasks like tactile imaging and detecting changes in the palpated object.</li>
                    
                    <li>Developed both a simulation environment and collected a real-world dataset using a robot with a tactile sensor to palpate soft objects.</li>
                    
                    <li>Ground truth for real-world data was obtained through Magnetic Resonance Imaging (MRI) to provide accurate internal object structures.</li>
                    
                    <li>Trained a model to predict sensory readings at various positions, demonstrating the utility of the learned representation in imaging and change detection.</li>
                    
                    <li>Aims to capture intricate tactile patterns that are currently almost exclusively interpreted by human medical professionals.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves an encoder-decoder neural network framework trained in a self-supervised manner. It takes sequences of tactile measurements as input to learn a compressed representation, which is then used to predict sensory readings at different object positions. Validation includes a custom simulation environment and a real-world dataset collected using a robot equipped with a tactile sensor palpating soft objects, with ground truth provided by Magnetic Resonance Imaging (MRI).</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The study demonstrates that a self-supervised learning approach can effectively learn a meaningful representation of touch from palpation sequences. This learned representation is shown to be useful for tactile imaging and for detecting changes within soft objects, indicating the feasibility of artificial palpation for diagnostic tasks.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The development of artificial palpation could lead to objective, reproducible, and highly sensitive diagnostic tools, potentially improving early detection of abnormalities (e.g., tumors, cysts), reducing inter-examiner variability, and extending diagnostic capabilities to remote or underserved areas. It could also enhance robotic surgery by providing nuanced haptic feedback.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract frames the work as a "proof of concept," implying an early stage of development. The effectiveness is conjectured to rely on "enough training data," suggesting current limitations in data quantity or diversity. The complexity of "intricate patterns" captured compared to human expert palpation remains an area for further validation beyond simple force maps.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future work involves scaling up training data collection to capture more intricate tactile patterns, further refining the learned representations, and exploring the robustness and generalizability of the method across a wider range of soft body types and pathologies. Advancing the integration of these representations into real-world clinical diagnostic pipelines is also a key long-term direction.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">oncology</span>
                    
                    <span class="tag">surgery</span>
                    
                    <span class="tag">dermatology</span>
                    
                    <span class="tag">physical examination</span>
                    
                    <span class="tag">diagnostics</span>
                    
                    <span class="tag">rehabilitation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">artificial palpation</span>
                    
                    <span class="tag tag-keyword">representation learning</span>
                    
                    <span class="tag tag-keyword">self-supervised learning</span>
                    
                    <span class="tag tag-keyword">tactile sensing</span>
                    
                    <span class="tag tag-keyword">robotics</span>
                    
                    <span class="tag tag-keyword">medical imaging</span>
                    
                    <span class="tag tag-keyword">change detection</span>
                    
                    <span class="tag tag-keyword">soft bodies</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Palpation, the use of touch in medical examination, is almost exclusively performed by humans. We investigate a proof of concept for an artificial palpation method based on self-supervised learning. Our key idea is that an encoder-decoder framework can learn a $\textit{representation}$ from a sequence of tactile measurements that contains all the relevant information about the palpated object. We conjecture that such a representation can be used for downstream tasks such as tactile imaging and change detection. With enough training data, it should capture intricate patterns in the tactile measurements that go beyond a simple map of forces -- the current state of the art. To validate our approach, we both develop a simulation environment and collect a real-world dataset of soft objects and corresponding ground truth images obtained by magnetic resonance imaging (MRI). We collect palpation sequences using a robot equipped with a tactile sensor, and train a model that predicts sensory readings at different positions on the object. We investigate the representation learned in this process, and demonstrate its use in imaging and change detection.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>