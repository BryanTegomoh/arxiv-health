<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data - Health AI Hub</title>
    <meta name="description" content="This study evaluates multiple language models, including LLMs (Llama, GPT) and transformer-based architectures (BERT, XLNet, Distil-RoBERTa), against classical ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.07044v1" target="_blank">2511.07044v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-10
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Mihael Arcan, David-Paul Niland
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.07044v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.07044v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This study evaluates multiple language models, including LLMs (Llama, GPT) and transformer-based architectures (BERT, XLNet, Distil-RoBERTa), against classical machine learning for detecting anxiety, depression, and stress from clinical interview text. Using the DAIC-WOZ dataset and synthetic data augmentation, the research demonstrates that transformer models achieve high F1 scores, with Distil-RoBERTa excelling in anxiety detection and XLNet in depression. The findings highlight the significant potential of advanced language models combined with synthetic data to improve automated mental health assessment, enhancing recall and generalization.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant for addressing the global burden of mental health disorders by advancing accurate and automated early detection from textual data. By identifying effective methods for recognizing subtle symptom expressions, it offers a scalable approach to enhance screening, facilitate timely interventions, and support clinical decision-making.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development and evaluation of Large Language Models and transformer-based architectures for automated detection and classification of anxiety, depression, and stress from textual data, serving as a tool for mental health assessment and screening.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The study systematically compares LLMs (Llama, GPT), classical machine learning, and transformer-based models (BERT, XLNet, Distil-RoBERTa) for mental health disorder detection.</li>
                    
                    <li>The DAIC-WOZ dataset, comprising clinical interview transcripts, was used for fine-tuning models for anxiety (GAD-2), depression (PHQ), and stress classification.</li>
                    
                    <li>Synthetic data generation was applied to mitigate class imbalance, proving effective in improving recall and generalization for mental health detection.</li>
                    
                    <li>Distil-RoBERTa achieved the highest F1 score of 0.883 for Generalized Anxiety Disorder (GAD-2) detection.</li>
                    
                    <li>XLNet demonstrated superior performance on Patient Health Questionnaire (PHQ) tasks for depression detection, reaching an F1 score of up to 0.891.</li>
                    
                    <li>A zero-shot synthetic approach (SD+Zero-Shot-Basic) achieved an F1 of 0.884 and ROC AUC of 0.886 for stress detection.</li>
                    
                    <li>The research emphasizes that careful calibration is crucial when utilizing synthetic data to prevent a loss in precision while improving other metrics.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study involved fine-tuning various language models, including LLMs (Llama, GPT) and transformer-based architectures (BERT, XLNet, Distil-RoBERTa), on the DAIC-WOZ dataset, which consists of clinical interview transcripts. Synthetic data generation was employed as a data augmentation strategy to mitigate class imbalance for anxiety (GAD-2), depression (PHQ), and stress classification tasks. Model performance was evaluated using metrics such as F1 score and ROC AUC.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Transformer-based models proved highly effective, with Distil-RoBERTa achieving the highest F1 score (0.883) for GAD-2 detection and XLNet outperforming others on PHQ tasks (F1 up to 0.891). A zero-shot synthetic approach (SD+Zero-Shot-Basic) demonstrated strong performance for stress detection (F1 0.884, ROC AUC 0.886). The research also found that synthetic data significantly improved model recall and generalization, though careful calibration is necessary to prevent precision loss.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The findings suggest a strong potential for developing more accurate and efficient automated tools for mental health assessment from patient-generated text or clinical notes. This could enable earlier and more widespread detection of anxiety, depression, and stress, allowing clinicians to identify at-risk individuals more rapidly and facilitate timely therapeutic interventions, thereby improving patient outcomes and potentially alleviating healthcare system strain.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>A notable limitation highlighted is the requirement for careful calibration when using synthetic data to prevent precision loss, indicating a trade-off that needs optimization. Additionally, the study relies on a specific dataset (DAIC-WOZ), suggesting that the generalizability of these findings to diverse patient populations or other forms of textual data may require further validation.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research should focus on optimizing the combination of advanced language models and data augmentation techniques, including exploring more sophisticated synthetic data generation methods and refining calibration strategies to balance precision and recall. Further validation across broader, more diverse datasets and in real-world clinical settings will be crucial for translating these findings into practical applications for automated mental health assessment.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Psychiatry</span>
                    
                    <span class="tag">Clinical Psychology</span>
                    
                    <span class="tag">Mental Health Informatics</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Mental Health Detection</span>
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Transformers</span>
                    
                    <span class="tag tag-keyword">Anxiety</span>
                    
                    <span class="tag tag-keyword">Depression</span>
                    
                    <span class="tag tag-keyword">Stress</span>
                    
                    <span class="tag tag-keyword">Synthetic Data</span>
                    
                    <span class="tag tag-keyword">Automated Assessment</span>
                    
                    <span class="tag tag-keyword">Natural Language Processing</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Mental health disorders affect over one-fifth of adults globally, yet
detecting such conditions from text remains challenging due to the subtle and
varied nature of symptom expression. This study evaluates multiple approaches
for mental health detection, comparing Large Language Models (LLMs) such as
Llama and GPT with classical machine learning and transformer-based
architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ
dataset of clinical interviews, we fine-tuned models for anxiety, depression,
and stress classification and applied synthetic data generation to mitigate
class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score
(0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to
0.891). For stress detection, a zero-shot synthetic approach
(SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings
demonstrate the effectiveness of transformer-based models and highlight the
value of synthetic data in improving recall and generalization. However,
careful calibration is required to prevent precision loss. Overall, this work
emphasizes the potential of combining advanced language models and data
augmentation to enhance automated mental health assessment from text.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>