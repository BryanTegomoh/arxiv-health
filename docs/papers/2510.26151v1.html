<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction - Health AI Hub</title>
    <meta name="description" content="This paper introduces MV-MLM, a novel Multi-View Mammography and Language Model designed for breast cancer diagnosis and risk prediction. MV-MLM leverages multi">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26151v1" target="_blank">2510.26151v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26151v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26151v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MV-MLM, a novel Multi-View Mammography and Language Model designed for breast cancer diagnosis and risk prediction. MV-MLM leverages multi-view supervision and cross-modal self-supervision using paired mammograms and synthetic radiology reports, achieving state-of-the-art performance in malignancy, subtype, and risk classification tasks with strong data efficiency.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly significant for advancing breast cancer detection and risk assessment by providing a data-efficient and robust AI model, potentially leading to earlier diagnoses, more personalized patient management, and reduced reliance on costly manual annotations in clinical settings.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application involves using Vision-Language Models (VLMs) to analyze multi-view mammography images alongside associated (synthetic) radiology reports. This MV-MLM aims to develop robust and data-efficient CAD models that can automatically classify breast cancer malignancy and subtypes, distinguish specific cancer characteristics (e.g., calcification, mass), and predict an individual's risk of developing breast cancer, thereby assisting radiologists and clinicians in early detection and patient management.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of expensive and time-consuming fine-detailed annotations for robust Computer-Aided Diagnosis (CAD) models in breast cancer.</li>
                    
                    <li>Proposes MV-MLM, a Multi-View Mammography and Language Model, trained on a unique dataset of paired mammogram images and *synthetic* radiology reports.</li>
                    
                    <li>Utilizes multi-view supervision and cross-modal self-supervision across image-text pairs, including multiple mammography views and corresponding pseudo-radiology reports, to learn rich representations.</li>
                    
                    <li>Employs a novel joint visual-textual learning strategy to enhance generalization and accuracy in distinguishing breast tissues and cancer characteristics like calcification and mass.</li>
                    
                    <li>Achieves state-of-the-art performance in three critical classification tasks: malignancy classification, breast cancer subtype classification, and image-based cancer risk prediction.</li>
                    
                    <li>Demonstrates strong data efficiency, outperforming existing fully supervised or VLM baselines, notably without requiring actual radiology reports for training.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The MV-MLM model integrates multi-view mammography images with language understanding through a Vision-Language Model architecture. It is trained using multi-view supervision and cross-modal self-supervision on a custom dataset of mammogram images paired with *synthetic* radiology reports. A novel joint visual-textual learning strategy is applied to learn rich representations that can discern specific breast tissue and cancer characteristics (e.g., calcification, mass) and to improve generalization across various diagnostic tasks.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The MV-MLM model achieved state-of-the-art performance across three crucial breast cancer tasks: malignancy classification, subtype classification, and image-based cancer risk prediction. A standout finding is its robust data efficiency, as it outperformed existing fully supervised or VLM baselines while being trained exclusively on synthetic text reports, eliminating the need for real radiology reports.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This model has the potential to significantly enhance Computer-Aided Diagnosis (CAD) systems in breast cancer screening and diagnosis, offering more accurate and efficient tools for radiologists. Its ability to operate effectively with synthetic reports could streamline AI model development and deployment by reducing the need for costly and time-consuming manual annotation by expert radiologists, making advanced diagnostic tools more accessible and scalable in clinical practice.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed MV-MLM model, highlighting the use of synthetic reports as a strength rather than a constraint.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Future research directions are not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Preventive Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Breast Cancer</span>
                    
                    <span class="tag tag-keyword">Mammography</span>
                    
                    <span class="tag tag-keyword">Vision-Language Model (VLM)</span>
                    
                    <span class="tag tag-keyword">Multi-View Learning</span>
                    
                    <span class="tag tag-keyword">Self-Supervision</span>
                    
                    <span class="tag tag-keyword">Risk Prediction</span>
                    
                    <span class="tag tag-keyword">Computer-Aided Diagnosis (CAD)</span>
                    
                    <span class="tag tag-keyword">Synthetic Reports</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)
  Workshop at ICCV 2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>