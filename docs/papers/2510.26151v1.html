<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction - Health AI Hub</title>
    <meta name="description" content="This paper introduces MV-MLM, a novel Multi-View Mammography and Language Model, designed to enhance breast cancer diagnosis and risk prediction. By utilizing m">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26151v1" target="_blank">2510.26151v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26151v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26151v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MV-MLM, a novel Multi-View Mammography and Language Model, designed to enhance breast cancer diagnosis and risk prediction. By utilizing multi-view mammograms paired with synthetic radiology reports, and employing cross-modal self-supervision and joint visual-textual learning, MV-MLM achieves state-of-the-art performance. The model demonstrates high data efficiency across malignancy classification, subtype classification, and image-based cancer risk prediction without requiring actual expert-annotated reports.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant as it addresses the critical challenge of developing robust and data-efficient Computer-Aided Diagnosis (CAD) models for early breast cancer detection and accurate risk stratification, which can significantly improve patient outcomes and resource utilization in radiology.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is a Computer-Aided Diagnosis (CAD) system for breast cancer. The MV-MLM model aims to enhance the accuracy and efficiency of breast cancer detection, classification of cancer characteristics (e.g., malignancy, subtype, calcification, mass), and prediction of individual cancer risk from mammogram images. It leverages vision-language models to learn from multi-view mammography and even synthetic radiology reports, potentially reducing the need for costly manual annotations and improving generalization.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Novel MV-MLM Architecture:** Introduces a Multi-View Mammography and Language Model specifically for breast cancer diagnosis and risk prediction.</li>
                    
                    <li>**Synthetic Data Training:** Trained on multi-view mammogram images paired with *synthetic* radiology reports, mitigating the cost and time of acquiring detailed expert annotations.</li>
                    
                    <li>**Multi-View & Cross-Modal Self-Supervision:** Leverages multi-view mammography supervision and cross-modal self-supervision between images (multiple views) and their corresponding pseudo-radiology reports to learn rich representations.</li>
                    
                    <li>**Joint Visual-Textual Learning:** Proposes a novel learning strategy that combines visual and textual information to improve generalization and accuracy across various data types and tasks, distinguishing cancer characteristics like calcification and mass.</li>
                    
                    <li>**State-of-the-Art Performance:** Achieves SOTA results in three critical classification tasks: malignancy classification, subtype classification, and image-based cancer risk prediction.</li>
                    
                    <li>**High Data Efficiency:** Demonstrates superior data efficiency compared to existing fully supervised or VLM baselines, performing well despite relying solely on synthetic text reports.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The paper proposes MV-MLM, a multi-view mammography and language model. It is trained on a dataset comprising multi-view mammogram images combined with synthetically generated radiology reports. The core methodology involves leveraging multi-view supervision and employing cross-modal self-supervision across the paired image-text data. A novel joint visual-textual learning strategy is utilized to enhance generalization and accuracy, allowing the model to distinguish breast tissues and specific cancer characteristics (e.g., calcification, mass) for understanding images and predicting cancer risk.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The MV-MLM model achieved state-of-the-art performance in three critical classification tasks: (1) malignancy classification, (2) subtype classification, and (3) image-based cancer risk prediction. A significant finding is its strong data efficiency, outperforming existing fully supervised and VLM baselines, even when trained exclusively on synthetic radiology reports and without the need for actual, often scarce, expert-annotated reports.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This model has the potential to significantly enhance the accuracy and efficiency of breast cancer screening and diagnosis. By providing highly accurate tools for malignancy, subtype, and risk classification, it could aid radiologists in making more informed decisions, potentially leading to earlier detection, more tailored treatment plans, and reduced workload. Its reliance on synthetic data for training makes it a more accessible and scalable solution for clinical deployment, especially in settings with limited access to extensive annotated datasets.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the proposed MV-MLM model or the study.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly suggest future research directions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">mammography</span>
                    
                    <span class="tag tag-keyword">breast cancer</span>
                    
                    <span class="tag tag-keyword">vision-language model</span>
                    
                    <span class="tag tag-keyword">multi-view</span>
                    
                    <span class="tag tag-keyword">self-supervision</span>
                    
                    <span class="tag tag-keyword">risk prediction</span>
                    
                    <span class="tag tag-keyword">synthetic reports</span>
                    
                    <span class="tag tag-keyword">CAD</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)
  Workshop at ICCV 2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>