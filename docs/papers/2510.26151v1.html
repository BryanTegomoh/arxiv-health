<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction - Health AI Hub</title>
    <meta name="description" content="This paper introduces MV-MLM, a novel Multi-View Mammography and Language Model, designed for breast cancer diagnosis and risk prediction. It addresses the chal">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.26151v1" target="_blank">2510.26151v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-30
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.26151v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.26151v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces MV-MLM, a novel Multi-View Mammography and Language Model, designed for breast cancer diagnosis and risk prediction. It addresses the challenge of limited finely annotated medical datasets by leveraging multi-view mammography images paired with synthetic radiology reports, employing cross-modal self-supervision. The model achieves state-of-the-art performance across malignancy classification, subtype classification, and image-based cancer risk prediction, demonstrating strong data efficiency without requiring actual radiology reports.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant for improving breast cancer detection and risk assessment by providing robust, data-efficient Computer-Aided Diagnosis (CAD) models. It offers a practical solution to the scarcity of finely annotated medical imaging datasets, potentially accelerating the development and deployment of AI in clinical settings for enhanced patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is a novel Multi-View Mammography and Language Model (MV-MLM) designed to serve as a Computer-Aided Diagnosis (CAD) tool for breast cancer. It processes multi-view mammography images and synthetic radiology reports to classify malignancy, determine cancer subtypes, and predict an individual's risk of breast cancer. This system aims to enhance the accuracy and efficiency of breast cancer screening and diagnosis, potentially assisting radiologists and clinicians in clinical decision-making.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the scarcity and cost of fine-detailed annotated datasets for breast cancer CAD by leveraging synthetic radiology reports.</li>
                    
                    <li>Introduces MV-MLM, a novel Multi-View Mammography and Language Model, employing multi-view and cross-modal self-supervision across paired mammogram images and pseudo-radiology reports.</li>
                    
                    <li>Proposes a joint visual-textual learning strategy specifically designed to learn representations distinguishing breast tissues and cancer characteristics (e.g., calcification, mass).</li>
                    
                    <li>Achieves state-of-the-art (SOTA) performance across three critical clinical tasks: malignancy classification, breast cancer subtype classification, and image-based cancer risk prediction.</li>
                    
                    <li>Demonstrates superior data efficiency compared to existing fully supervised and VLM baselines.</li>
                    
                    <li>Successfully trained and evaluated using only synthetic text reports, eliminating the dependence on actual, often unavailable or costly, radiology reports.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The MV-MLM model is trained on a dataset of paired multi-view mammogram images and synthetic radiology reports. It leverages multi-view supervision and cross-modal self-supervision across these image-text pairs. A novel joint visual-textual learning strategy is proposed to learn rich representations, enabling the model to distinguish breast tissues and specific cancer characteristics like calcification and mass, and subsequently utilize these patterns for mammography understanding and cancer risk prediction.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The MV-MLM model achieved state-of-the-art performance on both private and publicly available datasets across three critical classification tasks: malignancy classification, breast cancer subtype classification, and image-based cancer risk prediction. Crucially, it demonstrated strong data efficiency, outperforming existing fully supervised and VLM baselines while being trained exclusively on synthetic text reports, thereby obviating the need for actual, often unavailable or costly, radiology reports.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The proposed MV-MLM has the potential to significantly advance clinical breast cancer screening and diagnostics by offering a robust and data-efficient Computer-Aided Diagnosis (CAD) tool. It could assist radiologists in more accurately identifying malignancy, classifying cancer subtypes, and predicting patient risk, leading to earlier interventions, more tailored treatment plans, and ultimately improving patient outcomes, particularly in scenarios where comprehensive human-annotated datasets are scarce.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Oncology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Preventive Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Mammography</span>
                    
                    <span class="tag tag-keyword">Breast Cancer</span>
                    
                    <span class="tag tag-keyword">Vision-Language Models (VLMs)</span>
                    
                    <span class="tag tag-keyword">Computer-Aided Diagnosis (CAD)</span>
                    
                    <span class="tag tag-keyword">Risk Prediction</span>
                    
                    <span class="tag tag-keyword">Multi-view</span>
                    
                    <span class="tag tag-keyword">Self-supervision</span>
                    
                    <span class="tag tag-keyword">Synthetic Data</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)
  Workshop at ICCV 2025</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>