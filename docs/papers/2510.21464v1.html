<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray Diagnosis - Health AI Hub</title>
    <meta name="description" content="This paper introduces CXR-LanIC, a novel framework designed to enhance the interpretability of deep learning models for chest X-ray diagnosis, addressing the cr">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray Diagnosis</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.21464v1" target="_blank">2510.21464v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-24
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Yiming Tang, Wenjia Zhong, Rushi Shah, Dianbo Liu
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.21464v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.21464v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces CXR-LanIC, a novel framework designed to enhance the interpretability of deep learning models for chest X-ray diagnosis, addressing the critical 'black-box' problem. It achieves this by training transcoder-based sparse autoencoders on a diagnostically-trained BiomedCLIP classifier to discover approximately 5,000 clinically relevant, monosemantic visual patterns from MIMIC-CXR data. CXR-LanIC provides transparent attribution for predictions, decomposing them into these verifiable patterns, while maintaining competitive diagnostic accuracy on key findings.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for increasing clinician trust in AI-driven diagnoses, enabling the identification of potential AI failure modes, and ultimately facilitating the safer and more widespread adoption of deep learning models in real-world clinical settings for chest X-ray interpretation.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The paper describes an AI application designed to function as an interpretable diagnostic assistant for chest X-rays. It aims to provide clinicians with transparent, verifiable explanations for AI-generated diagnoses, thereby building trust and facilitating the safer and more effective integration of artificial intelligence into clinical practice and decision-making.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the 'black-box' nature of deep learning in chest X-ray diagnosis, which hinders clinical adoption despite high accuracy.</li>
                    
                    <li>Introduces CXR-LanIC (Language-Grounded Interpretable Classifier for Chest X-rays), a framework using task-aligned pattern discovery for interpretability.</li>
                    
                    <li>Employs transcoder-based sparse autoencoders, trained on a BiomedCLIP diagnostic classifier, to decompose medical image representations into interpretable visual patterns.</li>
                    
                    <li>An ensemble of 100 transcoders was trained on multimodal embeddings from the MIMIC-CXR dataset, discovering ~5,000 monosemantic patterns across categories like cardiac, pulmonary, pleural, structural, device, and artifact.</li>
                    
                    <li>Enables transparent prediction attribution, where diagnoses are decomposed into 20-50 interpretable patterns with verifiable activation galleries, demonstrating consistent activation behavior.</li>
                    
                    <li>Achieves competitive diagnostic accuracy on five key findings, proving that interpretability does not come at the cost of performance.</li>
                    
                    <li>Key innovation lies in extracting interpretable features from a classifier trained on *specific diagnostic objectives* rather than general-purpose embeddings, ensuring clinical relevance of discovered patterns.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves training transcoder-based sparse autoencoders on a pre-trained BiomedCLIP diagnostic classifier. An ensemble of 100 transcoders is utilized, processing multimodal embeddings derived from the MIMIC-CXR dataset. This process decomposes complex medical image representations into approximately 5,000 discrete, monosemantic visual patterns, ensuring they are task-aligned and directly relevant to diagnostic objectives.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>['Discovery of approximately 5,000 monosemantic visual patterns covering diverse radiological categories (cardiac, pulmonary, pleural, structural, device, artifact).', 'These patterns exhibit consistent activation behavior, allowing for transparent prediction attribution where diagnostic outputs are decomposed into 20-50 interpretable patterns with verifiable activation galleries.', 'CXR-LanIC achieves competitive diagnostic accuracy on five key findings, demonstrating that high performance can be maintained alongside strong interpretability.', 'The approach successfully extracts clinically relevant features by training on a diagnostic classifier rather than general embeddings, enhancing the utility of the discovered patterns.']</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>CXR-LanIC significantly improves the transparency and trustworthiness of AI in chest X-ray diagnosis, a critical step for clinical integration. Clinicians can verify AI predictions through interpretable patterns and activation galleries, aiding in identifying potential errors or edge cases. This fosters safer clinical deployment by providing clinically grounded explanations directly relevant to patient care and diagnostic decision-making, thereby accelerating the adoption of accurate AI tools in radiology departments.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations. However, it notes that natural language explanations are a 'planned' future direction, implying the current system primarily offers visual pattern-based interpretability rather than direct textual explanations.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The primary stated future direction is the generation of natural language explanations for the discovered patterns through planned annotation using large multimodal models, which would further enhance the clinical utility and user-friendliness of the system.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Pulmonology</span>
                    
                    <span class="tag">Cardiology</span>
                    
                    <span class="tag">Critical Care Medicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Chest X-ray</span>
                    
                    <span class="tag tag-keyword">Interpretability</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Medical Imaging AI</span>
                    
                    <span class="tag tag-keyword">Sparse Autoencoders</span>
                    
                    <span class="tag tag-keyword">BiomedCLIP</span>
                    
                    <span class="tag tag-keyword">Diagnostic Classifier</span>
                    
                    <span class="tag tag-keyword">MIMIC-CXR</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Deep learning models have achieved remarkable accuracy in chest X-ray
diagnosis, yet their widespread clinical adoption remains limited by the
black-box nature of their predictions. Clinicians require transparent,
verifiable explanations to trust automated diagnoses and identify potential
failure modes. We introduce CXR-LanIC (Language-Grounded Interpretable
Classifier for Chest X-rays), a novel framework that addresses this
interpretability challenge through task-aligned pattern discovery. Our approach
trains transcoder-based sparse autoencoders on a BiomedCLIP diagnostic
classifier to decompose medical image representations into interpretable visual
patterns. By training an ensemble of 100 transcoders on multimodal embeddings
from the MIMIC-CXR dataset, we discover approximately 5,000 monosemantic
patterns spanning cardiac, pulmonary, pleural, structural, device, and artifact
categories. Each pattern exhibits consistent activation behavior across images
sharing specific radiological features, enabling transparent attribution where
predictions decompose into 20-50 interpretable patterns with verifiable
activation galleries. CXR-LanIC achieves competitive diagnostic accuracy on
five key findings while providing the foundation for natural language
explanations through planned large multimodal model annotation. Our key
innovation lies in extracting interpretable features from a classifier trained
on specific diagnostic objectives rather than general-purpose embeddings,
ensuring discovered patterns are directly relevant to clinical decision-making,
demonstrating that medical AI systems can be both accurate and interpretable,
supporting safer clinical deployment through transparent, clinically grounded
explanations.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>