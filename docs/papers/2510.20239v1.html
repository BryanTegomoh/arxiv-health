<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders - Health AI Hub</title>
    <meta name="description" content="This paper presents a unified tri-modal affective severity framework for simultaneously diagnosing co-occurring depression (5 classes) and post-traumatic stress">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20239v1" target="_blank">2510.20239v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Filippo Cenacchi, Deborah Richards, Longbing Cao
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20239v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20239v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper presents a unified tri-modal affective severity framework for simultaneously diagnosing co-occurring depression (5 classes) and post-traumatic stress disorder (PTSD, 3 classes) using interview text, audio, and facial signals. The framework fuses standardized features via a calibrated late fusion classifier to output graded severities and feature-level attributions. It significantly outperforms unimodal baselines, improves robustness under noisy conditions, and offers explainable insights crucial for clinical decision-making.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to mental healthcare by providing an advanced, automated, and explainable diagnostic tool for complex cases of co-occurring depression and PTSD, enabling more precise severity assessment and personalized treatment planning for patients.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is a multi-modal machine learning system designed for the automated, severity-aware diagnosis and assessment of depression and PTSD. It fuses data from interview text (transformer embeddings), audio (log Mel statistics), and facial signals (action units, gaze, head, pose descriptors) to predict graded severities for these disorders. This system serves as a clinical decision support tool, providing per-disorder probabilities and feature-level attributions to aid clinicians in their diagnostic and assessment processes.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the clinical need for severity-aware, cross-disorder diagnosis of co-occurring depression and PTSD, moving beyond traditional binary and disorder-specific assessments.</li>
                    
                    <li>Proposes a tri-modal framework integrating sentence-level transformer embeddings from text, log Mel statistics with deltas from audio, and action units, gaze, head, and pose descriptors from facial signals.</li>
                    
                    <li>Outputs graded severities for depression (PHQ-8; 5 classes) and PTSD (3 classes) through a calibrated late fusion classifier, providing per-disorder probabilities and feature-level attributions.</li>
                    
                    <li>Evaluated on DAIC-derived corpora using stratified cross-validation, demonstrating superior performance over unimodal and ablation baselines, particularly in decision curve utility and robustness to noisy or missing modalities.</li>
                    
                    <li>Specifically for PTSD, the fusion model significantly reduces regression error and improves class concordance, reliably identifying extreme severity classes.</li>
                    
                    <li>Ablation studies show that text features are most critical for depression severity assessment, while audio and facial cues play a dominant role in PTSD diagnosis, with attributions aligning with established clinical markers.</li>
                    
                    <li>The approach supports reproducible evaluation and offers clinician-in-the-loop decision support, enhancing the utility of automated affective clinical assessments.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves a unified tri-modal affective severity framework. It extracts features from three modalities: interview text (sentence-level transformer embeddings), audio (log Mel statistics with deltas), and facial signals (action units, gaze, head, pose descriptors). These standardized features are then integrated using a calibrated late fusion classifier to predict graded severities for depression and PTSD. The model generates per-disorder probabilities and feature-level attributions. Evaluation is conducted via stratified cross-validation on DAIC-derived corpora, comparing performance against unimodal and ablation baselines, and analyzing robustness, decision curve utility, and error patterns.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>['The fused tri-modal model outperforms unimodal and ablation baselines across various metrics, demonstrating improved decision curve utility and enhanced robustness, particularly when modalities are noisy or missing.', 'It matches the strongest unimodal baseline in accuracy and weighted F1, while offering superior overall diagnostic capabilities.', 'For PTSD, the fusion approach specifically reduces regression error and improves class concordance, indicating more accurate severity estimation.', 'Extreme severity classes for both disorders are reliably identified, though errors tend to cluster between adjacent severity levels.', 'Ablation analyses reveal distinct contributions from modalities: text is most influential for depression severity, while audio and facial cues are critical for PTSD diagnosis.', 'Feature-level attributions provide explainability, aligning well with known linguistic and behavioral markers associated with depression and PTSD.']</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This approach offers a significant advancement in clinical decision support by providing clinicians with a robust, severity-aware, and explainable tool for concurrent depression and PTSD diagnosis. It enables more nuanced assessment than binary diagnoses, facilitating personalized treatment planning and interventions. The ability to identify modality-specific contributions and provide feature-level attributions fosters greater trust and integration of AI in clinician-in-the-loop diagnostic processes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>While extreme severity classes are identified reliably, the abstract notes that prediction errors tend to cluster between adjacent severity levels, indicating potential challenges in distinguishing very fine-grained differences in severity.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper suggests that its approach offers reproducible evaluation and supports clinician-in-the-loop decision-making. Future directions would likely involve further validation in diverse and larger clinical populations, exploring real-world deployment for integrated clinical support, and potentially refining the interpretability features to make them even more actionable for clinicians.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Psychiatry</span>
                    
                    <span class="tag">Clinical Psychology</span>
                    
                    <span class="tag">Mental Health Assessment</span>
                    
                    <span class="tag">Computational Psychiatry</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Depression</span>
                    
                    <span class="tag tag-keyword">PTSD</span>
                    
                    <span class="tag tag-keyword">Multimodal Fusion</span>
                    
                    <span class="tag tag-keyword">Severity Diagnosis</span>
                    
                    <span class="tag tag-keyword">Affective Computing</span>
                    
                    <span class="tag tag-keyword">Machine Learning</span>
                    
                    <span class="tag tag-keyword">Clinical Decision Support</span>
                    
                    <span class="tag tag-keyword">Transformer Embeddings</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>