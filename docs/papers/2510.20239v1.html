<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders - Health AI Hub</title>
    <meta name="description" content="This paper introduces a unified tri-modal framework for severity-aware, cross-disorder diagnosis of depression (5 classes) and PTSD (3 classes) by fusing interv">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">← Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20239v1" target="_blank">2510.20239v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Filippo Cenacchi, Deborah Richards, Longbing Cao
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20239v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20239v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a unified tri-modal framework for severity-aware, cross-disorder diagnosis of depression (5 classes) and PTSD (3 classes) by fusing interview text, audio, and facial signals. Utilizing advanced features like transformer embeddings and action units, the approach employs a calibrated late fusion classifier to output graded severities and feature-level attributions. The fused model demonstrates superior robustness and utility over unimodal baselines, particularly for PTSD, offering enhanced decision support for clinicians.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to mental healthcare by addressing the complex co-occurrence of depression and PTSD. It offers an objective, severity-aware, and interpretable automated diagnostic tool, which can significantly aid clinicians in precise assessment, tailored treatment planning, and monitoring of mental health conditions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is a 'tri-modal affective severity framework' that fuses interview text, audio, and facial signals to automatically output graded severities for diagnosing depression (PHQ-8; 5 classes) and PTSD (3 classes). It aims to provide a severity-aware, cross-disorder diagnostic estimate with feature-level attributions, serving as a 'clinician in the loop support' system for affective clinical decision making in mental health.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>**Tri-modal Fusion Framework:** Develops a unified system integrating interview text (transformer embeddings), audio (log Mel statistics with deltas), and facial signals (action units, gaze, head/pose descriptors).</li>
                    
                    <li>**Severity-Aware Cross-Disorder Diagnosis:** Aims to provide graded severity estimates for both depression (PHQ-8; 5 classes) and PTSD (3 classes) concurrently, moving beyond binary and disorder-specific assessments.</li>
                    
                    <li>**Calibrated Late Fusion & Attributions:** Standardized features are fused using a calibrated late fusion classifier, yielding per-disorder probabilities and crucial feature-level attributions for interpretability.</li>
                    
                    <li>**Enhanced Robustness and Utility:** The fused model, validated via stratified cross-validation on DAIC-derived corpora, outperforms unimodal/ablation baselines, improving decision curve utility and robustness under noisy or missing modalities.</li>
                    
                    <li>**Disorder-Specific Feature Importance:** Ablation studies show that text contributes most to depression severity, while audio and facial cues are critical for PTSD severity assessment.</li>
                    
                    <li>**Reliable Identification of Extreme Severities:** Errors primarily cluster between adjacent severity classes, indicating that extreme (e.g., severe or no-disorder) classes are identified reliably.</li>
                    
                    <li>**Clinician Decision Support:** The approach is designed to offer reproducible evaluation and clinician-in-the-loop support for affective clinical decision making through interpretable attributions and graded severities.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study proposes a unified tri-modal affective severity framework. It extracts features from interview text using sentence-level transformer embeddings, from audio using log Mel statistics with deltas, and from facial signals using action units, gaze, head, and pose descriptors. These standardized features are synchronized and fused via a calibrated late fusion classifier. The model outputs graded severities for depression (PHQ-8; 5 classes) and PTSD (3 classes), along with feature-level attributions. Evaluation is performed using stratified cross-validation on DAIC-derived corpora, comparing the fused model against unimodal and ablation baselines.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The tri-modal fused model significantly outperforms unimodal and ablation baselines, demonstrating improved decision curve utility and robustness when modalities are noisy or missing. For PTSD specifically, fusion reduces regression error and improves class concordance. The model reliably identifies extreme severity classes, with errors predominantly occurring between adjacent severities. Ablation studies reveal that text contributes most to depression severity, while audio and facial cues are critical for PTSD. The generated feature attributions align well with established linguistic and behavioral markers of the disorders.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This approach has the potential to significantly improve the accuracy and efficiency of diagnosing co-occurring depression and PTSD by providing objective, graded severity estimates. The interpretable feature-level attributions can empower clinicians with deeper insights into patient symptomatology, facilitating more precise and personalized treatment strategies. Its robustness to missing data makes it suitable for real-world clinical settings, potentially supporting remote mental health assessments and enabling 'clinician in the loop' decision-making for more effective and accessible patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned as future research directions in the abstract, but the framework is positioned to offer 'clinician in the loop support for affective clinical decision making', implying further integration and validation within actual clinical workflows.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Psychiatry</span>
                    
                    <span class="tag">Mental Health</span>
                    
                    <span class="tag">Clinical Psychology</span>
                    
                    <span class="tag">Telemedicine</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Depression</span>
                    
                    <span class="tag tag-keyword">PTSD</span>
                    
                    <span class="tag tag-keyword">Multi-modal fusion</span>
                    
                    <span class="tag tag-keyword">Severity assessment</span>
                    
                    <span class="tag tag-keyword">Machine learning</span>
                    
                    <span class="tag tag-keyword">Affective computing</span>
                    
                    <span class="tag tag-keyword">Clinical decision support</span>
                    
                    <span class="tag tag-keyword">Transformer embeddings</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">← Back to all papers</a></p>
    </footer>
</body>
</html>