<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders - Health AI Hub</title>
    <meta name="description" content="This paper introduces a unified tri-modal framework that synchronizes and fuses interview text, audio, and facial signals to diagnose and assess graded severity">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
            </nav>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2510.20239v1" target="_blank">2510.20239v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-10-23
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Filippo Cenacchi, Deborah Richards, Longbing Cao
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CL, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.98 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2510.20239v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2510.20239v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a unified tri-modal framework that synchronizes and fuses interview text, audio, and facial signals to diagnose and assess graded severity for co-occurring depression (PHQ-8, 5 classes) and PTSD (3 classes). The severity-aware approach outperforms unimodal baselines, enhances diagnostic utility and robustness, and provides explainable feature attributions, offering clinician decision support.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant as it advances automated diagnosis for complex mental health conditions, providing severity-aware, cross-disorder estimates and explainable insights. This can lead to more accurate, nuanced, and personalized clinical decision support for patients experiencing co-occurring depression and PTSD.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research describes an AI system that processes multi-modal data (interview text, audio, facial signals) to generate graded severity estimates and diagnoses for depression and PTSD. It is designed to function as a clinical decision support tool, offering automated assessment and explanations to assist clinicians in diagnosing and managing these mental health conditions.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the challenge of automated diagnosis for co-occurring depression and PTSD, moving beyond binary, disorder-specific assessments to graded severity estimates.</li>
                    
                    <li>Proposes a unified tri-modal affective severity framework integrating interview text (transformer embeddings), audio (log Mel statistics with deltas), and facial signals (action units, gaze, head, pose descriptors).</li>
                    
                    <li>Features are standardized and fused via a calibrated late fusion classifier, yielding per-disorder probabilities and feature-level attributions for explainability.</li>
                    
                    <li>Evaluated on DAIC-derived corpora using stratified cross-validation, the fused model significantly outperforms unimodal and ablation baselines in overall utility and robustness, especially under noisy or missing modalities.</li>
                    
                    <li>For PTSD specifically, the fusion approach reduces regression error and improves class concordance, reliably identifying extreme severity classes.</li>
                    
                    <li>Ablation studies and attributions reveal text as the primary contributor to depression severity, while audio and facial cues are critical for PTSD assessment.</li>
                    
                    <li>The framework offers reproducible evaluation and supports clinicians with explainable decision-making in affective clinical assessment.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employs a unified tri-modal affective severity framework that synchronizes and fuses features extracted from interview text (sentence-level transformer embeddings), audio (log Mel statistics with deltas), and facial signals (action units, gaze, head, and pose descriptors). Standardized features are then combined using a calibrated late fusion classifier to output graded severities (5 classes for PHQ-8 depression, 3 classes for PTSD), along with per-disorder probabilities and feature-level attributions. Evaluation is conducted using stratified cross-validation on DAIC-derived corpora.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The fused model outperforms unimodal and ablation baselines, matching the strongest unimodal baseline on accuracy and weighted F1, while significantly improving decision curve utility and robustness under noisy or missing modalities. For PTSD, fusion specifically reduces regression error and improves class concordance, with extreme severity classes identified reliably. Errors predominantly cluster between adjacent severity levels. Ablation studies indicate text contributes most to depression severity, whereas audio and facial cues are critical for PTSD.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The approach offers reproducible and explainable diagnostic support for clinicians, enabling more precise and severity-aware assessment of co-occurring depression and PTSD. This can improve the efficiency and accuracy of clinical decision-making, facilitate personalized treatment planning, and enhance patient management by providing clear insights into the contribution of different behavioral and linguistic markers.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the proposed method. However, the evaluation is based on "DAIC derived corpora," which might imply a limitation in generalizability to broader patient populations or different clinical contexts. The finding that "errors cluster between adjacent severities" suggests that fine-grained distinctions between very similar severity levels might still be challenging for the model, although extreme classes are identified reliably.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly propose future research directions. It highlights the immediate application of offering "clinician in the loop support for affective clinical decision making" as a key output and impact of the research.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Psychiatry</span>
                    
                    <span class="tag">Clinical Psychology</span>
                    
                    <span class="tag">Mental Health</span>
                    
                    <span class="tag">Affective Disorders</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Depression</span>
                    
                    <span class="tag tag-keyword">PTSD</span>
                    
                    <span class="tag tag-keyword">Multimodal Fusion</span>
                    
                    <span class="tag tag-keyword">Severity Assessment</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">Affective Computing</span>
                    
                    <span class="tag tag-keyword">Mental Health Diagnosis</span>
                    
                    <span class="tag tag-keyword">Clinical Decision Support</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>