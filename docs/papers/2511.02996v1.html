<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics - Health AI Hub</title>
    <meta name="description" content="SCALE-VLP introduces a novel soft-weighted contrastive vision-language pre-training framework specifically designed for volumetric medical data, like CT scans. ">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.02996v1" target="_blank">2511.02996v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-04
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Ailar Mahdizadeh, Puria Azadi Moghadam, Xiangteng He, Shahriar Mirabbasi, Panos Nasiopoulos, Leonid Sigal
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.02996v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.02996v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">SCALE-VLP introduces a novel soft-weighted contrastive vision-language pre-training framework specifically designed for volumetric medical data, like CT scans. By integrating volumetric spatial semantics and domain-aware, knowledge-infused semantics, it overcomes limitations of 2D-focused VLMs that often ignore crucial spatial coherence and structured dependencies. The framework demonstrates significant improvements across various tasks, achieving state-of-the-art performance in CT-report retrieval, abnormality classification, and report generation, while showcasing strong cross-task and zero-shot cross-domain generalizability.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly relevant to medical imaging by enabling AI models to better interpret complex 3D radiological data in conjunction with clinical text. By preserving spatial and domain-specific semantic information, SCALE-VLP promises more accurate and robust AI tools for diagnosis, prognosis, and treatment planning in radiology, directly addressing current limitations in medical VLM applications.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>SCALE-VLP proposes an AI model designed to enhance the understanding and processing of medical volumetric data (like CT scans). Its applications include AI-assisted diagnosis through abnormality classification in medical images, automated generation of radiology reports, and efficient retrieval of medical reports based on image content, thereby streamlining clinical workflows and potentially improving diagnostic accuracy and efficiency in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical gap of Vision-Language Models (VLMs) being primarily limited to 2D data and binary supervision, extending them to volumetric medical data (e.g., CT scans).</li>
                    
                    <li>Proposes a soft-weighted contrastive pre-training framework that explicitly models continuous and structured dependencies inherent in 3D medical images.</li>
                    
                    <li>Integrates two crucial semantic components: (i) volumetric spatial semantics to preserve anatomical structure, and (ii) domain-aware, knowledge-infused semantics (e.g., radiological ontologies) to guide precise vision-language alignment.</li>
                    
                    <li>Achieves up to 4.3x higher top-1 CT-report retrieval performance compared to the previous state-of-the-art methods.</li>
                    
                    <li>Demonstrates a significant 10-point improvement in abnormality classification accuracy.</li>
                    
                    <li>Excels in report generation tasks, reaching ROUGE-L 0.44 and BERT-F1 0.89.</li>
                    
                    <li>Proves strong cross-task transferability and cross-domain generalizability, showing consistent performance gains in zero-shot evaluation on out-of-domain external datasets without further fine-tuning.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>SCALE-VLP utilizes a soft-weighted contrastive vision-language pre-training framework. Its core innovation lies in simultaneously integrating volumetric spatial semantics to ensure anatomical consistency and domain-aware, knowledge-infused semantics (derived from sources like radiological ontologies) to guide the cross-modal alignment process. This approach moves beyond simplistic 2D slice processing and binary supervision to create structurally consistent and semantically grounded representations for 3D medical data.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>SCALE-VLP achieved up to 4.3x higher top-1 CT-report retrieval, improved abnormality classification by 10 points, and reached ROUGE-L 0.44 and BERT-F1 0.89 for report generation. Crucially, it demonstrated consistent gains in zero-shot evaluation on an out-of-domain external dataset, affirming its robust cross-task and cross-domain generalization capabilities without fine-tuning.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The enhanced performance of SCALE-VLP in tasks like CT-report retrieval, abnormality classification, and report generation has the potential to significantly improve the efficiency and accuracy of radiological workflows. It could assist radiologists in faster and more precise diagnoses, streamline report drafting, facilitate relevant case retrieval, and ultimately contribute to better patient outcomes by providing more reliable AI-driven insights from complex 3D medical images. Its generalizability across datasets is vital for real-world clinical deployment.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state limitations of the SCALE-VLP method itself. Instead, it frames the work as overcoming several key limitations of existing VLM approaches, such as their restriction to 2D data, reliance on binary supervision, and failure to preserve spatial coherence and leverage rich clinical semantics in volumetric data.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions. However, the demonstrated strong generalizability and performance suggest potential for expansion to other volumetric imaging modalities, integration of even richer and more diverse clinical knowledge bases, and exploration of its utility in new downstream tasks beyond retrieval, classification, and generation.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Clinical Decision Support Systems</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Volumetric Vision-Language Pre-training</span>
                    
                    <span class="tag tag-keyword">CT Scans</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Contrastive Learning</span>
                    
                    <span class="tag tag-keyword">Spatial Semantics</span>
                    
                    <span class="tag tag-keyword">Radiological Ontologies</span>
                    
                    <span class="tag tag-keyword">Report Generation</span>
                    
                    <span class="tag tag-keyword">Abnormality Classification</span>
                    
                    <span class="tag tag-keyword">Zero-shot Learning</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Vision-language models (VLMs) have demonstrated strong cross-modal
capabilities, yet most work remains limited to 2D data and assumes binary
supervision (i.e., positive vs. negative pairs), overlooking the continuous and
structured dependencies present in volumetric data such as CT. Existing
approaches often treat volumetric scans as independent 2D slices, compromising
spatial coherence and underutilizing rich clinical semantics. We propose
SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework
that integrates (i) volumetric spatial semantics to preserve anatomical
structure and (ii) domain-aware, knowledge-infused semantics (e.g.,
radiological ontologies) to guide alignment. This yields structurally
consistent and semantically grounded representations under limited supervision,
demonstrating strong cross-task transferability (retrieval, report generation,
and classification), and cross-domain generalizability with consistent gains
without further fine-tuning. In particular, compared to the previous state of
the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval,
improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and
BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an
out-of-domain external dataset, we observe consistent gains, indicating the
cross-task and cross-domain generalization ability of SCALE-VLP.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>