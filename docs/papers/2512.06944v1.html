<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Unifying Human-Centered AI Fairness Framework - Health AI Hub</title>
    <meta name="description" content="This paper introduces a unifying human-centered AI fairness framework designed to systematically integrate eight distinct fairness metrics, addressing the chall">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>A Unifying Human-Centered AI Fairness Framework</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2512.06944v1" target="_blank">2512.06944v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-12-07
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Munshi Mahbubur Rahman, Shimei Pan, James R. Foulds
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, cs.AI, cs.CY
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2512.06944v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2512.06944v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a unifying human-centered AI fairness framework designed to systematically integrate eight distinct fairness metrics, addressing the challenge of navigating trade-offs between competing fairness notions and predictive accuracy. The framework allows stakeholders to assign weights across multiple fairness objectives, facilitating the practical and value-sensitive deployment of fair AI systems in critical domains like healthcare.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This framework is highly relevant to medicine and health by offering a systematic method to ensure ethical and equitable application of AI in healthcare, mitigating biases in critical decision-making processes such as patient risk assessment, resource allocation, and diagnostic support across diverse populations, as demonstrated through its application to healthcare utilization data.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The framework develops methods for ensuring fairness in AI systems, which are then demonstrated in healthcare contexts. This includes ensuring equitable outcomes when AI is used for tasks like predicting healthcare utilization, assessing patient risk, or allocating medical resources, thereby mitigating bias related to race, gender, or socioeconomic status in healthcare AI applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical challenge of ensuring AI fairness by providing a structured approach to manage trade-offs between various fairness metrics and predictive accuracy in sensitive applications.</li>
                    
                    <li>Proposes a novel human-centered framework that systematically unifies eight distinct fairness metrics, derived from combinations of individual/group fairness, infra-marginal/intersectional assumptions, and outcome-based/equality-of-opportunity perspectives.</li>
                    
                    <li>The framework uses a consistent and easy-to-understand formulation for all metrics, which lowers the learning curve for non-experts and allows stakeholders to align interventions with their specific values and contextual considerations.</li>
                    
                    <li>Enables multi-stakeholder compromise by allowing users to assign customizable weights across multiple fairness objectives, reflecting diverse priorities rather than privileging a single notion of fairness.</li>
                    
                    <li>Demonstrated application across four real-world datasets, including the MEPS dataset for healthcare utilization, showing that adjusting fairness weights reveals nuanced trade-offs between different fairness metrics.</li>
                    
                    <li>Highlights practical utility through case studies in judicial decision-making and healthcare, illustrating how the framework can inform the value-sensitive deployment of fair AI systems.</li>
                    
                    <li>Aims to bridge the gap between theoretical fairness research and practical deployment by offering a flexible and comprehensive tool for mitigating unequal treatment across sensitive attributes in AI applications.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors developed a unifying framework that systematizes eight distinct fairness metrics by combining individual/group fairness, infra-marginal/intersectional assumptions, and outcome-based/equality-of-opportunity perspectives. They formulated these metrics with a consistent, easy-to-understand structure and enabled the assignment of customizable weights across multiple fairness objectives. The framework's utility was then evaluated by applying it to four real-world datasets, including the MEPS dataset for healthcare utilization, to demonstrate how weight adjustments reveal nuanced trade-offs, further supported by practical case studies in healthcare.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The framework successfully unifies diverse fairness metrics into a consistent and manageable formulation, allowing stakeholders to systematically explore and manage the inherent trade-offs between different fairness objectives and predictive accuracy. Adjusting weights across these objectives was shown to reveal nuanced compromises, underscoring the framework's effectiveness in aligning AI interventions with specific values and contextual considerations in domains like healthcare.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This framework can profoundly impact clinical practice by guiding the development and deployment of fair AI systems, ensuring equitable access to care, accurate diagnostic predictions, and unbiased treatment recommendations across all patient demographics. It provides a practical, multi-stakeholder tool for clinicians, policymakers, and AI developers to explicitly define, measure, and optimize for fairness in healthcare AI, thereby reducing health disparities and building trust in AI-driven medical decisions.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state specific limitations or caveats of the proposed framework or its application.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state specific future research directions, beyond its immediate goal of informing the practical and value-sensitive deployment of fair AI systems.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Healthcare Utilization</span>
                    
                    <span class="tag">Medical Diagnostics</span>
                    
                    <span class="tag">Treatment Planning</span>
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Health Equity</span>
                    
                    <span class="tag">Algorithmic Bias in Medicine</span>
                    
                    <span class="tag">Health Policy</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">AI fairness</span>
                    
                    <span class="tag tag-keyword">human-centered AI</span>
                    
                    <span class="tag tag-keyword">fairness metrics</span>
                    
                    <span class="tag tag-keyword">trade-offs</span>
                    
                    <span class="tag tag-keyword">multi-objective optimization</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">ethical AI</span>
                    
                    <span class="tag tag-keyword">bias mitigation</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>