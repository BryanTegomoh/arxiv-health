<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation - Health AI Hub</title>
    <meta name="description" content="This paper addresses the critical limitation of current multi-modal large language models (MLLMs) in medicine, which are largely confined to single-image unders">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.22232v1" target="_blank">2511.22232v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-27
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Zhen Chen, Yihang Fu, Gabriel Madera, Mauro Giuffre, Serina Applebaum, Hyunjae Kim, Hua Xu, Qingyu Chen
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI, cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 1.00 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.22232v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.22232v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper addresses the critical limitation of current multi-modal large language models (MLLMs) in medicine, which are largely confined to single-image understanding, by developing a framework for multi-image composite reasoning. It leverages license-permissive compound figures from biomedical literature to create M3LLM, a medical multi-image MLLM that significantly outperforms existing models and generalizes effectively to longitudinal chest X-ray analysis.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Medical diagnosis and monitoring frequently demand synthesizing information from multiple images across different modalities or time points. This work enables MLLMs to perform such composite reasoning, bridging a significant gap in AI's ability to support complex clinical workflows and improve patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>This research develops M3LLM, a medical multi-modal large language model designed to provide composite understanding from multiple medical images (different modalities or time points) to assist in complex medical diagnosis and progression monitoring. Its application includes advanced interpretation of medical imaging data, such as longitudinal chest X-ray analysis, thereby enhancing clinical decision support.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the critical gap in medical MLLMs by enabling multi-image understanding, crucial for comprehensive clinical diagnosis and progression monitoring.</li>
                    
                    <li>Proposes a novel data generation framework utilizing license-permissive compound figures from biomedical literature, overcoming the scarcity of high-quality, large-scale multi-image training data.</li>
                    
                    <li>Implements a five-stage, context-aware instruction generation paradigm based on a divide-and-conquer strategy to teach MLLMs complex spatial, temporal, and cross-modal relationships.</li>
                    
                    <li>Develops M3LLM, a medical multi-image multi-modal large language model, trained by parsing over 237,000 compound figures and their contextual text.</li>
                    
                    <li>Introduces PMC-MI-Bench, a new benchmark for composite understanding, which has been manually validated by medical experts.</li>
                    
                    <li>Demonstrates M3LLM's superior performance against both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios.</li>
                    
                    <li>Exhibits strong generalization capabilities, particularly in longitudinal chest X-ray analysis using the real-world MIMIC dataset, validating its practical applicability.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The methodology involves leveraging license-permissive compound images within biomedical literature as a primary data source for multi-image instruction generation. A five-stage, context-aware instruction generation paradigm, underpinned by a divide-and-conquer strategy, decomposes complex multi-image analysis into manageable sub-tasks. Over 237,000 such compound figures and their contextual text are parsed to train M3LLM. For evaluation, a new benchmark, PMC-MI-Bench, is constructed and manually validated by medical experts, alongside testing generalization on longitudinal chest X-ray data from MIMIC.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>M3LLM significantly outperforms both general-purpose and specialized medical MLLMs in multi-image, single-image, text-only, and multi-choice tasks. Crucially, it demonstrates strong generalization to real-world longitudinal chest X-ray analysis using the MIMIC dataset, indicating its robustness and clinical utility. The proposed framework establishes a scalable and efficient paradigm for developing MLLMs capable of composite reasoning.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work directly impacts clinical practice by enabling AI models to perform sophisticated multi-image analysis, essential for accurate medical diagnosis, monitoring disease progression across time, and integrating information from diverse imaging modalities. It offers a scalable pathway for developing AI tools that can provide a more holistic understanding of patient conditions, thereby enhancing clinical decision support and potentially improving patient outcomes.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the developed M3LLM or the proposed framework. It primarily focuses on addressing and overcoming limitations of existing single-image MLLMs.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning. This implies future directions towards broader application and integration into real-world clinical workflows, bridging the gap between biomedical literature-derived knowledge and practical clinical applications.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Clinical Decision Support</span>
                    
                    <span class="tag">Disease Progression Monitoring</span>
                    
                    <span class="tag">AI in Healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Multi-modal LLM</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                    <span class="tag tag-keyword">Compound Figures</span>
                    
                    <span class="tag tag-keyword">Composite Understanding</span>
                    
                    <span class="tag tag-keyword">Longitudinal Analysis</span>
                    
                    <span class="tag tag-keyword">Biomedical Literature</span>
                    
                    <span class="tag tag-keyword">Chest X-ray</span>
                    
                    <span class="tag tag-keyword">Clinical AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.</p>
            </section>

            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>