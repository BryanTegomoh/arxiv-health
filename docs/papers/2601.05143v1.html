<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering - Health AI Hub</title>
    <meta name="description" content="This paper introduces a lightweight and explainable vision-language framework designed for visual question answering (VQA) concerning crop disease identificatio">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.05143v1" target="_blank">2601.05143v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-08
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Siam Ansary
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.80 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.05143v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.05143v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper introduces a lightweight and explainable vision-language framework designed for visual question answering (VQA) concerning crop disease identification from leaf images. The framework combines a Swin Transformer vision encoder with sequence-to-sequence language decoders, achieving high accuracy in crop and disease identification and strong natural language generation metrics. It outperforms larger baseline models with significantly fewer parameters, highlighting the effectiveness of task-specific pretraining.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>Ensuring crop health is fundamental to global food security and nutritional quality, which are direct determinants of human health, well-being, and disease prevention. The development of efficient and explainable AI for agricultural diagnostics also provides a scalable model that can be adapted for similar VQA applications in human medical imaging, aiding in diagnostic accuracy and clinical decision support.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>While the AI methodology could be adapted for medical image analysis, this specific application is not directly for human health. It is an AI application for agricultural biosecurity, focusing on plant health and food security rather than human medicine or healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Proposes a lightweight vision-language framework for crop disease VQA, integrating a Swin Transformer vision encoder and sequence-to-sequence language decoders.</li>
                    
                    <li>Employs a two-stage training strategy to enhance visual representation learning and cross-modal alignment.</li>
                    
                    <li>Achieves high accuracy in both crop and disease identification on a large-scale crop disease dataset.</li>
                    
                    <li>Demonstrates strong natural language generation performance, evidenced by high scores on BLEU, ROUGE, and BERTScore metrics.</li>
                    
                    <li>Outperforms large-scale vision-language baselines while utilizing substantially fewer parameters, indicating computational efficiency.</li>
                    
                    <li>Provides explainability through Grad-CAM and token-level attribution, allowing for robust performance interpretation under diverse queries.</li>
                    
                    <li>Emphasizes the critical role and effectiveness of task-specific visual pretraining for improved performance in crop disease VQA.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The framework utilizes a Swin Transformer as its vision encoder, coupled with sequence-to-sequence language decoders for generating natural language answers. A two-stage training approach is implemented, first focusing on visual representation learning and then on cross-modal alignment. Performance is evaluated on a large-scale crop disease dataset using both classification accuracy for crop/disease identification and natural language generation metrics (BLEU, ROUGE, BERTScore). Explainability is assessed using Grad-CAM for visual saliency and token-level attribution for language interpretation.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The proposed models achieve high accuracy for both crop and disease identification. They exhibit strong performance on natural language generation metrics. Crucially, they outperform large-scale vision-language baselines while requiring significantly fewer parameters, demonstrating superior efficiency. The framework also offers robust explainability, confirming the effectiveness of task-specific visual pretraining for this domain.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>By enabling rapid and accurate identification of crop diseases, this technology can significantly contribute to food security by reducing crop loss and ensuring a stable food supply, thereby directly impacting public health outcomes related to nutrition and economic stability for farming communities. Methodologically, its lightweight and explainable VQA design presents a transferable paradigm for developing efficient AI-powered diagnostic tools in clinical settings, potentially aiding clinicians in interpreting medical images and providing actionable, explainable insights in fields like radiology or pathology.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract, but common limitations for such models could include generalization to novel diseases or environmental conditions, robustness to varying image quality, and the reliance on extensive annotated datasets for training.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract, but implied directions from the findings might include deployment in real-world agricultural settings, exploration of transfer learning to other visual diagnostic domains (e.g., human medicine), and further enhancements in robustness and explainability.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Public Health</span>
                    
                    <span class="tag">Nutrition</span>
                    
                    <span class="tag">Global Health</span>
                    
                    <span class="tag">Diagnostic Imaging (potential transfer)</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Vision-Language Models</span>
                    
                    <span class="tag tag-keyword">Crop Disease</span>
                    
                    <span class="tag tag-keyword">Visual Question Answering</span>
                    
                    <span class="tag tag-keyword">Explainable AI</span>
                    
                    <span class="tag tag-keyword">Swin Transformer</span>
                    
                    <span class="tag tag-keyword">Food Security</span>
                    
                    <span class="tag tag-keyword">Agricultural AI</span>
                    
                    <span class="tag tag-keyword">Public Health</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Preprint, manuscript is under review</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>