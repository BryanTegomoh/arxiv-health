<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting - Health AI Hub</title>
    <meta name="description" content="PatchFormer is a novel patch-based time series foundation model that achieves state-of-the-art zero-shot multi-horizon forecasting by employing hierarchical mas">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.20845v1" target="_blank">2601.20845v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-28
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.LG, eess.SP
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.20845v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.20845v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">PatchFormer is a novel patch-based time series foundation model that achieves state-of-the-art zero-shot multi-horizon forecasting by employing hierarchical masked reconstruction for self-supervised pretraining and efficient cross-domain transfer learning. It significantly reduces mean squared error by 27.3% and requires 94% less task-specific data compared to strong baselines, while demonstrating superior scalability and inference speed across diverse datasets, including healthcare.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This model holds significant promise for healthcare by enabling accurate multi-horizon forecasting with minimal labeled data and zero-shot capabilities, which are crucial for applications like patient prognosis, disease progression prediction, and vital sign monitoring where data can be scarce, highly variable, or necessitate rapid deployment for new conditions.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>PatchFormer can be applied as a medical AI tool for zero-shot multi-horizon forecasting across various healthcare time series. This enables proactive predictions of patient conditions (e.g., deterioration, recovery), disease outbreaks, healthcare resource needs, and personalized treatment outcomes, without requiring extensive, task-specific retraining for each new forecasting problem. This could improve clinical decision-making, optimize hospital operations, and enhance public health preparedness.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Addresses the common challenge in time series forecasting of requiring extensive domain-specific feature engineering and large labeled datasets for each task.</li>
                    
                    <li>Introduces PatchFormer, a foundation model that segments time series into patches and learns multiscale temporal representations with learnable aggregation.</li>
                    
                    <li>Utilizes a self-supervised pretraining strategy based on hierarchical masked reconstruction, incorporating dynamic masking and objectives for both local accuracy and global consistency.</li>
                    
                    <li>Employs cross-domain knowledge distillation and lightweight adapters for efficient transfer learning, enabling zero-shot multi-horizon forecasting capabilities.</li>
                    
                    <li>Achieves state-of-the-art performance across 24 diverse benchmark datasets (including healthcare, weather, energy, traffic, finance), reducing mean squared error by 27.3% relative to strong baselines.</li>
                    
                    <li>Demonstrates remarkable data efficiency, requiring 94% less task-specific training data compared to existing methods for comparable performance.</li>
                    
                    <li>Exhibits near log-linear scaling with pretraining data up to 100 billion points and offers significant computational efficiency, processing length-512 sequences 3.8 times faster than full-sequence transformers.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>PatchFormer employs a patch-based architecture to segment time series into learnable temporal patches, followed by multiscale representation learning with learnable aggregation. Its core is self-supervised pretraining via hierarchical masked reconstruction, applying dynamic masking to encourage reconstruction of both local accuracy and global consistency. Cross-domain knowledge distillation refines the pretrained model, and lightweight adapters facilitate efficient transfer learning for specific downstream forecasting tasks, enabling zero-shot application.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>PatchFormer achieved state-of-the-art zero-shot multi-horizon forecasting, demonstrating a 27.3% reduction in mean squared error over strong baselines. It required 94% less task-specific training data, exhibited near log-linear scalability with pretraining data up to 100 billion points, and showed a 3.8x speed improvement for length-512 sequence processing compared to full-sequence transformers.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The ability to perform accurate multi-horizon forecasting with significantly less labeled data and in a zero-shot manner can revolutionize medical applications by providing timely and precise predictions for patient outcomes, disease trajectories, and resource allocation, especially in rare conditions or new clinical scenarios where large datasets are unavailable. Its computational efficiency further supports real-time clinical decision support systems and monitoring devices.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the PatchFormer model or the experimental setup.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions for this work.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">patient monitoring</span>
                    
                    <span class="tag">disease progression prediction</span>
                    
                    <span class="tag">prognostics</span>
                    
                    <span class="tag">clinical decision support</span>
                    
                    <span class="tag">epidemiology</span>
                    
                    <span class="tag">drug response prediction</span>
                    
                    <span class="tag">resource allocation in healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">time series forecasting</span>
                    
                    <span class="tag tag-keyword">foundation model</span>
                    
                    <span class="tag tag-keyword">self-supervised learning</span>
                    
                    <span class="tag tag-keyword">zero-shot learning</span>
                    
                    <span class="tag tag-keyword">deep learning</span>
                    
                    <span class="tag tag-keyword">healthcare AI</span>
                    
                    <span class="tag tag-keyword">patch-based models</span>
                    
                    <span class="tag tag-keyword">multi-horizon forecasting</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>5 pages; 2 figures; 7 tables</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>