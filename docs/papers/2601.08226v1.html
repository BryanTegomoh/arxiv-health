<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Knowledge-based learning in Text-RAG and Image-RAG - Health AI Hub</title>
    <meta name="description" content="This research developed and compared a multi-modal approach, combining Vision Transformers (EVA-ViT) with Large Language Models (LLMs) like Llama or ChatGPT, wi">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Knowledge-based learning in Text-RAG and Image-RAG</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2601.08226v1" target="_blank">2601.08226v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2026-01-13
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Alexander Shim, Khalil Saieh, Samuel Clarke
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2601.08226v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2601.08226v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This research developed and compared a multi-modal approach, combining Vision Transformers (EVA-ViT) with Large Language Models (LLMs) like Llama or ChatGPT, within Text-RAG and Image-RAG frameworks for disease detection in chest X-ray images. The study aimed to mitigate LLM hallucination and improve diagnostic accuracy. It found that Text-RAG effectively reduces hallucination using external knowledge, while Image-RAG improves prediction confidence and calibration via KNN methods, with GPT LLM demonstrating superior performance, lower hallucination, and better calibration than Llama.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research directly addresses the crucial issues of AI hallucination and prediction reliability in medical imaging diagnostics, particularly for chest X-ray interpretation. By improving the trustworthiness and accuracy of AI outputs, it can significantly enhance clinical decision support and patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is the development and evaluation of multi-modal AI systems (combining Vision Transformers with LLMs like Llama or ChatGPT) for automated disease detection from chest x-ray images. It specifically focuses on improving diagnostic accuracy, reducing AI hallucination, and enhancing prediction confidence and calibration, which are critical for deploying AI safely and effectively in healthcare settings.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>The primary goal was to reduce hallucination and improve disease detection in chest X-ray images using a multi-modal AI system.</li>
                    
                    <li>The system integrated a Vision Transformer (EVA-ViT) based image encoder with either Llama or ChatGPT Large Language Models (LLMs).</li>
                    
                    <li>The research compared three main approaches: Text-based Retrieval Augmented Generation (RAG), Image-based RAG, and a baseline model.</li>
                    
                    <li>Text-based RAG was found to effectively reduce the hallucination problem by incorporating external knowledge information, enhancing the reliability of generated text.</li>
                    
                    <li>Image-based RAG improved prediction confidence and calibration, particularly through the utilization of K-Nearest Neighbors (KNN) methods for contextual understanding.</li>
                    
                    <li>The GPT LLM demonstrated superior performance, a lower hallucination rate, and better Expected Calibration Error (ECE) when compared to the Llama-based model.</li>
                    
                    <li>Challenges identified include data imbalance and the inherent complexity of the multi-stage model structure, suggesting a need for a large experience environment and balanced use examples.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employed a multi-modal deep learning architecture, combining a Vision Transformer (EVA-ViT) for processing chest X-ray images with Large Language Models (Llama or ChatGPT). It utilized Retrieval Augmented Generation (RAG) in both text-based (leveraging external knowledge) and image-based (using KNN for context) forms. The models were trained and evaluated on the NIH Chest X-ray image dataset, with performance assessed based on hallucination rates, prediction confidence, and Expected Calibration Error (ECE) in comparison to a baseline.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Text-based RAG effectively reduced LLM hallucination by integrating external knowledge. Image-based RAG improved prediction confidence and calibration through KNN methods. The GPT LLM consistently outperformed the Llama model, exhibiting superior overall performance, a lower hallucination rate, and better Expected Calibration Error (ECE).</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>The advancements in reducing hallucination and improving model calibration can lead to more reliable and trustworthy AI tools for chest X-ray interpretation. This could enhance diagnostic accuracy, reduce misdiagnosis rates, streamline radiologist workflows by providing more dependable insights, and ultimately contribute to earlier and more effective treatment planning for patients.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The research identified challenges related to data imbalance within the NIH Chest X-ray dataset and the intrinsic complexity associated with its multi-stage model architecture.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The authors suggest that future research should focus on developing models within a large experience environment and ensuring a balanced example of use, implying the need for broader real-world data application and more robust evaluation across diverse patient populations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Diagnostic Medicine</span>
                    
                    <span class="tag">Artificial Intelligence in Healthcare</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Multi-modal AI</span>
                    
                    <span class="tag tag-keyword">Chest X-ray</span>
                    
                    <span class="tag tag-keyword">Disease Detection</span>
                    
                    <span class="tag tag-keyword">Vision Transformer</span>
                    
                    <span class="tag tag-keyword">LLM</span>
                    
                    <span class="tag tag-keyword">RAG</span>
                    
                    <span class="tag tag-keyword">Hallucination</span>
                    
                    <span class="tag tag-keyword">Calibration</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>9 pages, 10 figures</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>