<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CARE-RAG - Clinical Assessment and Reasoning in RAG - Health AI Hub</title>
    <meta name="description" content="This paper addresses the critical gap between retrieval and reasoning in Large Language Models (LLMs) within clinical settings, using Written Exposure Therapy (">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>CARE-RAG - Clinical Assessment and Reasoning in RAG</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.15994v1" target="_blank">2511.15994v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-20
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Deepthi Potluri, Aby Mammen Mathew, Jeffrey B DeWitt, Alexander L. Rasgon, Yide Hao, Junyuan Hong, Ying Ding
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.AI, cs.CL
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.15994v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="https://arxiv.org/pdf/2511.15994v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper addresses the critical gap between retrieval and reasoning in Large Language Models (LLMs) within clinical settings, using Written Exposure Therapy (WET) guidelines as a testbed. It proposes an evaluation framework to assess the accuracy, consistency, and fidelity of LLM reasoning, highlighting that errors persist even when authoritative information is provided. The research underscores that while Retrieval-Augmented Generation (RAG) can constrain outputs, safe clinical deployment mandates rigorous evaluation of reasoning capabilities, not just retrieval.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for ensuring the safe and effective integration of AI, specifically LLMs and RAG, into clinical practice by addressing fundamental reasoning flaws that could compromise patient care, lead to medical errors, or non-adherence to established medical guidelines and protocols.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>Evaluating the reasoning capabilities and safety of Large Language Models (LLMs) augmented with Retrieval-Augmented Generation (RAG) for applications in clinical settings, specifically for adhering to medical guidelines (e.g., therapy protocols like WET) and assisting with clinical assessment and decision-making. This research contributes to the development and responsible deployment of AI tools in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>LLMs frequently fail to reason correctly with provided evidence, a problem especially acute in clinical contexts requiring strict adherence to structured protocols.</li>
                    
                    <li>Written Exposure Therapy (WET) guidelines served as the specific clinical testbed for evaluating LLM responses to clinician-vetted questions.</li>
                    
                    <li>Even when authoritative clinical passages were explicitly provided, LLMs exhibited persistent reasoning errors.</li>
                    
                    <li>The paper introduces a novel evaluation framework designed to systematically measure the accuracy, consistency, and fidelity of LLM reasoning in clinical scenarios.</li>
                    
                    <li>Retrieval-Augmented Generation (RAG) is shown to be effective in constraining LLM outputs, demonstrating its potential for generating controlled and relevant responses.</li>
                    
                    <li>Safe and reliable deployment of RAG systems in clinical practice necessitates assessing the reasoning capabilities of these models with the same rigor applied to their information retrieval functions.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The authors studied the gap between retrieval and reasoning by using Written Exposure Therapy (WET) guidelines as a testbed. They curated clinician-vetted questions and evaluated LLM responses, finding persistent errors. To address this, they proposed and applied an evaluation framework designed to measure the accuracy, consistency, and fidelity of the models' clinical reasoning.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Despite access to authoritative passages, LLMs demonstrated persistent errors in reasoning when responding to clinician-vetted questions based on WET guidelines. While RAG can effectively constrain model outputs, the study revealed that safe clinical deployment requires a dedicated and rigorous assessment of the model's reasoning capabilities, not solely its information retrieval prowess.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This work directly impacts the development and deployment of safe and reliable AI systems in clinical settings by providing a framework to critically evaluate their reasoning abilities. It emphasizes that for LLMs to be practically useful in healthcare, especially for tasks requiring adherence to structured protocols like therapy guidelines, their reasoning must be rigorously verified to prevent erroneous advice or actions, thereby enhancing patient safety and quality of care.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract implicitly notes that even with retrieval-augmented generation (RAG) providing relevant context, current LLMs demonstrate inherent limitations in correctly reasoning with this information, leading to persistent errors in clinical applications despite authoritative evidence being present.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The paper implies a future direction of developing and applying more robust evaluation frameworks for LLM reasoning across diverse clinical scenarios and improving LLM architectures to close the identified reasoning gap, paving the way for safer and more reliable AI integration in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Psychiatry</span>
                    
                    <span class="tag">Mental Health</span>
                    
                    <span class="tag">Clinical Psychology</span>
                    
                    <span class="tag">Medical Informatics</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Large Language Models (LLMs)</span>
                    
                    <span class="tag tag-keyword">Retrieval-Augmented Generation (RAG)</span>
                    
                    <span class="tag tag-keyword">Clinical Reasoning</span>
                    
                    <span class="tag tag-keyword">Medical Guidelines</span>
                    
                    <span class="tag tag-keyword">Written Exposure Therapy (WET)</span>
                    
                    <span class="tag tag-keyword">AI Evaluation</span>
                    
                    <span class="tag tag-keyword">Healthcare AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>The Second Workshop on GenAI for Health: Potential, Trust, and Policy Compliance</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>