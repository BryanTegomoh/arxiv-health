<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets - Health AI Hub</title>
    <meta name="description" content="This paper addresses the critical issue of Deep Learning (DL) models failing to generalize to out-of-distribution (OOD) medical imaging data, particularly for C">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03855v1" target="_blank">2511.03855v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Duong Mai, Lawrence Hall
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.90 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03855v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03855v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper addresses the critical issue of Deep Learning (DL) models failing to generalize to out-of-distribution (OOD) medical imaging data, particularly for COVID-19 detection from Chest X-rays (CXRs), due to learning source-specific artifacts. The study proposes and empirically demonstrates that injecting fundamental noise (Gaussian, Speckle, Poisson, Salt and Pepper) during model training significantly improves OOD generalization, drastically reducing the performance gap between in-distribution and OOD evaluation.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is crucial for deploying reliable AI diagnostics in healthcare, as it tackles a major limitation where models trained on one dataset fail when encountering data from different hospitals or devices, ensuring more consistent and trustworthy clinical decision support.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The research aims to improve the robustness and generalizability of deep learning models used for medical image analysis, specifically for diagnostic tasks like COVID-19 detection from Chest X-rays. By reducing the performance gap between in-distribution and out-of-distribution data, it contributes to making AI-powered diagnostic tools more reliable and deployable across diverse clinical environments and patient populations.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>DL models for medical image recognition (e.g., CXR COVID-19 detection) often fail OOD generalization due to 'shortcut learning' from source-specific artifacts.</li>
                    
                    <li>The study investigates the use of fundamental noise injection techniques (Gaussian, Speckle, Poisson, Salt and Pepper) applied during model training.</li>
                    
                    <li>The primary goal is to render DL models more robust to distribution shifts encountered in real-world clinical settings.</li>
                    
                    <li>Empirical results show a substantial reduction in the performance gap between ID and OOD evaluation, from 0.10-0.20 to 0.01-0.06.</li>
                    
                    <li>Performance improvement is observed across key metrics including AUC, F1-score, accuracy, recall, and specificity.</li>
                    
                    <li>Results are robust, averaged over ten random seeds, indicating statistical significance.</li>
                    
                    <li>The source code for the methodology is publicly available, promoting reproducibility and further research.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employs fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper noise) directly during the training phase of Deep Learning models. This approach aims to make models more robust to variations and artifacts that might distinguish OOD data. The performance was evaluated by comparing ID and OOD generalization metrics (AUC, F1, accuracy, recall, specificity) across ten random seeds.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>Injecting fundamental noise during training significantly improves OOD generalization for DL models in medical image analysis. The performance gap between in-distribution and out-of-distribution evaluation was dramatically reduced from 0.10-0.20 to a narrow range of 0.01-0.06 across multiple critical metrics (AUC, F1, accuracy, recall, specificity).</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This technique could lead to more robust and clinically applicable AI systems for medical diagnosis, reducing the need for constant re-training on new data sources. It enhances the trustworthiness of AI models in diverse hospital environments, improving diagnostic accuracy for conditions like COVID-19 even when encountering novel data distributions, thus facilitating wider and safer adoption of AI in healthcare.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>The abstract does not explicitly state any limitations of the study, such as the specific DL architectures used, the size or diversity of the datasets beyond mentioning OOD challenges, or the generalizability of these noise types to all medical imaging modalities.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>The abstract does not explicitly state future research directions. However, potential directions implied by the research could include exploring other types of data augmentation/regularization, investigating the optimal noise parameters or combinations, applying this technique to different medical imaging tasks and modalities, and studying the underlying mechanisms by which noise injection improves OOD generalization.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Medical Imaging</span>
                    
                    <span class="tag">Infectious Diseases (COVID-19)</span>
                    
                    <span class="tag">Diagnostic AI</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Out-of-Distribution Generalization</span>
                    
                    <span class="tag tag-keyword">Noise Injection</span>
                    
                    <span class="tag tag-keyword">Chest X-ray</span>
                    
                    <span class="tag tag-keyword">COVID-19 Detection</span>
                    
                    <span class="tag tag-keyword">Distribution Shift</span>
                    
                    <span class="tag tag-keyword">Shortcut Learning</span>
                    
                    <span class="tag tag-keyword">Medical Imaging</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Abstract accepted for oral presentation at SPIE Medical Imaging 2026:
  Computer-Aided Diagnosis</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>