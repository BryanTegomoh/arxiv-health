<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets - Health AI Hub</title>
    <meta name="description" content="This paper addresses the critical issue of deep learning models failing to generalize to out-of-distribution (OOD) medical imaging data, particularly in COVID-1">
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">‚Üê Back to all papers</a>
                <a href="../index.html" class="home-btn">üè† Home</a>
            </div>
        </div>
    </header>

    <main class="container paper-detail">
        <article>
            <h1>Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets</h1>

            <div class="paper-metadata">
                <div class="meta-row">
                    <strong>arXiv ID:</strong> <a href="http://arxiv.org/abs/2511.03855v1" target="_blank">2511.03855v1</a>
                </div>
                <div class="meta-row">
                    <strong>Published:</strong> 2025-11-05
                </div>
                <div class="meta-row">
                    <strong>Authors:</strong> Duong Mai, Lawrence Hall
                </div>
                <div class="meta-row">
                    <strong>Categories:</strong> cs.CV, cs.AI
                </div>
                <div class="meta-row">
                    <strong>Relevance Score:</strong> 0.95 / 1.00
                </div>
            </div>

            <div class="action-buttons">
                <a href="http://arxiv.org/abs/2511.03855v1" target="_blank" class="btn btn-primary">View on arXiv</a>
                <a href="http://arxiv.org/pdf/2511.03855v1" target="_blank" class="btn btn-primary">Download PDF</a>
            </div>

            <section class="paper-section">
                <h2>Summary</h2>
                <p class="summary-text">This paper addresses the critical issue of deep learning models failing to generalize to out-of-distribution (OOD) medical imaging data, particularly in COVID-19 detection from Chest X-rays (CXRs), due to learning source-specific shortcuts. The authors propose using fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during training to enhance model robustness. Their empirical results demonstrate that this method significantly reduces the performance gap between in-distribution and OOD evaluation from 0.10-0.20 to 0.01-0.06 across key metrics, improving the reliability of medical AI.</p>
            </section>

            <section class="paper-section">
                <h2>Medical Relevance</h2>
                <p>This research is highly medically relevant as it tackles a fundamental barrier to the clinical deployment of AI: the lack of robust generalization to unseen patient populations, devices, or institutions. By improving out-of-distribution performance, it ensures that AI diagnostic tools can be trusted across varied healthcare environments, preventing misdiagnosis and improving patient care.</p>
            </section>

            
            <section class="paper-section">
                <h2>AI Health Application</h2>
                <p>The AI application is to improve the reliability and generalizability of deep learning models for medical image analysis, such as diagnosing conditions like COVID-19 from Chest X-rays. By making these models more robust to distribution shifts across different clinical sources, the research contributes to developing more trustworthy and widely deployable AI tools for clinical diagnosis and decision support in healthcare.</p>
            </section>
            

            <section class="paper-section">
                <h2>Key Points</h2>
                <ul class="key-points">
                    
                    <li>Deep learning models for medical image recognition (e.g., COVID-19 detection from CXRs) commonly exhibit poor generalization to out-of-distribution (OOD) data.</li>
                    
                    <li>This OOD generalization failure is attributed to models exploiting 'shortcuts' ‚Äì source-specific artifacts that do not translate to new data distributions ‚Äì rather than learning clinically relevant biomarkers.</li>
                    
                    <li>The study investigates the use of fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) applied during the model training phase.</li>
                    
                    <li>The primary goal of noise injection is to make deep learning models more robust to distribution shifts, forcing them to learn more generalizable features.</li>
                    
                    <li>Empirical results show a substantial reduction in the performance gap between in-distribution (ID) and OOD evaluation, narrowing it from an initial 0.10-0.20 to a significantly smaller 0.01-0.06.</li>
                    
                    <li>These improvements were consistently observed across multiple crucial performance metrics including AUC, F1-score, accuracy, recall, and specificity, averaged over ten random seeds.</li>
                    
                    <li>The technique offers a simple yet effective strategy to enhance the trustworthiness and real-world applicability of AI diagnostic tools in diverse clinical settings.</li>
                    
                </ul>
            </section>

            <div class="two-column">
                <section class="paper-section">
                    <h2>Methodology</h2>
                    <p>The study employs fundamental noise injection techniques during the training of deep learning models for image recognition. Specifically, four types of noise (Gaussian, Speckle, Poisson, and Salt and Pepper) are applied to the training data. This augmentation strategy aims to increase the model's robustness to distribution shifts by preventing it from relying on source-specific artifacts. Performance is evaluated by comparing the gap between in-distribution (ID) and out-of-distribution (OOD) metrics (AUC, F1, accuracy, recall, specificity) on a COVID-19 detection task using Chest X-rays.</p>
                </section>

                <section class="paper-section">
                    <h2>Key Findings</h2>
                    <p>The key finding is that noise injection during training significantly reduces the performance gap between ID and OOD evaluation. The ID-OOD performance gap was narrowed from an initial range of 0.10-0.20 down to a range of 0.01-0.06. This improvement was consistent across crucial metrics such as AUC, F1, accuracy, recall, and specificity, with results averaged over ten random seeds, indicating a robust enhancement in OOD generalization.</p>
                </section>
            </div>

            <section class="paper-section">
                <h2>Clinical Impact</h2>
                <p>This research has a substantial potential clinical impact by enabling the safer and more widespread deployment of AI-powered diagnostic tools in healthcare. By making models more robust to variations in data from different clinical sources (e.g., hospitals, X-ray machines, patient demographics), it increases the reliability of AI for critical tasks like COVID-19 detection. This reduces the risk of models failing in real-world scenarios due to unseen data, fostering greater clinician trust and facilitating the integration of AI into routine clinical workflows.</p>
            </section>

            
            <section class="paper-section">
                <h2>Limitations</h2>
                <p>Not explicitly mentioned in the abstract. However, typical considerations for such a study (not stated here) might include the generalizability of these specific noise types to other medical imaging tasks or modalities, the optimal magnitude of noise injection, and the potential computational overhead.</p>
            </section>
            

            
            <section class="paper-section">
                <h2>Future Directions</h2>
                <p>Not explicitly mentioned in the abstract.</p>
            </section>
            

            <section class="paper-section">
                <h2>Medical Domains</h2>
                <div class="tags">
                    
                    <span class="tag">Radiology</span>
                    
                    <span class="tag">Infectious Diseases</span>
                    
                    <span class="tag">Diagnostic Imaging</span>
                    
                    <span class="tag">Medical Artificial Intelligence</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Keywords</h2>
                <div class="tags">
                    
                    <span class="tag tag-keyword">Out-of-Distribution Generalization</span>
                    
                    <span class="tag tag-keyword">Noise Injection</span>
                    
                    <span class="tag tag-keyword">Deep Learning</span>
                    
                    <span class="tag tag-keyword">Chest X-rays</span>
                    
                    <span class="tag tag-keyword">COVID-19 Detection</span>
                    
                    <span class="tag tag-keyword">Medical Imaging AI</span>
                    
                    <span class="tag tag-keyword">Distribution Shift</span>
                    
                    <span class="tag tag-keyword">Generalizability</span>
                    
                </div>
            </section>

            <section class="paper-section">
                <h2>Abstract</h2>
                <p class="abstract">Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood</p>
            </section>

            
            <section class="paper-section">
                <h2>Comments</h2>
                <p>Abstract accepted for oral presentation at SPIE Medical Imaging 2026:
  Computer-Aided Diagnosis</p>
            </section>
            

            
        </article>
    </main>

    <footer class="container">
        <p><a href="../index.html">‚Üê Back to all papers</a></p>
    </footer>
</body>
</html>